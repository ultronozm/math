\documentclass[reqno]{amsart} \input{common.tex}


\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{graphicx, amsmath, amssymb, amsfonts, amsthm, stmaryrd, tikz, amscd}
\usepackage[all]{xy}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{tikz}
\def\eps{\varepsilon}
\usetikzlibrary{decorations.markings}

\newcommand{\C}[1]{\mathbb{C}^{#1}}

\newcommand{\dynkinradius}{.04cm}
\newcommand{\dynkinstep}{.35cm}
\newcommand{\dynkindot}[2]{\fill (\dynkinstep*#1,\dynkinstep*#2) circle (\dynkinradius);}
\newcommand{\dynkinXsize}{1.5}
\newcommand{\dynkincross}[2]{
  \draw[thick] (#1*\dynkinstep-\dynkinXsize,#2*\dynkinstep-\dynkinXsize) -- (#1*\dynkinstep+\dynkinXsize,#2*\dynkinstep+\dynkinXsize);
  \draw[thick] (#1*\dynkinstep-\dynkinXsize,#2*\dynkinstep+\dynkinXsize) -- (#1*\dynkinstep+\dynkinXsize,#2*\dynkinstep-\dynkinXsize);
}
\newcommand{\dynkinline}[4]{\draw[thin] (\dynkinstep*#1,\dynkinstep*#2) -- (\dynkinstep*#3,\dynkinstep*#4);}
\newcommand{\dynkindots}[4]{\draw[dotted] (\dynkinstep*#1,\dynkinstep*#2) -- (\dynkinstep*#3,\dynkinstep*#4);}
\newcommand{\dynkindoubleline}[4]{\draw[double,postaction={decorate}] (\dynkinstep*#1,\dynkinstep*#2) -- (\dynkinstep*#3,\dynkinstep*#4);}

\newenvironment{dynkin}{\begin{tikzpicture}[decoration={markings,mark=at position 0.7 with {\arrow{>}}}]}
  {\end{tikzpicture}}

\title{Notes on Lie Groups}

\begin{document}

\begin{abstract}
  Notes from a Lie groups course given at ETH Zurich, Fall 2016, unedited
\end{abstract}

\maketitle
\tableofcontents

\section{Disclaimers\label{sec:disclaimers}}
\label{sec:org8827c4a}
The notes recorded here are intended to \emph{supplement} the lectures, but I have included more material here than is necessary for the course.  You are \emph{not} expected to read these notes, but I hope that providing them as a reference may be helpful.  For example, although I have recorded here a fair bit of background from differential geometry, it should not be necessary for the purposes of the course.

A principal aim of the lectures is that after attending them, you should be able to do the homework problems given below their summaries.

It is very likely that the notes and exercises will contain some mistakes; any corrections would be much appreciated.

\section{Summary of classes and homework assignments}
\label{sec:orgeacc8f6}
\newpage
\subsection{9/20: The definition of a Lie group}
\label{sec:org5df22f0}
\textbf{Objectives.}  You should be able to explain the definition of "Lie group" and to prove that basic examples (e.g., orthogonal groups) are in fact Lie groups.

\textbf{Summary.}
\begin{enumerate}
\item review of one-variable calculus and how it relates global properties of functions (e.g., monotonicity) to infinitesimal ones (e.g., positivity of derivative)
\item review of multivariable calculus:
  \begin{enumerate}
  \item partial and total derivatives of a function
  \item inverse function theorem
  \item implicit function theorem
  \end{enumerate}
\item very brief review of differential geometry:
  \begin{enumerate}
  \item the definition of submanifolds of open subsets of Euclidean spaces
  \item how (in practice) to check that a subset is a submanifold
  \item how (in practice) to compute tangent spaces of submanifolds
  \item immersions, embeddings, submersions
  \end{enumerate}
\item review of group theory:
  \begin{enumerate}
  \item functorial definition of "group" in terms of multiplication and inversion maps
  \item permutation groups; Cayley's theorem
  \item definition of topological group
  \end{enumerate}
\item basic Lie-theoretic definitions:
  \begin{enumerate}
  \item Lie group (without recalling what a "manifold" is, other than to note that open subsets of Euclidean space and submanifolds thereof are examples of manifolds)
  \item Lie subgroup
  \item immersed Lie subgroup (e.g., irrational winding of the 2-torus)
  \item the Lie algebra of a Lie group (without justifying the "algebra" in "Lie algebra")
  \item linear Lie group
  \end{enumerate}
\item how to compute Lie algebras of Lie groups in practice; examples of \(\GL_n, \SL_n, O_n\)
\end{enumerate}

\begin{homework}
[Due Oct 4]
  Write down the definitions of ``Lie group'' and ``Lie subgroup''.  Using some lemmas from class, prove that $\O(n) := \{g \in \GL_n(\mathbb{R}) : g g^t = 1\}$ is a Lie subgroup of $\GL_n(\mathbb{R})$ of dimension $n(n-1)/2$ with Lie algebra $\mathfrak{o}(n) := \Lie(\O(n))$ given by the space $\{ X \in M_n(\mathbb{R}) : X + X^t = 0 \}$ of skew-symmetric matrices.
\end{homework}

\newpage
\subsection{9/22: The connected component}
\label{sec:org254d9af}
\textbf{Objectives.} You should be able to define the "connected component" of a Lie group, explain its importance, and determine it in some basic examples (such as linear, orthogonal or unitary groups).

\textbf{Summary.}
\begin{enumerate}
\item review of the general topological notion of connected components of a topological space, and how it specializes when the space is a manifold
\item basics on the connected component of the identity \(G^0\) in a Lie group \(G\):
  \begin{enumerate}
  \item it is a normal Lie subgroup whose cosets are the connected components of \(G\)
  \item \(G^0\) (and more generally, any connected topological group) is generated by any neighborhood of the identity
  \end{enumerate}
\item most of the classical groups were introduced and their number of connected components described, with some cases proved (\(\SL_n, \GL_n, \O(n), \SO(n), \U(n)\)) and others left as exercises (\(\O(p,q)\), \ldots{}); a large part of the class consisted of filling in and explaining the entries in the three-column table depicted in \S\ref{sec:connectedness-examples}
\item review on the matrix exponential, as in \S\ref{sec:matrix-exp}
\end{enumerate}

\begin{homework}[Due Oct 4]\label{hw:2}
  Let $p, q \geq 1$ and $n := p + q$.  Recall that
  \begin{align*}
    \O(p,q) &:= \{g \in \GL_n(\mathbb{R}) : Q(g v) = Q(v)
              \text{ for all } v \in \mathbb{R}^n\} \\
            &=
              \{g \in \GL_n(\mathbb{R}): g^t J g = J \},
  \end{align*}
  where $Q(v) := v_1^2 + \dotsb + v_p^2 - v_{p+1}^2 - \dotsb - v_n^2$ and $J := \diag(1,\dotsc,1,-1,\dotsc,-1)$, and that $\SO(p,q) := \SL_n(\mathbb{R}) \cap \O(p,q)$.  For $a \in \mathbb{R}$, set $V_a := \{v \in \mathbb{R}^n : Q(v) = a\}$.  Denote by $e_1,\dotsc,e_n$ the standard basis vectors for $\mathbb{R}^n$.
  \begin{enumerate}
  \item Suppose $p = q = 1$, so that $n = 2$.
    \begin{enumerate}
    \item Show that every matrix of the form
      \begin{equation}\label{eq:elements-o-1-1}
        g = 
        \begin{pmatrix}
          \eps_1 &  \\
                 & \eps_2
        \end{pmatrix}
        \begin{pmatrix}
          \cosh(t) & \sinh(t) \\
          \sinh(t) & \cosh(t)
        \end{pmatrix}
      \end{equation}
      with $\eps_1,\eps_2 \in \{\pm 1\}$, $t \in \mathbb{R}$ belongs to $\O(1,1)$.  Show that
      \begin{equation*}
        \begin{pmatrix}
          \cosh(t) & \sinh(t) \\
          \sinh(t) & \cosh(t)
        \end{pmatrix}
        = \exp (t 
\begin{pmatrix}
          0 & 1 \\
          1 & 0
        \end{pmatrix}
).
      \end{equation*}
      Show that if $\eps_1 = \eps_2 = 1$, then $g \in \O(1,1)^0$.
    \item Show that for each $v \in V_1$ with $v_1 > 0$ there is an element $g \in \O(1,1)^0$ so that $g e_1 = v$.
    \item Show that every $g \in \O(1,1)$ with $g e_1 = e_1$ is of the form \eqref{eq:elements-o-1-1} with $\eps_1 = 1, \eps_2 = \pm 1$ and $t = 0$.
    \item Show for $a = \pm 1$ that the space $V_a$ has two connected components and that $\O(1,1)$ acts transitively on $V_a$.  Determine the orbit of $e_1$ under $\SO(1,1)$.
    \item Using some of the previous steps (or direct calculation), show that every element of $\O(1,1)$ is of the form \eqref{eq:elements-o-1-1} and that $\O(1,1)$ has four connected components.
    \end{enumerate}
  \item Suppose now that $p = 1, q = 2, n = 3$.
    \begin{enumerate}
    \item Observe (by drawing a picture, say) that $V_{-1}$ is connected, that $V_{1}$ has two connected components, and that $e_1 \in V_{1}$.  Denote by $V_{1}^0$ the connected component of $V_{1}$ containing $e_1$.  Show that for each $v \in V_{1}^0$ there exists an $h \in \SO(1,2)^0$ so that $h v = e_1$.  [Hint: one can reduce to part (b) of the previous exercise.]
    \item Show that the stabilizer of $e_1$ in $\SO(1,2)$ is isomorphic to $\SO(2)$, hence is connected.
    \item Show that any $g \in \SO(1,2)$ for which $g e_1 \in V_{1}^0$ belongs to the connected component $\SO(1,2)^0$.
    \item Deduce that $\SO(1,2)$ has two connected components.
    \end{enumerate}
  \end{enumerate}
\end{homework}

\newpage
\subsection{9/27: One-parameter subgroups and the exponential map}
\label{sec:orge89327d}
\textbf{Objectives.} You should be able to define one-parameter subgroups and apply their basic uniqueness theorem.  You should be able to define and characterize the exponential map on a Lie group in a few different ways, and be able to apply these characterizations. You should be able to apply the exponential map to relate global symmetries to infinitesimal ones (as in the example from lecture or the homework problem below).  You should be able to apply the fact that the image of the exponential map contains a neighborhood of the identity, which in turn generates the connected component.

\textbf{Summary.}
\begin{enumerate}
\item Review of the matrix exponential and its various characterizations:
  \begin{enumerate}
  \item as a series \(\exp(X) = \sum X^n/n!\)
  \item as a limit \(\exp(X) = \lim (1 + X/n)^n\), or more generally, \(\exp(X) = \lim \gamma(1/n)^n\) for any curve with basepoint \(\gamma(0) = 1\) and initial velocity \(\gamma'(0) = X\)
  \item by requiring that for each \(X\), the function \(\Phi_X(t) = \exp(t X)\) is the unique solution to the ODE \(\Phi'(t) = X \Phi(t)\) with initial condition \(\Phi(0) = 1\)
  \item by requiring that for each \(X\), the function \(\Phi_X(t)\) as above is the unique smooth group homomorphism with initial velocity \(\Phi_X'(0) = X\).
  \end{enumerate}
\item We explained how the above generalizes to any Lie group.  The key was the existence/uniqueness of one-parameter subgroups.
  \begin{enumerate}
  \item The uniqueness was reduced to uniqueness theorems for ODE's.
  \item We gave a direct proof of the existence of one-parameter subgroups for \(\GL_n\), deduced it for linear Lie groups via the second characterization above, and indicated how it follows for general \(G\) by solving some ODE's and extending their solutions.
  \end{enumerate}
\item As a basic application we explained how to characterize the rotation-invariant functions on \(\mathbb{R}^n\) as the solutions to a finite system of homogeneous linear differential equations.
\end{enumerate}

\begin{homework}[Due Oct 4]~\label{hw:3-lie-first}
  \begin{enumerate}
  \item Use Lie's first theorem (Theorem \ref{thm:exp-local-diffeo}) and the results of Homework \ref{hw:2} to show that the following are equivalent for a smooth function $f : \mathbb{R}^3 \rightarrow \mathbb{R}$:
    \begin{enumerate}
    \item $f$ is constant on each connected component of $\{(x,y,z) \in \mathbb{R}^3 : z^2 - x^2 - y^2 = 1\}$.
    \item $f$ satisfies the differential equations
      \begin{equation*}
        x \frac{\partial f }{\partial y } - y \frac{\partial f}{ \partial x} = 0,
      \end{equation*}
      \begin{equation*}
        z \frac{\partial f }{\partial x } + x \frac{\partial f}{ \partial z} = 0,
      \end{equation*}
      \begin{equation*}
        z \frac{\partial f }{\partial y } + y \frac{\partial f}{ \partial z} = 0
      \end{equation*}
      on $\{(x,y,z) \in \mathbb{R}^3 : z^2 - x^2 - y^2 = 1\}$.
    \end{enumerate}
  \item Let $G$ be a topological group and $H \leq G$ a subgroup with the property that there is a neighborhood $U$ in $G$ of the identity element so that $U \cap H = \{1\}$.  Show that $H$ is a discrete subgroup of $G$.
  \item Let $G$ be a connected commutative Lie group with Lie algebra $\mathfrak{g}$.  Show that $\exp : \mathfrak{g} \rightarrow G$ is a surjective homomorphism and with discrete kernel.
  \end{enumerate}
\end{homework}

\newpage
\subsection{9/29: The Lie algebra of a Lie group}
\label{sec:org102c8ed}
\textbf{Objectives.} You should be able to explain how the Lie bracket arises as an infinitesimal commutator of group elements.  You should be able to explain the meaning of the sentence "the differential of a morphism of Lie groups is a morphism of Lie algebras"; in particular, you should be able to define all of its terms.  Given a fairly explicit morphism of Lie groups (such as the representations on polynomials discussed in lecture or in the homework below), you should be able to compute the induced infinitesimal action of the Lie algebra.

\textbf{Summary.}
\begin{enumerate}
\item tying up loose ends on application of exponential map:
  \begin{enumerate}
  \item connected Lie subgroups are determined by their Lie algebras
  \item the exponential map intertwines morphisms of Lie groups with their differentials
  \item morphisms of Lie groups with connected domain are characterized by their differentials
  \end{enumerate}
\item the commutator of infinitesimal elements on the general linear group compared with the commutator bracket \([X,Y] := X Y - Y X\) on the matrix algebra; generalization to arbitrary Lie groups
\item definition of Lie algebra and morphism of Lie algebra
\item examples of Lie algebras:
  \begin{enumerate}
  \item \(\Lie(G)\) for \(G\) a Lie group
  \item \(\End(V)\) for \(V\) a vector space
  \item \(\Der(A)\) for \(A\) an algebra
  \item \(\operatorname{Vect}(M) = \Der(C^\infty(M))\) for \(M\) a manifold
  \end{enumerate}
\item proof that morphisms of Lie groups induce morphisms of Lie algebras
\item definition of a representation of a Lie group, matrix coefficients with respect to a basis; example involving trigonometric functions and their addition law
\end{enumerate}
\begin{homework}[Due Oct 4]\label{hw:sl2-rep-verify-commutator}
~
  Let $G$ be the Lie group $\SL_2(\mathbb{C})$, $\mathfrak{g} := \Lie(G) = \slLie_2(\mathbb{C})$, and let $n$ be a positive integer.  Let $V \leq \mathbb{C}[x,y]$ be the $(n+1)$-dimensional vector space consisting of homogeneous polynomials of degree $n$ in the variables $x,y$, so that a basis of $V$ is given by the set of monomials
  \begin{equation*}
    \mathcal{B} := \{x^n, x^{n-1} y, \dotsc, x y^{n-1}, y^n\}.
  \end{equation*}
  Let $R : G \rightarrow \GL(V)$ be the map given for $\phi \in V$ by
  \begin{equation*}
    (R(g) \phi)(x,y) := \phi((x,y) g),
  \end{equation*}
  where $(x,y) g$ denotes matrix multiplication, so that more explicitly
  \begin{equation*}
    (R( 
\begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
 )
    \phi)(x,y)
    =
    \phi (a x + c y, b x + d y).
  \end{equation*}
  \begin{enumerate}
  \item Verify that $R$ defines a representation of $G$ on $V$, hence (by a general theorem from class) that $d R : \mathfrak{g} \rightarrow \End(V)$ is a morphism of Lie algebras.
  \item Verify that the basis elements
    \begin{equation*}
      H := 
\begin{pmatrix}
        1 & 0 \\
        0 & -1
      \end{pmatrix}
,
      \quad
      X :=
      \begin{pmatrix}
        0 & 1 \\
        0 & 0
      \end{pmatrix}
      \quad Y :=
      \begin{pmatrix}
        0 & 0 \\
        1 & 0
      \end{pmatrix}
    \end{equation*}
    of $\mathfrak{g}$ satisfy $[X,Y] = H$.
  \item Compute the actions of $d R(H), d R(X), d R(Y)\in \End(V)$ explicitly with respect to the basis $\mathcal{B}$ of $V$ and verify directly (without appeal to the general theorem from class) that $[d R(X), d R(Y)] = d R(H)$.  [See \S\ref{sec:stability-subspaces} if the definition of $d R(X)$ is unclear.]
  \end{enumerate}
\end{homework}

\newpage
\subsection{10/4 (half-lecture) and 10/6: Representations of \texorpdfstring{$\SL(2)$}{SL2}}
\label{sec:org04627a1}
\textbf{Objectives.} You should be able to analyze (finite-dimensional) representations of \(\SL_2(\mathbb{C})\) by differentiating them to obtain representations of \(\slLie_2(\mathbb{C})\), breaking the latter up into weight spaces, and studying how the weight spaces are permuted by raising and lowering operators.

\textbf{Summary.}
\begin{enumerate}
\item definition of representations of Lie groups and algebras
\item example of polynomial representations of linear Lie groups; explicit calculation of the induced representation on the Lie algebra
\item discussion of the action of the standard basis of \(\SL_2(\mathbb{C})\) on the \$(n+1)\$-dimensional representation \(W_n\) from Homework \ref{hw:sl2-rep-verify-commutator}
\item definition of invariant subspaces, irreducibility
\item \(W_n\) is irreducible
\item every irreducible finite-dimensional representation of \(\SL_2(\mathbb{C})\) is isomorphic to some \(W_n\)
\end{enumerate}

\begin{homework}
[Due Oct 11]~ Let $G$ be the Lie group $\SL_2(\mathbb{C})$.  Let $H,X,Y$ be the basis of $\mathfrak{g} := \Lie(G)$ as in Homework \ref{hw:sl2-rep-verify-commutator}.
  \begin{enumerate}
  \item For $\lambda \in \mathbb{C}$, let $V_\lambda$ be the vector space with basis $(v_k)_{k \in \mathbb{Z}_{\geq 0}}$.  Show that the action
    \begin{align*}
      H v_k &= (\lambda - 2 k) v_k \\
      X v_k &= k ( \lambda - k + 1) v_{k-1}
              \quad (v_{-1} := 0) \\
      Y v_k &= v_{k+1}
    \end{align*}
    defines a Lie algebra representation $\mathfrak{g} \rightarrow \End(V_{\lambda})$.  Determine the invariant subspaces of $V_\lambda$.
  \item Same question, but for the spaces $U_\nu$ ($\nu \in \mathbb{C}$) with basis $(v_k)_{k \in \mathbb{Z}}$ and action
    \begin{align*}
      H v_k &= 2 k v_k \\
      X v_k &= (\nu + k) v_{k+1} \\
      Y v_k &= (\nu - k) v_{k-1}.
    \end{align*}
  \item Let $\mathfrak{b} \leq \mathfrak{g}$ be the subspace spanned by $H,X$.  Verify that $\mathfrak{b}$ is a Lie subalgebra.  Let $\rho : \mathfrak{g} \rightarrow \End(V)$ be a finite-dimensional representation.  Show that the following are equivalent for $v \in V$:
    \begin{enumerate}
    \item $v$ is an eigenvector for every element of $\mathfrak{b}$.
    \item $v$ is an eigenvector of $H$ and satisfies $X v = 0$.
    \end{enumerate}
  \item Let $V$ be a finite-dimensional representation of $G$.  Let $v \in V$ be a nonzero element satisfying $H v = \lambda v$ for some integer $\lambda \in \mathbb{Z}$.  Define $v' \in V$ by
    \begin{equation*}
      v' :=
      \begin{cases}
        Y^\lambda v & \lambda \geq 0
        \\
        X^{-\lambda} v & \lambda \leq 0.
      \end{cases}
    \end{equation*}
    \begin{enumerate}
    \item Verify that $H v' = - \lambda v'$.
    \item Prove that $v' \neq 0$.  [Hint: Use the classification theorem for $V$ proved in lecture.]
    \end{enumerate}
  \item (Optional) The $n$th Legendre polynomial $P_n$ may be defined (perhaps up to a sign) by
    \begin{equation*}
      P_n(x) := \sum_{k=0}^n {\binom{n}{k}}^2 \left( \frac{x-1}{2} \right)^{n-k} \left( \frac{x+1}{2} \right)^{k}.
    \end{equation*}
    The purpose of the exercise is to establish the formula
    \begin{equation}\label{eq:legendre-composition}
      P_n(\cos \theta_1) P_n(\cos \theta_2)
      = \int_{\phi = -\pi}^{\pi}
      P_n(\cos(\theta_1) \cos(\theta_2) - \sin(\theta_2)
      \sin(\theta_2) \cos(\phi))
      \,
      \frac{d \phi }{2 \pi }.
    \end{equation}
    The geometric interpretation of the argument in the integrand is that if one fixes a point $O \in S^2$ at spherical distance $\theta_1$ from the north pole $N$, then $\cos(\theta_1) \cos(\theta_2) - \sin(\theta_2) \sin(\theta_2) \cos(\phi)$ is the vertical coordinate of the point $P \in S^2$ at spherical distance $\theta_2$ from $O$ for which the angle between the arcs $O N$ and $O P$ is $\phi$.  [You might wish first to attempt to prove \eqref{eq:legendre-composition} directly.]
    \begin{enumerate}
    \item Let $R : G \rightarrow \GL(V)$ be the $(2 n+1)$-dimensional representation $V := W_{2 n}$ defined in the lectures.  Let $(v_k)_{k=-n..n}$ be the basis of $V$ given by $v_k := x^{n+k} y^{n-k}$.  For $i,j \in \{-n..n\}$, let $R_{i j}(g)$ denote the matrix entry of $g \in G$ with respect to this basis, i.e., the coefficient of $v_i$ in $R(g) v_j$.  For $\theta,\phi \in \mathbb{R}$, set
      \begin{equation*}
        \kappa(\theta) := 
\begin{pmatrix}
          \cos (\theta/2)  & i\sin (\theta/2)  \\
          i\sin (\theta/2) & \cos (\theta /2)
        \end{pmatrix}
,
        \quad
        \delta(\phi) :=
        \begin{pmatrix}
          e^{i \phi/2}  &  \\
                     & e^{-i \phi/2}
        \end{pmatrix}
.
      \end{equation*}
      Verify that
      \begin{equation*}
        R_{0 0}(
\begin{pmatrix}
          a & b \\
          c & d
        \end{pmatrix}
)
        = \sum_{k=0}^n
        {\binom{n}{k}}^2
        (a d)^k (c y)^{n-k}.
      \end{equation*}
      Deduce that
      \begin{equation*}
        P_n(\cos(\theta)) = R_{0 0}(\kappa(\theta)).
      \end{equation*}
    \item Show that for each $\theta_1,\theta_2,\phi$ there exist $\phi_1,\phi_2,\theta$ so that
      \begin{equation*}
        \kappa(\theta_1) \delta(\phi) \kappa(\theta_2) = \delta(\phi_1) \kappa(\theta) \delta(\phi_2)
      \end{equation*}
      and moreover
      \begin{equation*}
        \cos(\theta) = \cos(\theta_1) \cos(\theta_2) - \sin(\theta_1) \sin(\theta_2) \cos(\phi).
      \end{equation*}
      This can be proved directly via the geometric interpretation mentioned after \eqref{eq:legendre-composition} using the map $\SU(2) \rightarrow \SO(3)$ to be discussed next week; if one wishes to attempt an algebraic proof, it may help to note that
      \begin{enumerate}
      \item \emph{every} element of $\SU(2)$ may be decomposed as $\delta(\phi_1) \kappa(\theta) \delta(\phi_2)$,
      \item the function
        \begin{equation*}
          f : \SU(2) \rightarrow \mathbb{R}
        \end{equation*}
        given by
        \begin{equation*}
          f(
\begin{pmatrix}
            \alpha  & \beta  \\
            - \overline{\beta } & \overline{\alpha }
          \end{pmatrix}
          ) := 2 |\alpha|^2 - 1
        \end{equation*}
        satisfies $f(\delta(\phi_1) g \delta(\phi_2)) = f(g)$ and $f(\kappa(\theta)) = \cos(\theta)$.
      \end{enumerate}
      It may also help to treat first the case $\phi = 0$.
    \item Verify that $R_{k l}(\delta(\phi_1) g \delta(\phi_2)) = e^{i(-k \phi_1 + l \phi_2)}$ for all relevant indices and arguments.
    \item Prove \eqref{eq:legendre-composition} by taking the $(0,0)$th matrix coefficient of the identity
      \begin{equation*}
        R(g_1 g_2) = R(g_1) R(g_2)
      \end{equation*}
      with
      \begin{equation*}
g_1 := \kappa(\theta_1) \delta(\phi),
\end{equation*}
      \begin{equation*}
g_2 := \kappa(\theta_2)
\end{equation*}
 and integrating over $\phi$.  [It may be helpful to recall the Fourier inversion formula
      \begin{equation*}
        \int_{\phi = -\pi}^{\pi} e^{i k \phi} \, \frac{d \phi }{2 \pi } = 
\begin{cases}
          1 & k = 0 \\
          0 & k \neq 0
        \end{cases}
      \end{equation*}
      for $k \in \mathbb{Z}$.]
    \end{enumerate}
  \end{enumerate}
\end{homework}

\newpage
\subsection{10/11 (half-lecture) and 10/13: The unitary trick}
\label{sec:orgc4f94de}
\textbf{Objectives.}  Given a real form of a complex Lie algebras, you should be able to relate representations of the two.  You should be able to verify that the classical complex Lie groups have compact real forms and apply this fact to deduce their linear reductivity.  You should know the definitions of \(\Ad\) and \(\ad\) and be able to apply the fact that they are morphisms.

\textbf{Summary.}
\begin{enumerate}
\item introduction to and overview of the "unitary trick"
\item defn of real form of a complex Lie algebra, comparison between representations
\item defn of real form of a connected complex Lie group
\item example of a representation that is not completely reducible
\item lemma: complete reducibility is the same as invariant subspaces having invariant complements
\item stated theorems that the following classes of groups are linearly reductive:
  \begin{enumerate}
  \item finite groups
  \item (more generally) compact groups
  \item complex connected Lie groups with a compact real form
  \end{enumerate}
\item (Thursday onwards) we proved the above theorems.
\item along the way, we proved the useful fact that any finite-dimensional representation of a compact group is unitarizable, i.e., admits an invariant inner product.
\item we spent some time talking about examples of real forms and complexifications.
\item we introduced Ad and ad.  we related them, proved some of their basic properties, and interpreted the Jacobi identity in terms of properties of ad.
\end{enumerate}

\begin{homework}[Due Oct 18]~\label{hw:all-about-Ad}
  \begin{enumerate}
  \item Prove that if $f : G \rightarrow H$ is a Lie group morphism, then $d f(\Ad(g) X) = \Ad(f(g)) d f(X)$.
  \item Do Exercise \ref{exe:complexifications}.
  \item Let $G := \SL_2(\mathbb{C})$; it is a three-dimensional complex Lie group.  Regard $\Ad : G \rightarrow \GL(\mathfrak{g})$ as a three-dimensional holomorphic representation of $G$.  Write down an explicit isomorphism between $\Ad$ and the representation $W_2 = \mathbb{C} x^2 \oplus \mathbb{C} x y \oplus \mathbb{C} y^2$ discussed in lecture.
  \item Let $\mathfrak{g}$ be a Lie algebra (the case $\mathfrak{g} = \End(V)$ is already interesting), let $n \geq 1$, and let
    \begin{equation*}
      M = [Z_1,[Z_2,\dotsc,[Z_{n-1},Z_n] \dotsb ] = \ad(Z_1) \ad(Z_2) \dotsb \ad(Z_{n-1}) Z_n
    \end{equation*}
    be an $n$-fold iterated commutator of elements $Z_1,\dotsc,Z_n \in \mathfrak{g}$.  Let $M' \in \mathfrak{g}$ be the result of formally expanding $M$ as a sum of degree $n$ monomials $Z_{i_1} \dotsb Z_{i_n}$ and replacing each such monomial by the corresponding commutator $\ad(Z_{i_1}) \dotsb \ad(Z_{i_{n-1}}) Z_{i_n}$.  For example:
    \begin{enumerate}
    \item If $M = [X,Y]$, then we expand and set
      \begin{align*}
        M &= X Y - Y X,\\
        M' &:= [X,Y] - [Y,X]
      \end{align*}
      and obtain $M' = 2 [X,Y]$ after some simplification.
    \item If $M = [X,[Y,X]]$, then we expand and set
      \begin{align*}
        M &= X Y X - X X Y - Y X X + X Y X, \\
        M' &:= [X,[Y,X]] - [X,[X,Y]] - [Y,[X,X]] + [X,[Y,X]]
      \end{align*}
      and obtain $M ' = 3 [X,[Y,X]]$ after some simplification.
    \end{enumerate}
    Show that $M' = n M$.  [Hint: induct on $n$.  Use the consequence $[\ad(Z_{i_1}), [\ad(Z_{i_2}), \dotsc, [\ad_{Z_{i_{n-1}}}, \ad_{Z_{i_n}}]]] = \ad([Z_{i_1},[Z_{i_2}, \dotsc, [Z_{i_{n-1}}, Z_{i_n}]]])$ of iterated application of the Jacobi identity in the form $\ad([X,Y]) = [\ad(X),\ad(Y)]$.]
  \end{enumerate}
\end{homework}

\newpage
\subsection{10/18 (half-lecture): The adjoint representation}
\label{sec:org019e4bd}
\textbf{Objectives.} You should be able to use the adjoint representation to describe some low-dimensional exceptional isomorphisms and to relate representations of the involved Lie groups and Lie algebras.

\textbf{Summary.}
\begin{enumerate}
\item recap of what we've shown about representations of \(\SL_2(\mathbb{C})\) and \(\SU(2)\)
\item the exceptional isomorphisms \(\SL_2(\mathbb{C})/ \{\pm 1\} \cong \SO_3(\mathbb{C})\), \(\SU(2) / \{\pm 1\} \cong \SO(3)\), \(\SL_2(\mathbb{R})/ \{\pm 1\} \cong \SO(1,2)^0\), (plus some generalities on quadratic spaces)
\end{enumerate}
\begin{homework}
[Due Oct 25]~
  \begin{enumerate}
  \item Write down a careful proof that the adjoint representation $\Ad : G \rightarrow \GL(\mathfrak{g})$ of the group $G := \SL_2(\mathbb{R})$ induces an isomorphism of Lie groups $f : \PSL_2(\mathbb{R}) \xrightarrow{\cong} \SO(1,2)^0$.  Give an explicit isomorphism of Lie algebras $d f : \slLie_2(\mathbb{R}) \xrightarrow{\cong } \so(1,2)$.
  \item Explain why the adjoint representation of $G = \GL_2(\mathbb{C})$ does \emph{not} induce an isomorphism between $G$ and $\SO_4(\mathbb{C})$.
  \item Let $\mathbb{H}$ denote Hamilton's quaternion algebra over $\mathbb{R}$, realized as the subalgebra of $M_2(\mathbb{C})$ given by
    \begin{equation*}
      \mathbb{H} := \left\{
        \begin{pmatrix}
          z & w \\
          -\overline{w} & \overline{z}
        \end{pmatrix}
        :z,w \in \mathbb{C} \right\}.
    \end{equation*}
    Set
    \begin{equation*}
      \mathbb{H}^{(1)} := \left\{ g \in \mathbb{H}^\times : \det(g) = 1 \right\}
    \end{equation*}
    and
    \begin{equation*}
      \mathbb{H}_0 := \left\{ v \in \mathbb{H} : \trace(v) = 0 \right\}.
    \end{equation*}
    \begin{enumerate}
    \item Verify that $\mathbb{H}^{(1)} = \SU(2)$.  Deduce in particular via the embedding $(z,w) \hookrightarrow \mathbb{C}^2 \hookrightarrow \mathbb{R}^4$ that $\SU(2)$ is diffeomorphic to the three-dimensional sphere $S^3$.
    \item Show that $(\mathbb{H}_0,\det)$ is a quadratic space over $\mathbb{R}$ of signature $(3,0)$.
    \item Let $\alpha : \mathbb{H}^\times \rightarrow \GL(\mathbb{H}_0)$ be the conjugation action $\alpha(g)(v) := g v g^{-1}$ ($g \in \mathbb{H}^\times, v \in \mathbb{H}_0$).  Show that $\alpha(\mathbb{H}^{(1)}) = \SO(\mathbb{H}_0,\det) \cong \SO(3)$ and that $\{g \in \mathbb{H}^{(1)} : \alpha(g) = 1\} = \{\pm E\}$ where $E := \left(
        \begin{smallmatrix}
          1&\\
           &1
        \end{smallmatrix}
      \right)$.  Deduce that
      \begin{equation*}
        \mathbb{H}^\times / \mathbb{R}^\times \cong \mathbb{H}^{(1)} / \{\pm 1\} \cong \SO(3).
      \end{equation*}
 [Use the connectedness of $\SO(3)$ to reduce the problem to one involving Lie algebras.]
    \item Deduce that $\alpha$ induces an isomorphism $\SU(2) / \{\pm 1\} \cong \SO(3)$.  Compare with the proof given in class by showing that one has $\Lie(\mathbb{H}^{(1)}) = \mathbb{H}_0$ under the natural identification $\Lie(\mathbb{H}^{\times}) = \mathbb{H}$.
    \end{enumerate}
  \item (Optional) Here we understand how the map $\SU(2) \rightarrow \SO(3)$ may be defined by comparing the standard actions $\mathbb{P}^1(\mathbb{C}) \circlearrowleft \SU(2)$ and $S^2 \circlearrowleft \SO(3)$ under the identification $\mathbb{P}^1(\mathbb{C}) \cong S^2$ given by stereographic projection:
    \begin{enumerate}
    \item Let $\mathbb{P}^1(\mathbb{C})$ be the complex projective line, that is, the set of equivalence classes $[z:w]$ of row vectors $(z,w) \in \mathbb{C}^2 - \{0\}$ under the equivalence relation $(z,w) \simeq (\lambda z,\lambda w)$ for all $\lambda \in \mathbb{C}^\times$.  Verify that $\SU(2)$ acts on $\mathbb{P}^1(\mathbb{C})$ via
      \begin{equation*}
 [\xi:\eta] \cdot g := [a \xi + c \eta : b \xi + d \eta ] \text{ for } g =
        \begin{pmatrix}
          a & b \\
          c & d
        \end{pmatrix}
        \in \SU(2).
      \end{equation*}
    \item Let $S^2 := \{(x,y,z) : x^2 + y^2 + z^2\} \subseteq \mathbb{R}^3$ be the standard two-dimensional sphere.  Let $\SO(3)$ act on $S^2$ in the usual way: for $v \in S^2$ and $g \in G$, $v \cdot g$ is given by matrix multiplication.  Verify that an element of $\SO(3)$ is determined by its action on $S^2$.
    \item Let $p := (0,0,-1) \in S^2$ denote the ``south pole'' and let $P := \{(u,v,0) : u,v \in \mathbb{R}\} \subseteq \mathbb{R}^3$ denote the ``equatorial plane.''  Let $\pi : S^2 - \{p\} \rightarrow P$ denote the result of stereographic projection from $p$, thus $\pi(x,y,z) = (u,v,0)$ means that the points $(0,0,-1)$, $(u,v,0)$, $(x,y,z)$ are collinear.  Let $\rho : P \hookrightarrow \mathbb{P}^1(\mathbb{C})$ be the map $\rho(u,v,0) := [u+i v : 1]$.  Verify that the composition $\rho \circ \pi : S^2 - \{p\}$ extends to a homeomorphism
      \begin{equation*}
        \iota : S^2 \rightarrow \mathbb{P}^1(\mathbb{C})
      \end{equation*}
      for which $\iota(p) = [1:0]$.
    \item Show that for each $g \in \SU(2)$ there is a unique $\alpha(g) \in \SO(3)$ so that for all $s \in S^2$, one has $\iota(s \cdot \alpha(g)) = \iota(s) \cdot g$.  Show that the map $\alpha : \SU(2) \rightarrow \SO(3)$ is a surjective morphism of Lie groups.
    \item Read about the ``Hopf fibration'' somewhere and understand its relevance.
    \end{enumerate}
  \end{enumerate}
\end{homework}

\newpage
\subsection{10/20 (first half): Character theory for \texorpdfstring{$\SL(2)$}{SL2} (algebraic)}
\label{sec:org7c4df3a}
\textbf{Objectives.} Given a (reasonably explicit) representation \(\SL_2(\mathbb{C})\) or some closely related group, you should be able to determine its reduction into irreducibles by computing its character and multiplying by the Weyl denominator.

\textbf{Summary.}
\begin{enumerate}
\item definitions of direct sum and tensor product of representations of Lie groups and Lie algebras
\item characters of representations of \(\SL_2(\mathbb{C})\) as Laurent polynomials in one variable \(z\)
\item compatibility with direct sum and direct product
\item the characters of the irreducibles
\item the Weyl denominator \(z - 1/z\)
\item Clebsch--Gordon decomposition
\end{enumerate}
\begin{homework}[Due Oct 25]~\label{hw:characters-sl2}
  \begin{enumerate}
  \item Verify that if $\rho_j : \mathfrak{g} \rightarrow \End(V_j)$ ($j=1,2$) are representations of a Lie algebra, then the map $\rho_1 \otimes \rho_2 : \mathfrak{g} \rightarrow \End(V_1 \otimes V_2)$, defined as in class by linear extension of its definition on pure tensors $v_1 \otimes v_2 \in V_1 \otimes V_2$ by
    \begin{equation*}
      ((\rho_1 \otimes \rho_2)(X))(v_1 \otimes v_2) := \rho_1(X) v_1 \otimes v_2 + v_1 \otimes \rho_2(X) v_2,
    \end{equation*}
    or in abbreviated form simply by
    \begin{equation*}
      X(v_1 \otimes v_2) := X v_1 \otimes v_2
    + v_1 \otimes X v_2,
    \end{equation*}
    defines a representation of Lie algebras.
  \item
    Verify that the map
    $W_2 \oplus W_0 \rightarrow W_1 \otimes W_1$
    defined in class
    is an
    isomorphism of $\SL_2(\mathbb{C})$-representations.
  \item
    Show that there does \emph{not} exist
    a representation $V$ of $\SL_2(\mathbb{C})$
    whose weight spaces $V[m] := \{ v \in V : H v = m v \}$ ($m \in \mathbb{Z}$) have dimensions
    given by
    \begin{equation*}
      \dim V[m] = 
\begin{cases}
        1 & m \in \{-7,-6,-5,5,6,7\}, \\
        0 & \text{otherwise.}
      \end{cases}
    \end{equation*}
    More generally, for which functions $\nu : \mathbb{Z} \rightarrow \mathbb{Z}_{\geq 0}$
    does there exist a finite-dimensional representation $V$ of $\SL_2(\mathbb{C})$
    with $\dim V[n] = \nu(n)$ for all $n$?
    [Hint: write $V \cong \oplus W_m^{\oplus \mu(m)}$ and apply $D \cdot \operatorname{ch}(.)$ to both sides.]
  \item
    (Optional)
    Given $k \in \mathbb{Z}_{\geq 0}$ and
    a representation $R : G \rightarrow \GL(V)$
    of a Lie group $G$,
    one obtains a symmetric power representation
    $\Sym^k(R) : G \rightarrow \GL(\Sym^k(V))$
    on the symmetric power vector space $\Sym^k(V)$;
    see \S\ref{sec:constructing-sym-power} or Google for some details.
    The purpose of this exercise is to relate
    the character of $\Sym^k(V)$ to that of $V$.
    We restrict to the case $G := \SL_2(\mathbb{C})$,
    although the arguments are somewhat more general.
    Let $A := \mathbb{Z}[z,z^{-1}]$ be as in lecture.
    \begin{enumerate}
      \item
    Define $\sigma(V)$ to be the element of the formal power
    series ring
    $A[[T]]$ in the variable $T$ with coefficients in $A$
    given by the formula
    \begin{equation*}
      \sigma(V) := \sum_{k \in \mathbb{Z}_{\geq 0}}
      \operatorname{ch}(\Sym^k V) T^k.
    \end{equation*}
    Show that
    \begin{equation*}
      \sigma(V) = \exp \sum _{k \geq 1}
      \frac{\Psi^k(\operatorname{ch}(V)) T^k}{k }
    \end{equation*}
    where $\Psi^k$
    is defined
    via the substitution $z \mapsto z^k$,
    i.e., by setting
    $\Psi^k(\chi)(z) := \chi(z^k)$ for $\chi \in A$.
    [Hint:
    the identity
    \begin{equation*}
      \exp \sum_{k \geq 1}
      \frac{x^k}{k}
      =
      \frac{1}{1-x}
      = \sum_{k \geq 0}
      x^k
    \end{equation*}
    is relevant.]
  \item 
    Deduce the recursion relation
    \begin{equation*}
      n \operatorname{ch}(\Sym^k (V)) = \sum_{k = 1}^n
      \Psi^k(\operatorname{ch}(V))
      \operatorname{ch}(\Sym^{n-k}(V)).
    \end{equation*}
    Check that this is consistent
    with the isomorphisms
    $\Sym^n(W_1) \cong W_n$.
  \item
    Specialize the above relation to the case $n = 2$
    to obtain
    \begin{equation*}
      \operatorname{ch}(\Sym^2(V))
      = \frac{\operatorname{ch}(V)^2 - \Psi^2(\operatorname{ch}(V))}{2}.
    \end{equation*}
    For each irreducible
    representation $W_m$ of $G$,
    compute
    $\operatorname{ch}(\Sym^2(W_m))$
    by the above formula and use it to
    derive
    the decomposition
    \begin{equation*}
      \Sym^2(W_m)
      \cong
      W_{2 m} \oplus W_{2 m - 4} \oplus \dotsb
      = \oplus _{\substack{
          0 \leq j \leq 2 m : \\
          j \equiv 2 m (4)
        }}
      W_j.
    \end{equation*}
    of $\Sym^2(W_m)$ into irreducibles.
    (It is also instructive, and not difficult,
    to derive this decomposition directly.)
    Write down an explicit isomorphism $\Sym^2(W_2) \cong W_4 \oplus W_0$.
  \end{enumerate}
  \end{enumerate}
\end{homework}

\subsection{10/20 (second half): Maurer-Cartan equations; lifting morphisms of Lie algebras}
\label{sec:org5473899}
\textbf{Objectives.} You should know that \(\Hom(G,H) \rightarrow
\Hom(\mathfrak{g},\mathfrak{h})\) is injective when \(G\) is connected
and surjective when \(G\) is simply-connected and be able to give some
basic counter-examples indicating the necessity of such conditions.
You should be able to describe the role played by the Maurer--Cartan
equations in establishing surjectivity in the simply-connected case.
Given hints, you should be able to apply the Maurer--Cartan equation to related
problems.

\textbf{Summary.}
\begin{enumerate}
\item statement of main theorem on lifts of Lie algebra morphisms
\item proof via paths and Maurer--Cartan equation
\end{enumerate}
\begin{homework}[Due Oct 25]~\label{hw:diff-exp}
  \begin{enumerate}
  \item
    For a smooth scalar-valued function
    $f : \mathbb{R} \xdashrightarrow{} \mathbb{R}$,
    the chain rule implies that
    \begin{equation}\label{eq:exp-path-abelian-case}
            \frac{d}{d t} \exp(f(t))
      = \exp(f(t)) f'(t).
    \end{equation}
    The purpose of this exercise is to
    generalize
    the above identity as an application of a technique introduced in lecture.
    Let $G$ be a real Lie group with Lie algebra $\mathfrak{g}$;
    the problem is already interesting when
    $G = \GL_n(\mathbb{R})$, so feel free to assume that.  
    Prove that for a smooth function
    $f : \mathbb{R} \xdashrightarrow{} \mathfrak{g}$,
    one has
    \begin{equation}\label{eq:derivative-exponential-path}
      \frac{d}{d t} \exp(f(t))
      =
      \exp(f(t))
      \sum_{n=1}^{\infty}
      \frac{
        (- \ad_{f(t)})^{n-1} f'(t)
      }{n!}
    \end{equation}
    where we may write more explicitly
    \begin{equation*}
      (- \ad_{f(t)})^{n-1} f'(t)
      = [[[f'(t), f(t)], f(t)], \dotsc, f(t)].
    \end{equation*}
    Observe that
    \eqref{eq:derivative-exponential-path}
    specializes
    to \eqref{eq:exp-path-abelian-case}
    when $G$ is abelian,
    so that $\ad_{f(t)} = 0$.

    [Hint:
    Consider the map
    $g : \mathbb{R}^2 \xdashrightarrow{} G$
    given by
    \begin{equation*}
      g(s,t) := \exp(s f(t)).
    \end{equation*}
    Define $\xi : \mathbb{R}^2 \xdashrightarrow{}\mathfrak{g}$
    by $\frac{\partial g}{\partial t} = g \xi$,
    so that $\frac{d}{d t} \exp(f(t))
    = \exp(f(t)) \xi(t,1)$.
    Apply the Maurer--Cartan equation (\S\ref{sec:cartan-maurer-eqn})
    to characterize $\xi$ as the unique
    solution $F$ to the differential equation
    \begin{equation*}
      \frac{\partial F}{\partial s}(t,s)
      = f'(t)
      + [F(t,s), f(t)]
    \end{equation*}
    with initial condition $F(t,0) = 0$.
    On the other hand, verify that such a solution
    may be given explicitly by
    \begin{equation*}
      F(t,s) :=
      \sum_{n=1}^{\infty}
      s^n
      \frac{
        (- \ad_{f(t)})^{n-1} f'(t)
      }{n!}
    \end{equation*}
    and set $s := 1$ to conclude.]
  \end{enumerate}
\end{homework}

\newpage
\subsection{10/25 (half-lecture): universal covering group}
\label{sec:org37094a7}
\textbf{Objectives.} You should be able to classify the Lie groups having a
given Lie algebra in terms of discrete central subgroups of a
simply-connected group.  You should be able to describe some basic
examples of covering morphisms and use them to determine the
fundamental groups of some Lie groups.

\textbf{Summary.}
\begin{enumerate}
\item The main theorem was that for any connected Lie group \(G\) there
exists a simply-connected Lie group \(\tilde{G}\) and a covering
morphism \(p : \tilde{G} \rightarrow G\) whose kernel \(N = \ker(p)\)
is a discrete subgroup of the center of \(\tilde{G}\), with
\((\tilde{G},N)\) uniquely determined up to isomorphism.  Moreover,
\(\pi_1(G) \cong N\).
\item We gave several examples to which this applies.  Some further
examples are given on the homework.
\item We stated without proof that every finite-dimensional Lie algebra
arises from some Lie group.
\item By combining with a result from last time, we deduced that the
category of simply-connected Lie groups is equivalent to the
category of finite-dimensional Lie algebras.
\item We recalled the definition of "cover" (locally trivial fiber bundle
with discrete fiber).  We briefly recalled the construction of the
universal cover of a connected manifold and stated its universal
property (existence of unique lifts of paths).  We defined the
group structure on the simply-connected cover of a Lie group.
\item We reduced the remainder of the proof of the main theorem to some
lemmas left mostly as exercises.
\end{enumerate}
\begin{homework}[Due Nov 1]~\label{hw:universal-covering-group}
  \begin{enumerate}
  \item
    Let $n \geq 1$.
    For the purposes of this exercise,
    you may use without proof that $\SL_n(\mathbb{C})$
    and $\SU(n)$ are
    simply-connected.
    \begin{enumerate}
    \item 
      Show that $\pi_1(\PGL_n(\mathbb{C})) \cong \mathbb{Z}/n$.
      [Hint: show that the natural map $p : \SL_n(\mathbb{C}) \rightarrow
      \PGL_n(\mathbb{C})$
      is a covering morphism, determine the kernel of $p$,
      and appeal to the theorem from lecture.] 
    \item
      Set
      $\mathfrak{g} := \slLie_n(\mathbb{C})$.
      Determine the connected Lie groups $G$
      (up to isomorphism, and over either the real or complex numbers -- it doesn't matter)
      having Lie algebra (isomorphic to) $\mathfrak{g}$,
      and describe their fundamental groups $\pi_1(G)$.
      [Hint:
      start by determining the center of $\SL_n(\mathbb{C})$.]
      Interpret ``determine'' as you wish.  For instance, you should be able to answer the following question:
      How many isomorphism classes of connected Lie groups have
      Lie algebra isomorphic to $\slLie_{12}(\mathbb{C})$?
    \item
      Same question
      but for $\mathfrak{g} := \su(n)$.
    \end{enumerate}
  \item Verify that (at least one or two of)
    the following maps are covering morphisms of
    Lie groups and determine their kernels.
    [Hint: the lemma from lecture characterizing
    covering morphisms may help.]
    \begin{enumerate}
    \item The morphism of complex Lie groups
      \begin{equation*}
        \mathbb{C} \xrightarrow{\exp} \mathbb{C}^\times.
      \end{equation*}
    \item The morphism of complex Lie groups
      \begin{equation*}
        \SL_2(\mathbb{C}) \xrightarrow{\Ad}
        \SO(\slLie_2(\mathbb{C}),\det)
        \cong 
        \SO_3(\mathbb{C}).
      \end{equation*}
    \item The morphism of real Lie groups
      \begin{equation*}
        \SU(2) \xrightarrow{\Ad}
        \SO(\su(2),\det)
        \cong 
        \SO(3).
      \end{equation*}
    \item The morphism of real Lie groups
      \begin{equation*}
        \SL_2(\mathbb{R}) \xrightarrow{\Ad}
        \SO(\slLie_2(\mathbb{R}),\det)^0
        \cong 
        \SO(1,2)^0.
      \end{equation*}
    \item The morphism of complex Lie groups
      \begin{equation*}
        \SL_2(\mathbb{C}) \times \SL_2(\mathbb{C}) \rightarrow 
        \SO(M_2(\mathbb{C}),\det)
        \cong \SO_4(\mathbb{C}),
      \end{equation*}
      \begin{equation*}
        (g,h) \mapsto [x \mapsto g x h^{-1}].
      \end{equation*}
    \item The morphism of real Lie groups
      \begin{equation*}
        \SU(2) \times \SU(2)
        \rightarrow 
        \SO(\mathbb{H},\det)
        \cong \SO(4),
      \end{equation*}
      \begin{equation*}
        (g,h) \mapsto [x \mapsto g x h^{-1}],
      \end{equation*}
      where $\mathbb{H} = \left\{ 
\begin{pmatrix}
          z & w \\
          -\overline{w} & \overline{z}
        \end{pmatrix}
 \right\}\subseteq M_2(\mathbb{C})$ denotes
      Hamilton's quaternion algebra as considered on previous homeworks.
    \item The morphism of \emph{real} Lie groups
      \begin{equation*}
        \SL_2(\mathbb{C})
        \rightarrow 
        \SO(V,\det)^0
        \cong \SO(1,3)^0,
      \end{equation*}
      \begin{equation*}
        g \mapsto [x \mapsto g x \overline{g}^t],
      \end{equation*}
      where
      $V 
      := \left\{ X \in M_2(\mathbb{C}) : \overline{X} = X^t
      \right\}
      =
      \left\{ 
\begin{pmatrix}
          x &  z \\
          \overline{z} & y
        \end{pmatrix}
 : x,y \in \mathbb{R}, z \in \mathbb{C}
      \right\}$
      is the space of $2 \times 2$ hermitian matrices.
    \end{enumerate}
  \end{enumerate}
  % Assuming that
  %   $\SL_2(\mathbb{C})$ and $\SU(2)$ are simply-connected,
  %   determine the fundamental groups of
  %   $\SO_3(\mathbb{C}),\SO(3), \SO_4(\mathbb{C}),\SO(4)$ and
  %   $\SO(1,3)^0$. 
\end{homework}

\newpage
\subsection{10/27: Fundamental groups of Lie groups}
\label{sec:org6e185d7}
\textbf{Objectives}.  You should be able to analyze the topology of Lie
groups by
\begin{itemize}
\item applying the homotopy exact sequence to their transitive
actions, and
\item via the Cartan decomposition.
\end{itemize}

\textbf{Summary}
\begin{enumerate}
\item Description, without proof, of the fundamental groups of the
classical groups; empirical observation that complex Lie groups
and their compact real forms (if they exist) have the same
fundamental groups
\item Homotopy exact sequence and its consequences; proofs of some of the
descriptions
of fundamental groups given before (most were left as exercises).
For example, we showed inductively that \(\SL_n(\mathbb{C})\) is simply-connected.
\item Quotient groups (abstract, topological, Lie), quotient manifolds,
transitive action theorem; sketch of construction of smooth
structure on the quotient
\item Statement of Cartan decomposition; application to comparing
homotopy groups, recap on the unitary trick
\end{enumerate}

\begin{homework}
[Due Nov 1]~
  \begin{enumerate}
  \item
    Let $p,q \geq 1$.
    Set $G := \O(p,q)$,
    realized as usual as a subgroup of $\GL_{p+q}(\mathbb{R})$.
    Let $\Theta$ be given by $\Theta(g) := {}^t
    \overline{g}^{-1}$.
    \begin{enumerate}
    \item Show that the subgroup
      $K := \{g \in G : \Theta(g) = g\}$ fixed by $G$ identifies
      with $\O(p) \times \O(q)$.
    \item Use (without proof) the Cartan decomposition (\S\ref{sec:cartan-decmop})
      \begin{equation*}
K \times \mathfrak{p} \cong G
\end{equation*} 
\begin{equation*}
(k,Y) \mapsto k \exp(Y)
\end{equation*}
      to show that $G$ has four connected components.
    \item Describe the Cartan decomposition
      explicitly in the special case $p := 1, q := 1$,
      and compare with the related problem on Homework \ref{hw:2}.
    \end{enumerate}
  \item (Optional)
    Let 
\begin{equation*}
G := \SL_2(\mathbb{R}).
\end{equation*}
    Denote by $\mathbb{H} := \{x + i y : x,y \in \mathbb{R}, y >
    0\} \subseteq \mathbb{C}$ the upper half-plane.
    The group $G$ acts smoothly on $\mathbb{H}$ by fractional
    linear transformations:
    \begin{equation*}
      g z := \frac{a z + b}{c z + d}
      \text{ if }
      g = 
\begin{pmatrix}
        a & b \\
        c & d
      \end{pmatrix}
 \in G,
      \quad z \in \mathbb{H}.
    \end{equation*}
    Denote by $\tilde{G}$
    the set of all pairs
    $(g,\phi)$,
    where
    \begin{itemize}
    \item $g \in G$, and
    \item $\phi : \mathbb{H} \rightarrow \mathbb{C}$ is a
      holomorphic function
      with the property that
      \begin{equation*}
        \exp(\phi(z))
        =
        c z + d
        \text{ if we write }
        g = 
\begin{pmatrix}
          a & b \\
          c & d
        \end{pmatrix}
.
      \end{equation*}
    \end{itemize}
    In other words, $\phi$ is a branch of $\log(c z + d)$;
    it is determined by $g$ and any of its values, say $\phi(i)$.
    We may define on $\tilde{G}$ a smooth structure
    by regarding it as a submanifold of
    $G \times \mathbb{C}$
    via the embedding $(g,\phi) \mapsto (g,\phi(i))$.
    We define on $\tilde{G}$ a group structure
    by the law
    \begin{equation*}
      (g_1,\phi_1) \cdot (g_2,\phi_2)
      :=
      (g_1 g_2, \phi_1^{g_2} \phi_2)
      \text{ where }
      (\phi_1^{g_2} \phi_2)(z)
      :=
      \phi_1(g_2 z) \phi_2(z).
    \end{equation*}
    This group operation is then associative and smooth,
    and defines on $\tilde{G}$ the structure of a Lie group.
    The natural map
    \begin{equation*}
      \pi : \tilde{G} \rightarrow G
    \end{equation*}
    given by $\pi((g,\phi)) := g$,
    is smooth and surjective.
    % Let $N := \ker(\pi)$ denote its kernel;
    % then $N$ consists of pairs $(1,\phi)$
    % with $\phi$ of the form $\phi(z) = 2 \pi i n$ for some $n \in
    % \mathbb{Z}$,
    % thus $N \cong \mathbb{Z}$.
    The group $\tilde{G}$ inherits from $G$ an action on $\mathbb{H}$:
    $(g,\phi) \cdot z := g z$.
    The map
    \begin{equation*}
      \kappa : \mathbb{R} \rightarrow \tilde{G}
    \end{equation*}
    given
    by
    \begin{equation*}
      \kappa(\theta) :=
      (
      \begin{pmatrix}
        \cos(\theta) & \sin(\theta) \\
        -\sin(\theta) & \cos(\theta)
      \end{pmatrix}
,
      \phi_\theta),
    \end{equation*}
    where $\phi_\theta(z)$ is the unique branch
    of $\log(-\sin(\theta) z + \cos(\theta))$
    for which
    $\phi_\theta(i)
    = - i \theta$,
    is a morphism of Lie groups.
    [For the purposes of this exercise,
    all of the assertions just made may be regarded as sufficiently
    self-evident as not to require proof.]
    \begin{enumerate}
    \item Write down an isomorphism $N \cong \mathbb{Z}$.
    \item Show that $\tilde{G}$ is connected.
      [Hint: use $\kappa$ to show that $N \subseteq \tilde{G}^0$,
      and use
      that $G$ is connected.]
    \item
      Let $H \leq \tilde{G}$ denote the image of $\kappa$.
      Show that $H$ is the stabilizer in $\tilde{G}$
      of the point $i \in \mathbb{H}$.
    \item Show that $\tilde{G}$ is simply-connected.
      [The homotopy exact sequence gives one way to do this;
      alternatively, one can find a diffeomorphism
      $\tilde{G} \cong \mathbb{H} \times \mathbb{R}$.]
    \end{enumerate}
    In summary,
    $\tilde{G}$ is the simply-connected covering group of $G$,
    and $\pi_1(G) \cong N \cong \mathbb{Z}$.
  \end{enumerate}
\end{homework}

\newpage
\subsection{11/1: The Baker--Campbell--Hausdorff(--Dynkin) formula}
\label{sec:orged45fdb}
\textbf{Objectives.} You should be able to describe the BCH formula
(qualitatively), specialize it to the case of \$2\$-step nilpotent groups, and
apply it to derive asymptotic expansions in local exponential
coordinates of products in a Lie group.

\textbf{Summary.}
\begin{enumerate}
\item We defined what it means for a pair of Lie groups to be locally
isomorphic, and explained how the lifting theorem for
simply-connected Lie groups and the existence of the universal
cover of a given Lie group imply that two Lie groups are locally
isomorphic if and only if their Lie algebras are isomorphic.
\item Motivated by a ``local'' proof of this assertion, we initiated a
study of the \(x \ast y := \log(\exp(x) \exp(y))\) for a pair of
matrices \(x,y\).
\item We verified empirically that the first couple homogeneous
components \(z_n\) in the series expansion of \(x \ast y\) are Lie
polynomials, i.e., linear combinations of iterated Lie commutators
involving \(x\) and \(y\).  We stated the BCH theorem, which says that
this empirical observation holds for all \(n\).
\item We stated Dynkin's formula and indicated briefly how it follows
from the BCH theorem together with an earlier homework problem on
iterated commutators.
\item We proved the BCH theorem in its qualitative form using the
homework problem on the derivative of the exponential map.
\end{enumerate}

\begin{homework}[Due Nov 8]~\label{hw:bch-consequences}
  \begin{enumerate}
  \item
    Let $s \in \mathbb{Z}_{\geq 0}$.
    A group $G$ is said to be \emph{$s$-step nilpotent}
    if all
    for all $x_1,\dotsc,x_{s+1} \in G$,
    the iterated commutator
    $(x_1,(x_2,\dotsc,(x_{s},x_{s+1})))$
    is the identity element.
    Here $(x,y) := x y x^{-1} y^{-1}$.
    For example, $G$ is $1$-step nilpotent
    if and only if it is abelian.

    Similarly, a Lie algebra $\mathfrak{g}$ is said
    to be \emph{$s$-step nilpotent}
    if
    $[x_1,[x_2,\dotsc,[x_{s},x_{s+1}]]] = 0$
    for all
    $x_1,\dotsc,x_{s+1} \in \mathfrak{g}$.
    We call $\mathfrak{g}$ abelian if it is
    $1$-step nilpotent, or equivalently, if the commutator
    bracket vanishes identity.
    \begin{enumerate}
    \item
      Verify that the Lie group
      $G \leq \SL_{s+1}(\mathbb{R})$ consisting of strictly
      upper-triangular
      matrices
      is $s$-step nilpotent.
    \item
      Let $G$ be a connected Lie group
      with adjoint representation
      $\Ad : G \rightarrow \GL(\mathfrak{g})$.
      Show that $\ker(\Ad)$ is the center of $G$.
    \item Let $G$ be a connected Lie group with Lie algebra
      $\mathfrak{g}$.  Show for $s \leq 2$ that $G$ is $s$-step
      nilpotent if and only if $\mathfrak{g}$ is $s$-step
      nilpotent.  (The same conclusion holds for all $s$, and
      can be proved similarly; the assumption $s \leq 2$ is just
      to simplify the homework problem.)
    \item
      If $G$ is $2$-step nilpotent,
      show that the BCH formula
      takes the simple form
      \begin{equation}\label{eq:bchd-2-step}
        x \ast y = x + y + \frac{1}{2} [x,y]
      \end{equation} 
      for small enough $x,y \in \mathfrak{g}$.
      Show that the quantities
      \begin{equation}\label{eq:lie-algebra-elements-that-are-similar}
        x \ast (y-x),
        \quad
        y + \frac{1}{2} [x,y],
        \quad
        \frac{x}{2} \ast y
        \ast (\frac{-x}{2})
      \end{equation}
      coincide.
    \item Let $G \leq \SL_3(\mathbb{R})$ be the three-dimensional Lie group
      consisting of strictly upper-triangular matrices;
      we have seen already that it is $2$-step nilpotent.
      Establish the formula
      \begin{equation*}
        \exp (
\begin{pmatrix}
          & x & 0 \\
          &  & 0 \\
          &  & 
        \end{pmatrix}
)
        \exp (
\begin{pmatrix}
          & 0 & 0 \\
          &  & y \\
          &  & 
        \end{pmatrix}
)
        = 
        \exp (
\begin{pmatrix}
          & x & xy/2 \\
          &  & y \\
          &  & 
        \end{pmatrix}
)
      \end{equation*}
      in two ways:
      \begin{enumerate}
      \item By direct calculation with power series.
      \item By application of the BCHD formula
        to
        \begin{equation*}
          X := 
\begin{pmatrix}
            & x &  \\
            &  &  \\
            &  & 
          \end{pmatrix}
,
          \quad
          Y := 
\begin{pmatrix}
            &  &  \\
            &  & y \\
            &  & 
          \end{pmatrix}
.
        \end{equation*}
      \end{enumerate}
    \end{enumerate}
  \item
    Let $G$ be a Lie group.
    Equip its Lie algebra $\mathfrak{g}$ with some
    norm $|.|$.
    Use the BCHD formula (or
    part of its proof)
    to show that for small enough
    $x,y \in \mathfrak{g}$,
    \begin{equation}\label{eq:bchd-cons-0}
      x \ast y \ast (-x)
      =
      \exp(\ad_x) y
      =
      y + [x,y]
      + \frac{[x,[x,y]]}{2}
      + \dotsb,
    \end{equation}
    \begin{equation}\label{eq:bchd-cons-1}
      x \ast y
      =
      x +
      F_1(\ad_x) y + O(|y|^2),
      \quad
      F_1(z) :=
      \frac{z}{1 - \exp(-z)}
      =
      1 + \frac{z}{2} + \dotsb,
    \end{equation}
    \begin{equation}\label{eq:bchd-cons-2}
      x \ast (y-x)
      =
      F_2(\ad_x) y + O(|y|^2),
      \quad
      F_2(z) :=
      \frac{\exp(z) - 1}{z}
      =
      1 + \frac{z}{2} + \dotsb.
    \end{equation}
    Deduce that
    the quantities \eqref{eq:lie-algebra-elements-that-are-similar}
    coincide up to a possible error
    of size $O(|x|^2 |y| + |y|^2)$.
    [Hint:
    For \eqref{eq:bchd-cons-1} and \eqref{eq:bchd-cons-2},
    one can specialize the BCHD formula directly.  Alternatively,
    let $t \in \mathbb{R}$
    be small
    and
    let $f(t)$ denote one of the expressions $x \ast t y$
    or $x \ast (t y - x)$.
    Then the BCH formula in its qualitative
    form, together with Taylor's theorem,
    reduces the problem
    to computing $f(0)$ and $f'(0)$.
    For this,
    Homework \ref{hw:diff-exp} and \eqref{eq:diff-exp}
    may be helpful.]
  \end{enumerate}
\end{homework}

\subsection{11/3: Closed subgroups are Lie; virtual subgroups vs. Lie subalgebras}
\label{sec:org3214742}
\textbf{Objectives.} 
You should know the following trivia, and some of their basic consequences:
\begin{enumerate}
\item closed subgroups of a Lie group are the same as Lie subgroups.
\item connected immersed Lie subgroups of a given Lie group correspond to the Lie
subalgebras of its Lie algebra.
\end{enumerate}

\textbf{Summary.}
\begin{enumerate}
\item I explained how the BCH formula implies directly that isomorphisms
of Lie algebras lift to local isomorphisms of Lie groups, and how
Lie theory is the same whether one starts with "smooth manifolds"
or "analytic manifolds".
\item I stated the theorem that closed subgroups of Lie groups are Lie
subgroups, and indicated briefly how it implies that continuous
homomorphisms between Lie groups are automatically smooth (hence,
by BCH, analytic with respect to exponential coordinates).  I then
proved that theorem.
\item I explained the correspondence between Lie subalgebras and immersed
Lie subgroups and briefly mentioned some ideas of the proof.
\end{enumerate}

\begin{homework}[Due Nov 8]~\label{hw:including-symplectic-group-etc}
  \begin{enumerate}
  \item
    Following the sketch indicated
    in lecture,
    write down a careful proof
    that a continuous homomorphism
    of Lie groups $G \rightarrow H$ is automatically smooth.
  \item Let $n \geq 1$.
    Denote by $1_n$ the $n \times n$ identity matrix.
    Set $J := 
\begin{pmatrix}
      & 1_n \\
      -1_n & 
    \end{pmatrix}
$; it is a $2 n \times 2 n$ matrix.
    Set
    \begin{equation*}
      \Sp_{2n}(\mathbb{C})
      :=
      \{g \in \SL_{2n}(\mathbb{C}):
      g^t J g = J
      \}
    \end{equation*}
    \begin{equation*}
      \Sp_{2n}(\mathbb{R})
      :=
      \Sp_{2n}(\mathbb{C}) \cap \SL_{2n}(\mathbb{R})
      \}
    \end{equation*}
    and
    \begin{equation*}
      \Sp(2 n)
      := U(2 n) \cap \Sp_{2n}(\mathbb{C}).
    \end{equation*}
    (In practice,
    one alternates between writing
    $\Sp(2n)$ and $\Sp(n)$ to mean the same thing.
    Beware conventions.)
    Check that this definition is the same as what we gave
    earlier using quaternions.
    Show that $\Sp(2 n)$ is a compact real form
    of $\Sp_{2n}(\mathbb{C})$
    and that
    \begin{equation*}
      \sp_{2 n}(\mathbb{C}) := \Lie(\Sp_{2 n}(\mathbb{C}))
      = \left\{
        \begin{pmatrix}
          a & b \\
          c & d
        \end{pmatrix}
 : a,b,c,d \in M_n(\mathbb{C}),
        d = - a^t,
        b^t = b, c^t = c\right\}.
    \end{equation*}
    Show for $\mathbf{k} = \mathbb{R}, \mathbb{C}$
    and $a \in \GL_n(\mathbf{k})$
    and $b \in M_n(\mathbf{k})$ with $b^t = b$
    that
    $\Sp_{2n}(\mathbf{k})$
    contains the matrices
    \begin{equation*}
      \begin{pmatrix}
        0_n & 1_n \\
        -1_n & 0_n
      \end{pmatrix}
,
      \quad
      \begin{pmatrix}
        a &  \\
         & {}^t a^{-1}
       \end{pmatrix}
,
       \quad
       \begin{pmatrix}
         1_n & b \\
          & 1_n
       \end{pmatrix}
.
    \end{equation*}
    Show that $\Sp_{2n}(\mathbb{C})$ and $\Sp(2 n)$
    are connected and simply-connected.
    Show that $\Sp_{2n}(\mathbb{R})$
    is connected,
    that $\Sp_{2n}(\mathbb{R}) \cap \U(2n)$
    is isomorphic to $\U(n)$,
    and that $\pi_1(\Sp_{2n}(\mathbb{R})) \cong \mathbb{Z}$.
    [Hint: one way is as follows.  Study $\Sp_{2n}(\mathbb{C})$
    inductively on $n$ 
    by considering the natural action
    on $\mathbb{C}^{2 n} - \{0\}$,
    analyzing stabilizers, and using the homotopy exact sequence.
    Study
    $\Sp(2 n)$ 
    using the Cartan decomposition.
    Study 
    $\Sp_{2n}(\mathbb{R})$ using either the Cartan decomposition or the homotopy exact sequence.]
  \end{enumerate}
\end{homework}

\newpage
\subsection{11/8: Simplicity of Lie groups and Lie algebras}
\label{sec:org6225804}
\textbf{Objectives.} You should be able to explain what it means for a Lie
group to be ``simple as a Lie group'' and how this differs from being
simple as an abstract group.

\textbf{Summary.}
\begin{enumerate}
\item I defined what it means for Lie algebras and Lie groups to be
simple and proved the equivalence of the following assertions
concerning a connected Lie group:
\begin{enumerate}
\item It is simple (no nontrivial proper normal connected virtual Lie subgroups).
\item Its Lie algebra is simple (no nonzero proper ideals).
\item Its proper normal subgroups are discrete.
\end{enumerate}
\item I recalled the classical groups (complex forms, compact real forms,
Lie algebras) and stated as a motivating goal the theorem
describing when they are simple and what are the exceptional
isomorphisms between them.
\end{enumerate}

\begin{homework}
[Due Nov 15]~
  \begin{enumerate}
  \item
    Check carefully
    that the following are equivalent
    for a connected Lie group $G$ with Lie algebra
    $\mathfrak{g}$ and connected virtual Lie subgroup $H$
    with Lie algebra $\mathfrak{h}$:
    \begin{enumerate}
    \item $H$ is a normal subgroup of $G$.
    \item $\Ad(G) \mathfrak{h} \subseteq \mathfrak{h}$
    \item $\mathfrak{h}$ is an ideal of $\mathfrak{g}$.
    \end{enumerate}
    [Hint:
    Exercise
    \ref{exercise:conjugation-and-Ad-are-intertwined-by-exponential}
    may be useful.]
  \item
    \begin{enumerate}
    \item Prove by hand that $\slLie_2(\mathbb{C})$ is simple.
    \item Complete the following sentence: ``a Lie algebra
      $\mathfrak{g}$ is simple if and only if its adjoint
      representation
      $\ad : \mathfrak{g} \rightarrow \GL(\mathfrak{g})$ is
      (...).''  Explain then how we have already secretly proven
      that $\slLie_2(\mathbb{C})$ is simple.
    \end{enumerate}
  \end{enumerate}
\end{homework}

\newpage
\subsection{11/10: Simplicity of the special linear Lie algebra}
\label{sec:orgf1075d9}
\textbf{Objectives.} You should be to analyze ideals in classical Lie
algebras by decomposing them into root spaces.

\textbf{Summary.}
\begin{enumerate}
\item We recalled briefly some facts we established long ago concerning
\(\SL(2)\).
\item We recalled, with sketch of proof, the theorem from linear algebra
that commuting diagonalizable operators are simultaneously
diagonalizable.  We then reformulated this result in terms of
representations of abelian Lie algebras.
\item We defined the set of weights of a semisimple representation of an
abelian Lie algebra, and illustrated the definition in the basic
case of the standard representation of the diagonal subalgebra of
the matrix algebra.
\item We defined the set of roots for the diagonal subalgebra of
\(\slLie_n(\mathbb{C})\) and described the root spaces and their
commutation relations explicitly.
\item We proved that \(\slLie_n(\mathbb{C})\) is simple by splitting any
nonzero ideal as a sum of root spaces and
applying suitable commutators.
\end{enumerate}

\begin{homework}
[Due Nov 17]~
  Set $\mathfrak{g} := \sp_{2n}(\mathbb{C})$.
  The main purpose of this exercise is to carry
  out the analogue for $\mathfrak{g}$
  of what was done in lecture for $\slLie_n(\mathbb{C})$.
  We include some additional computations of independent interest;
  they are straightforward but (I think) instructive.

  \begin{enumerate}
  \item 
    Recall the description of  $\mathfrak{g}$ from Homework \ref{hw:including-symplectic-group-etc}.
    Verify that $\dim(\mathfrak{g}) = 2 n^2 + n$.
    For $1 \leq j, k \leq 2 n$,
    let $E_{j k} \in M_{2 n}(\mathbb{C})$ denote the elementary
    matrix
    with $1$ in the $(i,j)$th entry and $0$ elsewhere,
    thus $E_{j k} e_k = e_j$ and $e_{j k} e_l = 0$ for $l \neq k$,
    where $e_1,\dotsc,e_{2 n}$ denotes the standard basis
    of $\mathbb{C}^{2 n}$.
    Verify that a basis for $\mathfrak{g}$ is given
    by elements of the following form,
    where $1 \leq j, k \leq n$:
    \begin{itemize}
    \item $E_{j j} - E_{n+j,n+j}$
    \item $E_{j, k} - E_{n+k, n+j}$ for $j \neq k$
    %\item $E_{j,n+j}$
    \item $E_{j,n+k} + E_{k,n+j}$ for $j \leq k$
    %\item $E_{n+j,j}$
    \item $E_{n+j,k} + E_{n+k,j}$ for $j \leq k$
    \end{itemize}
    Write all these elements out explicitly
    when $n = 2$.

  \item Let $\mathfrak{h} \leq \mathfrak{g}$
    denote the subalgebra of diagonal matrices.
    Then $\dim(\mathfrak{h}) = n$.
    Explicitly,
    $\mathfrak{h}$ has the basis $E_{j j} - E_{n+j,n+j}$ ($j=1,\dotsc,n$) and consists of of matrices of the form
    \begin{equation}\label{eq:cartan-in-sp2n}
      H = 
\begin{pmatrix}
        \lambda_1(H) & 0 & 0 & 0 & 0 & 0 \\
        0 & \dotsb & 0 & 0 & 0 & 0 \\
        0 & 0 & \lambda_n(H) & 0 & 0 & 0 \\
        0 & 0 & 0 & -\lambda_1(H) & 0 & 0\\
        0 & 0 & 0 & 0 & \dotsb & 0 \\
        0 & 0 & 0 & 0 & 0 & -\lambda_n(H)
      \end{pmatrix}
.
    \end{equation}
    A basis for the dual space $\mathfrak{h}^*$
    consists of the elements $\lambda_1,\dotsc,\lambda_n$
    defined by \eqref{eq:cartan-in-sp2n}, or equivalently, by $\lambda_k(E_{j j} - E_{n+j,n+j}) := \delta_{j k}$.

    Let $R$ denote the set of \emph{roots} for
    the pair $(\mathfrak{g},\mathfrak{h})$,
    defined
    exactly as in the case of $\slLie_n(\mathbb{C})$
    to consist of all nonzero elements $\alpha \in \mathfrak{h}^*$
    for which the eigenspace
    \begin{equation*}
      \mathfrak{g}^\alpha := \left\{ X \in \mathfrak{g} : [H,X]
        = \alpha(H) X \text{ for all }  H \in \mathfrak{h}\right\}
    \end{equation*}
    is nonzero.
    The same argument as in lecture
    shows that
    \begin{equation*}
      \mathfrak{g} = \mathfrak{h} \oplus (\oplus_{\alpha \in R} \mathfrak{g}^\alpha).
    \end{equation*}

    Show that
    \begin{equation*}
      R = \left\{ \pm (\lambda_j \pm \lambda_k) : j < k \right\} \cup \{ \pm 2 \lambda_k \},
    \end{equation*}
    where the signs $\pm$ vary independently.
    Verify that $R$ spans $\mathfrak{h}^*$.
  \item 
    For each $\alpha \in R$:
    \begin{enumerate}
    \item Verify that
      $\{n \in \mathbb{Z} : n \alpha \in R\} = \{\pm 1\}$.
    \item Verify that $\dim(\mathfrak{g}^\alpha) = 1$.
    \item Find an explicit basis element $X_\alpha \in
      \mathfrak{g}^\alpha$.
    \item Show that there exists $Y_\alpha \in
      \mathfrak{g}^{-\alpha}$
      so that the element $H_\alpha := [X_\alpha,Y_\alpha]$
      of $\mathfrak{h}$ satisfies $\alpha(H_\alpha) = 2$; write
      down $H_\alpha$ explicitly.
    %\item Verify (at least in a few representative cases) that for $\beta \in R$, one has
    %  $\beta - \beta(H_\alpha) \alpha \in R$.
    \item     Verify that for all $\alpha,\beta \in R$,
    \begin{equation*}
      [\mathfrak{g}^\alpha, \mathfrak{g}^\beta]
      = 
\begin{cases}
        \mathfrak{g}^{\alpha + \beta} & \text{ if } \alpha + \beta \in R \\
        \mathbb{C} H_\alpha  & \text{ if } \alpha + \beta = 0 \\
        0 & \text{ otherwise.}
      \end{cases}
    \end{equation*}
    \item (Optional) Show that the subspace
      $\mathbb{C} H_\alpha \oplus \mathbb{C} X_\alpha
      \oplus \mathbb{C} Y_\alpha$
      of $\mathfrak{g}$ is a Lie subalgebra
      that is isomorphic to $\slLie_2(\mathbb{C})$.
    \end{enumerate}
\item (Optional)
  Set
  $S := \{\lambda_1 - \lambda_2, \lambda_2 - \lambda_3,
    \dotsc, 
    \lambda_{n-1} - \lambda_n, 2 \lambda_n \} \subseteq R$.
  Verify that $S$ is a basis
  for $\mathfrak{h}^*$
  and that every $\beta \in R$
  may be written in the form
  $\beta = \sum_{\alpha \in S}
    m_\alpha \alpha$,
  where the $m_\alpha$ are integers which are either all $\geq 0$ or all $\leq 0$.
  Let $C$
  denote the set of all $\lambda \in \mathfrak{h}^*$
  for which $\lambda(H_\alpha) \geq 0$ for all $\alpha \in S$.
  Verify that
  $C = \{l_1 \lambda_1 + \dotsb + l_n \lambda_n :
  l_1 \geq l_2 \geq \dotsb \geq l_{n-1} \geq |l_n|
  \}$.
  \item
    By adapting the argument given in lecture
    for $\slLie_n(\mathbb{C})$,
    prove that $\mathfrak{g}$ is simple.
    [This is the only part of this homework that is not
    a straightforward computation.
    The same argument as in lecture shows that no nonzero ideal is contained in $\mathfrak{h}$.
    The key point is then to show that if an ideal contains one root space, then it contains every other root space.
    For this, one can certainly
    imitate the proof given in lecture,
    but it may be simpler
    to show that Lemma \ref{lemma:clean-way-to-get-simplicity}
    applies also to $\sp_{2n}(\mathbb{C})$ with $\lambda_{\max} := 2 \lambda_1$;
    the rest of the proof then goes through unmodified.]
  \end{enumerate}
\end{homework}

\newpage
\subsection{11/22, 11/24: How to classify classical simple complex Lie algebras}
\label{sec:org808d1e4}
\textbf{Objectives.} You should be to classify classical simple complex Lie
algebras by computing their Dynkin diagrams.  You should be able to
explain why this process is well-defined.  You should develop some
intuition for the following important concepts by reference to the
classical examples: Cartan subalgebras, roots, simple roots, positive
roots, Cartan matrix, root reflections, Weyl group, Weyl chambers and
their relation with simple systems

\textbf{Summary.} (Tuesday.) We recalled the classical simple complex Lie algebras,
explained (mechanically) how to compute their Dynkin diagrams (by
reference to the handout copied in \S\ref{sec:dynkin-diagrams-classical-examples}), and started explaining why the process of
doing so is well-defined (i.e., depends only upon the isomorphism
class of the Lie algebra); namely, we briefly discussed why Cartan
subalgebras of classical simple algebras are conjugate.

(Thursday.) We started explaining why the Cartan matrix (hence the
Dynkin diagram) is independent of the choice of simple root system.
We divided the proof into three parts:
\begin{enumerate}
\item The definition of root reflections and the observation that each
root reflection stabilizes the set of roots.
\item The interpretation of root reflections as geometric reflections
with respect to the "obvious" inner product on the real span of the
roots.
\item The claim that the Weyl group, i.e., the group generated by the
root reflections, acts transitively on the sets of simple systems.
\end{enumerate}
We explained how these points imply that the Cartan matrix is
independent of the simple system.  

We did not explain why points 1,2 hold (other than by brute-force
inspection of the handout, \S\ref{sec:dynkin-diagrams-classical-examples}); we will return to them later.
We started explaining point 3 by introducing the real span of the roots,
the regular subset of that span, the Weyl chambers, the dominant Weyl
chamber for a given simple system.  For each classical example, we
described the root reflections, the Weyl groups, the regular elements,
and the dominant Weyl chambers explicitly.  For the low-dimensional
families B\textsubscript{2} = C\textsubscript{2}, A\textsubscript{1} and A\textsubscript{2}, we drew pictures of the root systems,
indicating the simple roots, the irregular hyperplanes, the Weyl
chambers, the dominant Weyl chamber, etc.  We observed that the number
of Weyl chambers is the same as the order of the Weyl group in those
examples.  We stated the theorem that simple systems and Weyl chambers
are in natural bijection with each other, equivariantly for the action
of the Weyl group, and that the Weyl group acts simply transitively on
the set of Weyl chambers.  We described both directions of the
bijection between simple systems and Weyl chambers, without yet
proving that we get well-defined bijections in this way; we explicated
the bijection in the case of A\textsubscript{2}.  We explained
how these facts imply that the Weyl group acts simply-transitively on
the set of simple systems.

\begin{homework}
[Due Nov 29]~
  \begin{enumerate}
  \item
    (Optional, but highly recommended)
    Carefully study the computation of the Cartan
    matrices $N$ on the handout, \S\ref{sec:dynkin-diagrams-classical-examples}.
    Check that all entries are as they should be;
    report to me any errors that you find.
    (I worked them out by hand; it might be a healthy exercise
    to redo them.)
    Verify by inspection that the formula
    \begin{equation*}
    \alpha(H_\beta)
    = 2 \frac{(\alpha,\beta)}{(\beta,\beta)}.
    \end{equation*}
    holds (see \S\ref{sec:cartan-via-inner-products} for
    details).

  \item
    Let $X := 
\begin{pmatrix}
      0 & 1 \\
      0 & 0
    \end{pmatrix}
 \in \mathfrak{g} := \slLie_2(\mathbb{C})$.
    Show that $\ad_X$ is not semisimple (i.e., diagonalizable).
    Show that $\mathbb{C} X$ is a maximal abelian subalgebra
    of $\mathfrak{g}$ (i.e., that if $\mathfrak{h}$ is
    an abelian subalgebra of $\mathfrak{g}$ that contains
    $\mathbb{C} X$,
    then $\mathfrak{h} = \mathbb{C} X$.

  \item Which of the classical simple complex Lie algebras have
    the property that there exists an element $w$ of the Weyl
    group for which $w(\alpha) = -\alpha$ for all roots
    $\alpha$?

  \item Let $\mathfrak{g}$ be a classical simple complex Lie
    algebra with Weyl group $W$.  Equip $\mathfrak{g}$ with the scalar product $(,)$
    as in lecture or as in
    \S\ref{sec:cartan-via-inner-products}.
    \begin{enumerate}
    \item
      Suppose
      $\mathfrak{g} = A_n$ or $D_n$.
      Verify (by inspecting the handout (\S\ref{sec:dynkin-diagrams-classical-examples}), say) that $(\alpha,\alpha) = 2$
      for all roots $\alpha \in R$.
      Verify that $W$ acts transitively on $R$.
    \item Suppose $\mathfrak{g} = B_n$ or $C_n$.  Observe that
      $(\alpha,\alpha)$ takes two distinct values as $\alpha$
      traverses $R$; call $\alpha$ \emph{long} or \emph{short}
      according as it takes the larger or the smaller of these
      values.
      \begin{enumerate}
      \item  Verify (by inspecting the handout (\S\ref{sec:dynkin-diagrams-classical-examples})) that $W$ acts transitively
        on the set of long roots and also on the set of short
        roots.
      \item Verify that for each root $\alpha$
        there is a root $\beta$ so that $\gamma := \alpha + \beta$ is a
        root
        and $(\alpha,\alpha) \neq (\gamma,\gamma)$.
      \end{enumerate}
    \end{enumerate}
    % Use the above observations to give new proofs of the simplicity
    % of $\mathfrak{g}$.
    Remark: we may explain next week how these observations
    yield new proofs of the simplicity of $\mathfrak{g}$,
    ``simpler'' than the proofs we gave earlier; the key point
    will be to explain why the Weyl group $W$ permutes the root
    spaces belonging to any ideal.

  \item
    Let $\mathfrak{g}$ be a classical simple complex Lie
    algebra, and let notation be otherwise as usual; in particular, $S \subset R$ is the ``standard'' simple system,
    $R^+$ the associated set of positive roots, and $(,)$ the standard inner product.
    \begin{enumerate}
    \item Verify that if $\alpha,\beta \in S$ are distinct
      simple roots,
      then $(\alpha,\beta) \leq 0$, or equivalently,
      $\alpha(H_\beta), \beta(H_\alpha) \leq 0$. [The intention here is to observe that this holds 
      in every example; we will explain ``why'' next week.]
    \item Verify that if $\alpha \in S$ is a simple root,
      then the root reflection
      $s_\alpha$ stabilizes the set $R^+ - \{\alpha \}$
      consisting of all positive roots other than $\alpha$.
      [This can be verified by inspection; alternatively, use the previous part of this exercise
      and appeal to the definition of ``simple system'' and the observation that $\{n \in \mathbb{Z} : n \alpha \in R\} = \{\pm 1\}$.]
    \item Let $\rho := (1/2) \sum_{\alpha \in R^+} \alpha \in
      \mathfrak{h}^*$ denote the half-sum of positive roots.
      Compute that
      \begin{equation}\label{eqn:explicit-rho-half-sum-computation-classical-families}
        \rho
        = 
\begin{cases}
          \sum_{j=1}^n
          \frac{n + 1 - 2 j}{2} \lambda_j
           & \mathfrak{g} = A_{n-1} \\
           \sum_{j=1}^n
           (n + \frac{1}{2} - j) \lambda_j
           & \mathfrak{g} = B_n \\
           \sum_{j=1}^n
           (n + 1 - j) \lambda_j
           & \mathfrak{g} = C_n \\
           \sum_{j=1}^n
           (n-j) \lambda_j
           & \mathfrak{g} = D_n.
        \end{cases}
      \end{equation}
      Show for each $\alpha \in S$
      that      
      \begin{equation}\label{eqn:explicit-rho-half-sum-computation-classical-families-consequence-1}
        s_\alpha \rho = \rho - \alpha,
      \end{equation}
      or equivalently,
      that
      \begin{equation}\label{eqn:explicit-rho-half-sum-computation-classical-families-consequence-2}
        \rho(H_\alpha) = 1.
      \end{equation}
      [The two assertions are obviously equivalent.
      The first assertion \eqref{eqn:explicit-rho-half-sum-computation-classical-families-consequence-1} can be deduced from the previous part.
      The second assertion \eqref{eqn:explicit-rho-half-sum-computation-classical-families-consequence-2} 
      may be compared with
      \eqref{eqn:explicit-rho-half-sum-computation-classical-families}.]
    \item
      Verify that if $\alpha \in R^+ - S$ is a positive
      root that is not simple,
      then there exist positive roots $\beta, \gamma \in R^+$
      such that $\alpha = \beta + \gamma$.
      [This is unrelated to the previous parts of this exercise; the intention is to observe 
      that it holds in all examples.]
    \item
      Let $\lambda \in \mathfrak{h}^*_\mathbb{R}$
      be a regular element.
      Verify that $\{w \in W : w (\lambda) = \lambda \} =
      \{1\}$.
      [This is again unrelated to the previous parts of this
      exercise;
      the intent is that you verify it
      using the explicit description of $W$.]
    \end{enumerate}

  %\item
  %  Maybe something about length of longest Weyl element?  Etc.

  \item (Optional)
    The exceptional isomorphisms between
    classical complex simple Lie algebras
    that we have not already seen
    are the following:
    \begin{enumerate}
    \item $A_3 \cong D_3$, i.e.,
      $\slLie_4(\mathbb{C}) \cong \soLie_{6}(\mathbb{C})$.
      On the group level,
      denote by
      \begin{equation*}
      V := \Lambda^2 \mathbb{C}^4
      = \oplus_{i < j} \mathbb{C} e_i \wedge e_j
      \end{equation*}
      the six-dimensional vector space given by the exterior
      square of $\mathbb{C}^4$.  Equip it with the quadratic form
      $Q : V \rightarrow \mathbb{C}$
      defined by requiring that
      $v \wedge v = Q(v) e_1 \wedge e_2 \wedge e_3 \wedge e_4$.
      Then $(V,Q)$ is a quadratic space
      (see \S\ref{sec:low-rank-exceptional-isomorphisms})
      and the natural map
      $\Lambda^2 : \SL_4(\mathbb{C}) \rightarrow \GL(V)$
      given by $\Lambda^2(g) (v_1 \wedge v_2)
      := g v_1 \wedge g v_2$
      defines a covering morphism
      $\Lambda^2 : \SL_4(\mathbb{C}) \rightarrow \SO(V) \cong \SO_6(\mathbb{C})$.
    \item $C_2 \cong B_2$, i.e.,
      $\spLie_4(\mathbb{C}) \cong \soLie_{5}(\mathbb{C})$.
      [Restrict the map $\SL_4(\mathbb{C}) \rightarrow
      \SO_6(\mathbb{C})$
      to $\Sp_4(\mathbb{C})$;
      show that its image is the stabilizer of a one-dimensional
      subspace $L$ of $V$,
      hence identifies with the orthogonal group of the orthogonal
      complement $L^\perp$. 
      One can read off $L$ from the definition of $\Sp_4(\mathbb{C})$.]
    \end{enumerate}
  \end{enumerate}
\end{homework}

\newpage

\subsection{11/29: 11/31: Why simple Lie algebras give rise to root systems}
\label{sec:org6f2526e}
\textbf{Objectives.} You should be able to use $\slLie\textsubscript{2}$-triples to explain
why simple complex Lie algebras (which contain Cartan subalgebras
satisfying the expected properties) give rise to root systems.

\textbf{Summary.} (Tuesday.) 
\begin{enumerate}
\item We finished the proof from last time that the Weyl group of a
classical simple complex Lie algebra acts transitively on the
simple systems; this was deduced by choosing an element that
maximizes the inner product of a pair of elements of the
corresponding Weyl chambers and verifying that the map from simple
systems to Weyl chambers is one-to-one.
\item We stated the definition of a Cartan subalgebra of a simple complex
Lie algebra and the theorem concerning the existence and uniqueness
of such subalgebras; we included in this theorem also
\begin{itemize}
\item the existence of a real form of the Cartan subalgebra on which
the roots are real-valued, and
\item the existence of a scalar product on the ambient Lie algebra
(i.e., a non-degenerate symmetric bilinear form) whose
restriction to the real form of the Cartan subalgebra is
positive-definite.
\end{itemize}
We observed that the conclusion of this theorem holds for the
classical families "by inspection" and gave a reference for the
general case; discussing its proof would take us too far afield
from (what I think are) more interesting topics to present in a
first course on Lie groups.
\item We stated the definition of a (reduced) root system: it is a finite
subset of a real inner product space that satisfies some axioms
that we had observed empirically last week.
\item We stated the theorem that simple complex Lie algebras (satisfying
the conclusion of the "Cartan subalgebra theorem") give rise to
root systems.  Our aim next time is to prove this theorem.
\end{enumerate}

(Thursday.)
We explained in detail how $\slLie\textsubscript{2}$-triples allow us to prove that
simple Lie algebras possessing Cartan subalgebras give rise to root
systems that satisfy the various properties we had observed for the
classical families.


\begin{homework}[Due Dec 6]\label{hw:Falphaz}
 The main purposes of the lectures
  this week were to demystify last week's observations
  concerning root systems for the classical Lie algebras and to
  demonstrate the power of $\slLie_2(\mathbb{C})$
  in proving results about more general Lie algebras.
  This week there is one multipart exercise whose purpose is to complement that
  discussion
  by showing how one might instead use $\SL_2(\mathbb{C})$
  to establish such properties.

  Thus, let $\mathfrak{g}$ be a simple Lie algebra over
  $\mathbb{C}$,
  let $\mathfrak{h} \leq \mathfrak{g}$ be a Cartan subalgebra
  with real form  $\mathfrak{h}_\mathbb{R}$
  so that the set $R$ of roots of $\ad  : \mathfrak{h}
  \rightarrow \End(\mathfrak{g})$
  satisfies $R \subseteq \mathfrak{h}_\mathbb{R}^*  =
  \Hom_\mathbb{R}(\mathfrak{h}_\mathbb{R},\mathbb{R})
  \cong \{\lambda \in \mathfrak{h}^* : \lambda(\mathfrak{h}_\mathbb{R}) \subseteq \mathbb{R} \}$.
  % let $(,) : \mathfrak{g} \otimes \mathfrak{g} \rightarrow
  % \mathbb{C}$ be
  % a $\mathfrak{g}$-invariant scalar product (i.e.,
  % non-degenerate symmetric bilinear form)
  % whose restriction to $\mathfrak{h}_\mathbb{R}$ is real-valued
  % and positive-definite,
  % denote also by $(,)$ the induced scalar product on
  % $\mathfrak{g}^*$
  % and inner product on $\mathfrak{h}_\mathbb{R}^*$ (see
  % \S\ref{sec:some-stuff-about-scalar-products}).
  Fix $\alpha \in R$.
  Assume given nonzero elements $X_\alpha \in
  \mathfrak{g}^\alpha,
  Y_\alpha \in \mathfrak{g}^{-\alpha}, H_\alpha \in
  \mathfrak{h}_\mathbb{R}$
  satisfying the relations indicated just before
  the statement of Lemma \ref{lem:root-spaces-one-dimensional},
  so that the map 
\begin{equation*}
\phi_\alpha : \slLie_2(\mathbb{C})
  \rightarrow \mathfrak{s}_\alpha  := \mathbb{C} X_\alpha
  \oplus \mathbb{C} H_\alpha \oplus \mathbb{C} Y_\alpha
  \subseteq \mathfrak{g}
\end{equation*}
  given by $X,Y,H \mapsto X_\alpha,Y_\alpha,H_\alpha$
  is an isomorphism of Lie algebras.
  Finally, fix a complex Lie group $G$
  so that $\mathfrak{g} = \Lie(G)$.

  Our aim here is to give alternative
  proofs
  (using Lie group methods rather than Lie algebra methods)
  of the first part of Lemma
  \ref{lem:alpha-string-thru-beta-description}.
  Thus, it would be best not to invoke the statement of that lemma in the arguments to follow.
  \begin{enumerate}
  \item
    Show that there is a morphism of complex Lie groups
    $F_\alpha : \SL_2(\mathbb{C}) \rightarrow G$
    so that $d F_\alpha = \phi_\alpha$.

    For an element $t$ of any complex vector space on which $\exp$ is defined
    (e.g., $t$ can be a complex scalar
    or an element of a Lie algebra over $\mathbb{C}$),
    introduce the abbreviation $e(t) := \exp(2 \pi i t)$.
  \item
    Show that $e(H) = 1$.
  \item
    Show that $e(H_\alpha) = 1$.
  \item
    Let $\beta \in R$.
    Show for $t \in \mathbb{C}$
    and $v \in \mathfrak{g}^\beta$
    that $\Ad(e(t H_\alpha)) v = e(t \beta(H_\alpha)) v$.
  \item Deduce that $e(\beta(H_\alpha)) = 1$, hence that
    $\beta(H_\alpha) \in \mathbb{Z}$.
  \item Set $w := 
\begin{pmatrix}
      0 & 1 \\
      -1 & 0
    \end{pmatrix}
 \in \SL_2(\mathbb{C})$.
    Verify that $w = e^X e^{-Y} e^X$
    and that $\Ad(w) H = - H$.
  \item Set $w_\alpha := F_\alpha(w) \in G$.
    Show that $\Ad(w_\alpha) H_\alpha = - H_\alpha$
    and that $\Ad(w_\alpha)^2 = 1$.
  \item Suppose $H \in \mathfrak{h}$ satisfies
    $\alpha(H) = 0$.
    \begin{enumerate}
    \item Show that $[x,H] = 0$ for all $x \in
      \mathfrak{s}_\alpha$.
    \item Show that $\Ad(g) H = H$ for all $g$ in the image of
      $F_\alpha$.
      Deduce in particular that $\Ad(w_\alpha) H = H$.
    \end{enumerate}
  \item
    Deduce that $\Ad(w_\alpha) \mathfrak{h} = \mathfrak{h}$.
  \item
    Recall that for $\lambda \in \mathfrak{h}^*$,
    we set $s_\alpha \lambda := \lambda - \lambda(H_\alpha)
    \alpha$.
    Show that for all $H \in \mathfrak{h}$,
    one has
    \begin{equation}
      \lambda(\Ad(w_\alpha) H) = (s_\alpha \lambda)(H).
    \end{equation}
  \item
    For any $\lambda \in \mathfrak{h}^*$,
    set
    $\mathfrak{g}^\lambda := \{v \in \mathfrak{g} : [H,v] =
    \lambda(H) v \text{ for all } H \in \mathfrak{h}\}$.
    Show that
    $\Ad(w_\alpha)$ induces a well-defined isomorphism
    \begin{equation}
      \Ad(w_\alpha) : \mathfrak{g}^{\lambda} \rightarrow \mathfrak{g}^{s_\alpha(\lambda)}.
    \end{equation}
  \item Deduce in particular
    that if $\beta \in R$, then $s_\alpha(\beta) \in R$.
  \end{enumerate}
\end{homework}

\newpage
\subsection{12/6: Complex reductive vs. compact real}
\label{sec:orge92c104}
\textbf{Objectives.} You should be able to explain the relationships between
complex simple Lie algebras, complex semisimple Lie algebras, complex
reductive Lie algebras, and compact real Lie algebras.

\textbf{Summary.}
\begin{enumerate}
\item We explained that the association constructed in previous weeks
from
\begin{enumerate}
\item simple complex Lie algebras to
\item irreducible reduced root systems to
\item connected Dynkin diagrams
\end{enumerate}
is bijective, and indicated briefly how the Serre relations explain
this (without proving them).
\item We defined semisimple and reductive Lie algebras and indicated how
the above bijection generalizes to them.
\item We defined compact real Lie algebras and explained why the Lie
algebra of any compact real Lie group is compact; we stated the
theorem that every compact real Lie algebra arises in this way.
\item We stated the theorem that compact real Lie algebras and complex
reductive Lie algebras are in natural bijection.  We gave a proof
using the unitary trick and the Serre relations.
\end{enumerate}
\subsection{12/8: Compact Lie groups: center, fundamental group}
\label{sec:org291b534}
\textbf{Objectives.} You should be able to describe the center and
fundamental group of a copmact Lie group in terms of its root/weight
lattices and the kernel of the exponential map.

\textbf{Summary.} We defined the root, weight, coroot and coweight lattices
of a semisimple Lie algebra as well as the lattices of integral
elements attached to a compact Lie group.  We explained how the
center and fundamental group are described in terms of these, and
illustrate with the examples of tori and \(\SL_n\).

\begin{homework}
[Due Dec 13]~
  Let
    $K$ be a compact Lie group
    with finite center.
    Let other notation be as in
    \S\ref{sec:center-pi1-compact}.
    Thus
    $\mathfrak{h}$ be a Cartan subalgebra of a complex
    semisimple Lie algebra $\mathfrak{g}$.  Let $R$ be the set
    of roots and $S \subseteq R$ a base.
    Let $\mathfrak{h}_\mathbb{R}^*$ denote the span of $R$
    and $\mathfrak{h}_\mathbb{R} := \{H \in \mathfrak{h} :
    \alpha(H) \in \mathbb{R}
    \text{ for all } \alpha \in R\}$,
    as usual.
  \begin{enumerate}
  \item Verify that the Weyl group
    $W$ (generated by the $s_\alpha$ for $\alpha \in R$,
    as usual)
    acts on
    the root and weight lattices.
    Verify that the transposes of elements of the Weyl group
    act on the coroot and coweight lattices.
    One can thus form the semidirect product
    $W \ltimes \mathbb{Z} R^\wedge$.
  \item Let
    $\mathfrak{h}_\mathbb{R}^{\sreg} = \{H \in
    \mathfrak{h}_\mathbb{R} : \alpha(H) \notin \mathbb{Z} \text{
      for all } \alpha\in R \}$.
    \begin{enumerate}
    \item Verify that $W$ acts on
      $\mathfrak{h}_{\mathbb{R}}^{\sreg}$.
    \item Verify that $\mathbb{Z} R^\wedge$ acts on
      $\mathfrak{h}_\mathbb{R}^{\sreg}$ (by translation).
    \item Verify that the actions of $W$ and $\mathbb{Z} R^\wedge$
      on $\mathfrak{h}_\mathbb{R}$
      induce an action of their semidirect product $W \ltimes
      \mathbb{Z} R^\wedge$ on $\mathfrak{h}_{\mathbb{R}}$,
      preserving  $\mathfrak{h}_{\mathbb{R}}^{\sreg}$.
      Let $T \leq \GL(\mathfrak{h}_{\mathbb{R}})$ denote the image of $W \ltimes \mathbb{Z}
      R^\wedge$.
    \item Let $n \in \mathbb{Z}$, $\alpha \in R$.
      Let $s_{\alpha,n} : \mathfrak{h}_{\mathbb{R}} \rightarrow
      \mathfrak{h}_{\mathbb{R}}$
      be the reflection in the hyperplane $\alpha(H) = n$,
      thus $s_{\alpha,n}(H) = H - (\alpha(H) - n) H_\alpha$.
      Show that
      $s_{\alpha,n}$ belongs to $T$.
    \item Show that $T$ is generated
      by the $s_{\alpha,n}$.
    \end{enumerate}

  \item 
    Choose an enumeration
    $S = \{\alpha_1,\dotsc,\alpha_l\}$.
    \begin{enumerate}
    \item Show that the $\mathbb{Z}$-span $\mathbb{Z} R$
      is has basis $S$ in the sense that $\mathbb{Z} R = \mathbb{Z} \alpha_1 \oplus
      \dotsb \oplus \mathbb{Z} \alpha_l$.
    \item Show that there exist unique elements
      $\pi_1,\dotsc,\pi_l \in \mathfrak{h}_\mathbb{R}$
      so that $H_{\alpha_i}(\pi_j) = \delta_{i j}$ for all $i,j
      \in \{1..l\}$.
      (These are called the \emph{fundamental weights}.)
    \item Show that $(\mathbb{Z} R^\wedge)^* = \mathbb{Z} \pi_1
      \oplus 
      \dotsb \oplus \mathbb{Z} \pi_l$.
    \item Show that matrix $(a_{i j})$ for which
      $\alpha_i = \sum_j a_{i j} \pi_j$
      is given by the Cartan matrix.
    \item
      (Optional) Compute $\pi_1, \dotsc, \pi_l$
      for the classical families.      (This can be done by hand, or by inverting the Cartan
      matrix.)


    \end{enumerate}
  \end{enumerate}
\end{homework}
\newpage

\subsection{12/13 Maximal tori in compact Lie groups}
\label{sec:org070bbf1}
\textbf{Objectives.} You should be able to explain the role played by maximal
tori in the study of compact connected Lie groups.

\textbf{Summary.} 
(Tuesday)
See \S\ref{sec:tori-compact-lie-gps} for details.
Throughout, let \(K\) be a compact connected Lie group.
\begin{enumerate}
\item We defined tori, and characterized them as compact connected
abelian Lie groups.
\item We defined maximal tori in \(K\) and characterized them as closed
connected subgroups whose Lie algebras are maximal abelian subalgebras.
\item We recorded that closed connected abelian subgroups of \(K\) are
tori.
\item As an example of the latter, we gave the connected components of
closures of subgroups generated by individual elements.
\item We indicated why the Lie algebras of maximal tori give rise to
Cartan subalgebras after taking complexifications.
\item We stated (without proof yet) the big theorem that \(K\) is the union
of the conjugates of any one of its maximal tori.
\item We derived from this the consequence that the center of \(K\) is the
intersection of all (maximal) tori and indicated briefly how this
implies the description given last time of the center in terms of
roots.
\end{enumerate}

(Thursday)
See \S\ref{sec:tori-compact-lie-gps} for details.
\begin{enumerate}
\item \(\Aut(T)\)
\item \(N(T)_0\)
\item \(\Lambda(f)\)
\item Proof.
\item \(T_1, T_2\)
\item \(Z(T) = T\)
\end{enumerate}



\newpage

\section{Selected homework solutions}
\label{sec:org814ace2}
\begin{itemize}
\item Homework \ref{hw:2}, 2d.
  Here is a quick and fairly intuitive way to see that $G := \SO(1,2)$
  has two connected components.
  (We've seen later in the course
  that this is established
  more efficiently using the Cartan decomposition.)
  We realize $G$ as a subgroup of $\SL_3(\mathbb{R})$
  in the evident way.
  It contains the subgroups
  \begin{equation*}
  H_1 :=
  \left\{
    \begin{pmatrix}
      a & b & 0 \\
      c & d & 0 \\
      0 & 0 & 1
    \end{pmatrix}
 :
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
 \in \O(1,1)
  \right\}
  \end{equation*}
  and
  \begin{equation*}
  H_2 :=
  \left\{
    \begin{pmatrix}
      1 & 0 & 0 \\
      0 & a & b \\
      0 & c & d
    \end{pmatrix}
    :
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
 \in \SO(2)
  \right\}.
  \end{equation*}
  Note that $H_2$ is connected,
  so $H_2 \subseteq G^0$.

  Let $V_1$ be as in the homework problem.
  Let $V_1^+$ denote the connected component
  containing $e_1$,
  and $V_1^-$ the other component.
  (Thus $V_1^+ = V_1^0$ in the notation of the homework
  problem.)
  We now make the following observations:
  \begin{enumerate}
  \item
    Since $V_1^{\pm}$ is connected
    and $G$ acts on $V_1 = \cup_{\pm} V_1^{\pm}$,
    for each $g \in G$ we have either
    $g V_1^+ \subseteq V_1^+$ or $g V_1^+ \subseteq V_1^-$.
    It follows that $G$ permutes
    the two connected components $V_1^{\pm}$ of $V_1$,
    and so the subgroup $\{g \in G : g V_1^+ \subseteq V_1^+\}$
    of $G$ 
    has index at most $2$.
  \item 
    There exist elements
    $g \in G$ which map
    $V_1^+$ to $V_1^-$, and vice-versa.
    For instance,
    one can take $g := \diag(-1,-1,1)$.
    The subgroup $\{g \in G : g V_1^+ \subseteq V_1^+\}$ of $G$
    thus has index exactly $2$.
  \item
    Since
    $V_1^+$ is connected,
    we have
    $G^0 \subseteq \{g \in G : g V_1^+ \subseteq V_1^+\}$.
  \item
    Let $v \in V_1^+$.
    It is of the form $v = (x,y,z)$
    with $x^2 - y^2 - z^2 = 1$.
    Choose an element $h_2 \in H_2$
    so that $h_2 v = (x,r,0)$,
    where $r = \sqrt{y^2 + z^2}$.
    By part 1b of the same homework,
    we can then find $h_1 \in H_1^0$
    so that $h_2 v = h_1 e_1$.
    Consequently
    $v = g e_1$ where $g := h_2^{-1} h_1 \in G^0$.
  \item 
    Now let $g \in G$ with $g V_1^+ \subseteq V_1^+$.
    Then $g e_1 \in V_1^+$.
    By what was shown in the previous item, we can find $g_0 \in G^0$
    so that $g e_1 = g_0 e_1$,
    thus $g \in g_0 H_2 \subseteq G^0$.
    Thus,
    $G^0 \supseteq \{g \in G : g V_1^+ \subseteq V_1^+\}$.
  \item 
    We have seen that $G^0 = \{g \in G : g V_1^+ \subseteq V_1^+\}$
    and that $\{g \in G : g V_1^+ \subseteq V_1^+\}$ has index $2$ in
    $G$.
    Therefore $G$ has two connected components.
  \end{enumerate}
  One can tidy this discussion up a bit with some lemmas
  from lecture.
\item Homework \ref{hw:3-lie-first}, part 1.
  Let $G := \SO(2,1)$.
  It acts on $M := V_1 := \{(x,y,z) : z^2 - x^2 - y^2 = 1\}$
  as well as its connected components
  $V_1^{\pm}$.
  Let us realize $M$ as a space of row vectors,
  so that $G$ acts on $M$ by right matrix multiplication:
  $m \mapsto m g$.
  The group $G$ then acts on smooth functions
  $f : M \rightarrow \mathbb{R}$
  by the formula:
  for $g \in G$, $m \in M$,
  \begin{equation*}
  g f(m) :=
  f(m g).
  \end{equation*}
  We saw in an earlier homework problem
  that $G$ acts transitively on the connected components of $M$,
  hence $f$ is constant on each such component if and only if
  \begin{equation}\label{eq:f-invariant-by-group-for-so-1-2-hw-problem}
    g f = f \text{ for all } g \in G.
  \end{equation}

  Set $\mathfrak{g} := \Lie(G)$.
  We may differentiate the action of $G$ on $C^\infty(M)$
  to the action of $\mathfrak{g}$ on $C^\infty(M)$
  given for $X \in \mathfrak{g}$
  by
  \begin{equation*}
  X f(m) :=
  \partial_{t=0} f(m \exp(t X)).
  \end{equation*}
  Explicitly, the Lie algebra $\mathfrak{g}$
  consists of matrices
  $X \in M_3(\mathbb{R})$
  satisfying ${}^t X J + J X = 0$,
  where $J := \diag(1,1,-1)$;
  an explicit basis for $\mathfrak{g}$ is given by the matrices
  \begin{equation*}
  X_1
  :=
  \begin{pmatrix}
    0 & 1 & 0 \\
    -1 & 0 & 0 \\
    0 & 0 & 0
  \end{pmatrix}
,
  \quad
  X_2
  :=
  \begin{pmatrix}
    0 & 0 & 1 \\
    0 & 0 & 0 \\
    1 & 0 & 0
  \end{pmatrix}
,
  X_3
  :=
  \begin{pmatrix}
    0 & 0 & 0 \\
    0 & 0 & 1 \\
    0 & 1 & 0
  \end{pmatrix}
.
  \end{equation*}
  Using that
  \begin{equation*}
  (x,y,z) X_1
  =
  x e_2
  - y e_1,
  \end{equation*}
  \begin{equation*}
  (x,y,z) X_2
  =
  z e_1
  + x e_3,
  \end{equation*}
  \begin{equation*}
  (x,y,z) X_3
  =
  z e_2
  + y e_3,
  \end{equation*}
  we obtain
  \begin{equation*}
  X_1 f(x,y,z)
  =
  (x \partial_y - y \partial_x) f(x,y,z),
  \end{equation*}
  \begin{equation*}
  X_2 f(x,y,z)
  =
  (z \partial_x + x \partial_z) f(x,y,z),
  \end{equation*}
  \begin{equation*}
  X_3 f(x,y,z)
  =
  (z \partial_y + y \partial_z) f(x,y,z).
  \end{equation*}
  Therefore assertion (b) in the homework problem
  is equivalent to saying that
  $X_i f = 0$ for $i=1,2,3$,
  or equivalently,
  that
  \begin{equation}\label{eq:f-invariant-by-algebra-for-so-1-2-hw-problem}
    X f = 0 \text{ for all }X \in \mathfrak{g}.
  \end{equation}

  It remains only to verify
  that
  \eqref{eq:f-invariant-by-group-for-so-1-2-hw-problem}
  and
  \eqref{eq:f-invariant-by-algebra-for-so-1-2-hw-problem}
  are equivalent.
  This follows from the connectedness of $G$
  by the same argument (the ``exponentiation/differentiation trick'')
  as in \S\ref{sec:appl-inv-by-connected}.
\item Homework \ref{hw:all-about-Ad},
  part 1.
  We want to show for $f : G \rightarrow H$
  that $d f(\Ad(g) X) = \Ad(f(g)) d f(X)$.
  The curve $t \mapsto \exp(t \Ad(g) X) = \exp(\Ad(g) t X)$
  in $G$
  has initial velocity  $\Ad(g) X$,
  hence
  \begin{equation}\label{eqn:proving-ad-df-compatibility-1}
    d f(\Ad(g) X)
    = \partial_{t=0} f(\exp(\Ad(g) t X)).
  \end{equation}
  In particular,
  \begin{equation}\label{eqn:proving-ad-df-compatibility-2}
    d f(X)
    = \partial_{t=0} f(\exp(t X)).
  \end{equation}
  By Exercise
  \ref{exercise:conjugation-and-Ad-are-intertwined-by-exponential},
  we have
  $\exp(\Ad(g) t X))
  = g \exp(t X) g^{-1}$;
  since $f$ is a group homomorphism,
  it follows that
  \begin{equation}\label{eqn:proving-ad-df-compatibility-3}
    d f(\Ad(g) X)
    =
    \partial_{t=0}
    f(g)g
    f(\exp(t X)) f(g)^{-1}
    =
    \Ad(f(g))
    \partial_{t=0}
    f(\exp(t X))
  \end{equation}
  Combining \eqref{eqn:proving-ad-df-compatibility-2}
  with \eqref{eqn:proving-ad-df-compatibility-3}
  gives the required identity.
\item
  Homework \ref{hw:all-about-Ad},
  part 2.
  We want to determine the complexifications
  of
  $\SL_n(\mathbb{H}),
  \SU(p,q)$,
  and $\U_m(\mathbb{H})$.
  (!!! to be written)

\item
  Homework \ref{hw:characters-sl2},
  part 3.
  The answer is:
  those functions $\nu : \mathbb{Z} \rightarrow \mathbb{Z}_{\geq
    0}$
  for which
  \begin{enumerate}
  \item $\nu(n) = 0$ for all but finitely many $n$,
  \item $\nu(n) = \nu(-n)$ for all $n \in \mathbb{Z}_{\geq 0}$,
  \item 
    $\nu(0) \geq \nu(2) \geq \nu(4) \geq \nu(6) \geq \dotsb$, and
  \item 
    $\nu(1) \geq \nu(3) \geq \nu(5) \geq \nu(7) \geq \dotsb$.
  \end{enumerate}
  Given such a seuqence,
  we can define
  $\mu : \mathbb{Z}_{\geq 0} \rightarrow \mathbb{Z}_{\geq 0}$
  by $\mu(m) := \nu(m) - \nu(m + 2)$
  and set
  \begin{equation*}
  V := \oplus_{\substack{
      m \in \mathbb{Z}_{\geq 0}
    }
  }
  W_m^{\oplus \mu(m)}.
  \end{equation*}
  Conversely, the claimed inequalities
  are clearly satisfied
  for $V$ of this form
  (by the hint suggested in the homework problem,
  or by directly writing out the characters).
\item Homework \ref{hw:universal-covering-group}, part 1:
  It is easy to see (by considering
  elementary matrices)
  that the center of $\SL_n(\mathbb{C})$
  is the subgroup $\mu_n$ of scalar matrices $\diag(z,z,\dotsc,z)$
  whose entries $z$ are $n$th roots of unity.
  We have $\mathbb{Z}/n \cong \mu^n$
  via the map $x \mapsto e^{2 \pi i x}$.


  The inclusion $\SL_n(\mathbb{C}) \rightarrow
  \GL_n(\mathbb{C})$
  has differential
  given by the inclusion
  $\slLie_n(\mathbb{C}) \rightarrow \gl_n(\mathbb{C})$
  from the space of traceless matrices to the space of all
  matrices.
  The group $\PGL_n(\mathbb{C})$
  is the quotient of $\GL_n(\mathbb{C})$
  by the normal subgroup $Z$ of scalar matrices
  $\diag(z,z,\dotsc,z)$
  ($z \in \mathbb{C}^\times$).
  Thus (by some theorem from lecture)
  the surjective quotient map
  $\GL_n(\mathbb{C}) \rightarrow \PGL_n(\mathbb{C})$
  has differential given by the surjective linear map
  \begin{equation*}
  \gl_n(\mathbb{C}) \rightarrow \mathfrak{p}\mathfrak{g}\mathfrak{l}_n(\mathbb{C}),
  \end{equation*}
  where
  $\mathfrak{p}\mathfrak{g}\mathfrak{l}_n(\mathbb{C})$
  denotes the quotient of
  $\gl_n(\mathbb{C})$ by the subgroup of diagonal scalar matrices
  of the form $\diag(Z,Z,\dotsc,Z)$ ($Z \in \mathbb{C}$).
  The composite map
  \begin{equation*}
  \slLie_n(\mathbb{C}) \rightarrow \gl_n(\mathbb{C})
  \twoheadrightarrow
  \mathfrak{p}\mathfrak{g}\mathfrak{l}_n(\mathbb{C})
  \end{equation*}
  is an isomorphism (indeed,
  one may invert it by sending
  an element 
  of $\mathfrak{p}\mathfrak{g}\mathfrak{l}_n(\mathbb{C})$
  to its unique traceless representative)
  so we may identify
  $\mathfrak{p}\mathfrak{g}\mathfrak{l}_n(\mathbb{C}) = \slLie_n(\mathbb{C})$.
  The map $p : \SL_n(\mathbb{C}) \rightarrow \PGL_n(\mathbb{C})$
  is a morphism between connected Lie groups
  whose differential
  $d p$ is then the ``identity map'' on $\slLie_n(\mathbb{C})$;
  in particular, $d p$ is an isomorphism.
  Thus $p$ is a covering morphism.
  We have seen that $\SL_n(\mathbb{C})$ is simply-connected.
  By the homotopy exact sequence
  (or the uniqueness of the discrete central subgroup ``$N$''
  appearing in the
  theorem on the universal covering group),
  it follows that $\pi_1(\PGL_n(\mathbb{C})) \cong \ker(p) =
  \mu_n \cong \mathbb{Z}/n$,
  as required.

  An identical argument works for $G = \SU(n)$.

  The connected Lie groups having Lie algebra isomorphic
  to $\slLie_n(\mathbb{C})$
  are in bijection with the discrete central subgroups
  of the simply-connected Lie group $\SL_n(\mathbb{C})$
  having that Lie algebra;
  since the center of that group is $\mu_n \cong \mathbb{Z}/n \mathbb{Z}$
  and since all subgroups of the latter
  are uniquely of the form $d \mathbb{Z} / n \mathbb{Z} \cong \mathbb{Z} / (n/d) \mathbb{Z}$
  for some positive divisor $d$ of $n$,
  we obtain a bijection between the isomorphism classes of such
  Lie groups $G$ 
  and the positive divisors $d$ of $n$,
  where $\pi_1(G) \cong \mathbb{Z} / (n/d) \mathbb{Z}$.
\item Homework \ref{hw:bch-consequences},
  1a.
  Let $G \leq \SL_{n}(\mathbb{R})$
  be the group of unipotent upper-triangular matrices;
  for $n = 3$,
  one has
  \begin{equation*}
  G = 
\begin{pmatrix}
    1 & \ast & \ast \\
    & 1 & \ast \\
    &  & 1
  \end{pmatrix}
.
  \end{equation*}
  For subgroups $A,B$ of $G$,
  let $(A,B)$ denote the subgroup of $G$
  generated by all commutators $(a,b) := a b a^{-1} b^{-1}$
  with $a \in A, b \in B$.
  In the special case that $B$ is a normal
  subgroup of $A$,
  we may interpret $B/(A,B)$ as the \emph{maximal quotient
    of $B$ on which $A$ acts trivially by conjugation}:
  if $\phi : B \rightarrow K$ is a surjective group homomorphism
  with the property that $\phi(a b a^{-1}) = \phi(b)$
  for all $a \in A, b \in B$,
  then $\phi$ factors uniquely
  as a composition
  $B \twoheadrightarrow B/(A,B) \xrightarrow{\psi} K$.

  For $k \in \mathbb{Z}_{\geq 1}$,
  define $G_k$ inductively
  by
  $G_1 := G$
  and $G_{k+1} := (G,G_k)$.
  The problem is to show that $G_{n} = \{1\}$.

  We will establish the stronger assertion
  that $G_k = U_k$,
  where
  \begin{equation*}
  U_k := \{a \in G : a_{i j} = 0 \text{ if } i < j < i + k\}.
  \end{equation*}
  For example,
  if $n = 4$,
  then
  \begin{equation*}
  U_1 = 
\begin{pmatrix}
    1 & \ast & \ast & \ast \\
    & 1 & \ast & \ast  \\
    &  & 1 & \ast \\
    & & & 1
  \end{pmatrix}
,
  \quad 
  U_2 = 
\begin{pmatrix}
    1 & 0 & \ast & \ast \\
    & 1 & 0 & \ast  \\
    &  & 1 & 0 \\
    & & & 1
  \end{pmatrix}
,
  \end{equation*} 
\begin{equation*}
  U_3 = 
\begin{pmatrix}
    1 & 0 & 0 & \ast \\
    & 1 & 0 & 0  \\
    &  & 1 & 0 \\
    & & & 1
  \end{pmatrix}
,
  \quad 
  U_4 = 
\begin{pmatrix}
    1 & 0 & 0 & 0 \\
    & 1 & 0 & 0  \\
    &  & 1 & 0 \\
    & & & 1
  \end{pmatrix}
  = \{1\}.   
  \end{equation*}
  For notational convenience,
  set $U_m := \{1\}$ if $m \geq n$.

  To show that $G_k = U_k$,
  it suffices (since $G_1 = U_1$
  and $G_{k+1} = (G,G_k)$)
  to show
  that
  \begin{equation}
    (U,U_k) = U_{k+1}.
  \end{equation}

  For $i, j \in \{1..n\}$ with $i < j$,
  let $E_{i j} \in G$
  denote the ``elementary matrix'' that has a $1$ in the $(i,j)$th entry
  and vanishes
  on all other off-diagonal entries.
  For example, if $n = 3$,
  then
  \begin{equation*}
  E_{12}
  = 
\begin{pmatrix}
    1 & 1 & 0 \\
    & 1 & 0 \\
    &  & 1
  \end{pmatrix}
,
  \quad 
  E_{13}
  = 
\begin{pmatrix}
    1 & 0  & 1 \\
    & 1 & 0 \\
    &  & 1
  \end{pmatrix}
,
  \quad 
  E_{23}
  = 
\begin{pmatrix}
    1 & 0  & 0 \\
    & 1 & 1 \\
    &  & 1
  \end{pmatrix}
.
  \end{equation*}
  By matrix multiplication, one has the commutation relations
  \begin{equation*}
  [E_{i j}, E_{j k}] = E_{i k},
  \quad
  [E_{ij},E_{k l}] = 0 \text{ if } j \neq k.
  \end{equation*}

  By Gaussian elimination, $G$ is generated by the elements
  $E_{i j}$ taken over all $i < j$.  Similarly, $U_k$ is
  generated by those $E_{i j}$ for which $j \geq i + k$.  It
  follows from this observation and the commutation relations
  that
  \begin{enumerate}
  \item[(i)]
    $U_k$ is normal in $U_1$ for all $k$,
  \item[(ii)]
    the conjugation action of $U_1$ on the quotient $U_k / U_{k+1}$ is
    trivial,
    and
  \item [(iii)]
    $(U_1,U_k) \geq U_{k+1}$ for all $k$.
  \end{enumerate}
  On the other hand,
  $U_k / (U_1,U_k)$ is the \emph{maximal} quotient of $U_k$ on
  which $U_1$ acts trivially by conjugation,
  hence
  $(U_1,U_k) \leq U_{k+1}$
  and
  therefore $(U_1,U_k) = U_{k+1}$.
  This completes the proof.

  Remark:
  One can alternatively argue using the matrix
  logarithm/exponential.
  The series defining the logarithm converges
  everywhere on $G$ because $g-1$ is nilpotent for $g \in G$,
  hence the series is actually finite;
  similarly, the exponential series
  $\mathfrak{g} \rightarrow G$ is actually a polynomial.

  Remark:
  One can formulate the definition
  of $U_k$  more geometrically in terms of
  the standard complete flag
  $\mathbb{R}^n = V_0 \supset V_1 \supset \dotsb \supset V_n
  = \{0\}$,
  where
  $V_k$ denotes the span
  of the first $k$ standard basis vectors $e_1,\dotsc,e_k$.
  Then $U_k = \{g  \in \GL_n(\mathbb{R}) :
  (g - 1) V_i \subseteq V_{i + k} \text{ for all } i\}$,
  where $V_m := \{0\}$ for $m \geq n$.


\item Homework \ref{hw:bch-consequences}, 2.
  In what follows,
  $x,y$ denote small enough elements of $\mathfrak{g}$,
  while $g$ denotes an element of the group $G$.
  \begin{enumerate}
  \item
    Recall (from Exercise
    \ref{exercise:conjugation-and-Ad-are-intertwined-by-exponential})
    the general identity
    $\exp(\Ad(g) x) = g \exp(x) g^{-1}$.
    Recall also (from \S\ref{sec:Ad-ad-intertwined-exp})
    that
    $\Ad(\exp(x)) = \exp(\ad_x)$.
    From these identities it follows that
    \begin{equation}
      \exp(x \ast y \ast (-x))
      = \exp(x) \exp(y) \exp(-x)
      = \exp(\Ad(\exp(x)) y)
    \end{equation}
    and thus
    \begin{equation}
x \ast y \ast (-x) = \Ad(\exp(x)) y
      = \exp(\ad_x) y,
    \end{equation}
    as required.
  \item
    Set $f(t) := x \ast t y$.
    By BCH,
    $f$ is analytic near $0$.
    We have $f(0) = x$.
    We have
    \begin{equation}
      \exp(f(t)) =  \exp(x) \exp(t y)
    \end{equation}
    and thus
    \begin{equation}
      \exp(-f(0))
      \partial_{t=0}
      \exp(f(t))
      = y.
    \end{equation}
    On the other hand,
    by Homework \ref{hw:diff-exp},
    \begin{equation}\label{eqn:consequence-of-maurer-cartan-for-computing-exp-dfdfd}
      \exp(-f(0))
      \partial_{t=0}
      \exp(f(t))
      =
      \Psi(\ad_x)
      f'(0)
    \end{equation}
    where
    \begin{equation}
      \Psi(z)
      = \sum_{n=1}^{\infty}
      \frac{(-z)^{n-1}}{n!}
      = \frac{1 - \exp(-z)}{z}.
    \end{equation}
    It follows that
    $f'(0) = \Psi(\ad_x)^{-1} y$,
    or more verbosely, that
    \begin{equation}
      f'(0)
      =
      \frac{\ad_x}{1 - \exp(-\ad_x)}
      y
      =
      \sum_{n \geq 0}
      c_n \ad_x^n y
      =
      y + \frac{[x,y]}{2} + \dotsb
    \end{equation}
    for some explicit coefficients $c_n$ (Bernoulli numbers).
    Since $f$ is analytic,
    we deduce (upon setting $t := 1$,
    taking $x,y$ small enough and appealing to Taylor's theorem)
    that
    \begin{equation}
      x \ast y
      = x
      + \frac{\ad_x}{1 - \exp(-\ad_x)} y + O(|y|^2).
    \end{equation}
  \item Take $f(t) := x \ast (t y  - x)$.
    Then
    $\exp(f(t)) = \exp(x) \exp(t y - x)$ and $f(0) = 0$;
    we want to compute $f'(0)$.
    To that end, we compute the quantity
    \begin{equation}
      Q := \exp(-f(0)) \partial_{t=0} \exp(f(t))
      =
      \partial_{t=0}
      \exp(x) \exp(t y - x).
    \end{equation}
    in two ways.
    First,
    by the rearrangement
    \begin{equation}
      Q =
      \exp(x)
      \partial_{t=0}
      \exp(t y - x)
    \end{equation}
    and the formula \eqref{eq:diff-exp},
    we have
    \begin{equation}
      Q 
      = \Psi(-\ad_x) y
    \end{equation}
    with $\Psi$ as above.
    On the other hand,
    by direct application of
    \eqref{eqn:consequence-of-maurer-cartan-for-computing-exp-dfdfd}
    (which remains valid in this context),
    \begin{equation}
      Q = \Psi(\ad_{f(0)}) f'(0) = f'(0),
    \end{equation}
    since $f(0) = 0$.
    Therefore
    \begin{equation}
      f'(0) = \Psi(-\ad_x) y
      =
      \frac{1 - \exp(\ad_x)}{- \ad_x} y
      =
      \frac{\exp(\ad_x)-1}{\ad_x} y
      = y + \frac{[x,y]}{2} + \dotsb.
    \end{equation}
    The asymptotic formula
    \begin{equation}
      x \ast (y- x)
      = \frac{\exp(\ad_x) - 1}{\ad_x} y + O(|y|)^2
    \end{equation}
    for small enough $x,y$
    follows as in the previous part of the problem.
  \item
    In particular,
    since $|\ad_x^n y| = O(|x|^n |y|)$,
    we see by Taylor's theorem that
    \begin{equation}
      \frac{x}{2} \ast y
      \ast \frac{-x}{2}
      = x + \frac{[x,y]}{2} + O(|x|^2 |y| + |y|^2),
    \end{equation}
    \begin{equation}
      x \ast (y - x)
      = x + \frac{[x,y]}{2} + O(|x|^2 |y| + |y|^2).
    \end{equation}
  \end{enumerate}
\end{itemize}

\section{Some notation}
\label{sec:org8f383a2}
\subsection{Local maps\label{sec:notation-partial-functions}}
\label{sec:orgafcf8b3}
\subsubsection{Motivation}
\label{sec:org37c609c}
We shall often have occasion to consider continuous maps defined
on open subsets of topological spaces for which the precise
choice of domain is unimportant (other than, perhaps, in that it
contains a specific point).
This circumstance motivates introducing the notation
and terminology to follow.

\subsubsection{Definition}
\label{sec:org4f4ed31}
Let $X$, $Y$ be topological spaces.  
By a \emph{map $f$ from $X$ to $Y$}, denoted $f : X \rightarrow Y$, we shall mean a continuous function.
By a \emph{local map
  $f$ from $X$ to $Y$}, denoted\footnote{The notation is inspired by that used in algebraic geometry for rational maps.}  
\begin{equation*}
f : X \xdashrightarrow{} Y,
\end{equation*}
we shall mean more precisely a pair $(U,f)$, where:
\begin{itemize}
\item $U$ is an
open subset of $X$, called the \emph{domain of definition} or
simply the \emph{domain} of $f$
and denoted $U =: \dom(f)$, and
\item $f : U \rightarrow Y$ is continuous.
\end{itemize}
We say that $f : X \xdashrightarrow{} Y$ is
\emph{defined} at a point $p \in X$ if $p$
belongs to the domain of $f$.
\begin{example}
  Let $X := Y := \mathbf{k}$.
  The pair $(U,f)$,
  where $U := \mathbf{k}^\times$
  and $f(x) := 1/x$,
  is a local map
  $f : \mathbf{k} \xdashrightarrow{} \mathbf{k}$.
\end{example}

\subsubsection{Equivalence}
\label{sec:orgcfbef67}
Two local maps $f_1, f_2  : X \xdashrightarrow{} Y$
will be called \emph{equivalent}
if they coincide on the intersection of their domains of definition.

\subsubsection{Images\label{sec:partial-map-images}}
\label{sec:orgf825d0e}
Given a local map $f : X \xdashrightarrow{} Y$
and a subset $S$ of $X$,
we denote by $f(S)$ the image of the intersection
of $S$ with the domain of $f$,
i.e., if $U = \dom(f)$,
then $f(S) := f(U \cap S)$.


\subsubsection{Composition}
\label{sec:org7f9108b}
Given local maps $f : X \xdashrightarrow{} Y$
and $g : Y \xdashrightarrow{} Z$,
we may define their composition
to be the local map 
\begin{equation*}
g \circ f : X \xdashrightarrow{} Z
\end{equation*}
with $\dom(g \circ f) := \dom(f) \cap f^{-1}(\dom(g))$ given as usual by $(g \circ f)(x) := g(f(x))$.
It can happen that $\dom(g \circ f) = \emptyset$.

\subsubsection{Inverses}
\label{sec:orga0b7c1f}
A local map $f : X \xdashrightarrow{} Y$
with domain $U := \dom(f)$
will be called \emph{invertible}
if $f(U)$ is open
and the induced map $f : U \rightarrow f(U)$
is a homeomorphism.
In that case,
the \emph{inverse} of $f$
is defined to be the local map
\begin{equation*}
f^{-1} : Y \xdashrightarrow{} X
\end{equation*}
with $\dom(f^{-1}) := f(U)$
given as usual by $f^{-1}(y) := x$ if $y = f(x)$.


% Given a pair of topological spaces $X$ and $Y$, we shall often have
% the occasion to consider continuous maps $f : U \rightarrow Y$ defined on
% some open subset $U$ of $X$.  We call such a pair $(U,f)$ a
% \emph{partial function} from $X$ to $Y$ and denote this
% relationship symbolically by $f : X \xdashrightarrow{} Y$.
% This notation omits the $U$,
% whose precise shape is often unimportant;
% if we need to refer back to it, we shall call
% $U$ the \emph{domain of definition} or simply the \emph{domain} of $f$.
% Given $p \in X$,
% we say that $f : X \xdashrightarrow{} Y$ is \emph{defined at}
% $p$ if $p$ belongs to the domain of definition of $f$.

\section{Some review of calculus}
\label{sec:org2f1e843}
To explain what will happen in this course,
it will help to recall some
background from calculus.
Let $\mathbf{k}$ denote either of the fields $\mathbb{R}$ or $\mathbb{C}$.

\subsection{One-variable derivatives}
\label{sec:org428341e}
We say that $f : \mathbf{k} \xdashrightarrow{} \mathbf{k}$
is \emph{smooth} if all of its derivatives (of arbitrary order) exist at all points
in the domain of definition.
The first derivative $f' : \mathbf{k} \xdashrightarrow{} \mathbf{k}$
may be characterized by
the relation
\begin{equation*}
f(p + v) = f(p) + f'(p) v + o(|v|)
\end{equation*}
holding for each fixed point $p$ as $|v| \rightarrow 0$.  
In a single-variable calculus course, one learns
to relate (apparently complicated) \emph{global} properties of a
function to simpler \emph{local} (or \emph{infinitesimal})
properties involving its derivatives.  For example, when $\mathbf{k} = \mathbb{R}$, one learns
that the following are equivalent:
\begin{itemize}
 \item $f$ is
increasing (an ostensibly \emph{global} property, as it requires one to
check that $f(x) < f(y)$ for every pair of points with $x < y$,
and such points
might be quite far apart!);
\item $f'$ is positive (a \emph{local} property, as it
only requires one to check that $f'(x) > 0$ for each point $x$).
\end{itemize}
This test and others (concerning the second derivative, for instance) often suffice to piece together an
approximate portrait of the global shape of a function from the
local behavior of its derivatives at the critical points.

\subsection{Multi-variable total and partial derivatives\label{sec:calc-multi}}
\label{sec:org2290879}
We say that $f : \mathbf{k}^m \xdashrightarrow{} \mathbf{k}^n$ is \emph{smooth}
if all of its partial derivatives exist at all points in the domain of definition.
For each $p$ in the domain of $f$, the total derivative
$T_p f$ is a \emph{linear} map $T_p f : \mathbf{k}^m
\rightarrow \mathbf{k}^n$
that may be characterized as above by the relation
\begin{equation*}
  f(p+v) = f(p) + (T_p f)(v) + o(|v|)
\end{equation*}
holding for each fixed $p$ as $|v| \rightarrow 0$.
One can express $T_p f$ in matrix form
\begin{equation*}
  T_p f
  = 
\begin{pmatrix}
    \frac{\partial f_1}{\partial x_1}(p) & \dotsb  & \frac{\partial f_1}{\partial x_m}(p) \\
    \dotsb  & \dotsb  & \dotsb  \\
    \frac{\partial f_n}{\partial x_1}(p) & \dotsb  & \frac{\partial f_n}{\partial x_m}(p)
  \end{pmatrix}
\end{equation*}
where the function $f$ is expressed
as a tuple $f =  (f_1,\dotsc,f_n)$ of components $f_i : \mathbf{k}^m \xdashrightarrow{}
\mathbf{k}$,
elements $x \in \mathbf{k}^m$ are equipped with the standard coordinates $x =
(x_1,\dotsc,x_n)$,
and the partial derivatives are characterized by
\begin{equation*}
  f_i(p + t e_j)
  =
  f_i(p)
  +
  \frac{\partial f_i}{\partial x_j}(p)
  t +
  o(|t|)
\end{equation*}
for $t \in \mathbf{k}$ with $|t| \rightarrow 0$, where $e_j$
denotes the standard $j$th basis element of $\mathbf{k}^m$ dual
to the coordinate $x_j$;
for a vector $v = (v_1,\dotsc,v_m) \in \mathbf{k}^m$
and a coordinate index $i = 1,\dotsc,n$,
one then has
\begin{equation*}
((T_p f) v)_i
=
\sum_{j=1}^m
\frac{\partial f_i}{\partial x_j}(p)
v_j.
\end{equation*}
As in single-variable calculus, one learns various ways to
relate the global behavior of
a $f$
to the local behavior of its
total derivative $T_p f$ at various critical points $p$ (Hessian test, etc).
A basic case to keep in mind is that of a linear
function $A : \mathbf{k}^m \rightarrow \mathbf{k}^n$,
for which one has $T_p A = A$ for all $p \in \mathbf{k}^m$.

When $m = 1$, so that $f = (f_1,\dotsc,f_n)$
may be thought of as an $n$-tuple of functions $f_i : \mathbf{k} \rightarrow \mathbf{k}$,
the total derivative $T_p f$
is then a linear transformation $\mathbf{k} \rightarrow \mathbf{k}^n$
and so may be identified with the vector
\begin{equation*}
f'(p) = (f_1'(p), \dotsc, f_n'(p)) \in \mathbf{k}^n
\end{equation*}
characterized by the same relation as in the one-variable case.

\subsection{The chain rule}
\label{sec:org56f7558}
Given a pair of smooth functions
$f : \mathbf{k}^m \xdashrightarrow{} \mathbf{k}^n$
and $g : \mathbf{k}^n \xdashrightarrow{} \mathbf{k}^l$,
one can  form the composition
$h := g \circ f : \mathbf{k}^m \xdashrightarrow{} \mathbf{k}^l$, which is smooth.
One knows the chain rule: at a point $p \in \mathbf{k}^m$ at which $h$ is defined,
the derivative of the composition $h$ is the linear map
$T_p h  : \mathbf{k}^m \rightarrow \mathbf{k}^l$
given by the composition
\begin{equation*}
T_p h = T_{f(p)} g \circ T_p f
\end{equation*}
of the derivatives of $f,g$.
Expanding out in terms of matrices
and using standard coordinates $x_1,\dotsc,x_m$
on $\mathbf{k}^m$
and $y_1,\dotsc,y_n$ on $\mathbf{k}^n$,
the chain rule reads:
for $i \in \{1..l\}$
and $k \in \{1..m\}$,
\begin{equation*}
\frac{\partial h_i}{\partial x_k }(p)
=
\sum_{j=1}^n
\frac{\partial g_i}{\partial y_j}(f(p))
\frac{\partial f_j}{\partial x_k}(p).
\end{equation*}
Specialized to the case $m=1$, the chain rule reads
$h'(p) = (T_{f(p)} g) f'(p)$,
or expanding out further in terms in terms of components, as
$h_i'(p)
= \sum_{j=1}^n
\frac{\partial h_i}{\partial y_j}(f(p))
f_j(p)$.

\subsection{Inverse function theorem\label{sec:calc-inv-func-thm}}
\label{sec:org81b78cb}
The \emph{inverse function
  theorem} is a fundamental tool
for controlling the local behavior of a function $f$
\emph{near} a point
$p$ 
in terms of \emph{linear algebraic} properties
of the derivative $T_p f$.
\begin{theorem}\label{thm:inverse-function-theorem-euclidean}
  Let $f : \mathbf{k}^n \xdashrightarrow{} \mathbf{k}^n$
  be smooth and defined at $p$.
  The following are equivalent:
  \begin{enumerate}
  \item The map $T_p f : \mathbf{k}^n \rightarrow
    \mathbf{k}^n$
    is a linear isomorphism of vector spaces,
    or equivalently, has nonzero Jacobian determinant $\det(T_p
    f)$.
  \item
    There is an open neighborhood $U$ of $p$,
    contained in the domain of $f$,
    so that
    $V := f(U)$ is open
    and the induced map
    $f : U \rightarrow V$ is a diffeomorphism,
    i.e., admits a smooth two-sided inverse
    $g : V \rightarrow U$.
    %Thus for $x = (x_1,\dotsc,x_n) \in U$,
    %one has
    %$x_i = g_i(f(x_1,\dotsc,x_n)))$,
    %and for $y = (y_1,\dotsc,y_n) \in V$,
    %one has $y_j = f_j(g(y_1,\dotsc,y_n))$.
  \end{enumerate}
\end{theorem}

\subsection{Implicit function theorem\label{sec:calc-impl-func-thm}}
\label{sec:orgc7b25df}
We state it here rather verbosely, saving a tidier formulation
for the generalization to manifolds given below in \S\ref{sec:diff-geom-inv-func-thm}.
\begin{theorem}\label{thm:implicit-function-thm-euclidean}
  Let $n,m,d$ be nonnegative integers with $n = m + d$.  Suppose
  given a point $p \in \mathbf{k}^n$ and smooth maps
  $f_1,\dotsc,f_m : \mathbf{k}^n \xdashrightarrow{} \mathbf{k}$
  defined at $p = (p_1,\dotsc,p_n)$ satisfying
  either of the following evidently equivalent properties:
  \begin{enumerate}
  \item The
    $m \times n$ matrix of partial derivatives
    $\frac{\partial f_i}{\partial x_j}(p)$ ($i=1..m, j = 1..n$) has
    rank $m$.
  \item
    The function $f := (f_1,\dotsc,f_m) : \mathbf{k}^n
    \xdashrightarrow{} \mathbf{k}^m$
    has the property that its total derivative $T_p f : \mathbf{k}^n
    \rightarrow \mathbf{k}^m$ at $p$ is surjective.
  \item $\dim \ker(T_p f) = d$.
  \item The space of solutions
    $(d x_1,\dotsc, d x_n) \in \mathbf{k}^n$
    to the system of homogeneous linear equations
    $\sum_{j=1}^n \frac{\partial f_i}{\partial x_j} (p) d x _j = 0$
    ($i=1,\dotsc,m$)
    is $d$-dimensional.
  \end{enumerate}
  Then after suitably reordering the
  coordinates
  indices, one can find smooth functions
  $\psi_{d+1}, \dotsc, \psi_{n} : \mathbf{k}^n \xdashrightarrow{}
  \mathbf{k}$
  defined at 
  $(p_1,\dotsc,p_d,f_1(p),\dotsc,f_m(p))$
  so for any point $x = (x_1,\dotsc,x_n)$ close
  enough to $p$,
  one has
  \begin{equation*}
  x_j
  =
  \psi_j(x_1,\dotsc,x_d,f_1(x),\dotsc,f_m(x))
  \quad \text{ for } j = d+1,\dotsc,n.
  \end{equation*}
  In particular, if we suppose moreover
  that $f_1(p) = \dotsb = f_m(p) = 0$,
  then the following are equivalent
  for $x$ close enough to $p$:
  \begin{enumerate}
  \item $f_1(x) = \dotsb = f_m(x) = 0$
  \item $x_j = \psi_j(x_1,\dotsc,x_d,0,\dotsc,0)$ for $j = d+1, \dotsc, n$.
  \end{enumerate}
  Consequently the map
  \begin{equation*}
\Psi : \mathbf{k}^d \xdashrightarrow{} \mathbf{k}^n
\end{equation*}
  \begin{equation*}
\Psi(x_1,\dotsc,x_d) :=
  (x_1,\dotsc,x_d,\psi_{d+1}(x_1,\dotsc,x_d,0,\dotsc,0),
  \dotsc,
  \psi_{n}(x_1,\dotsc,x_d,0,\dotsc,0))
\end{equation*}
  parametrizes the set $\{ x : f_1(x) = \dotsb = f_m(x) = 0\}$ near $p$.
\end{theorem}
\begin{proof}
  Suppose after suitably relabeling indices that the rightmost
  $m \times m$ minor of the matrix $T_p f$ is nonsingular.
  Consider then the map
  $\phi : M \xdashrightarrow{} \mathbf{k}^n$ defined near $p$ by
  the formula
  $\phi(x) := (x_1,\dotsc,x_d,f_1(x),\dotsc,f_m(x))$.  The total
  derivative $T_p \phi$ is then given by the matrix
  \begin{equation*}
  \begin{pmatrix}
    1 & 0 & \dotsb & 0 & 0 & \dotsb & 0 \\
    0 & 1 & \dotsb & 0 & 0 & \dotsb & 0 \\
    \dotsb & \dotsb & \dotsb & \dotsb & \dotsb & \dotsb & 0 \\
    0 & 0 & \dotsb & 1 & 0 & \dotsb & 0 \\
    \frac{\partial f_1}{\partial x_{1}}(p)  &   \frac{\partial f_1}{\partial x_{2}}(p) & \dotsb & \frac{\partial f_1}{\partial x_d}(p) & \frac{\partial f_1}{\partial x_{d+1}}(p) & \dotsb & \frac{\partial f_1}{\partial x_{n}}(p) \\
    \dotsb & \dotsb & \dotsb & \dotsb & \dotsb & \dotsb & \dotsb \\
    \frac{\partial f_m}{\partial x_{1}}(p)  &   \frac{\partial f_m}{\partial x_{2}}(p) & \dotsb & \frac{\partial f_m}{\partial x_d}(p) & \frac{\partial f_m}{\partial x_{d+1}}(p) & \dotsb & \frac{\partial f_m}{\partial x_{n}}(p) 
  \end{pmatrix}
  \end{equation*}
  By our assumption on $T_p f$, it follows that $T_p \phi$ is
  nonsingular.  By the inverse function theorem, we can find a
  local inverse
  $\psi = (\psi_1,\dotsc,\psi_n) : \mathbf{k}^n \xdashrightarrow{}
  \mathbf{k}^n$
  to $\phi$ defined at $\phi(p)$.
  By construction,
  $\psi_i(x) = x_i$ for $i=1,\dotsc,d$,
  while the $\psi_i$ for $i > d$
  satisfy the requirements of the conclusion.
\end{proof}
% the components $y_1,\dotsc,y_n$
% of $\phi$ then define a coordinate system at $p$.  But visibly
% $y_{d+1} = \dotsb = y_n = 0$ if and only if
% $f_1 = \dotsb = f_m = 0$.  Therefore $S$ is given in the
% coordinate system $(y_1,\dotsc,y_n)$ by
% $y_{d+1} = \dotsb = y_n = 0$, and so it is a $d$-dimensional
% submanifold.

\section{Some review of differential geometry}
\label{sec:org53eb85b}
As indicated already in \S\ref{sec:disclaimers}, this section is
intended to be used mainly as a reference.  I don't plan to use much
differential geometry in the actual course.
\subsection{Charts}
\label{sec:org8cf0641}
Recall the notation and terminology of \S\ref{sec:notation-partial-functions}.
By a \emph{topological chart} on a topological space $X$
we shall mean a pair $(\phi,n)$, where $n$ is a nonnegative integer and
\begin{equation*}
\phi : X \xdashrightarrow{} \mathbf{k}^n
\end{equation*}
is an invertible local map.
More verbosely, this means that a topological chart
is a triple
$(U,\phi,n)$, where $n$ is a nonnegative integer, $U$ is an open
subset of $X$, and $\phi : U \rightarrow \mathbf{k}^n$ is a
continuous map for which $\phi(U)$ is open and
$\phi : U \rightarrow \phi(U)$ is a homeomorphism onto its
image.

\subsection{Manifolds}
\label{sec:orgdbe668d}
For us, an \emph{$n$-manifold} (over the field $\mathbf{k}$)
is a triple
$(M,n,\mathcal{A})$, often abbreviated simply as $M$
when the data $n, \mathcal{A}$ are understood by context, where:
\begin{enumerate}
\item $M$ is a topological space (which we assume
   to be Hausdorff and second-countable, hence metrizable),
\item $n$ is a nonnegative integer,
\item $\mathcal{A}$ is an \emph{maximal smooth atlas}, that is to say, a
  collection of topological charts
  $\phi : M \xdashrightarrow{} \mathbf{k}^n$ whose domains cover
  $M$ and which are \emph{smoothly compatible} in the sense that
  for each pair $\phi, \psi$ of charts in $\mathcal{A}$, the
  compositions
  \begin{equation*}
\phi \circ \psi^{-1}, \psi \circ \phi^{-1} : \mathbf{k}^n \xdashrightarrow{}
  \mathbf{k}^n
\end{equation*}
  are smooth on their respective domains of definition.
  ``Maximal'' means that $\mathcal{A}$ contains every chart on $M$ that is compatible with every chart 
  in $\mathcal{A}$.
  By a \emph{smooth chart} on $M$
  we then mean an element of $\mathcal{A}$.
\end{enumerate}
A \emph{manifold} is an $n$-manifold $M$ for some $n$, which is
called the \emph{dimension} of $M$ and denoted $n = \dim(M)$.
\begin{example}
  An open subset subset $M$ of $\mathbf{k}^n$
  is a $n$-manifold
  if we take for $\mathcal{A}$ the set of all
  charts $\phi : M \xdashrightarrow{} \mathbf{k}^n$
  which are smooth and have smooth inverses in 
  the ordinary sense of \S\ref{sec:calc-multi}.
  In particular, $\mathbf{k}^n$ is an $n$-manifold.
  More generally, any open subset $U$ of an $n$-manifold $M$ has the natural structure 
  of an $n$-manifold:
  if $\mathcal{A}$ is a maximal smooth atlas on $M$,
  then $\{ \phi \in \mathcal{A} : \dom(\phi) \subseteq U \}$ is a maximal smooth atlas on $U$.
\end{example}
% \begin{example}
%   Let $\mathbb{P}^n_\mathbf{k}$ denote the set of lines in $\mathbf{k}^{n+1}$.
%   Each $L \in \mathbb{P}^n_\mathbf{k}$
%   consists of the multiples of some
%   nonzero element $(x_0,\dotsc,x_n)$.
%   Denote by

%   Let $m,n$ be integers with $n > m > 0$.
%   Denote by $\Gr(n,m)$ the set of all
%   $m$-dimensional subspaces $L$ of $\mathbf{k}^n$.
%   For each such $L$
%   there is a tuple $(i_1,\dotsc,i_m)$ of coordinate indices
%   so that the map
%   $\pi : L \rightarrow \mathbf{k}^m$
%   given by $(x_1,\dotsc,x_n) \mapsto (x_{i_1},\dotsc,x_{i_m})$
%   is an isomorphism.
%   Denote by $f : \mathbf{k}^m \rightarrow L$ the inverse of $\pi$
%   and by $g : L \rightarrow \mathbf{k}^{n-m}$
%   the map $g(x) := (x_{j_1},\dotsc,x_{j_{n-m}})$,
%   where
%   $(j_1,\dotsc,j_{n-m})$ is some tuple for which
%   $\{1,\dotsc,n\} = \{i_1,\dotsc,i_m\} \sqcup
%   \{j_1,\dotsc,j_{n-m}\}$.
%   Set $\phi_\alpha (L) := (f(e_1),\dotsc,f(e_m)) \in (\mathbf{k}^n)^m
% \end{example}<++>
\begin{remark}
~
  \begin{enumerate}
  \item One could also work with manifolds
    having multiple components of varying dimension,
    but we will not have occasion to do so.
  \item One can show that the dimension of a manifold is determined by its topological structure,
    making the inclusion of $n$ in the definition redundant, but
    this fact
    is not important for our purposes.
  \end{enumerate}
\end{remark}

\subsection{Smooth maps}
\label{sec:org92b178d}
Given manifolds $M$, $N$, a local map
$f : M \xdashrightarrow{} \mathbf{k}$ is \emph{smooth} if it is
smooth with respect to every pair of charts
$\phi : M \xdashrightarrow{} \mathbf{k} ^m$,
$\psi : N \xdashrightarrow{} \mathbf{k} ^n$ in the sense that
the composition
\begin{equation*}
\mathbf{k}^m \xdashrightarrow{\phi^{-1}}
M
\xdashrightarrow{f}
N
\xdashrightarrow{\psi}
\mathbf{k}^n
\end{equation*}
is smooth in the sense of \S\ref{sec:calc-multi}.
When $M, N$ are open subsets of Euclidean space,
this notion generalizes the earlier one.

\subsection{Coordinate systems}
\label{sec:org9cf6a98}
Let $M$ be an $m$-manifold
and let $p \in M$ be a point.
By a \emph{coordinate system for $M$ at $p$},
or more simply
a \emph{coordinate system at $p$} or  \emph{local coordinates at $p$}
when the ambient manifold $M$ is clear from context,
we shall mean a tuple of 
smooth maps $x_1,\dotsc,x_m : M \xdashrightarrow{} \mathbf{k}$
arising as the components
of a smooth chart $\phi = (x_1,\dotsc,x_m) : M \xdashrightarrow{} \mathbf{k}^m$
on $M$ defined at $p$.
Such a coordinate system allows us to identify points $x \in M$
near $p$ with tuples $x = (x_1,\dotsc,x_m)$
and smooth functions
$f : M \xdashrightarrow{} \mathbf{k}$
defined near $p$ with smooth functions $f(x_1,\dotsc,x_m)$ of 
the local coordinates
$x_1,\dotsc,x_m$.
In particular, it makes sense
to define partial derivatives
$\partial f /\partial x_j \in \mathbf{k}$
in this setting.
For example, if $M$ is an open subset of $\mathbf{k}^m$,
then the standard coordinate functions $x_1,\dotsc,x_m : M
\rightarrow \mathbf{k}$
form a coordinate system at every point $p \in M$.

Given an $m$-manifold $M$,
an $n$-manifold $N$,
a smooth map $f : M \xdashrightarrow{} N$
and a point $p \in M$
at which $f$ is defined,
one can choose local coordinates
$x_1,\dotsc,x_m$ at $p$
and
$y_1,\dotsc,y_n$ at $f(p)$.
With respect to such coordinates,
the smooth map $f$ identifies
with a smooth map
$(f_1,\dotsc,f_n) : \mathbf{k}^m \xdashrightarrow{} \mathbf{k}^n$
defined at $(x_1(p),\dotsc,x_m(p))$
and $T_p f$ identifies 
with the matrix of partial derivatives
$\partial f_i/ \partial x_j$
as in \S\ref{sec:calc-multi}.


% we will often tacitly assume here
% that the domains
% of the $x_i$ are contained in
% the domain of $f$.
% We obtain in this way
% smooth charts
% \begin{equation*}
% (x_1,\dotsc,x_m) : M \xdashrightarrow{} \mathbf{k}^m
% \end{equation*}
% defined at $p$
% and
% \begin{equation*}
% (y_1,\dotsc,y_n) : N \xdashrightarrow{} \mathbf{k}^n
% \end{equation*}
% defined at $f(p)$
% under which $f$ identifies
% with a smooth map
% \begin{equation*}
% (f_1,\dotsc,f_m) : \mathbf{k}^m \xdashrightarrow{}
% \mathbf{k}^n
% \end{equation*}
% defined

\subsection{Tangent spaces\label{sec:diff-geom-tangent-spaces}}
\label{sec:orgd99d069}
Let $M$ be an open subset of $\mathbf{k}^m$.  A \emph{curve on $M$}
is a smooth map
$\gamma : \mathbf{k} \xdashrightarrow{} M$ whose domain is connected and contains $0$;
the point $p := \gamma(0) \in M$ is referred to as its
\emph{basepoint} and the vector
$v := \gamma'(0) \in \mathbf{k}^m$ as its \emph{initial
  velocity}.
Given an open subset $N \subseteq \mathbf{k} ^n$
and a smooth map $f : M \rightarrow N$, the composition
$f \circ \gamma : \mathbf{k} \xdashrightarrow{} N$ is then a curve on
$N$ with basepoint $f(p)$; thanks to the chain rule, its initial velocity
is the image $(f \circ \gamma)'(0) = (T_{p} f) v$ of that of the original curve $\gamma$
under the derivative of the map $f$ at the basepoint of the original curve.
\begin{remark}
\label{rmk:lots-of-curves}
  Observe also that for each point $p \in M$
  and vector $v \in \mathbf{k}^m$
  there exists a curve $\gamma$ with basepoint $p$ and initial
  velocity $v$;
  for example, one can set
  $\gamma(t) := p + t v$
  and take for the domain of $\gamma$ any sufficiently small neighborhood of $0$.
\end{remark}

We now recall the generalization of the above notion to an arbitrary $n$-manifold $M$:
\begin{definition}
Let $M$ be a manifold. 
A \emph{curve on $M$} is defined as above to be a smooth map
$\gamma : \mathbf{k} \xdashrightarrow{} M$ whose domain is connected and contains $0$.
\end{definition}
We continue to refer to the point $p := \gamma(0) \in M$ as
the \emph{basepoint} of $\gamma$. 
If one now ponders
how to define ``the initial velocity $\gamma'(0)$'' (e.g., to which space should it belong?),
one is eventually led to introduce the \emph{tangent space} $T_p M$
to the manifold $M$ at the point $p$.
To motivate the definition, note first that for any smooth chart
$\phi : M \xdashrightarrow{} \mathbf{k}^n$ defined at $p$, the
composition $\phi \circ \gamma : \mathbf{k}  \xdashrightarrow{}
\mathbf{k}^n$ is
(after suitably shrinking its domain so as to be connected)
a curve on $\mathbf{k}^n$ with basepoint $\phi(p)$ and initial
velocity
\begin{equation}\label{eq:velocity-of-curve-in-chart}
  v_\phi := (\phi \circ \gamma)'(0)
\end{equation}
in the Euclidean sense defined previously.
Moreover, for any other smooth chart
$\psi : M \xdashrightarrow{} \mathbf{k}^n$ defined at $p$,
we can use the chain rule
in the form
\begin{equation*}
T_0 (\psi \circ \gamma)
= 
T_0 (\psi \circ \phi^{-1} \circ \phi \circ  \gamma)
=
T_{\phi(p)} (\psi \circ \phi^{-1}) \circ T_{0}(\phi \circ  \gamma)
\end{equation*}
to read off the initial velocity $v_{\psi}$
of the curve $\psi \circ \gamma$
from that of $\phi \circ \gamma$:
\begin{equation}\label{eq:consistency-tangent-vectors-in-charts}
  v_\psi = T_{\phi(p)}(\psi \circ \phi^{-1}) v_\phi.
\end{equation}
This observation suggests the following
definition:
\begin{definition}
  The \emph{tangent space
    $T_p M$ to $M$ at $p$}
  is the set of all tuples $v = (v_\phi)_{\phi}$,
  where $\phi$ traverses the set of smooth charts
  defined at $p$
  and the
  $v_\phi$ are elements of $\mathbf{k}^n$
  satisfying the consistency
  condition \eqref{eq:consistency-tangent-vectors-in-charts}.
\end{definition}
Since the consistency condition is linear,
it is clear that $T_p M$ is a vector space.
We can also now define, for each curve
$\gamma$ on $M$ with basepoint $p = \gamma(0)$,
the initial velocity of $\gamma$ to be the  vector
$\gamma '(0) \in T_p M$
given by $v = (v_\phi)_\phi$,
where the  components $v_\phi$ are as in \eqref{eq:velocity-of-curve-in-chart}.
We have moreover the following:
\begin{lemma}\label{lem:describe-tangent-space}
  For any smooth chart $\chi$ on $M$ defined at $p$,
  the map $v \mapsto v_\chi$
  defines a linear isomorphism
  $T_p M \cong \mathbf{k}^n$.
  In particular, $\dim T_p M = \dim M$.
\end{lemma}
\begin{proof}
  The consistency condition \eqref{eq:consistency-tangent-vectors-in-charts} implies that
  $v$ is determined by any one of its components
  $v_\chi$,
  so the map in question
  is clearly injective.
  Conversely, given any element $u \in \mathbf{k}^n$,
  we may define a tuple $v = (v_\phi)_\phi$
  by the rule
  $v_\phi := T_{\chi(p)}(\phi \circ \chi^{-1}) u$.
  An application of the chain rule to the composition
  $(\psi \circ \phi^{-1}) \circ (\phi \circ  \chi^{-1})$
  implies for any smooth charts $\phi,\psi$
  defined at $p$ that \eqref{eq:consistency-tangent-vectors-in-charts} is
  satisfied,
  and so $v$ belongs to $T_p M$.
  Since $v_\chi = u$ and $u$ was arbitrary, we deduce that the
  map in question is
  surjective.
    % \begin{equation}\label{eq:}
    %   v_\psi =
    %   T_{\chi(p)}(\psi \circ \chi^{-1}) u
    %   =
    %   T_{\chi(p)}(\psi \circ \phi^{-1} \circ \phi \circ  \chi^{-1}) u
    %   =
    %   T_{\phi(p)}(\psi \circ \phi^{-1}) T_{\chi(p)}(\phi \circ \chi^{-1}) u
    %   =
    %   T_{\phi(p)}(\psi \circ \phi^{-1}) v_\phi,
    % \end{equation}
    % hence
\end{proof}



\begin{example}\label{example:tangent-space-open-subset-euclidean-space}
  If $M$ is an open subset of $\mathbf{k}^n$,
  then the inclusion $\phi : M \rightarrow \mathbf{k}^n$
  is a smooth chart defined at all points of $M$,
  and the lemma gives a natural identification $T_p M \cong
  \mathbf{k}^n$
  for all $p \in M$.
  The two senses in which we have defined the initial velocity
  $\gamma'(0)$ of a curve $\gamma$ on $M$ with basepoint $p$
  (first as the ordinary derivative $\frac{d }{d t}
  \gamma(t)|_{t=0}$, 
  then later as a tuple $(v_\phi)_\phi$)
  are compatible under this identification.
\end{example}

\begin{remark}\label{rmk:tangent-vectors-via-curves}
  A more customary definition is that $T_p M$ is the space of
  equivalence classes $[\gamma]$ of curves $\gamma$ on $M$ with
  basepoint $p$, with two curves $\gamma_1, \gamma_2$ declared
  equivalent precisely when
  $(\phi \circ \gamma_1)'(0) = (\phi \circ \gamma_2)'(0)$ for
  all charts $\phi$ on $M$ defined at $p$.  That definition is
  isomorphic to the one we've used.  An isomorphism from the
  former to the latter may be given by
  $[\gamma] \mapsto (v_\phi)_\phi$ with $v_\phi$ as in \eqref{eq:velocity-of-curve-in-chart}; this
  map is well-defined by
  \eqref{eq:consistency-tangent-vectors-in-charts}, injective by
  definition of the equivalence relation defining $[\gamma]$,
  and surjective by Lemma \ref{lem:describe-tangent-space} and
  Remark \ref{rmk:lots-of-curves}.
\end{remark}

\subsection{Derivatives}
\label{sec:org3a5d387}
Given a smooth map $f : M \xdashrightarrow{} N$ of manifolds
a point $p \in M$ at which $f$ is defined,
a chart $\phi : M \xdashrightarrow{} \mathbf{k} ^m$
at $p$
and a chart $\psi : N \xdashrightarrow{} \mathbf{k} ^n$
at $f(p)$,
we can form the smooth map
\begin{equation*}
f_{\phi \psi} := \psi \circ f \circ \phi ^{-1} :
\mathbf{k}^m \xdashrightarrow{} \mathbf{k}^n
\end{equation*}
and consider its total derivative in the sense of \S\ref{sec:calc-multi},
which is a linear map
\begin{equation*}
T_{\phi(p)}(f_{\phi \psi})
:
\mathbf{k}^m \xdashrightarrow{} \mathbf{k}^n.
\end{equation*}
We can piece these linear maps
together to form a linear map
\begin{equation*}
T_p f : T_p M \rightarrow T_{f(p)} N,
\end{equation*}
called the \emph{derivative of $f$},
by setting, for $v = (v_\phi)_\phi \in T_p M$,
\begin{equation}\label{eq:defn-derivative-manifolds}
  (T_p f(v))_\psi
  :=
  T_{\phi(p)} (f_{\phi \psi}) v_\phi.
\end{equation}
An application of the chain rule confirms
that the RHS of
\eqref{eq:defn-derivative-manifolds}
is independent of $\phi$
and that the the
tuple $T_p f(v)$ defined componentwise above
actually belongs to $T_{f(p)} N$;
moreover, for a composition
$L \xdashrightarrow{g} M \xdashrightarrow{f} N$
defined at a point $p \in L$,
one has again
\begin{equation}\label{eqn:chain-rule-general}
T_p (f \circ g)
=
T_{g(p)} f \circ T_p g.
\end{equation}
\begin{remark}
  \begin{enumerate}
~
  \item   If $M \subseteq \mathbf{k}^m, N \subseteq \mathbf{k}^n$
    are open
    and we identify $T_p M \cong \mathbf{k}^m, T_{f(p)} N \cong
    \mathbf{k}^n$,
    then the derivative
    $T_p f : T_p M \rightarrow T_{f(p)} N$
    defined just now identifies the derivative
    $T_p f : \mathbf{k}^m \rightarrow \mathbf{k}^n$
    as in \S\ref{sec:calc-multi}.
  \item   
    If $M$ is an $n$-manifold,
    $p \in M$ is a point,
    and $\phi$ is a chart on $M$ at $p$,
    then the map
    $T_\phi : T_p M \rightarrow T_{\phi(p)} \mathbf{k}^n \cong  \mathbf{k}^n$
    is just the projection
    $v \mapsto v_\phi$.
  \item 
    If we had instead defined tangent spaces
    using equivalence classes of smooth curves
    as in Remark \ref{rmk:tangent-vectors-via-curves},
    then the definition of the derivative of $f$
    would look like:
    for a curve $\gamma$ on $M$ with basepoint $p$
    and equivalence class $[\gamma]$,
    \begin{equation*}
    T_p f([\gamma]) := [f \circ \gamma].
    \end{equation*}
    As noted earlier in the Euclidean case,
    one has the following identity of elements
    of $T_{f(p)} N$:
    \begin{equation*}
    (T_p f) \gamma'(0)
    =
    (f \circ \gamma)'(0).
    \end{equation*}
  \end{enumerate}
\end{remark}

It is occasionally
notationally cumbersome
to refer explicitly to the basepoint
$p$ in the total derivative $T_p f : T_p M \rightarrow T_{f(p)} N$;
when that is the case, we might denote the latter as simply
\begin{equation*}
d f : T_p M \rightarrow T_{f(p)} N,
\end{equation*}
with the point $p$ understood by context as the domain of any
vector
$v$ to which we apply $d f$.
To illustrate, consider a composition of smooth maps
of manifolds
\begin{equation*}
\phi : K \xrightarrow{h} L \xrightarrow{g} M \xrightarrow{f} N.
\end{equation*}
For a point $p \in K$ and a tangent vector $v \in T_p K$,
the chain rule \eqref{eqn:chain-rule-general}
gives the slightly unwieldly formula
\begin{equation*}
T_p \phi(v)
=
T_{g(h(p))} f
(
T_{h(p)} g(
T_p h (v)
)
)
\end{equation*}
which we abbreviate to simply
\begin{equation*}
d \phi(v)
= d f (d g( d h(v))).
\end{equation*}
% A special case is worth emphasizing.
% Let $f : M \xdashrightarrow{} \mathbf{k}$
% be smooth,
% and let $p \in M$ belong to the domain of $f$.
% Since $T_{f(p)} \mathbf{k}$ identifies naturally with
% $\mathbf{k}$,
% the derivative map
% identifies with a linear functional
% $T_p f : T_p M \rightarrow \mathbf{k}$.
% We might sometimes denote this functional
% by $(d f)_p$
% and its value at an element $v \in T_p M$
% by simply \begin{equation*}d f(v) := (d f)_p(v) := T_p f(v)\end{equation*}
% when it might become too notationally cumbersomethe precise
% A bit more generally,
% for $f = (f_1,\dotsc,f_m) : M \xdashrightarrow{} \mathbf{k}^m$,
% we might write $d f := (d f_1, \dotsc, d f_m) :
% T_p M \rightarrow \mathbf{k}^m$.

\subsection{Jacobians}
\label{sec:org9589e79}
Given an $n$-dimensional vector space $V$, we denote by
$\det (V) := \Lambda^n V$ its highest wedge power.  It is a
one-dimensional vector space.  Given a pair $V,W$ of
$n$-dimensional vector spaces and a linear map
$f : V \rightarrow W$, one obtains an induced map
$\det(f) : \det(V) \rightarrow \det(W)$.  If $W = V$, then
$\det(f) : \det(V) \rightarrow \det(V)$ acts on the
one-dimensional space $\det(V)$ via multiplication by the
determinant of $f$ in the sense of a first course on linear algebra.

In particular, given a manifold $M$,
we obtain for each point $p \in M$
a one-dimensional vector space $\det T_p M$.
Given a pair of manifolds $M,N$ of the same dimension,
a smooth map $f : M \xdashrightarrow{} N$ between them,
and a point $p$ at which $f$ is defined,
we obtain a linear map
$\det(T_p f) : \det(T_p M) \rightarrow \det(T_{f(p)} N)$
between one-dimensional spaces,
called the \emph{Jacobian determinant} of $f$.
In the special case that $M,N$ are open subsets
of $\mathbf{k}^n$,
we may identify $T_p M, T_{f(p)} N$ with $\mathbf{k}^n$
and
the Jacobian determinant
with the linear map
$\det(\mathbf{k}^n) \rightarrow \det(\mathbf{k}^n)$
given by multiplication by
the determinant of the
Jacobian matrix
describing the
total derivative
$T_p f : \mathbf{k}^n \rightarrow \mathbf{k}^n$.
In general,
one has
$\det(T_p f) \neq 0$ if and only if $T_p f$ is a linear
isomorphism.

\subsection{Inverse function theorem\label{sec:diff-geom-inv-func-thm}}
\label{sec:org63376d9}
We record the generalization
of what was given earlier in the Euclidean case.
\begin{definition}
  A smooth map $f : M \xdashrightarrow{} N$ between $n$-manifolds is
  said to be a \emph{local diffeomorphism} at a point $p \in M$
  if $f$ is equivalent to an invertible local map defined at
  $p$ with smooth inverse, or more verbosely, if there is an
  open neighborhood $U$ of $p$ in the domain of $f$ so that
  $f(U)$ is open and the induced map $f : U \rightarrow f(U)$ is
  a diffeomorphism.  In that case,
  there are
  coordinate systems $x_1,\dotsc,x_n$
  on $M$ at $p$
  and $y = (y_1,\dotsc,y_n)$ on $N$ at $f(p)$
  with respect to which $p$ is the origin and
  so that $f$ is given near $p$ in these coordinates
  by the identity map $f : (x_1,\dotsc,x_n) \mapsto (x_1,\dotsc,x_n)$.
\end{definition}
For example, if $M$ is an $n$-manifold,
then a map $f : M \xdashrightarrow \mathbf{k}^n$
is a local diffeomorphism at $p$ if and only if it is equivalent
to a smooth chart at $p$.
\begin{theorem}
  Let $f : M \rightarrow N$ be a smooth map of manifolds of the same dimension.
  The following are equivalent:
  \begin{enumerate}
\item $T_p f : T_p M \rightarrow T_{f(p)} N$
  is a \emph{linear isomorphism} of vector spaces,
  or equivalently, has nonzero Jacobian determinant
  $\det(T_p f)$.
\item $f$ is a local diffeomorphism near $p$.
\end{enumerate}
\end{theorem}
The problem is local,
so it suffices to consider the case that $M,N$ are open subsets
$\mathbf{k}^n$, in which case the theorem reduces to
special case given above in \S\ref{sec:calc-inv-func-thm}.

Here is a particularly useful consequence:
\begin{corollary}
  Let $M$ be an $n$-manifold
  and $p \in M$.
  Let $\phi : M \xdashrightarrow{} \mathbf{k}^n$
  be a smooth map defined at $p$.
  Suppose that $\det(T_p \phi) \neq 0$.
  Then $\phi$ is equivalent to a smooth chart on $M$ at $p$;
  in other words,
  there is a neighborhood $p \in U \subseteq M$
  so that
  if we write $\phi|_U = (x_1,\dotsc,x_n)$
  for some component functions $x_i : U \rightarrow \mathbf{k}$,
  then
  $x_1,\dotsc,x_n$ defines a coordinate system at $p$.
\end{corollary}

\subsection{Local linearization of smooth maps}
\label{sec:org7d02d39}
\subsubsection{Linear maps in terms of coordinates}
\label{sec:org7f15f91}
Suppose given an $m$-dimensional vector space $V$
an $n$-dimensional vector $W$ and a linear
map $f : V \rightarrow W$
between them.
Recall that the \emph{rank} of $f$ is
the dimension of its image, or equivalently,
the codimension of its kernel.
Denote by $k$ the rank of $f$.
Then $k \leq m$ and $k \leq n$.
One can always
find bases $e_1,\dotsc,e_m$ of $V$
and $\eps_1,\dotsc,\eps_n$ of $W$
so that
$f(\sum_{i=1}^m x_i e_i)
= \sum_{i=1}^k x_i \eps_i$;
in coordinates, $f$ takes the form
$(x_1,\dotsc,x_m) \mapsto (x_1,\dotsc,x_k,0,\dotsc,0)$.

\subsubsection{The constant rank theorem}
\label{sec:org52585b3}
Suppose now given an $m$-manifold $M$,
an $n$-manifold $N$,
a smooth map $f : M \xdashrightarrow{} N$
and a point $p$ in the domain of $f$.
\begin{definition}
  We say that $f$ is \emph{linearizable} at $p$
  if there are local coordinates at $p$ with respect
  to which $f$ is given by a linear map.
  The \emph{rank} of $f$ at $p$ is then the rank of that linear map.
\end{definition}
Since any linear  map is its own derivative,
the rank of $f$ at $p$ is the same as the rank of $T_p f$.
Denoting that rank by $k$
(necessarily $k \leq \min(m,n)$),
one can
find
local coordinates
$x_1,\dotsc,x_m$ at $p$ and $y_1,\dotsc,y_n$ at $f(p)$,
putting both $p$ and $f(p)$ at the origin,
so that $f$ is given in the particularly
concrete form
\begin{equation*}
  f : (x_1,\dotsc,x_m) \mapsto (x_1,\dotsc,x_k,0,\dotsc,0).
\end{equation*}
Since a linear map has constant rank, an obvious necessary condition for $f$
to be linearizable is the following:
\begin{definition}
  We say that $f$ has \emph{constant rank} at $p$
  if the rank of $T_x f$ takes some constant value
  in a neighborhood of $p$.
\end{definition}
In fact, the two conditions are equivalent:
\begin{theorem}\label{thm:constant-rank}
  $f$ is linearizable at $p$ if and only if $f$ has constant rank at $p$.
\end{theorem}
\begin{proof}
  The interesting direction (showing that if $f$ has constant
  rank, then it is linearizable) reduces to the rank theorem
  from multivariable calculus, whose proof is similar to that of
  the implicit function theorem given in
  \S\ref{sec:calc-impl-func-thm}.
\end{proof}

\subsubsection{The case of maximal rank\label{sec:linearization-subimm}}
\label{sec:org6479c62}
Given $f : M \xdashrightarrow{} N$ as above, the function
$x \mapsto \rank(T_x f)$ is \emph{lower semicontinuous}, i.e.,
has the property that $\{x : \rank(T_x f) \geq k\}$ is open for
all $k$.  This is because the condition $\rank(T_x f) \geq k$ is
detected by the nonvanishing of any $k \times k$ minor, which is
an open condition.  In other words, as $x$ varies, the rank can
only ``jump'' downwards.

The quantity $k_0 := \min(m,n)$
is the largest possible value
for the rank of $f$ at any point,
i.e., one has $\rank(T_x f) \leq k_0$ for all $x$.
It follows from the lower semicontinuity
noted above that the set
$\{x : \rank(T_x f) = k_0 \}$ of points
at which $f$ attains its maximal rank is \emph{open}:
if $f$ has rank $k_0$ at some point at some $p$,
it automatically has rank $k_0$ in some small neighborhood of
$p$.
This observation motivates the utility
of the following definition:
\begin{definition}
  For $m \geq n$,
  we say that $f$ is \emph{submersive} at $p$
  if it satisfies any of the following
  equivalent conditions:
  \begin{enumerate}
  \item $\rank(T_p f) = n$.
  \item $T_p f$ is surjective.
  \item $\dim \ker(T_p f) = m - n$.
  \end{enumerate}

  For $m \leq n$,
  we say that $f$ is \emph{immersive} at $p$
  if it satisfies any of the following
  equivalent conditions:
  \begin{enumerate}
  \item $\rank(T_p f) = m$.
  \item $T_p f$ is injective.
  \item $\dim \coker(T_p f) = n-m$,
    where $\coker(T_p f) := T_{f(p)}N/\image(T_p f)$.
  \end{enumerate}

  We say that $f$ is a submersion (resp. immersion)
  if it is submersive (resp. immersive) at all points $p$.
\end{definition}
\begin{theorem}
  Suppose $f : M \xdashrightarrow{} N$ as above is submersive at
  $p$.  Then there are coordinate systems at $p$ with respect to
  which $f$ is given by a surjective linear map.  For instance,
  there are coordinate systems $x_1,\dotsc,x_m$ on $M$ at $p$
  and $y_1,\dotsc,y_n$ on $N$ at $f(p)$, putting both $p$ and
  $f(p)$ at the origin, so that $f$ is given by
  $f : (x_1,\dotsc,x_m) \mapsto (x_1,\dotsc,x_n)$.
\end{theorem}
\begin{proof}
  The statement is local, and reduces to that of \S\ref{sec:calc-impl-func-thm}.
\end{proof}
\begin{theorem}
  If $f$ as above is immersive at $p$,
  then there are coordinate systems at $p$ with respect to
  which $f$ is given by an injective linear map.  For instance,
  there are coordinates
  $x_1,\dotsc,x_m$ on $M$ at $p$
  and $y_1,\dotsc,y_n$ on $N$ at $f(p)$,
  putting both $p$ and $f(p)$ at the origin,
  so that
  $f$ is given 
  by
  $(x_1,\dotsc,x_m) \mapsto (x_1,\dotsc,x_m,0,\dotsc,0)$.
\end{theorem}
\begin{proof}
 This reduces to a local statement which can be proved 
as in \S\ref{sec:calc-impl-func-thm}
using the inverse function theorem. 
\end{proof}

\begin{corollary}\label{cor:checking-smoothness-via-immersions}
  Let $K$ be a $k$-manifold,
  let $g : K \xdashrightarrow{} M$ be a continuous map,
  and let $f : M  \xdashrightarrow{} N$ be an
  immersion whose domain contains the image of $g$.
  Suppose that the composition
  \begin{equation*}
  K \xdashrightarrow{g} M \xdashrightarrow{f} N
  \end{equation*}
  is smooth.
  Then $g$ is smooth.
\end{corollary}
\begin{proof}
  Smoothness can be checked locally, so we may suppose that $K$
  is an open subset of $\mathbf{k}^k$.  By the local description
  of $f$ given the previous result, we may assume that
  $N = \mathbf{k}^n$ and
  $M = \mathbf{k}^m \cong \mathbf{k}^m  \times \{0\}  \subseteq \mathbf{k}^n$.
  We are then given
  a continuous map $\mathbf{k}^k \xdashrightarrow{g} \mathbf{k}^m$
  with the property that the
  composition
  $\mathbf{k}^k \xdashrightarrow{g} \mathbf{k}^m \hookrightarrow
  \mathbf{k}^n$
  is smooth (i.e., all partials exist).
  We want to deduce that
  $\mathbf{k}^k \xdashrightarrow{g} \mathbf{k}^m$ is smooth
  (i.e., all partials exist).
  What we want follows
  immediately from the definition.
\end{proof}

\subsection{Submanifolds\label{sec:submflds}}
\label{sec:orgde0d03e}
Submanifolds\footnote{
 What we call \emph{submanifold}
 might normally be more verbosely called  \emph{smooth submanifold}.
}
are subsets of manifolds that look like vector subspaces up to a local diffeomorphism.
More precisely:
\begin{definition}
  Given an $n$-manifold $M$,
  a subset $S$ of $M$ is said
  to be a \emph{$d$-dimensional submanifold}
  if for each $p \in S$,
  there is a smooth chart
  $\phi : M \xdashrightarrow{} \mathbf{k}^n$
  at $p$
  so that $\phi(S) = \phi(M) \cap \mathbf{k}^d \times \{0\} \subseteq \mathbf{k}^n$
  (as defined in \S\ref{sec:partial-map-images});
  said another way,
  there is a coordinate system $x_1,\dotsc,x_n$ on $M$ at $p$
  with respect to which $S$ is given near $p$ by the equation
  $x_{d+1} = \dotsb  = x_n = 0$.
\end{definition}

% \begin{enumerate}
% \item There is a smooth chart
%   $\phi : M \xdashrightarrow{} \mathbf{k}^n$ defined at $p$
%   and a $d$-dimensional subspace $E$ of $\mathbf{k}^n$ so that
%   $\phi(p) = 0$ and so that
%   $\phi(S) = E \cap \phi(M)$
%   (see \S\ref{sec:partial-map-images});
%   more verbosely,
%   denoting by $U$ the domain of $\phi$,
%   one has $\phi(S \cap U) = E \cap \phi(U)$.
% \item There is a coordinate system $x_1,\dotsc,x_n$ at $p$ so that $S$
%   is given near
%   $p$ by the equation $x_{d+1} = \dotsb = x_n = 0$;
%   more verbosely,
%   there is an open neighborhood $p \in U \subseteq M$ contained in the domain
%   of the $y_i$
%   for which
%   $S \cap U = \{x \in U : x_{d+1}(x) = \dotsb = x_n(x) = 0\}$. 
% \end{enumerate}
% (To go from (2) to (1), take for $E$ the standard
% inclusion of $\mathbf{k}^d$ in $\mathbf{k}^n$
% and write $\phi$ as a tuple of components
% $\phi = (x_1,\dotsc,x_n)$.
% For the opposite direction, choose a basis of $E$.)

The appropriateness of the term ``submanifold''
requires some justification:
\begin{theorem}\label{thm:characterize-submanifold-smooth-structure}
  Let $S$ be a $d$-dimensional submanifold
  of an $n$-manifold $M$, regarded as a topological space
  with the induced topology.
  Then $S$ possess a unique structure of a smooth $d$-manifold
  (i.e., a unique maximal smooth atlas)
  for which the inclusion map
  $\iota : S \rightarrow M$
  is immersive.
\end{theorem}
\begin{proof}
  The uniqueness follows from Corollary \ref{cor:checking-smoothness-via-immersions}: 
  if $\mathcal{A}_1, \mathcal{A}_2$ are two maximal smooth atlases on $S$
  with the property that $(S,d,\mathcal{A}_1)$
  and $(S,d,\mathcal{A}_2)$
  are $d$-manifolds
  for which $\iota$ is immersive,
  then each of the inclusions
  $(S,d,\mathcal{A}_i) \hookrightarrow M$ is smooth,
  so by Corollary \ref{cor:checking-smoothness-via-immersions},
  the identity maps
  $(S,d,\mathcal{A}_1) \rightarrow (S,d,\mathcal{A}_2),
  (S,d,\mathcal{A}_2) \rightarrow (S,d,\mathcal{A}_1)$
  are smooth two-sided inverses of each other;
  this shows that any smooth chart for $\mathcal{A}_1$
  is also a smooth chart for $\mathcal{A}_2$, and vice-versa,
  so we may conclude by the maximality of $\mathcal{A}_1$ and
  $\mathcal{A}_2$
  that they coincide.
  For the existence,
  we can
  use the local coordinates afforded by the definition of
  ``submanifold'' to define for each $p \in S$
  a smooth atlas $\mathcal{A}_p$ in some neighborhood $U$
  of $p$ for which $U \hookrightarrow M$ is an immersion;
  as $p$ varies, the atlases $\mathcal{A}_p$ are compatible with
  one another thanks
  to the uniqueness assertion shown before, hence their union
  extends to a maximal smooth atlas on $S$ with the required property.
  % note that special case in which $M$ is open in
  % $\mathbf{k}^n$ and $S = \mathbf{k}^d \cap M$ follows from
  % Example ??? (because $S$ is then open in $\mathbf{k}^d$, and
  % the inclusion $\mathbf{k}^d \hookrightarrow \mathbf{k}^n$ is
  % clearly immersive).  In general, we can find a family
  % $(\phi_i)$ of smooth charts $\phi_i$ on $M$ whose domains
  % cover $S$ and for which
  % $\phi_i(S) = \mathbf{k}^d \cap \phi_i(M)$. By the special case
  % already considered, there is an smooth atlas $\mathcal{B}_i$
  % on $\phi_i(S)$ for which $\phi_i(S) \hookrightarrow \phi_i(M)$
  % is an immersion.
  % Set $\mathcal{A}_i := \{\psi \circ \phi_i|_{S} : \psi \in
  % \mathcal{B}_i\}$.
  % Then $\mathcal{A}_i$ is an atlas on $S \cap \dom(\phi_i)$
  % for which $S \cap \dom(\phi_i) \hookrightarrow \dom(\phi_i)$
  % is an immersion.
  % By the already proven uniqueness,
  % any two atlases $\mathcal{A}_i, \mathcal{A}_j$
  % are compatible,
  % so we can extend their union to a maximal atlas
  % $\mathcal{A}$
  % with the required property.
\end{proof}

By Corollary \ref{cor:checking-smoothness-via-immersions},
we immediately obtain:
\begin{proposition}\label{prop:smoothness-preserved-codomain-pass-to-submfld}
  Let $f : M \rightarrow N$ be a smooth map of manifolds
  whose image is contained
  in some submanifold $S \subseteq N$.
  Then the induced map $f : M \rightarrow S$
  is also smooth.
\end{proposition}

\begin{remark}\label{rmk:submfld-locally-closed}
  A submanifold need not be open (think $\mathbf{k}^1
  \hookrightarrow \mathbf{k}^2$)
  and need not be closed
  (think $(0,1) \hookrightarrow \mathbb{R}$),
  but is always \emph{locally closed}
  in the following equivalent senses (as follows immediately
  from its local description):
  \begin{enumerate}
  \item $S$ is open in its closure in $M$.
  \item $S$ is the intersection of a closed subset of $M$ and an
    open subset of $M$.
  \item For each $p \in S$ there is an open neighborhood $p \in
    U \subseteq M$
    so that $S \cap U$ is closed in $U$.
  \end{enumerate}
\end{remark}

\begin{exercise}
  Let $S,M$ be manifolds and let $\iota : S \rightarrow M$ be an
  injective immersion with the property:
  \begin{itemize}
  \item for each $x \in M$ and each open
    $x \in U_1 \subseteq M$, there exists an open
    $x \in U \subseteq U_1 \subseteq M$
    so that the open subset $\iota^{-1}(U)$ of $S$
    is connected.
  \end{itemize}
  Show that $\iota(S)$ is a submanifold of $M$ and that $\iota$ is a diffeomorphism onto its image.
\end{exercise}

\subsection{A criterion for being a submanifold}
\label{sec:org6e7a46d}
In this section we record a handy criterion for determining when
a subset is actually a submanifold.
It amounts to the implicit function theorem from multivariable calculus.


\begin{proposition}\label{prop:submfld-criterion}
  Suppose given an $n$-manifold $M$
  and a natural number $d \leq n$.
  Let $S \subseteq M$ be a subset with the property
  that for each $p \in S$
  there are $m := n - d$ smooth functions
  $f_1,\dotsc,f_m : M \xdashrightarrow{} \mathbf{k}$, defined at
  $p$, so that
  \begin{enumerate}
  \item $S$ is given near $p$ by the equation
    $f_1 = \dotsb = f_m = 0$ (cf. \S\ref{sec:submflds}), and
  \item $(f_1,\dotsc,f_m) : M \xdashrightarrow{} \mathbf{k}^m$ is
    submersive at $p$.
  \end{enumerate}
  Then $S$ is a $d$-dimensional submanifold of $M$.
\end{proposition}
\begin{proof}
  This is immediate from the local description of submersive maps given in \S\ref{sec:linearization-subimm}.
\end{proof}

% \begin{remark}
% The proof (cf. the statement of Theorem \ref{thm:implicit-function-thm-euclidean})
%  also shows that if the rightmost $m \times m$ minor of
% $T_p f$ is nonvanishing, then $S$ may be given near $p$ in the
% parametric form
% \begin{equation*}\{(x_1,\dotsc,x_d,\phi_{d+1}(x_1,\dotsc,x_d), \dotsc,
% \phi_n(x_1,\dotsc,x_d)\}\end{equation*}
% for some smooth
% functions $\phi_{d+1},\dotsc,\phi_n : \mathbf{k}^d
% \xdashrightarrow{} \mathbf{k}$
% defined at $(p_1,\dotsc,p_d) \in \mathbf{k}^d$.
% \end{remark}

\begin{remark}
  The proposition is not an ``if and only if.''
  For example, consider $S := \{0\} \subseteq M := \mathbf{k}$.
  Clearly $S$ is a $0$-dimensional submanifold.
  On the other hand, one can (unwisely) define
  $S$ inside $M$ by the equation $f_1 = 0$,
  where $f_1(x) := x^2$.
  For this choice, the hypotheses of Proposition \ref{prop:submfld-criterion} fail
  because $f_1$ is not submersive at $0$: its derivative $2 x$ vanishes there.
\end{remark}

\subsection{Computing tangent spaces of submanifolds\label{sec:tangent-space-submfld}}
\label{sec:orgcf4ffee}
Let $M$ be an $n$-manifold and $S \subseteq M$
a $d$-dimensional submanifold.
For each $p \in S$,
the tangent space $T_p S$ then identifies with a $d$-dimensional vector
subspace of the $n$-dimensional vector space $T_p M$.
The following is computationally helpful:
\begin{proposition}\label{prop:compute-tangent-space-submfld}
  Suppose $S$
  is given near $p \in S$
  by a system of smooth equations
  \begin{equation}\label{eq:tangent-computation-original-system}
    f_1 = \dotsb = f_m = 0,
  \end{equation}
  where $m := n-d$ and $f := (f_1,\dotsc,f_m) : M \xdashrightarrow{}
  \mathbf{k}^m$
  is defined and submersive at $p$.
  Then $T_p(S)$ coincides with the space
  $\ker(T_p f)$
  of solutions to the system of linear
  equations obtained by differentiating
  \eqref{eq:tangent-computation-original-system}.
  Thus in local coordinates $x_1,\dotsc,x_n$
  at $p$,
  \begin{equation}\label{eq:tangent-space-as-solutions-to-linear-eqns}
    T_p(S)
    = 
    \left\{  (d x_1, \dotsc, d x_n) \in \mathbf{k}^n :
      \sum_{j=1}^{n}
      \frac{\partial f_i}{\partial x_j}(p) d x_j
      = 0 \text{ for } i=1..m
    \right\}.
  \end{equation}  
\end{proposition}
\begin{proof}
  This is again immediate from the local description
  of submersive maps.
\end{proof}

\subsection{Summary of how to work with submanifolds}
\label{sec:org75b7188}
Let $M$ be an $n$-manifold
and $S$ a subset that one expects is a $d$-dimensional submanifold.
Let's take a moment to explain how in practice one goes about
verifying this and computing tangent spaces.
First of all, the problem is local,
so for each point $p \in S$,
one fixes local coordinates
$x_1,\dotsc,x_n$ for $M$ at $p$.
(If $M$ is an open subset of $\mathbf{k}^n$, then one can just use
the default
global coordinates.)
Next,
one expresses $S$ near $p$
as the solution set of some smooth system $f_1 = \dotsb = f_m = 0$,
where $m := n - d$.
Next, one computes by hand the space $V$
of
solutions to the system of linear equations
arising in \eqref{eq:tangent-space-as-solutions-to-linear-eqns}.
If it happens that $\dim(V) = d$,
then it follows from Propositions \ref{prop:submfld-criterion} and \ref{prop:compute-tangent-space-submfld}
that $\dim \ker(T_p f) = d$,
hence that $T_p f$ is surjective, i.e.,
that $f$ is submersive at $p$,
hence that $S$ is a $d$-dimensional submanifold
and that $T_p S = V$
as subspaces of $T_p M \cong \mathbf{k}^n$.

\begin{example}
  Let $M = \mathbb{R}^3$
  and
  \begin{equation*}
    S := \{(x,y,z)  \in M : x^2 + y^2 + z^2 = 1\}.
  \end{equation*}
  Thus $S$ is defined by $f = 0$,
  where $f(x,y,z) := x^2 + y^2 + z^2 - 1$.
  The derivative of $f$ at a point $(x,y,z) \in S$ is
  the linear map $T_{(x,y,z)}f : \mathbb{R}^3 \rightarrow
  \mathbb{R}$
  given by
  \begin{equation*}
    T_{(x,y,z)} f(d x, d y, d z)
    =
    2 x \, d x + 2 y \, d y + 2 z \, d z.
  \end{equation*}
  Since at least one of $x,y,z$ is nonzero,
  we see that $T_{(x,y,z)} f$ is surjective,
  hence that $f$ is submersive
  at all points of $S$.
  Therefore $S$ is a submanifold.
  Its tangent space
  is given at a point $(x,y,z) \in S$ by
  \begin{equation*}
    T_{(x,y,z)} S
    = \{(d x , d y , d z) \in \mathbb{R}^3 :
    2 x \, d x + 2 y \, d y + 2 z \, d z = 0
    \},
  \end{equation*}
  which is a translate of  (as expected)  the plane tangent to $S$ at $(x,y,z)$ in the familiar geometric sense.
\end{example}

\section{Some review of differential equations\label{sec:diffeq}}
\label{sec:orgec9f688}
The following results will be needed only briefly in the
course;
we record them here as a reference for completeness.  

Suppose given a
continuous
map
$f : \mathbf{k} \times \mathbf{k}^n \xdashrightarrow{}
\mathbf{k}^n$,
with the first coordinate regarded as the ``time'' variable,
the second as the ``position'' variable, and elements of the range as ``velocities.''
We suppose given an initial time $t_0 \in \mathbf{k}$ and an initial position $y_0 \in \mathbf{k}^n$ 
for which (of course)
$(t_0,y_0) \in \dom(f)$,
and consider the existence and uniqueness
problem for the linear ordinary differential equation (ODE)
\begin{equation}\label{eq:linear-ode}
  y(t_0) = y_0,
  \quad
  y'(t) = f(t,y(t))
  \text{ for all } t \in U.
\end{equation}
\begin{example}
  If $n = 1$
  and $f(t,y) := y$
  and $(t_0,y_0) := (0,1)$,
  then we are considering
  the problem $y(0) = 1, y'(t) = y(t)$,
  for which it is well-known that the unique solution
  is the exponential map $y(t) := \exp(t) = \sum_{n=0}^{\infty} t^n/n!$.
\end{example}
\begin{theorem}
[Uniqueness]
  Assume that $f$ is uniformly Lipschitz in the second
  variable:
  \begin{equation*}
  |f(t,y) - f(t,z)| \leq C |y - z|.
  \end{equation*}
  Then for any convex open set $T$ containing
  $t_0$,
  there is at most one continuously differentiable
  $y : T \rightarrow \mathbf{k}^n$ satisfying
  \eqref{eq:linear-ode}.
\end{theorem}
\begin{proof}
  If $y(t), z(t)$ are two solutions to \eqref{eq:linear-ode}
  defined on $U$,
  then
  their difference
  $w(t) := y(t) - z(t)$
  satisfies
  $w(t_0) = 0$
  and
  \begin{equation*}
|w'(t)| = |f(t,y(t)) - f(t,z(t))| \leq
  C |y(t) - z(t)| = C |w(t)|.
\end{equation*}
  Our aim is to show that the vanishing set $\Omega := \{t \in T : w(t) =
  0\}$
  is in fact all of $T$.
  Since $\Omega$ is nonempty (it contains $t_0$)
  and closed ($w$ is continuous)
  and since $T$ is connected,
  it will suffice to verify $\Omega$ is open.
  The mean value theorem
  implies that for each each $t_1,t_2 \in T$
  there is some $t$
  on the line segment connecting $t_1$ and $t_2$
  so that
  \begin{equation}\label{eq:strong-condition-that-implies-vanishing}
    |w(t_1) - w(t_2)| \leq |w'(t)| \cdot |t_1
    - t_2| \leq C |w(t)| \cdot |t_1 - t_2|.
  \end{equation}
  We apply \eqref{eq:strong-condition-that-implies-vanishing}
  with $t_1 \in \Omega$  (so that $w(t_1) = 0$)
  and with $t_2$ in a closed ball $B \subseteq T$ with origin $t_1$
  and radius at most $1/(2 C)$
  to obtain
  $|w(t_2)| \leq (1/2) M$ with $M := \max_{t \in B} |w(t)|$.
  Since $t_2 \in B$ was arbitrary, it follows that $M \leq (1/2) M$,
  hence that $M = 0$,  hence that $B \subseteq \Omega$,
  hence that $\Omega$ is open at $t_1$, as required.
\end{proof}
\begin{theorem}[Existence]\label{thm:existence}
  There exists an open ball $T$ with origin $t_0$ and a continuously
  differentiable solution $y : T \rightarrow \mathbf{k}^n$ to
  \eqref{eq:linear-ode}.  If $f$ is smooth, then so is $y$.
\end{theorem}
\begin{proof}
  Let $T_1$ be a ball with origin $t_0$
  and let $Y$ be a ball with origin $y_0$
  so that $f$ is defined on
  $T_1 \times Y$.
  Let $T$ be a ball with origin $t_0$
  so that
  \begin{equation*}
  \radius(T) \cdot \max_{T_1 \times Y} |f| < \radius(Y).
  \end{equation*}
  We will show that a solution $y$ exists with domain $T$.  To
  that end, let $\eps > 0$ be small.  Define as follows a
  function $y_\eps : T \rightarrow Y \subseteq \mathbf{k}^n$:
  \begin{enumerate}
  \item Set
    \begin{equation}\label{eq:diff-eqn-euler-initial-cond}
      y_\eps(0) := y_0.
    \end{equation}
  \item Define $y_\eps$ on integral multiples $n \eps \in T$ of
    $\eps$ 
    inductively
    by requiring that
    \begin{equation}\label{eq:diff-eqn-euler-initial-step}
    y_\eps((n+1) \eps)
    = y_\eps(n \eps) + \eps f(n \eps, y(n \eps)).
  \end{equation}
  This makes sense: our construction of $T$
    implies that $y(n \eps) \in Y$ for all $n \eps \in T$.
    (This is the ``Euler method'' for solving ODEs.)
  \item
    Define $y_\eps(t)$ for general $t \in T$
    by rounding $t$ to the nearest integral multiple
    $n \eps \in T$ of $\eps$
    and setting $y_\eps(t) := y_\eps(n \eps)$.
  \end{enumerate}
  Using the elementary consequences
  \begin{equation*}
  y_\eps(t) \ll 1
  \end{equation*}
  and
  \begin{equation*}
  |y_\eps(t) - y_\eps(s)| \ll |t - s| + o_{\eps \rightarrow 0}(1)
  \end{equation*}
  of the construction of $y_\eps$
  and arguing as in the proof of Arzeli--Ascoli,
  we obtain a
  sequence $\eps_j \rightarrow 0$
  and a
  bounded Lipschitz function $y : T \rightarrow Y$
  (given by the uniform limit $y(t) = \lim_{j \rightarrow \infty} y_{\eps_j}(t)$)
  satisfying $y(0) = y_0$ and, for all $t_1,t_2 \in T$ with $t_1 \leq t_2$,
  \begin{align*}
    y(t_2) - y(t_1)
    &=
  \lim_{j \rightarrow \infty}
      y_{\eps_j}(t_2) - y_{\eps_j}(t_1)
      \\
    &=
  \lim_{j \rightarrow \infty}
  \sum _{
    \substack{
      n \in \mathbb{Z} : \\
      n \eps_j \in [t_1,t_2]
    }
  }
    \eps_j f(n \eps_j, y_{\eps_j}(n \eps_j))
    \\
    &=
  \lim_{\eps \rightarrow 0}
  \sum _{
    \substack{
      n \in \mathbb{Z} : \\
      n \eps \in [t_1,t_2]
    }
  }
    \eps f(n \eps, y(n \eps))
    \\
    &=
  \int_{t_1}^{t_2} f(t,y(t)) \, d t.
  \end{align*}
  By the fundamental theorem of calculus, we
  conclude that $y$ is differentiable and satisfies \eqref{eq:linear-ode}.
 Note finally that if $f$ is smooth, then iterated application
  of the differential equation implies that $y$ is also smooth.
\end{proof}
\begin{example}
  A simple (and well-known) example
  illustrating the necessity of taking $T$ sufficiently small
  is when
  $f : \mathbf{k} \times \mathbf{k}^2 \rightarrow \mathbf{k}^2$
  is given by
  $f(t,x) := (x_1^2,x_1 x_2)$;
  for $y_0 = (u,v) \in \mathbf{k}^2$
  with $u \neq 0$
  and $t_0 := 0$,
  the unique solution $y$ to \eqref{eq:linear-ode}
  is given for $t$ in a neighborhood of $t_0$ by
  \begin{equation*}
    y(t)
    = (\frac{u}{1 - t u}, \frac{v}{1 - t u}),
  \end{equation*}
  which blows up as $t \rightarrow 1/u$.
\end{example}

We finally discuss the dependence of the solution $y$
under ``smooth deformation of parameters'' in the initial
condition
or the differential equation.
\begin{theorem}
[Smooth dependence of solutions]
  Let $\Pi$
  be
  an  open subset of some
  Euclidean space.
  Let
\begin{equation*}
  f : \mathbf{k} \times \mathbf{k}^n \times \Pi \xdashrightarrow{} \mathbf{k}^n
\end{equation*}
\begin{equation*}
  y_0 : \Pi \xdashrightarrow{} \mathbf{k}^n
\end{equation*}
be smooth.
Suppose given an initial time $t_0 \in \mathbf{k}$ and initial parameter
$\pi_0 \in \Pi$
so that $y_0$ is defined at $\pi_0$
and $f$ is defined at
$(t_0,y_0(\pi_0),\pi_0)$.
Then there exist open balls
$T \subset \mathbf{k}$ with origin $t_0$
and $\Pi_0 \subseteq \Pi$ with origin $\pi_0$
and a smooth solution $y : T \times \Pi_0 \rightarrow
\mathbf{k}^n$
to the differential equation
\begin{equation}\label{eq:diffeq-with-params}
    \frac{\partial}{\partial t} y(t,\pi)
  = f(t,y(t,\pi),\pi).
\end{equation}
\end{theorem}
\begin{proof}
  Arguing as in the proof of Theorem \ref{thm:existence} and using
  only the continuity of $f$,
  we may choose $T,\Pi_0$
  as above and a ball $Y \subseteq \mathbf{k}^n$
  so that
  \begin{enumerate}
  \item $f$ is defined on a neighborhood of some compact set containing $T \times Y \times \Pi_0$,
  \item $y_0$ is defined on $\Pi_0$,
    and
  \item for each $\pi \in \Pi_0$, the ball in $\mathbf{k}^n$
    with origin $y_0(\pi)$ and radius $R$, where
    $R := \radius(T) \cdot \max_{T_1 \times Y \times \Pi_0}|f|$,
    is contained in a compact subset of the interior of $Y$.
  \end{enumerate}
  Running through the proof of Theorem \ref{thm:existence},
  we obtain a function $y : Y \times \Pi \rightarrow Y$
  that is smooth in the first variable
  and satisfies \eqref{eq:diffeq-with-params}.
  We now verify that $y$ is smooth in the second
  variable.
  By a compactness argument, it will suffice
  (after possibly shrinking $\Pi_0$ a bit)
  to verify
  that each $\pi_1 \in \Pi_0$ is
  contained in a small ball $\Pi_1 \subseteq \Pi_0$
  on which $y$ is smooth.
  To that end, fix $d \geq 1$
  and consider for $\pi \in \Pi_1$
  the Taylor series
  \begin{equation*}
    y_0(\pi)
    =
    \sum_{\alpha \leq |d|}
    (\pi - \pi_1)^\alpha y_0^{(\alpha)}(\pi_1)
    + O(|\pi - \pi_1|)^{d + 1},
  \end{equation*}
  \begin{equation*}
    f(t,y,\pi)
    =
    \sum_{\alpha \leq |d|}
    (\pi - \pi_1)^\alpha 
    f^{(\alpha)}(t,y,\pi_1)
    + O(|\pi - \pi_1|)^{d + 1}.
  \end{equation*}
  The errors are uniform
  thanks to the property (1) of $f$.
  Running through the proof of Theorem \ref{thm:existence}
  and staring
  at
  \eqref{eq:diff-eqn-euler-initial-cond}
  and
  \eqref{eq:diff-eqn-euler-initial-step} for a bit,
  % that if we vary $y_0,f$
  % in the supremum norm by some sufficiently small $\delta > 0$,
  % then the solution $y$ varies in the supremum norm
  % by at most $O(\delta)$.
  % and making use of the observation above
  % (the one involving ``$\delta$''),
  we obtain an expansion
  \begin{equation*}
    y(t,\pi)
    = \sum_{\alpha \leq |d|}
    (\pi - \pi_1)^\alpha 
    y^{(\alpha)}(t,\pi_1)
    + O(|\pi - \pi_1|)^{d + 1}.
  \end{equation*}
  %where
  %$y^{(\alpha)}$
  %is derived from $y_0^{(\alpha)},f^{(\alpha)}$
  %as $y$ was from $y_0,f$.
  %It follows the partial derivatives of $y$ exist (and are given by the $y^{(\alpha)}$),
  Thus $y$ is smooth in the second variable.
  By iterating the differential equation we conclude that $y$ is jointly smooth in both variables.
\end{proof}

\section{Some review of group theory}
\label{sec:orgaef96e9}
\subsection{Basic definition}
\label{sec:orgd7903af}
Recall that a \emph{group}
is a tuple $(G,m,i,e)$, often abbreviated simply by $G$,
where
\begin{enumerate}
\item $G$ is a set,
\item $m : G \times G \rightarrow G$ is a map
  called \emph{multiplication}
  and abbreviated $x y := m(x,y)$,
\item $i : G \rightarrow G$
  is a map called \emph{inversion}
  and abbreviated $x^{-1} := i(x)$,
  and
\item $e \in G$ is an element called the \emph{identity} element
\end{enumerate}
and so that the usual axioms of group theory
are satisfied; we do not recall them here.
For example, the associativity axiom reads
$m(x,m(y,z)) = m(m(x,y),z)$.

\subsection{Permutation groups}
\label{sec:orgcf966cd}
One of the first examples of groups
encountered in a basic course
is the symmetric group $S(n)$.
More generally,
one considers for any set $X$
the permutation group $\operatorname{Perm}(X)$,
defined to consist of bijections $\sigma : X \rightarrow X$
and with the group law given by composition:
$\sigma_1 \sigma_2 := \sigma_1 \circ \sigma_2$.
For example, $S(n) = \operatorname{Perm}(\{1,\dotsc,n\})$.

A particularly concrete class of groups
are the subgroups of the form $G \leq \operatorname{Perm}(X)$
for some set $X$.
Cayley's theorem
asserts that \emph{every} group is isomorphic
to one of this form:
indeed, one can take $X = G$
and define $G \hookrightarrow \operatorname{Perm}(G)$
via $g \mapsto [x \mapsto g x]$.
Moreover, if $G$ is finite, then one can take $X$ to be finite.

\subsection{Topological groups}
\label{sec:orgaff9727}
One reason to phrase the definition in the above way is that
it places the emphasis on the maps $m,i$.  By equipping $G$ with
some additional structure and then requiring that those maps
respect such structure, one obtains interesting classes of
groups.  For example:
\begin{definition}\label{defn:top-gp}
 A \emph{topological group} is defined to
be a group $G = (G,m,i,e)$ equipped with the structure of a
topological space and for which the maps $m, i$ are continuous. 
A \emph{morphism of topological groups} $f : G \rightarrow H$
is a continuous group homomorphism.
An \emph{action} of a topological group $G$ on a Hausdorff topological space
$X$ is a continuous map $\alpha : G \times X \rightarrow X$
with the property that
$\alpha(e,x) = x$
and
$\alpha(g_1 g_2,x) = \alpha(g_1,\alpha(g_2,x))$;
one typically abbreviates $g x := \alpha(g,x)$.
\end{definition}
This definition is simple, but already fairly rich:
\begin{exercise}\label{exercise:openness-top-groups-ambient-spaces}
  Let $X$ be a topological space.
  Let $G \subseteq X$ be a topological group
  that is also a subspace of $X$, equipped with the induced
  topology.
  Suppose there is an open $U
  \subseteq X$
  for which $e \in U \subseteq G$,
  where $e$ denotes the identity element of $G$.
  Show that $G$ is open in $X$.
\end{exercise}
\begin{exercise}
  Let $G$ be a topological group.  Let $H \leq G$ be a subgroup.
  Suppose that $H$ is \emph{locally closed} in $G$ in the sense
  that there is a neighborhood $U \subseteq G$ of the identity
  with the property that $H \cap U$ is closed in $U$.  Show that
  $H$ is closed in $G$.
\end{exercise}
\begin{exercise}
  Let $G$ be a topological group,
  and $H \leq G$ an open subgroup.
  Show that $H$ is closed.
\end{exercise}
\begin{exercise}\label{exercise:connected-topological-group-generated-by-any-neighborhood}
  Let $G$ be a connected topological group,
  and let $U$ be a neighborhood of the identity.
  Show that $U$ generates $G$.
\end{exercise}
\begin{exercise}
  Let $G$ be a topological group,
  and let $H \leq G$ be a subgroup.
  Equip the set $G/H$ with the quotient topology.
  Show that the following are equivalent:
  \begin{enumerate}
  \item $H$ is closed.
  \item $G/H$ is Hausdorff.
  \end{enumerate}
  [Hint:
  If $H$ is closed,
  show first that for each $g \in G - H$
  there is a neighborhood $U$ of the identity in $G$
  so that $U^{-1} g U \cap H = \emptyset$.]
\end{exercise}
\begin{exercise}
  Let $G$ be locally compact topological group,
  let $g \in G$,
  and let  $V \subseteq G$ be a neighborhood of $g$.
  Show that there is an open neighborhood
  $U \subseteq G$ of $e$
  so that
  \begin{enumerate}
  \item  $\overline{U}$ is compact,
  \item $U = U^{-1}$,
  \item $U^{n} g \subseteq V$ for all $n \leq 100$, and
  \item $g U^{n} \subseteq V$ for all $n \leq 100$.
  \end{enumerate}
  Here
  $U^{n} := \{u_1 \dotsb u_{n} : u_1,\dotsc,u_{n} \in U\}$.
\end{exercise}
\begin{exercise}
  Let $G$ be a second countable topological group.
  Let $U \subseteq G$ be a subset with nonempty interior.
  Show that there is a sequence $g_n \in G$
  so that $G = \cup U g_n = \cup g_n U$.
\end{exercise}
\begin{exercise}
  Let $G$ be a topological group,
  let $X$ be a Hausdorff  topological space,
  and suppose given a transitive action of $G$ on $X$.
  Let $U$ be a compact subset of $G$,
  and let $x \in X$.
  Show that $U x$ is closed.
\end{exercise}
\begin{exercise}
  Say that a topological space $X$ is \emph{countable at
    infinity}
  if $X$ can be written as a countalbe union of compact subsets.

  Show that if $X$ is locally  compact and second-countable,
  then $X$ is countable at infinity.
\end{exercise}
\begin{exercise}\label{exercise-topological-groups-quotient-map-homeomorphism}
  Let $G$ be a topological group,
  let $X$ be a Hausdorff  topological space,
  and suppose given a transitive action of $G$ on $X$.
  Let $x \in X$.
  Show that the stabilizer $H := \{g \in G : g x = x\}$ is a closed
  subgroup of $G$.
  Suppose that
  \begin{enumerate}
  \item $G$ is locally compact and is countable at infinity, and
  \item $X$ is locally compact.
  \end{enumerate}
  Show that the map $\pi : G/H \rightarrow X$ given by
  $\pi(g) := g x$ is a homeomorphism.
  [It suffices to show that
  $\pi$ is open.
  Use some of the previous exercises
  together with the following variant of the Baire
  category theorem: if a locally compact topological space $E$
  is a countable union of closed subsets $E_n$, then some $E_n$
  has nonempty interior.]
\end{exercise}

\section{Some review of functional analysis}
\label{sec:org66cd91a}
\subsection{Definitions and elementary properties of operators on Hilbert spaces}
\label{sec:org3082d00}
\begin{definition}
  Recall that an \emph{operator} on a Hilbert space (real or complex)
  $V$ is a linear map $T : V \rightarrow V$.
  It is \emph{bounded} if $\sup_{x \in V : |x| = 1} \|T x\| <
  \infty$,
  \emph{self-adjoint}
  if $\langle T x, y \rangle = \langle x, T y \rangle$
  for all $x,y \in V$,
  and \emph{compact}
  if $T x_n$ has a convergent subsequence
  whenever $x_n$ is a bounded sequence in $V$.
  An \emph{eigenvector} of $T$ is a nonzero element $v \in V$
  for which $T v = \lambda v$ for some scalar $\lambda$,
  called the \emph{eigenvalue}.
\end{definition}
\begin{lemma}
  Let $T$ be a self-adjoint operator on a Hilbert space $V$.
  \begin{enumerate}
  \item The eigenvalues of $T$ are real.
  \item The eigenspaces of $T$  are orthogonal to
    one another.
  \item If $T$ acts on a subspace $U$ of $V$,
    then it acts also on the orthogonal complement $U^\perp$.
  \end{enumerate}
\end{lemma}
\begin{proof}
~
  \begin{enumerate}
  \item
    If $T u = \lambda u$,
    then
    $\lambda \langle u, u \rangle = \langle T u, u \rangle
    = \langle u, T u \rangle = \overline{\langle T u, u \rangle}
    = \overline{\lambda } \langle u, u \rangle$.
  \item 
    If moreover $T v = \lambda ' v$,
    then
    $\lambda \langle u,v \rangle = \langle  T u, v \rangle
    = \langle u, T v \rangle = \lambda ' \langle u,v \rangle$,
    and so either $\lambda ' = \lambda$
    or $\langle u,v \rangle = 0$.
  \item
    Suppose $T u \in U$ for all $u \in U$.
    Let $v \in U^\perp$.
    For $u \in U$,
    we have $T u \in U$,
    and so $\langle T v, u \rangle = \langle v, T u \rangle = 0$.
    Thus $T v \in U^\perp$.
  \end{enumerate}
\end{proof}

\subsection{Compact self-adjoint operators on nonzero Hilbert spaces have eigenvectors}
\label{sec:org816b98f}
\begin{theorem}\label{thm:compact-self-adj-has-eigencetor}
  Let $V$ be a nonzero Hilbert space.
  Let $T$ be a compact self-adjoint operator on $V$.
  Then $T$ has an eigenvector.
\end{theorem}
The basic idea of the proof can seen most transparently when $V$ is a
finite-dimensional real Hilbert space:
if $x$ is an element of the unit sphere in $V$
at which $\langle T x, x \rangle$ assumes a local maximum,
then the first derivative test implies that for any $v$ orthogonal to $x$,
\begin{equation*}
0 = \frac{d}{d \eps } \langle T (x + v), x + v)
\rangle|_{\eps=0} = \langle T v, x \rangle + \langle T x, v
\rangle = 2 \langle T x, v \rangle.
\end{equation*}
It follows that $T x \in (x^\perp)^\perp = \mathbb{R} x$, and
so $x$ is the required eigenvector.  

To adapt the argument to the infinite-dimensional
case, we replace the role of differential calculus
with some artful application of the parallelogram law
\begin{equation*}
4 \Re  \langle T x, y \rangle
= \langle T(x+ y), x + y  \rangle
- \langle T(x- y), x- y \rangle.
\end{equation*}
One of the steps en route to the solution
is of independent interest:
\begin{lemma}
  Let $T$ be a self-adjoint operator on a Hilbert space $V$.
  Then
  \begin{equation*}
  \sup_{x \in V : |x| = 1} |\langle T x, x \rangle|
  =
  \sup_{x,y: |x|=|y| = 1} |\langle T x, y \rangle|.
  \end{equation*}
\end{lemma}
\begin{proof}
  Denote by $M$ the LHS and by $M'$ the RHS.
  Clearly $M \leq M'$.
  Conversely,
  for $x,y \in V$ with $|x| = |y| = 1$
  and $\theta \in \mathbb{C}^{(1)}$
  chosen so that
  $\langle T x, \theta y \rangle$ is real,
  the parallelogram law applied to $x$ and $\theta y$
  gives
  \begin{equation*}
  4 \theta  \langle T x, y \rangle
  = \langle T(x+ \theta  y), x + \theta y  \rangle
  - \langle T(x-\theta y), x-\theta y \rangle.
  \end{equation*}
  From this it follows that $M' \leq M$.
\end{proof}

\begin{proof}
[Proof of Theorem \ref{thm:compact-self-adj-has-eigencetor}]
  Since $T$ is compact, it is
  bounded, and so the quantity
  \begin{equation*}
  M := \sup_{x \in V : |x| = 1} |\langle T x, x \rangle|
  \end{equation*}
  is finite.
  If $M = 0$, then the self-adjointness
  of $T$
  and the parallelogram law applied to $x \in V$ and $y := T x$
  implies
  that
  \begin{equation*}
4 \|T x\|^2
  =
  \langle T (x+y), x+y \rangle -
  \langle T (x-y), x-y \rangle
  = 0,
\end{equation*}
  so $T$ is the zero operator
  and any nonzero element of $V$ 
  is an eigenvector.
  We turn to the remaining case $M \neq 0$.
  Recall from the lemma that $M$ coincides with the operator norm
  $\sup_{x,y: |x|=|y| = 1} |\langle T x, y \rangle|$.
  There
  is thus a nonzero real number $\lambda = \pm M$ and a sequence
  of unit vectors $x_n$ so that
  \begin{equation*}
\langle T x_n, x_n  \rangle \rightarrow \lambda,
\end{equation*}
  \begin{equation*}
\langle T x_n, T x_n \rangle \leq \lambda^2.
\end{equation*}
  It follows then from the identity
  \begin{equation*}
  \|T x_n - \lambda x_n\|^2
  = \langle T x_n, T x_n \rangle
  - 2 \lambda  \langle T x_n, x_n \rangle
  + \lambda^2
  \end{equation*}
  that
  \begin{equation}\label{eq:some-convergence-of-T-x-n}
    T x_n - \lambda x_n \rightarrow 0.
  \end{equation}
  Since $T$ is compact, the sequence $T x_n$ has a subsequential
  limit $y$.
  By \eqref{eq:some-convergence-of-T-x-n},
  one has
  $|y| = |\lambda|$, hence $y \notin 0$.
  By applying $T$ to \eqref{eq:some-convergence-of-T-x-n},
  one obtains
  $T y = \lambda y$.
  Thus $y$ is the required eigenvector of $T$.
\end{proof}

\begin{remark}
  A
  self-adjoint operator on a Hilbert space need not have any
  eigenvectors; consider $f(x) \mapsto x f(x)$ on $L^2([0,1])$.
  In this sense, the compactness assumption is necessary.
\end{remark}

\subsection{Spectral theorem for compact self-adjoint operators on a Hilbert space}
\label{sec:orgaec3806}
\begin{theorem}\label{thm:spectral-thm-self-adj}
  Let $T$ be a compact self-adjoint operator on a Hilbert space
  $V$.
  For $\lambda \in \mathbb{R}$,
  denote by $V_\lambda \leq V$ the $\lambda$-eigenspace of $T$.
  Then $V$ is the Hilbert space orthogonal direct sum
  \begin{equation}\label{eq:spectral-theorem-compact-self-adjoint-1}
    V = \oplus_\lambda V_\lambda
  \end{equation}
  of its kernel $V_0 = \ker(T)$ and its eigenspaces
  $V_\lambda$ with nonzero eigenvalue $\lambda$.  Moreover,
  for any $\eps > 0$,
  \begin{equation}\label{eq:spectral-theorem-compact-self-adjoint-2}
    \dim(\oplus_{\lambda : |\lambda| > \eps} V_\lambda ) < \infty.
  \end{equation}
  %In particular, $\{\lambda \in \mathbb{R} - \{0\} : V_\lambda
  %\neq \{0\} \}$
  %is a discrete subset of $\mathbb{R} - \{0\}$
  %whose image in $\mathbb{R}$ is bounded.
\end{theorem}
\begin{proof}
  The orthogonal complement of $\oplus_\lambda V_\lambda$
  is $T$-stable and contains no eigenvectors for $T$;
  by
  Lemma \ref{eq:spectral-theorem-compact-self-adjoint-1},
  it is the zero space, giving \eqref{eq:spectral-theorem-compact-self-adjoint-1}.

  For each $\eps > 0$, the space
  $\oplus_{\lambda : |\lambda| > \eps} V_\lambda$ admits an
  orthonormal basis of eigenvectors for $T$ with eigenvalues
  of magnitude at least $\eps$; if that basis were to contain an
  infinite sequence, then the image of that sequence under $T$
  would have no convergent subsequence, contradicting the
  compactness of $T$.  This establishes
  \eqref{eq:spectral-theorem-compact-self-adjoint-2}.
\end{proof}

\subsection{Basics on matrix coefficients}
\label{sec:org14df4a0}

\subsection{Finite functions on a compact group are dense\label{sec:finite-functions-dense}}
\label{sec:org39a802e}
% Let $R : G \rightarrow \GL(V)$
% be a representation of a group $G$ on a complex vector space $V$.
% An element $v \in V$ will be called \emph{finite}
% (with respect to the given action of $G$) if the span of
% $\{R(g) v : g \in G\}$ is finite-dimensional.

% We specialize now to the case that
% \begin{enumerate}
% \item $G$ is a compact topological group, and
% \item $V = L^2(G)$.
% \end{enumerate}

Let $G$ be a compact topological group.
Let $\mu$ denote the probability Haar measure on $G$.
We may define $L^2(G)$ with respect to $\mu$.
Denote by $\U(L^2(G))$ the group of unitary operators on $L^2(G)$.
We then have
the right regular representation $R : G \rightarrow \U(L^2(G))$
given by
\begin{equation*}
R(g) f(x) := f(x g)
\end{equation*}
as well as
the left regular representation $L : G \rightarrow \U(L^2(G))$
given by
\begin{equation*}
L(g) f(x) := f(g^{-1} x).
\end{equation*}
We may extend the latter map linearly
to $L : L^1(G) \rightarrow \End(L^2(G))$
given for $\phi \in L^2(G)$
by
\begin{equation*}
L(\phi) f(x) := \int_{g \in G} \phi(g) f(g^{-1} x).
\end{equation*}
\begin{lemma}\label{lem:right-finite-implies-left-finite}
  Let $f \in L^2(G)$.
  If the span of the right translates of $f$
  is finite-dimensional,
  then so is the span of its left translates, and vice-versa.
\end{lemma}
\begin{proof}
  Let $f_1,\dotsc,f_n$ be an orthonormal basis for the span of the right
  translates of $f$.
  Then for each $g \in G$ there are complex coefficients
  $a_1(g),\dotsc,a_n(g)$
  so that for all $x \in G$,
  \begin{equation*}
  R(g) f(x) = f(x g) = \sum_i a_i(g) f_i(x).
  \end{equation*}
  Explicitly, we may take $a_i := \langle R(g) f, f_i \rangle$,
  which defines a bounded function and thus an element
  $a_i  \in L^2(G)$.
  It follows that
  \begin{equation*}
  L(g) f(x) = f(g^{-1} x)
  = \sum_i f_i(g^{-1}) a_i(x),
  \end{equation*}
  thus the $a_i$ span the space of left translates of $f$.
  % Let $V$ denote the span of the right translates
  % of $f$.
  % Let $\delta \in V^*$ be the functional
  % $\delta(v) := v(e)$.
  % Let $G \times G$ act
  % \begin{itemize}
  % \item on $V^* \otimes V$
  %   by the recipe
  %   $(g,h) \cdot \ell \otimes v
  %   := (\ell \circ R[g^{-1}]) \otimes R[h]$, and
  % \item on $L^2(G)$ by
  %   $(g,h) \cdot v := L[g] R[h] v$.
  % \end{itemize}
  % The natural map $V^* \otimes V \rightarrow L^2(G)$ given
  % by $\ell \otimes v \mapsto [g \mapsto \ell(R(x) v)]$ is
  % $G \times G$-equivariant,
  % since
  % \begin{equation*}
  % (\ell \circ R[g^{-1}])(R(x) R(h) v)
  % = \ell( R(g^{-1} x h) v).
  % \end{equation*}
  % Its image is finite-dimensional,
  % $G \times G$-stable and contains $f$,
  % so the span of the left translates
  % of $f$ is finite-dimensional.  The other direction may be argued similarly.
\end{proof}
\begin{definition}
  We say that an element $f \in L^2(G)$
  is \emph{finite}
  if the span of its left and right translates
  under $G$ is finite-dimensional.
  Denote by $L^2(G)_{\fin}$ the space of finite functions.
  (Lemma \ref{lem:right-finite-implies-left-finite} says that to
  check that a given function is finite, it suffices to show
  either that its left translates or its right translates have
  finite span.)
\end{definition}
The main result of this subsection is as follows.
\begin{theorem}\label{thm:finite-elements-dense}
  Let $G$ be a compact group.  Then the finite elements of $L^2(G)$ are dense.
\end{theorem}
The proof requires a couple lemmas.

% \begin{theorem}
%   Let $G$ be a compact group.  Then $L^2(G)_{\fin}$ is dense in $L^2(G)$.
% \end{theorem}
% \begin{proof}
%   Let $G$ act on $L^2(G)$ by the right regular representation.
%   For $\phi \in C_c(G)$
%   and $f \in L^2(G)$,
%   \begin{equation*}
%   T_\phi f(g) := \int_{h \in G} \phi(h) f(g h) \, d \mu(h).
%   \end{equation*}
%   This makes sense and defines a bounded operator $T_\phi$
%   on $L^2(G)$ thanks to the trivial estimate
%   $\|T_\phi f\| \leq \|\phi \| \|f\|$, where $\|.\|$
%   denotes the $L^2$-norm on $G$.
%   (We can even take $\phi \in L^2(G)$, if we wish.)
%   Note that if $\phi$ is real-valued and \emph{symmetric}
%   in the sense that $\phi(x) = \phi(x^{-1})$,
%   then the operator $T_\phi$ is self-adjoint.

%   We show first that elements of $L^2(G)$ of the form $T_\phi v$ with
%   $\phi,v \in L^2(G)$, and with $\phi$ real-valued and symmetric,
%   are dense in $L^2(G)$.
%   Thus, let $v \in L^2(G)$  be arbitrary.
%   By the continuity of the right regular representation $R$,
%   one has for all $g$ in some small neighborhood
%   $U$ of the identity
%   in $G$ that $\|R(g) v - v \| < \eps$.
%   We may assume after shrinking $U$ as necessary that $U = U^{-1}$.
%   By Urysohn's lemma,
%   there exists $\phi \in C_c(U) \subseteq C_c(G)$
%   with $\mu(\phi) = 1$.
%   For such a $\phi$, the required estimate follows
%   from the triangle inequality.
%   We can arrange that $\phi$ be symmetric
%   by averaging it with the function $x \mapsto \phi(x^{-1})$ as necessary.

%   It will thus suffice to show
%   that any element of the form $T_\phi v$
%   with $v,\phi \in L^2(G)$
%   can be arbitrarily approximated
%   by finite functions.
%   For such $\phi$, the operator $T_\phi$ is self-adjoint.
%   We claim that it is also compact.

%   Denote by $L^2(G)_\lambda$ its eigenspaces.
%   Decompose $v = \sum v_\lambda$ with $v_\lambda \in V_\lambda$.
%   For each $\eps > 0$, we then have
%   $T_\phi v = u_\eps + O(\eps)$ where
%   $u_\eps := \sum_{\lambda : |\lambda| > \eps} T v_\lambda$.
%   Since $T_\phi$ is defined using the right regular
%   representation,
%   it commutes with left translation,
%   and so its eigenspaces are invariant by the left regular
%   representation;
%   Theorem \ref{thm:spectral-thm-self-adj}
%   implies that $\dim( \oplus_{\lambda : |\lambda| > \eps}
%   V_\lambda) < \infty$,
%   so the left translates of $u_\eps$ have finite span,
%   and so $u_\eps$ is a finite element of $L^2(G)$.
%   Since it converges to $T v$ as $\eps \rightarrow 0$, we are done.
% \end{proof}

\begin{lemma}\label{lem:approximate-vectors-by-convolutions}
  Set $V := L^2(G)$.
  For each $v \in V$ and $\eps > 0$
  there exists a real-valued symmetric $\phi \in C_c(G)$ so that
  $\|L(\phi) v - v \| < \eps$.
\end{lemma}
\begin{proof}
  By the continuity of the representation $L$,
  one has for all $g$ in some small neighborhood
  $U$ of the identity
  in $G$ that $\|L(g) v - v \| < \eps$.
  We may assume after shrinking $U$ as necessary that $U = U^{-1}$.
  By Urysohn's lemma,
  there exists a real-valued $\phi \in C_c(U) \subseteq C_c(G)$
  with $\mu(\phi) = 1$.
  For such a $\phi$, the required estimate follows
  from the triangle inequality.
  We can easily arrange that $\phi$ be symmetric
  by averaging it with the function $x \mapsto \phi(x^{-1})$.
\end{proof}

\begin{lemma}
  Let $\phi \in L^1(G) \cap L^2(G)$.
  Then the operator $L(\phi)$ is compact.
\end{lemma}
\begin{proof}
  We will use that
  \begin{equation}\label{eq:kernel-is-integrable}
    \int_{g_1,g_2 \in G}
    |\phi(g_1^{-1} g_2)|^2 < \infty
  \end{equation}
  as follows from the compactness of $G$.
  (The natural context for this result is thus
  in the setting of operators defined by kernels
  in $L^2(G \times G)$; see any book on functional analysis.)
  For concreteness, I'll give the proof
  of the compactness of $T := L(\phi)$ in the special case that
  $V := L^2(G)$
  is a separable Hilbert space; this case certainly suffices
  when $G$ is a compact Lie group (and perhaps somewhat more
  generally).
  We will show that the image under $T$ of the unit ball
  is precompact.
  Let $e_1,e_2,\dotsc$ be a Hilbert space basis of $V$.
  Write $A_{i j} := \langle e_i, T e_j \rangle$,
  so that for $v = \sum a_i e_i \in V$,
  one has $T v =
  \sum_{i} b_i e_i$
  where $b_i := \sum_j A_{i j} a_j$.
  If $\|v\| \leq 1$,
  then Cauchy--Schwartz
  implies that $|b_i|^2 \leq B_i$ where $B_i := \sum_{j} |A_{i
    j}|^2$,
  so
  the image under $T$ of the unit ball
  is contained in $S := \{\sum b_i e_i : |b_i|^2 \leq B_i\}$.
  Since
  \begin{equation}\label{eq:}
    \sum B_i =  \sum_{i,j} |A_{i j}|^2 < \infty,
  \end{equation}
  the set $S$ is precompact, as required.
  (If $v^{(n)} = \sum b_i^{(n)} e_i$ is a sequence in $S$,
  then we may assume by a diagonlization argument
  that after passing to a subsequence,
  one has $b_i^{(n)} \rightarrow b_i$ for some scalar $b_i$,
  which obviously satisfies $|b_i|^2 \leq B_i$;
  it then follows easily
  that $v := \sum b_i e_i$ belongs to $V$
  and that $v^{(n)} \rightarrow v$.)
\end{proof}

\begin{proof}
[Proof of Theorem \ref{thm:finite-elements-dense}]
  By Lemma \ref{lem:approximate-vectors-by-convolutions},
  elements of the form $L(\phi) v$ with
  $\phi \in L^1(G), v \in L^2(G)$, and with $\phi$ real-valued and symmetric,
  are dense in $L^2(G)$, so it
  suffices to approximate such elements by finite functions.
  Thus consider some such elements $\phi,v$.
  The operator $T := L(\phi)$ is compact and self-adjoint.
  Denote by $V_\lambda$ its eigenspaces.
  Decompose $v = \sum v_\lambda$ with $v_\lambda \in V_\lambda$.
  For each $\eps > 0$, we then have
  $L(\phi) v = u_\eps + O(\eps)$ where
  $u_\eps := \sum_{\lambda : |\lambda| > \eps} \lambda v_\lambda$.
  Since $T$ commutes with $R(G)$,
  the eigenspaces $V_\lambda$ of $T$ are $R(G)$-invariant;
  Theorem \ref{thm:spectral-thm-self-adj}
  implies that $\dim( \oplus_{\lambda : |\lambda| > \eps}
  V_\lambda) < \infty$,
  so the right translates of $u_\eps$ have finite span,
  and so $u_\eps$ is a finite element of $L^2(G)$.
  Since it converges to $T v$ as $\eps \rightarrow 0$, we are done.
\end{proof}

% \begin{definition}\label{defn:convolution-action}
%   % For $\phi \in L^1(G)$,
%   % define the operator $T_\phi$ on $L^2(G)$
%   % by averaging over the right translates of $f$:
%   % \begin{equation*}
%   % T_\phi f(g) := \int_{h \in G} f(g h) \phi(h) \, d \mu(h).
%   % \end{equation*}
%   % Note that the triangle inequality
%   % $\|T_\phi f\|_{L^2} \leq \|\phi \|_{L^1} \|f\|_{L^2}$
%   % implies that this makes sense.
%   Let $R : G \rightarrow \GL(V)$
%   be a continuous representation on a Banach space $V$.
%   For $\phi \in C_c(G)$,
%   we may define an operator $R(\phi)$ on $V$ by the formula
%   \begin{equation*}
%   R(\phi) v := \int_{g \in G} \phi(g) R(g) v \, d \mu(g).
%   \end{equation*}
%   The definition extends by continuity
%   to $L^1(G)$.
% \end{definition}

% \begin{lemma}\label{lem:approximate-vectors-by-convolutions}
%   Let $R : G \rightarrow  \GL(V)$
%   be as in Definition \eqref{defn:convolution-action}.
%   For each $v \in V$ and $\eps > 0$
%   there exists a real-valued symmetric $\phi \in C_c(G)$ so that
%   $\|R(\phi) v - v \| < \eps$.
% \end{lemma}
% \begin{proof}
%   By the continuity of the representations,
%   one has for all $g$ in some small neighborhood
%   $U$ of the identity
%   in $G$ that $\|R(g) v - v \| < \eps$.
%   We may assume after shrinking $U$ as necessary that $U = U^{-1}$.
%   By Urysohn's lemma,
%   there exists $\phi \in C_c(U) \subseteq C_c(G)$
%   with $\mu(\phi) = 1$.
%   For such a $\phi$, the required estimate follows
%   from the triangle inequality.
%   We can easily arrange that $\phi$ be symmetric
%   by averaging it with the function $x \mapsto \phi(x^{-1})$.
% \end{proof}

% \begin{lemma}
%   Let $R : G \rightarrow  \GL(V)$
%   be a continuous representation of $G$ on a Hilbert space $V$.
%   Let $\phi \in L^2(G)$.
%   Then the operator $R(\phi)$ is compact.
% \end{lemma}
% \begin{proof}
%   We will use that
%   \begin{equation}\label{eq:kernel-is-integrable}
%     \int_{g_1,g_2 \in G}
%     |\phi(g_1^{-1} g_2)|^2 < \infty
%   \end{equation}
%   as follows from the compactness of $G$.
%   (The natural context for this result is thus
%   in the setting of operators defined by kernels
%   in $L^2(G \times G)$; see any book on functional analysis.)
%   For concreteness, I'll give the proof
%   of the compactness of $R(\phi)$ in the special case that
%   $V$ is a separable Hilbert space; this case certainly suffices
%   when $G$ is a compact Lie group (and perhaps somewhat more
%   generally).
%   We will show that the image under $T$ of the unit ball
%   is precompact.
%   Let $e_1,e_2,\dotsc$ be a Hilbert space basis of $V$.
%   Write $A_{i j} := \langle e_i, R(\phi) e_j \rangle$,
%   so that for $v = \sum a_i e_i \in V$,
%   one has $R(\phi) v =
%   \sum_{i} b_i e_i$
%   where $b_i := \sum_j A_{i j} a_j$.
%   If $\|v\| \leq 1$,
%   then Cauchy--Schwartz
%   implies that $|b_i|^2 \leq B_i$ where $B_i := \sum_{j} |A_{i
%     j}|^2$,
%   so
%   the image under $R(\phi)$ of the unit ball
%   is contained in $S := \{\sum b_i e_i : |b_i|^2 \leq B_i\}$.
%   Since
%   \begin{equation}\label{eq:}
%     \sum B_i =  \sum_{i,j} |A_{i j}|^2 < \infty,
%   \end{equation}
%   the set $S$ is precompact.
%   (If $v^{(n)} = \sum b_i^{(n)} e_i$ is a sequence in $S$,
%   then we may assume by a diagonlization argument
%   that after passing to a subsequence,
%   one has $b_i^{(n)} \rightarrow b_i$ for some scalar $b_i$,
%   which obviously satisfies $|b_i|^2 \leq B_i$;
%   it then follows easily
%   that $v := \sum b_i e_i$ belongs to $V$
%   and that $v^{(n)} \rightarrow v$.)
% \end{proof}

% \begin{theorem}
%   Let $G$ be a compact group.  Then the finite elements of $L^2(G)$ are dense.
% \end{theorem}
% \begin{proof}
%   By Lemma \ref{lem:approximate-vectors-by-convolutions},
%   elements of the form $L(\phi) v$ with
%   $\phi \in L^1(G), v \in L^2(G)$, and with $\phi$ real-valued and symmetric,
%   are dense in $L^2(G)$, so it
%   suffices to approximate such elements by finite functions.
%   For such $\phi$, the operator $L(\phi)$ is compact and self-adjoint.
%   Denote by $V_\lambda$ its eigenspaces.
%   Decompose $v = \sum v_\lambda$ with $v_\lambda \in V_\lambda$.
%   For each $\eps > 0$, we then have
%   $L(\phi) v = u_\eps + O(\eps)$ where
%   $u_\eps := \sum_{\lambda : |\lambda| > \eps} \lambda v_\lambda$.
%   Since $L(\phi)$ commutes with $R(g)$ for all $g \in G$,
%   its eigenspaces are invariant by the right regular
%   representation;
%   Theorem \ref{thm:spectral-thm-self-adj}
%   implies that $\dim( \oplus_{\lambda : |\lambda| > \eps}
%   V_\lambda) < \infty$,
%   so the right translates of $u_\eps$ have finite span,
%   and so $u_\eps$ is a finite element of $L^2(G)$.
%   Since it converges to $T v$ as $\eps \rightarrow 0$, we are done.
% \end{proof}

\subsection{Schur orthogonality relations}
\label{sec:orga60f514}
We continue to assume here
that $G$ is a compact group.
\begin{lemma}\label{lem:schur-lemma-for-morphisms}
  Let $(\pi,V), (\rho,W)$ be finite-dimensional irreducible
  representations of $G$.
  Then the space
  \begin{equation*}
  \Hom_G(V,W)
  :=
  \{\phi : V \rightarrow W \lvert \phi(\pi(g) v) = \rho(g) \phi(v) \text{ for all } v \in V, g \in G \}
  \end{equation*}
  of $G$-equivariant linear maps
  from $V$ to $W$ satisfies
  \begin{equation*}
  \dim \Hom_G(V,W)
  = 
\begin{cases}
    1 & V \cong W \\
    0 & \text{otherwise.}
  \end{cases}
  \end{equation*}
  If $V \cong W$,
  then every nonzero element
  of $\Hom_G(V,W)$ is an isomorphism $V \rightarrow W$.
  In particular, $\Hom_G(V,V)$ is the one-dimensional space
  consisting of scalar operators of the form $v \mapsto c v$,
  $c \in \mathbb{C}$.
\end{lemma}
\begin{proof}
  For $\phi \in \Hom_G(V,W)$,
  we check that the kernel of $\phi$ is an invariant subspace
  of $V$ and the image of $\phi$ is an invariant subspace of
  $W$.
  So if $V$ is not the zero map, then its kernel is the zero
  space (since $V$ is irreducible)
  and its image is all of $W$ (since $W$ is irreducible).
  The conclusion follows.
\end{proof}

\begin{lemma}\label{lem:schur-ortho-in-useful-form}
  Let $V,W$ be finite-dimensional irreducible representations
  of $G$.
  Let $\ell \otimes v \in V^* \otimes V$
  and $w \in W$.
  If $V$ is not isomorphic to $W$,
  then
  \begin{equation}\label{eq:schur-orth-1-0}
    \frac{1}{\dim W}
    \int_{g \in G} \ell(g v) g^{-1} w
    = 0.
  \end{equation}
  Otherwise,
  let us fix an equivariant identification
  $W = V$.
  Then
  \begin{equation}\label{eq:schur-orth-1-1}
    \frac{1}{\dim W}
    \int_{g \in G} \ell(g v) g^{-1} w
    = \ell(w) v.
  \end{equation}
\end{lemma}
\begin{proof}
  Fix $\ell,w$, and denote by $S : V \rightarrow W$ the
  the linear function of $v$ defined by the LHS of
  \eqref{eq:schur-orth-1-0}.
  Then a quick change of variables shows that $S$ is
  equivariant,
  so by Lemma \ref{lem:schur-lemma-for-morphisms},
  it is the zero map unless $V \cong W$;
  this establishes \eqref{eq:schur-orth-1-0}.
  We turn to \eqref{eq:schur-orth-1-1}.
  By Lemma \ref{lem:schur-lemma-for-morphisms},
  we know that $S : V \rightarrow V$ is a scalar operator;
  we wish to verify that the scalar is $\ell(w)$.
  To that end,
  it will suffice to verify that $\trace(S) = (\dim V) \ell(w)$,
  or equivalently,
  that
  \begin{equation*}
  \sum_{i}
  \int_{g \in G} \ell(g e_i) e_i^*(g^{-1} w)
  = \ell(w)
  \end{equation*}
  where $(e_i)$ is a basis of $V$ with dual basis $(e_i^*)$ of $V^*$.
  The integrand is independent of $g$
  (since it is independent of the choice of basis,
  and the basis dual to $g e_1,\dotsc,g e_n$ is $e_1^* \circ g^{-1}, \dotsc, e_n^* \circ g^{-1}$)
  so we reduce to showing that
  \begin{equation*}
  \sum_i \ell(e_i) e_i^*(w) = \ell(w),
  \end{equation*}
  which is immediate.
\end{proof}

\begin{corollary}
  Let
  $V_1, V_2$ be finite-dimensional irreducible representations
  of $G$.
  Let $\ell_1 \otimes v_1 \in V_1^* \otimes V_1$
  and $\ell_2 \otimes v_2 \in V_2^* \otimes V_2$
  Then
  \begin{equation}\label{eq:schur-orth-1-0}
    \int_{g \in G} \ell_1(g v_1) \ell_2(g^{-1} v_2)
    =
    \begin{cases}
      \dim(V_1) \ell_1(v_2) \ell_2(v_1) & V_1 \cong V_2 \\
      0 & \text{otherwise.}
    \end{cases}
  \end{equation} 
  where $\ell_1(v_2)$ and $\ell_2(v_1)$ are defined
  in the first case by fixing an equivariant identification between $V_1$ and $V_2$.
\end{corollary}

\subsection{Peter--Weyl theorem}
\label{sec:org9fd6b0d}
Let $G$ be a compact group.
Let $L^2(G)_{\fin}$ denote the subspace of finite elements;
we saw in \S\ref{sec:finite-functions-dense} that it is dense in $L^2(G)$.
\begin{theorem}
  Let $V$ traverse the set of isomorphism classes
  of finite-dimensional irreducible representation of $G$.
  Then the canonical morphism of $G \times G$-modules
  \begin{equation*}
  \mathfrak{m} : \oplus V^* \otimes V \rightarrow L^2(G)_{\fin}
  \end{equation*}
  given in terms of matrix coefficients
  by setting, for $\ell \otimes v \in V^* \otimes V$,
  \begin{equation*}
  \mathfrak{m}(\ell \otimes v)(g) := \ell(g v),
  \end{equation*}
  is an isomorphism
  with inverse
  \begin{equation*}
  \mathcal{F} : L^2(G) \rightarrow \oplus \End(V)
  \end{equation*}
  where $\mathcal{F} = \oplus \mathcal{F}_V$
  where for $u \in V$,
  \begin{equation*}
  \mathcal{F}_V(f) u := \frac{1}{\dim (V)} \int_{g \in G}
  f(g) g^{-1} u.
  \end{equation*}
  (The action is as in the proof
  of Lemma \ref{lem:right-finite-implies-left-finite}.)
\end{theorem}
\begin{proof}
  We first check surjectivity.
  Let $v \in L^2(G)_{\fin}$.
  Its span under the right regular representation of $G$
  is then a finite-dimensional representation $W$ of $G$.
  We ``have seen'' (the chronology of the lectures
  differs from that of the notes)
  in \S\ref{sec:compl-red}
  that any finite-dimensional representation
  of a compact group is completely reducible.
  In particular, $W$ is completely reducible.
  By decomposing $W$ into irreducibles and $v$ into its
  irreducilbe components,
  we reduce to verifying in the special case in which $W$ is
  irreducible
  that $v$ belongs to the image of $W^* \otimes W$
  in $L^2(G)_{\fin}$.
  But this follows immediately
  from the proof of Lemma
  \ref{lem:right-finite-implies-left-finite}.

  We now check that $\mathcal{F} \circ \mathfrak{m}  = 1$.
  It suffices to show for each $V,W \in \Irr(G)$
  and $\ell \otimes v \in V^* \otimes V$
  that
  \begin{equation*}
  \mathcal{F}_W(\mathfrak{m}(\ell \otimes v))
  =
  \begin{cases}
    \ell \otimes v & W = V \\
    0 & W \neq V.
  \end{cases}
  \end{equation*}
  Thus, let $w \in W$ be given.
  Then
  \begin{equation*}
  \mathcal{F}_W(\mathfrak{m}(\ell \otimes v))
  w
  =
  \frac{1}{\dim W}
  \int_{g \in G} \ell(g v) g^{-1} w,
  \end{equation*}
  while $(\ell \otimes v)(w) = \ell(w) v$,
  so the required conclusion
  follows
  from Lemma \ref{lem:schur-ortho-in-useful-form}.
  % Define $S : V \rightarrow W$
  % by $S(v) := \mathcal{F}_W(\mathfrak{m}(\ell \otimes v)) w$.
  % Then $S$ is equivariant, hence vanishes unless $W = V$,
  % which gives the second formula above.
  % Assuming now that $W = V$,
  % we know that $S$ is a scalar, since $V$ is irreducible.
  % We want to show that
  % $S(v) = \ell(w) v$,
  % so it will suffice to show that $\trace(S) = (\dim V)
  % \ell(w)$,

  % % So it will suffice to show that
  % % \begin{equation*}
  % % \trace(S)
  % % \end{equation*}<++>

  % % Thus, 
  % % For $f \in L^2(G)_{\fin}$,
  % % and $V$ as above,
  % % define $f_V  \in \End(V) \cong V^* \otimes V$
  % % by the formula: for $u \in V$,
  % % \begin{equation*}
  % % f_V(u) := \int_{g \in G}
  % % f_v(g) g^{-1} u.
  % % \end{equation*}
  % % We claim that the map
  % % \begin{equation*}
  % % f \mapsto \oplus_V \frac{f_V}{\dim V}
  % % \end{equation*}
  % % gives the required inverse.
  % % It suffices to consider the case that
  % % $f(g) = \ell( g w)$ for some $W \in \Irr(G)$
  % % and some $\ell \otimes w \otimes W^* \otimes W$.
  % % Conside then the map
  % % $S : W \rightarrow V$
  % % given for $w \in W,
  % % v \in V$ by $S(w) := \int_{g \in G} \ell(g w) g^{-1} v$.
  % % Then $S$ is equivariant.
  % % By Schur's lemma, $S$ vanishes unless $W \cong V$.
  % % In that case,
  % % $S$ is a scalar multiple of the identity operator.
  % % We can compute the scalar
  % % by writing, for $e_i$ a basis of $V$ with dual basis $e_i^*$,
  % % \begin{equation*}
  % % \trace(S)
  % % = \sum_i e_i^*(S e_i)
  % % =
  % % \sum_i
  % % \int_{g \in G}
  % % \ell(g e_i)
  % % e_i^*(g^{-1} u)
  % % =
  % % \sum_i
  % % \ell(e_i)
  % % e_i^*(u)
  % % = \ell(u).
  % % \end{equation*}
  % % Thus $S(w) = (\dim V)^{-1} \ell(u) w$.
  % % Therefore the map
  % % \begin{equation*}
  % % f \mapsto \frac{1}{\dim V} f_V
  % % \end{equation*}
  % % gives the required inverse.
\end{proof}

\begin{corollary}
  Let $G$ act on $L^2(G)_{\fin}$
  by the right regular representation.
  Then as $G$-representations,
  \begin{equation*}
  L^2(G)_{\fin}
  = \oplus V^{\oplus \dim(V)}.
  \end{equation*}
  where $V^{\oplus \dim(V)}$ is the image of $V^* \otimes V$,
  regarded now only as a $G$-module rather than as a $G \times G$-module.
\end{corollary}

\section{Some facts concerning invariant measures\label{sec:inv-measures}}
\label{sec:org22598a8}
\subsection{Definition of Haar measures}
\label{sec:org171fce1}
Let $G$ be a locally compact topological group.
\begin{definition}
  By a \emph{Radon measure} on $G$ we shall mean a linear
  functional $\mu : C_c(G) \rightarrow \mathbb{C}$ for which
  $f \geq 0 \implies \mu(f) \geq 0$;
  thanks to the Riesz
  representation theorem, this
  definition may also be formulated in terms of
  countably additive functions on the Borel $\sigma$-algebra
  satisfying certain properties.
\end{definition}
For $y \in G$
and $f \in C_c(G)$,
define the left and right translates
$L_y f, R_y f \in C_c(G)$
by
setting $L[y] f(x) := f(y x), R[y] f(x) := f(x y)$.

To interpret some of the statements to follow,
we ``recall'' that it makes sense to integrate functions taking values in a Banach space.
The only spaces we'll really need in the end are finite-dimensional vector spaces
(where everything should be familiar)
and the Hilbert space $L^2(G)$, where one doesn't lose much
by interpreting everything in a pointwise fashion.  (TODO: dfdfd)
\begin{definition}
\label{defn:}
A \emph{left (resp. right) Haar measure} on $G$
is a nonzero Radon measure $\mu$
with the property
$\mu(L[y] f) = \mu(f)$
(resp. 
$\mu(R[y] f) = \mu(f)$).
\end{definition}
We may reformulate this definition in various ways.
For example, $\mu$ is a left Haar measure
if $\mu(g E) = \mu(E)$ for all $g \in G$
and all Borel subsets $E$ of $G$,
or in integral form, if
\begin{equation*}
  \int_{g \in G} f(h g) \, d \mu(g) = \int_{g \in G} f(g) \, d \mu(g)
\end{equation*}
for all $h \in G$ and all $f \in C_c(G)$.

\subsection{Existence theorem}
\label{sec:orga3b0a45}
\begin{theorem}\label{thm:basics-on-haar-measure}
~
  \begin{enumerate}
  \item   There exist left Haar measures and there exist right Haar measures on any locally
  compact group $G$.  They need neither coincide nor be scalar multiples of one another.
\item   Any two left (resp. right) Haar measures are positive
  multiples
  of one another.
\item   Any left or right Haar measure
  $\mu$ satisfies
  $\mu(f) > 0$ for any nonzero nonnegative $f \in C_c(G)$.
\item
  There is a continuous homomorphism $\Delta : G \rightarrow
  \mathbb{R}_+^\times$
  so that for any left (resp. right) Haar measure $\mu$
  and $f \in C_c(G)$,
  one has
  $\mu(R[g] f) = \Delta(g)$
  (resp.   $\mu(L[g] f) = \Delta(g^{-1})$).
  (TODO: check inverse here.)
\item   If $G$ is compact, then
  $\{\text{left Haar measures}\}
  = \{\text{right Haar measures}\}$.
  \end{enumerate}
\end{theorem}
We sketch the idea of one proof; filling in the details may be regarded as an exercise, or alternatively, looked up somewhere.  For each
nonnegative nonzero $\phi \in C_c(G)$ and each nonnegative
$f \in C_c(G)$, denote by $[f:\phi]$ the infinum of $\sum c_i$
taken over all finite tuples of positive coefficients
$c_1,\dotsc,c_n$ and group elements $g_1,\dotsc,g_n$ with the
property that $f \leq \sum c_i L[g_i] \phi$.  Fix also some
nonzero nonnegative $f_0 \in C_c(G)$.
We may then attempt to define a left Haar measure
$\mu$ on $G$ by requiring that  $\mu(f_0) = 1$
and that
\begin{equation}\label{eq:haar-formula}
  \frac{\mu(f)}{\mu(f_0)}
  =
  \lim_{\phi}
  \frac{[f:\phi]}{[f_0:\phi]}
\end{equation}
where $\phi$ traverses a net
consisting of nonzero nonnegative elements of $C_c(G)$
with support shrinking to the identity.
This turns out to work.
Conversely,
to establish uniqueness,
it suffices to show that \eqref{eq:haar-formula}
holds for any left Haar measure $\mu$.
The key lemma is that each nonnegative $f \in C_c(G)$
may uniformly approximated
by some finite sum $c_i L[g_i] \phi_\alpha$
as above
with support in a fixed compact;
it follows then that
$\mu(f)$ is approximated by $\mu(\sum c_i L[g_i] \phi_\alpha) =
\mu(\phi_\alpha) \sum c_i \approx \mu(\phi_\alpha) [f:\phi]$,
giving \eqref{eq:haar-formula}.


\begin{definition}
  A locally compact group is \emph{unimodular}
  if
  $\{\text{left Haar measures}\}
  = \{\text{right Haar measures}\}$,
  or equivalently, if $\Delta(g) = 1$ for all $g \in G$.
  On a unimodular group,
  we may speak unambiguously simply about a \emph{Haar measure}
  (without specifying ``left'' or ``right'').
  For example,
  Theorem \ref{thm:basics-on-haar-measure}
  says that compact groups are unimodular.
\end{definition}

\subsection{Unimodularity of compact groups}
\label{sec:org720d559}
Note that if $G$ is a compact group,
then the image of the continuous homomorphism $\Delta : G
\rightarrow \mathbb{R}_+^\times$
is a compact subgroup of $\mathbb{R}_+^\times$; the only such
subgroup
is $\{1\}$, so $\Delta$ is trivial, which explains why left and
right Haar measures
coincide on such a group.
It follows that on each compact group, there is a unique (left
and right)
invariant probability measure.

\subsection{Direct construction for Lie groups\label{sec:inv-measure-lie}}
\label{sec:orgb26c793}
When $G$ is a Lie group, a simpler proof may be given using differential forms.
Let $\omega_e$ be a nonzero element of $\det(T_e^* G)$.
Denote by $\omega$ the volume form
on $G$ whose components $\omega_g \in \det(T_g^* G)$ for
$g \in G$ are given by
the pullback
\begin{equation*}
  \omega_g := R[g^{-1}]^*_g \omega_e
\end{equation*}
under the differential $R[g^{-1}]_g : T_g G \rightarrow T_e G$.
Then $L[g]^* \omega = \omega$ for all $g \in G$, so $\omega$
is left-invariant.
The map $C_c(G) \ni f \mapsto \int_G f \, \omega$ then defines a
left Haar measure.

\subsection{Some exercises}
\label{sec:org02a337d}
\begin{exercise}
  Let $G$ be a Lie group with left Haar measure $d g$.
  Let $\Delta : G \rightarrow \mathbb{R}^\times_+$
  be the function
  \begin{equation*}
  \Delta(g) := \det(\Ad(g) | \mathfrak{g}).
  \end{equation*}
  Show that $\Delta(g) \, d g$ is a right Haar measure.
\end{exercise}

\begin{exercise}
  Determine a left and right Haar measure
  on the Lie group
  \begin{equation*}
    \operatorname{Aff}(\mathbb{R})
    :=
    \left\{ 
\begin{pmatrix}
      \ast & \ast \\
       & 1
     \end{pmatrix}
 \right\}
   \leq \GL_2(\mathbb{R}).
  \end{equation*}
\end{exercise}

\begin{exercise}
  \begin{enumerate}
  \item Let $G$ be a locally compact group for which
    $[G,G]$ is dense in $G$.
    Show that $G$ is unimodular.    
  \item Let $G,B,K$ be
    locally compact groups
    and let $\phi : B \times K \rightarrow G$
    be a morphism.
    Suppose that $G$ is unimodular,
    $K$ is compact, and $\phi$ has dense image.
    Let $d_l b$ be a left Haar measure on $G$
    and let $d k$ be a Haar measure on $K$.
    Show that
    \begin{equation*}
    \mu(f) := \int_{b \in B} \int_{k \in K} f(b k) \, d_l b \, d k
    \end{equation*}
    defines a Haar measure on $G$.
  \end{enumerate}
\end{exercise}

\subsection{Construction of a Haar measure on a compact group via averaging\label{sec:haar-compact-gp-via-avg}}
\label{sec:org0bc8f8b}
Let $G$ be a compact topological group
(not necessarily a Lie group).
There is a nice way to construct the unique Haar
probability measure $\mu$ on $G$ via averaging.
\begin{definition}
  Let $f : G \rightarrow \mathbb{C}$ be a continuous
  function on a compact group $G$.
  Let $\operatorname{Averages}(f)$ denote the space
  of functions $G \rightarrow \mathbb{C}$
  of the form
  \begin{equation*}
    G \ni x \mapsto
    \sum_{i=1}^n
    c_i
    f(\lambda_i x) \in \mathbb{C}
  \end{equation*}
  for some $n \in \mathbb{Z}_{\geq 1}$
  and $\lambda_1,\dotsc,\lambda_n \in G$
  and some $c_1,\dotsc,c_n \in [0,1]$
  with $c_1 + \dotsb + c_n = 1$.
\end{definition}
\begin{lemma}\label{lem:averages-give-unique-constant}
  There exists a unique constant function
  in the closure (with respect to the uniform
  topology) of $\operatorname{Averages}(f)$.
\end{lemma}
\begin{proof}
  One should be able to prove this
  as follows:
  \begin{enumerate}
  \item It suffices to consider the case that $f$ is real-valued.
  \item The space of continuous functions on $G$ is closed
    with respect to the uniform topology.
  \item
    For a continuous real-valued function $f$ on $G$,
    set
    \begin{equation*}
      \osc(f) := \max_{g \in G} f(g) - \min_{g \in G} f(g).
    \end{equation*}
    Note that $f$ is constant if and only if $\osc(f) = 0$.
    Note also that $\osc(f') \leq \osc(f)$ for all $f' \in \operatorname{Averages}(f)$.
  \item
    Given a continuous real-valued function $f$ on $G$
    that is non-constant,
    show that there exists $f' \in \operatorname{Averages}(f)$
    so that $\osc(f') < \osc(f)$.
    (Use the compactness of $G$ and hence the uniform
    continuity of $f$.
    If $f$ is smaller than typical in some part of $G$,
    translate $f$ around a bit to dampen the contribution
    from parts of $G$ where $f$ is large.)
  \item The family of functions $\operatorname{Averages}(f)$
    is equicontinuous.
  \item The function
    $\osc : C(G) \rightarrow \mathbb{R}_{\geq 0}$ is continuous
    with respect to the uniform topology on the domain.
  \item To prove the existence part of the theorem,
    we can take a sequence $f_i \in \operatorname{Averages}(f)$
    so that $\osc(f_i)$ tends to the infinum
    of $\osc(h)$ over all $h \in \operatorname{Averages}(f)$.
    After passing
    to a subsequence and appealing to Arzela--Ascoli,
    we get a limit $h$ of the sequence $f_i$.
    If $h$ is non-constant, then we can find
    $h' \in \operatorname{Averages}(h)$ for which $\osc(h') < \osc(h)$.
    But one should then be able to check that $h'$
    lies in the closure of $\operatorname{Averages}(f)$,
    giving the required contradiction.
  \item
    To get uniqueness,
    let $c_1,c_2$ be values taken by constant functions
    in the closure of $\operatorname{Averages}(f)$.
    Let $c'$ be a value taken by some constant function
    in the closure of the set defined analogously
    to $\operatorname{Averages}(f)$, but using right translations in place of
    left translations.
    Since left and right translations commute, it's not so hard
    to check that $c_i = c'$ for $i=1,2$,
    hence that $c_1 =c_2$.
  \end{enumerate}
\end{proof}
We may then define $\mu(f) = \int_G f \, d \mu$
to be the value taken by the constant function
arising in Lemma \ref{lem:averages-give-unique-constant}.
It's not hard to check that this defines a positive
linear functional on the space of continuous functions on $G$,
hence defines a measure;
the key point is to verify additivity, which follows
from some of the assertions made above.

\section{Definition and basic properties of Lie groups}
\label{sec:orgfa95dc3}
\subsection{Lie groups: definition}
\label{sec:org11b4fd5}
\begin{definition}\label{defn:lie-group}
  By a \emph{Lie group} we shall mean a group $G$ equipped with
  the structure of a manifold for which the maps
  $m : G \times G \rightarrow G$ and $i : G \rightarrow G$ are
  smooth.  The \emph{Lie algebra} of $G$ is the vector space
  $\Lie(G) := T_e(G)$, often denoted $\mathfrak{g}$, given by
  the tangent space at the identity; it is a vector space of
  dimension equal to the dimension of $G$.
  (The word ``algebra'' appearing in the term ``Lie
  algebra''
  will be justified later.)

  For Lie groups $G, H$,
  a \emph{morphism of Lie groups} or simply a \emph{morphism} $f : G \rightarrow H$
  is a smooth group homomorphism.

  For a Lie group $G$ and a manifold $X$,
  an \emph{action of $G$ on $X$}
  is a smooth map $\alpha : G \times X \rightarrow X$,
  abbreviated $g x := \alpha(g,x)$,
  that satisfies the same assumptions as in Definition \ref{defn:top-gp}.
\end{definition}

\begin{exercise}
  It suffices to check that $m$ is smooth; the
  smoothness of $i$ is automatic. [Hint: apply the inverse function theorem  to the map $(x,y) \mapsto (x,x y)$.]
\end{exercise}
% \begin{proof}
%   Consider the map $\phi : G \times G \rightarrow G \times G$
%   given by $\phi(x,y) \mapsto (x,x y)$.
%   Set $\mathfrak{g} := \Lie(G)$.
%   The total derivative of $\phi$ at the identity
%   element $(e,e)$ of $G \times G$
%   is the linear map
%   \begin{equation*}
%     T_{(e,e)} \phi : \mathfrak{g} \oplus  \mathfrak{g}
%     \rightarrow \mathfrak{g} \oplus \mathfrak{g}
%   \end{equation*}
%   respected in matrix form
%   by
%   \begin{equation*}
%     \begin{pmatrix}
%       1 & 0 \\
%       1 & 1
%     \end{pmatrix},
%   \end{equation*}
%   where $1$ denotes the identity transformation
%   from $\mathfrak{g}$ to itself.
%   By the inverse function theorem,
%   we can invert $\phi$ 
% \end{proof}

\subsection{Basic examples}
\label{sec:org7a9cd32}
The \emph{additive group} $(\mathbf{k},+)$
of the field $\mathbf{k}$ is a Lie group,
since the addition map $\mathbf{k} \times \mathbf{k}
\ni (x,y) \mapsto x + y \in \mathbf{k}$ has the property
that all of its partial derivatives exist.
Similarly, the \emph{multiplicative group}
$(\mathbf{k}^\times,\times)$ is a Lie group.
A slightly more interesting example
can be obtained by considering
any finite-dimensional unital associative algebra $A$ over
$\mathbf{k}$.
A good example to keep in mind is when $A$ is the algebra
\begin{equation*}
A := M_n(\mathbf{k})
:=
\Mat_{n \times n}(\mathbf{k})
\end{equation*}
of $n \times n$ matrices,
in which case
\begin{equation*}
A^\times = \GL_n(\mathbf{k})
\end{equation*}
is the general linear group.
The algebra $A$ is a vector space, hence a manifold.
Moreover, the unit group $A^\times$ is open
in $A$: by Exercise \ref{exercise:openness-top-groups-ambient-spaces},
it suffices to verify that $A^\times$ contains
a neighborhood of the identity element $1$,
and this follows from the observation
that for $x \in A$ small enough,
the element $1 + x$
has inverse given by the convergent series $\sum_{n \geq 0}
(-x)^n$.
Since the multiplication on $A^\times$ is bilinear, it is smooth,
and so $A^\times$ is a Lie group of dimension $\dim(A)$.
Moreover, one can naturally identify
$T_0(A) = A$ and
$T_1(A^\times) = A$, where $1 \in A^\times$ denotes the identity element, as in 
Example \ref{example:tangent-space-open-subset-euclidean-space}.

By what was shown above, $\GL_n(\mathbf{k})$ is an
$n^2$-dimensional Lie group
with
\begin{equation}\label{eq:lie-algebra-general-linear-group}
  T_1(\GL_n(\mathbf{k})) = M_n(\mathbf{k}).
\end{equation}

\subsection{Lie subgroups: definition}
\label{sec:orgc7b7f1f}
There are at least a couple different
conventions concerning what a ``Lie subgroup'' is.

\begin{definition}
  Given a Lie group $G$,
  we say that a subset $H$ of $G$ is a \emph{Lie subgroup}
  if it is a subgroup and a submanifold.
\end{definition}
\begin{lemma}
  Let $G$ be a Lie group.
  Let $H$ be a Lie subgroup of $G$.
  Then $H$ is a Lie group.
\end{lemma}
\begin{proof}
  We must show that the multiplication map
  $\mu_H : H \times H \rightarrow H$ is smooth,
  and that it coincides
  with the restriction
  $\mu_G|_{H \times H} : H \times H \rightarrow G$
  of the smooth multiplication map $\mu_G : G \times G \rightarrow
  G$.
  So we reduce to the following:
  if $f : M \rightarrow N$
  is a smooth map between manifolds
  whose image lands in
  some submanifold $S \subseteq N$,
  then the induced map $f : M \rightarrow S$
  is also smooth.
  This is given by Proposition \ref{prop:smoothness-preserved-codomain-pass-to-submfld}.
\end{proof}

\begin{definition}
A \emph{linear Lie group}
is a Lie subgroup $G$ of $\GL_n(\mathbf{k})$ for some $n$.
(Essentially all of our examples will be of this form.)
\end{definition}

\begin{definition}\label{defn:immersed-lie-subgroup}
  Given a Lie group $G$,
  by an \emph{immersed Lie subgroup}
  we will mean a subset $H$ of $G$
  so that there exists a pair
  $(\hat{H},\iota)$,
  where $\hat{H}$ is a Lie group
  and $\iota : \hat{H} \rightarrow G$ is an injective immersion
  with image $H$.
  (We will see much later in the course
  that such a pair is essentially uniquely
  determined by $H$, at least if $H$ is connected.)
\end{definition}
\begin{example}\label{example:immersed-subgroup}
  Let $G  := (\mathbb{R}/\mathbb{Z})^2$ be the two-dimensional
  torus,
  let $H := \mathbb{R}$ be the real line,
  let $\alpha \in \mathbb{R} - \mathbb{Q}$ be an irrational real
  number,
  and define $\iota : H \rightarrow G$
  by the formula
  \begin{equation*}
  \iota(x) := (x,\alpha x).
  \end{equation*}
  Since $\alpha$ is irrational, the map $\iota$ is injective.
  It is also an immersion, since its differential
  is given everywhere by the column matrix
  \begin{equation*}
  T_x \iota
  = 
\begin{pmatrix}
    1  \\
    \alpha 
  \end{pmatrix}
  \end{equation*}
  which defines an injective linear map $\mathbf{k}
  \rightarrow \mathbf{k}^2$.
  Thus $(H,\iota)$ is an immersed Lie subgroup of $G$.
  On the other hand, $\iota(H)$ is not a Lie subgroup
  because it is not a submanifold:
  submanifolds are open in their closure (Remark \ref{rmk:submfld-locally-closed}), 
  and $\overline{\iota(H)} = G$,
  but $\iota(H)$ is not open in $G$.
  On a related note, $\iota$ does not define a homeomorphism
  onto its image:
  for instance, there exist sequences $x_n \in \mathbb{R}$
  with $x_n \rightarrow \infty$ for which $\iota(x_n) \rightarrow \iota(0)$.
\end{example}

\begin{remark}
  An injective immersion
  is called an \emph{embedding} if it defines a homeomorphism
  onto its image.
  (This is not the case in Example \ref{example:immersed-subgroup}.)
  With this terminology, we could alternatively
  define a Lie subgroup
  to be a pair $(H,\iota)$,
  where $H$ is a Lie group
  and $\iota$ is an embedding.
\end{remark}

\subsection{A handy criterion for being a Lie subgroup}
\label{sec:orgd63ace1}
Here is a very handy criterion for checking that
a subgroup of a Lie group is a Lie subgroup;
we shall use it in several examples.
\begin{lemma}\label{lem:lie-subgroups-criterion}
  Let $G$ be a Lie group.
  Let $H \leq G$ be a subgroup
  that is given near the identity $e \in G$
  element by a system of equations
  \begin{equation}\label{eq:define-lie-group-near-identity-by-equations}
      f_1 = \dotsb = f_m = 0,
  \end{equation}
  where the $f_i : G \xdashrightarrow{} \mathbf{k}$ are smooth
  maps defined near $e$
  for which
  $f := (f_1,\dotsc,f_m) : G \xdashrightarrow{} \mathbf{k}^m$ is
  submersive,
  i.e.,
  satisfies
  either of the equivalent conditions
  $\rank(T_e f) = m$
  or
  $\dim (V) = d$,
  where $d := \dim(G) - m$ and
  $V := \ker(T_e f)$ is the vector space
  given in local coordinates $x_1,\dotsc,x_n$ at $e \in G$ ($n =
  \dim(G)$)
  by the
  space of solutions $(d x_1,\dotsc, d x _n) \in \mathbf{k}^n$
  to the
  system of homogeneous linear equations
  \begin{equation*}
    \sum_{j=1}^n \frac{\partial f_i}{\partial x_j}(e) d x _j = 0
    \quad (i=1..m)
    \end{equation*}
  obtained by differentiating \eqref{eq:define-lie-group-near-identity-by-equations}.
  Then $H$ is a $d$-dimensional Lie subgroup of $G$.
  Moreover, $\Lie(H) = T_e H$ is equal to $V$.
\end{lemma}
\begin{proof}
  Thanks to Propositions \ref{prop:submfld-criterion}, \ref{prop:compute-tangent-space-submfld},
  we need only verify that $H$ is a $d$-dimensional
  submanifold of $G$.  
  Set
  $n := \dim(G)$.
  As in the proof of Proposition \ref{prop:submfld-criterion}, there is a coordinate
  system $x_1,\dotsc,x_n$ on $G$ at $e$ so that $H$ is
  given near $e$ by
  $x_{1}= \dotsb = x_m = 0$.
  Let
  $p \in H$, and let $\psi : G \rightarrow G$ be the map
  $\psi(x) := p^{-1} x$.  Since $G$ is a Lie group, the map
  $\psi$ is a diffeomorphism with $\psi(p) = e$,
  and so
  $y_1 := x_1 \circ \psi, \dotsc, y_n := x_n \circ \psi$ defines
  a coordinate system on $G$ at $p$.  For $g \in G$ near $p$,
  the following are then visibly equivalent:
\begin{enumerate}
\item $y_1(g) = \dotsb = y_m(g) = 0$
\item $x_1(p^{-1} g) = \dotsb = x_m(p^{-1} g) = 0$
\item $p^{-1} g \in H$
\item $g \in H$.
\end{enumerate}
In relating the final two steps, we used that $H$ is a subgroup
and that $p \in H$.
\end{proof}

\begin{example}
  The subgroup $H := \SL_n(\mathbf{k})$ of $G :=
  \GL_n(\mathbf{k})$
  is defined by the single equation $\det(g) = 1$.
  Differentiating this equation and evaluating
  at the identity element gives the linear equation
  \begin{equation*}
  \trace(d g) = 0
  \end{equation*}
  in the matrix variable $d g \in M_n(\mathbf{k}) = T_e(G)$ (see \eqref{eq:lie-algebra-general-linear-group}).
  Since this equation has an $n^2-1$-dimensional solution space,
  we deduce from Lemma \ref{lem:lie-subgroups-criterion}
  that $H$ is a Lie subgroup
  of $G$, called the \emph{special linear group}.
  Moreover,
  \begin{equation*}
  T_e(\SL_n(\mathbf{k}))
  = \{d g \in M_n(\mathbf{k}) : \trace(d g) = 0\}
  \end{equation*}
  is the space of traceless $n \times n$ matrices.
\end{example}

\begin{exercise}
  Show that the \emph{orthogonal group}
  \begin{equation*}
  O_n(\mathbf{k}) := \{g \in \GL_n(\mathbf{k}) : g g^t =
  1\},
  \end{equation*}
  where $g \mapsto g^t$ denotes the transpose map,
  is defined
  by a system of $n(n+1)/2$ equations 
  having full rank at the identity 
  (i.e., satisfying the submersiveness condition).
  Deduce that $O_n(\mathbf{k})$ is a Lie group
  of dimension $n^2 - n(n+1)/2 = n(n-1)/2$.
\end{exercise}

\subsection{Lie subgroups are closed\label{sec:lie-subgroups-are-closed}}
\label{sec:orgc0ef6df}
\begin{theorem}\label{thm:lie-subgroups-are-closed}
 Let $G$ be a Lie group and $H \leq G$ a Lie subgroup.  Then $H$
is closed in $G$.  
\end{theorem}
Recalling from Remark \ref{rmk:submfld-locally-closed} 
that $H$ is locally closed in $G$,
the proof of Theorem \ref{thm:lie-subgroups-are-closed} reduces  to that of the following:
\begin{lemma}
  A locally closed
subgroup $H$ of a topological group $G$ is closed. 
\end{lemma}
\begin{proof}
   By
the continuity of the group operations in $G$, the closure $\overline{H}$ is itself a group.
For
$g \in \overline{H}$, the coset $g H$ is then an open subset of
$\overline{H}$ (using here that $H$ is locally closed).  Since $H$ is dense in $\overline{H}$, the
subsets $g H$ and $H$ intersect.  This means that we can write
$g x = y$ for some $x,y \in H$, whence $g = x^{-1} y$ belongs to
$H$.  Since $g$ was arbitrary, we conclude as required that
$\overline{H} = H$.
\end{proof}

\subsection{Translation of tangent spaces by group elements\label{sec:translate-tangent-spaces-gp-elts}}
\label{sec:org89c17c6}
Let $G$ be a Lie group.
An element $g \in G$
acts on $G$
by the left and right multiplication maps
\begin{equation*}
  L[g] : G \rightarrow G
\end{equation*}
\begin{equation*}
  h \mapsto g h,
\end{equation*}
\begin{equation*}
  R[g] : G \rightarrow G
\end{equation*}
\begin{equation*}
  h \mapsto h g.
\end{equation*}
One has
\begin{equation}\label{eq:hom-property-left-right-translation}
  L[g_1] \circ L[g_2] = L[g_1 g_2],
  \quad R[g_1] \circ R[g_2] = R[g_2 g_1].
\end{equation}
These maps are smooth, so it makes sense to differentiate
them at an element $h \in G$
to obtain linear maps of tangent spaces
\begin{equation*}
  T_h L[g] : T_{h} G \rightarrow T_{g h}(G)
\end{equation*}
\begin{equation*}
  T_h R[g] : T_{h} G \rightarrow T_{h g}(G).
\end{equation*}
By the chain rule and the identities \eqref{eq:hom-property-left-right-translation},
these maps are in fact linear isomorphisms of the tangent
spaces.
It will be convenient to introduce for $X \in T_h G$
and $g \in G$
the abbreviations
\begin{equation}\label{eq:abbrev-gX-Xg}
    g X := (T_h L[g])(X) \in T_{g h}(G),
  \quad 
  X g := (T_h R[g])(X) \in T_{h g}(G).
\end{equation}
These will be used most often when $h = 1$,
so that $X \in T_e G = \mathfrak{g}$.
The special case worth focusing on is
when $G$ is a linear Lie group $G \leq \GL_n(\mathbf{k})$.
In that case,
we can identify the various tangent spaces
$T_h(G)$
with subspaces of $M_n(\mathbf{k})$;
under this identification,
the quantities $g X, X g$ defined in \eqref{eq:abbrev-gX-Xg}
are given by the matrix products of $g \in \GL_n(\mathbf{k})$ and $X \in M_n(\mathbf{k})$.

\section{The connected component}
\label{sec:orgf603676}
\subsection{Generalities}
\label{sec:org948d2a7}
Any group may be regarded as a discrete topological group, or
even a discrete $0$-dimensional Lie group (provided the group is countable,
so as to satisfy the hypothesis of second-countability), but Lie theory has
nothing interesting to say about such Lie groups; its techniques
show their true strength only when the group is connected.
Recall, then, that a topological space $X$ is \emph{connected}
if it cannot be written as a disjoint union of two nonempty
closed subsets, or equivalently, if every continuous map from
$X$ to a discrete topological space is constant.  Any
topological space $X$ admits a unique decomposition
$X = \sqcup_{i} X_i$ into maximal connected subsets $X_i$,
called the \emph{connected components} of $X$; since the closure
of any connected set is connected, the connected components are
always closed, but need not be open in general.  However, if $X$
is \emph{locally connected}, that is to say, if each point has a
connected neighborhood, then the connected components are open.
In particular, any manifold is locally Euclidean, hence locally
connected, and so is the disjoint union of its connected
components which are in turn open submanifolds.  Moreover, since
manifolds are locally path-connected, we know that any connected
manifold is necessarily path-connected, even by smooth paths.

In particular, the connected components of a Lie group $G$ are 
submanifolds.  It is customary to denote by $G^0$ the connected
component of the identity element of $G$.  Then $G^0$ has the
defining property that
\begin{itemize}
\item if $C$ is any  connected subset of $G$
  that contains the identity element,
  then $C \subseteq G^0$.
\end{itemize}
We have the following:
\begin{theorem}
  $G^0$ is a normal Lie subgroup of $G$,
  and the connected components
  of $G$ are precisely the cosets of $G^0$.
\end{theorem}
\begin{proof}
  We have already observed that $G^0$ is a submanifold.  Let
  $g \in G$.  Since left and right multiplication maps
  $x \mapsto g x, x \mapsto x g$ define homeomorphisms from $G$
  to itself, they permute the connected components.  The various
  assertions follow easily from this:
  \begin{enumerate}
  \item If $g \in G^0$,
    then $g G^0$ is a connected component of $G$
    containing $g$.  Since $G^0$ is also a connected
    component of $G$ containing $g$, it follows that $g G^0 = G^0$.
    This implies that $G^0$ is a subgroup.
  \item For any $g \in G$,
    the conjugate $g G^0 g^{-1}$ is a connected component of $G$
    that contains $g g^{-1} = 1$, hence $g G^0 g^{-1} = G^0$.
    Thus $G^0$ is normal.
  \item If $C$ is any connected component of $G^0$,
    then it is nonempty;
    if $g \in C$ is any element,
    then $g^{-1} C$ is a connected component of $G^0$
    that contains $g^{-1} g = 1$,
    hence $g^{-1} C = G^0$ and so $C = g G^0$.
  \end{enumerate}
  % To show
  % that it is a subgroup, we must verify that the multiplication
  % map $m : G \rightarrow G$ and inversion map
  % $i : G \rightarrow G$ satisfy
  % $m(G^0 \times G^0) \subseteq G^0$ and $i(G^0) \subseteq G^0$.
  % We verify the first of these, the verification of the second
  % being similar.  Since $G^0$ is connected, so is the product
  % $G^0 \times G^0$ and its image $m(G^0 \times G^0)$ under the
  % continuous map $m$; since that image contains $1$,
  % it follows from ???
  % that $m(G^0 \times G^0) \subseteq G^0$, as required.
  % To see that $G^0$ is normal,
  % take $g \in G$.
  % Then the map $\psi : G \rightarrow G$
  % given by $\psi(x) := g x g^{-1}$
  % is (among other things) continuous,
  % hence $g G^0 g^{-1} = \psi(G^0)$ is connected and contains
  % the identity element $e = \psi(e)$
  % and so is contained in $G^0$, as required.
  % Finally, if $C$ is a connected component of $G$
  % and $g \in G$ is arbitrary,
  % then since the map $G \ni x \mapsto g^{-1} x \in G$
  % is a diffeomorphism,
  % we see that $g^{-1} C$ is a connected component of $G$
  % that contains $g^{-1} g = e$,
  % hence that $g^{-1} C = G^0$
  % and so $C = g G^0$, as required.
\end{proof}

\subsection{Some examples\label{sec:connectedness-examples}}
\label{sec:orgdc3e70e}
In the following table,
we list the number of connected components
of some Lie groups.
Here $\mathbf{k}$ is one of the fields
$\mathbb{R}$ or $\mathbb{C}$,
$n \geq 1$ and $p \geq q \geq 1$.
\begin{center}
\begin{tabular}
{||l|c|c||}
    \hline
  $1$ component & $2$ components & $4$ components                               \\ \hline
  $\GL_n(\mathbb{C})$
      & $\GL_n(\mathbb{R})$
      & $O(p,q)$ \\ [1ex]
  $\SL_n(\mathbf{k})$
      & $O(n)$ 
      &   \\ [1ex]
$\SO(n)$
      & $\SO(p,q)$
      &         \\ [1ex]
  $U(n), \SU(n)$
      & 
      &  \\
  $U(p,q), \SU(p,q)$
      &  
      &                                                       \\ \hline
\end{tabular}
\end{center}
These Lie groups were defined in lecture.
It was proved that $\GL_n(\mathbb{C})$, $\SL_n(\mathbf{k})$, $\O(n)$, $\SO(n)$, $\U(n)$
have the indicated number of connected components;
the remaining cases were left as exercises.
\begin{itemize}
\item For the case of $\SL_n(\mathbf{k})$,
  we argued using elementary matrices.
\item The group $\GL_n(\mathbb{C})$ is the image of the
  connected domain  $\mathbb{C}^\times \times \SL_n(\mathbb{C})$
  under the continuous homomorphism
  $(\zeta,g) \mapsto \zeta g$, which is
  surjective because $\det(\zeta g) = \zeta^n$ and the $n$th power map on $\mathbb{C}^\times$
  is surjective.
\item The proof in the case of $\SO(n)$ was by induction on $n$,
  using that the trivial group $\SO(1) = \{1\}$ is connected and
  that $\SO(n)$ acts transitively on the (connected) sphere $S^{n-1}$ with
  $\SO(n-1) \hookrightarrow \SO(n)$ as the stabilizer group of
  the point $e_n := (0,\dotsc,0,1)$.  Since several people
  asked for clarifications regarding this proof, I have written it down in the following subsection.
\item The proof in the case of
  $\U(n)$ was to recall that every conjugacy class in $\U(n)$
  contains
  a diagonal element,
  and that the diagonal elements
  \begin{equation*}
  \begin{pmatrix}
    e^{i \theta_1} &  &  \\
     & \dotsb  &  \\
     &  & e^{i \theta_n}
  \end{pmatrix}
  \end{equation*}
  are obviously in the connected component of the identity,
  because (for instance)
  they are the values $\gamma(1)$
  of the continuous maps
  \begin{equation*}
  \gamma(t) = 
  \begin{pmatrix}
    e^{i \theta_1 t} &  &  \\
     & \dotsb  &  \\
     &  & e^{i \theta_n t}
  \end{pmatrix}
  \end{equation*}
  for which $\gamma(0) = 1$ is the identity.
\end{itemize}

\subsection{Connectedness of \texorpdfstring{$\SO(n)$}{SO(n)}}
\label{sec:orga8ac11a}
We verify by induction on $n$ that $\SO(n)$
is connected.
The group $\SO(1) = \{1\}$ is trivial,
hence connected.
Let $n \geq 2$.
The group $\SO(n)$ acts smoothly on the unit sphere $S^{n-1} := \{x
\in \mathbb{R}^n : |x| = 1\}$.
The action is transitive.
In fact, the connected component already acts transitively:
\begin{lemma}\label{lem:so-n-connected-component-acts-transitively-on-sphere}
  Let $n \geq 2$.
  Then $\SO(n)^0$ acts transitively on $S^{n-1}$.
\end{lemma}
\begin{proof}
  Denote by $e_1,\dotsc,e_n$
  the standard basis elements; they all belong to $S^{n-1}$.
  Let $v \in S^{n-1}$.
  We will show that there exists $g \in \SO(n)^0$
  with $g e_n = v$.
  \begin{enumerate}
  \item Consider first the case $n = 2$.
    Define $\gamma : \mathbb{R} \rightarrow \SO(2)$
    by the formula
    \begin{equation*}
    \gamma(\theta) := 
\begin{pmatrix}
      \cos \theta  & \sin \theta  \\
      -\sin \theta  & \cos \theta 
    \end{pmatrix}
 \in \SO(2).
    \end{equation*}
    Since $\gamma$ is continuous and $\gamma(0) = 1$,
    we see that $\gamma(\theta) \in \SO(2)^0$ for all $\theta$.
    Now let $v \in S^{1}$.
    We may then write $v = (x,y)$ where $x^2 + y^2 = 1$
    and solve $x = \sin(\theta), y = \cos(\theta)$ for some
    $\theta$.
    Then
    $\gamma(\theta) e_2 = (x,y)$,
    as required.
  \item Suppose now that $n \geq 3$.
    Define $\gamma : \mathbb{R} \rightarrow \SO(n)$
    by
    \begin{equation*}
    \gamma(\theta)
    :=
    \begin{pmatrix}
      1 &  & & & \\
      & \dotsb & & & \\
      &  & 1 & & \\
      & & & \cos(\theta) & \sin(\theta) \\
      & & & -\sin(\theta) & \cos(\theta)
    \end{pmatrix}
.
    \end{equation*}
    As above, $\gamma(\theta) \in \SO(n)^0$ for all $\theta$.
    Let $v \in S^{n-1}$.  If $v$ belongs to the line spanned by
    $e_n$, then either $v = e_n$ (in which case $v = g e_n$ with
    $g = 1 \in \SO(n)^0$) or $v = - e_n$ (in which case
    $v = \gamma(\pi) e_n$); in either case, $v$ is of the form
    $v = g e_n$ for some $g \in \SO(n)^0$.  If $v$ and $e_n$ are
    linearly independent, we may use Gram--Schmidt to find an
    orthonormal basis $e_{n-1}', e_n$ for their span.  We may
    then extend this to an orthonormal basis
    $e_1', e_2', \dotsc, e_{n-1}', e_n$ for $\mathbb{R}^n$.
    We can find $g$ in $\O(n)$ taking one orthonormal basis
    to the other,
    so that $g v$ belongs to the span of
    $e_{n-1}, e_n$
    and $g e_n = e_n$
    Then $g v$ is of the form $(0,\dotsc,0,x,y)$
    with $x^2 + y^2 = 1$; we can then solve
    $x = \sin(\theta), y = \cos(\theta)$ as before to obtain
    $\gamma(\theta) e_n = g v$
    and thus
    $g^{-1} \gamma(\theta) g e_n = v$.
    Since $\SO(n)^0 = \O(n)^0$ is normal in $\O(n)$,
    we have $g^{-1} \gamma(\theta) g \in \SO(n)^0$.
  \end{enumerate}
\end{proof}
\begin{lemma}\label{lem:compute-stabilizer-north-pole-so-n}
  The stabilizer group
  $\Stab_{\SO(n)}(e_n)$
  is isomorphic to $\SO(n-1)$,
  with an isomorphism in the opposite direction
  given by
  \begin{equation*}
  \SO(n-1) \xrightarrow{\cong} \Stab_{\SO(n)}(e_n)
  \end{equation*}
  \begin{equation*}
  h \mapsto 
\begin{pmatrix}
    h &  \\
    & 1
  \end{pmatrix}
.
  \end{equation*}
\end{lemma}
\begin{proof}
  If $g \in \SO(n)$ fixes $e_n$,
  then
  the identity
  \begin{equation*}
  \langle g v, e_n \rangle = \langle v, g^t e_n \rangle
  = 
  \langle v, g^{-1} e_n \rangle
  \end{equation*}
  implies that $g^{-1}$ and hence $g$ also stabilizes
  the orthogonal complement $\langle e_n \rangle^\perp = \langle
  e_1,\dotsc,e_{n-1} \rangle$.
  We may thus put it in in the block-upper triangular
  form
  $g = 
\begin{pmatrix}
    h &  \\
    & 1
  \end{pmatrix}
$ for some $h \in \GL_{n-1}(\mathbb{R})$.
  The condition $g g^t = 1$ implies $h h^t = 1$,
  hence that $h \in \SO(n-1)$.
\end{proof}

Using the above lemmas,
we now complete the inductive step in the proof that $\SO(n)$ is
connected, i.e., that $\SO(n) = \SO(n)^0$.
Let $g \in \SO(n)$;
we wish to show that in fact $g \in \SO(n)^0$.
Consider the element $g e_n \in S^{n-1}$.
By Lemma
\ref{lem:so-n-connected-component-acts-transitively-on-sphere},
we may write $g e_n = \gamma e_n$ for some $\gamma \in
\SO(n)^0$.
We then have $\gamma^{-1} g e_n = e_n$,
i.e., $\gamma^{-1} g \in \Stab_{\SO(n)}(e_n)$.
By Lemma \ref{lem:compute-stabilizer-north-pole-so-n},
we have
$\gamma^{-1} g = h$ for some $h \in \SO(n-1) \hookrightarrow
\SO(n)$.
By the inductive hypothesis,
$\SO(n-1)$ is connected,
hence $\SO(n-1) \subseteq \SO(n)^0$.
Thus $g = \gamma h$ is the product
of two elements of $\SO(n)^0$;
since the latter is known to be a group,
we conclude that $g \in \SO(n)^0$.

\begin{remark}
  One doesn't actually need to prove Lemma
  \ref{lem:so-n-connected-component-acts-transitively-on-sphere}
  to complete the inductive argument;
  a slightly softer way to proceed
  is to make use of the following general lemma:
  \begin{lemma}
    Let $G$ be a topological group
    that acts on a topological space $X$
    (i.e., we assume given an action $\alpha : G \times X
    \rightarrow X$
    as in Definition \ref{defn:top-gp}).
    Assume that:
    \begin{enumerate}
    \item For $x_0 \in X$, the orbit map $G \rightarrow X$ given by $g \mapsto g x_0$ is a quotient map.
    \item The action is transitive.
    \item $X$ is connected.
    \item The stabilizer $H$ in $G$ of some (equivalently, any)
      point $x_0 \in X$
      is connected.
    \end{enumerate}
    Then $G$ is connected.
  \end{lemma}
  \begin{proof}
    If not, we may find a non-constant continuous map
    $f : G \rightarrow D$ for some discrete topological space $D$
    (e.g., $D = \{0,1\}$).  Since $H$ and hence any coset of $H$
    is connected, the restriction of $f$ to any coset of $H$ is
    constant,
    hence $f$ induces a continuous map
    $G/H \rightarrow D$,
    where $G/H$ is equipped with the quotient topology.
    Since the orbit map assumed to be a quotient map,
    we obtain a continuous map $X \rightarrow D$
    sending $g x_0$ to $f(g)$.
    Since $X$ is connected, this last map must be constant,
    hence so must the original map $f$.
    Therefore $G$ is connected.
  \end{proof}
  The orbit maps for the action $\SO(n) \circlearrowright S^{n-1}$
  are quotient maps
  because (for instance) $\SO(n)$ is Hausdorff and $S^{n-1}$ is
  compact.
  Alternatively, one can appeal here to Exercise \ref{exercise-topological-groups-quotient-map-homeomorphism},
  which tells us that the orbits map are open maps, hence are quotient maps.
\end{remark}

\section{Basics on the exponential map\label{sec:exp-map}}
\label{sec:org77618cc}
\subsection{Review of the matrix exponential\label{sec:matrix-exp}}
\label{sec:orgd602cac}
Let $\mathbf{k}$ be either $\mathbb{R}$ of $\mathbb{C}$.
Let $n \geq 1$
and
\begin{equation*}
  A := M_n(\mathbf{k}) := \Mat_{n \times n}(\mathbf{k}).
\end{equation*}
It is a finite-dimensional unital associative algebra over
$\mathbf{k}$ (and the discussion to follow applies more generally to any such algebra).
The operator norm $\|.\|$ on $A$ is given by
$\|x\| := \sup_{v \in \mathbf{k}^n : |v| = 1} |x v|$;
it satisfies the submultiplicativity property $\|x y\| \leq \|x\| \|y\|$.
The unit group of $A$ is
\begin{equation*}
  A^\times = \GL_n(\mathbf{k}).
\end{equation*}
For $x \in A$ with $\|x\| < 1$, the series
$\sum_{n \geq 0} x^n$ converges (by the same proof
as in the one-variable case, using the submultiplicativity).
Therefore $1 - x$ has the inverse $\sum_{n \geq 0} x^n$
and hence belongs to $A^\times$ whenever $\|x\| < 1$.
Therefore $A^\times$ is open in $A$.
(One can also see this more directly.)
Since $A$ is a Euclidean space,
it follows that we have natural identifications of tangent
spaces
\begin{equation*}
  T_0(A) = A,
  \quad
  T_1(A^\times) = A.
\end{equation*}
For any $x \in A$, the series
\begin{equation*}
  \exp(x) := \sum_{n \geq 0} \frac{x^n}{n!}
\end{equation*}
converges.
It satisfies the following properties:
\begin{enumerate}
\item $\exp$ is smooth
\item $\exp(0) = 1$
\item
  One has $\exp(x+y) = \exp(x) \exp(y)$ whenever $x,y$ commute
  (but not in general otherwise).
  In particular:
  \begin{enumerate}
  \item $\exp(x) \exp(-x) = \exp(0) = 1$,
    hence $\exp(A) \subseteq A^\times$.
  \item $\exp((s+t) x) = \exp(s x) \exp(t x)$ for all $s,t \in
    \mathbf{k}$,
    hence the map $\mathbf{k} \ni t \mapsto \exp(t x) \in
    A^\times$
    is morphism of Lie groups for each $x \in A$.
  \end{enumerate}
\item $\frac{d}{d t} \exp(t x) = x$,
  hence $T_0 \exp : A \rightarrow A$
  is the identity transformation.
  Consequently $\exp$ defines a local diffeomorphism
  at $0$,
  i.e., induces a diffeomorphism
  $\exp : U \rightarrow V$
  for some open $0 \in U \subseteq A$
  and $1 \in V \subseteq A^\times$.
\item
  An inverse to $\exp$ on the subset
  $\{1 - x : \|x\| < 1\}$ of $A^\times$
  is given by the logarithm
  \begin{equation*}
    \log(1-x) := - \sum_{n \geq 1} \frac{x^n}{n}.
  \end{equation*}
\item For $g \in A^\times$
  and $x \in A$
  one has $\exp(g x g^{-1}) = g \exp(x) g^{-1}$.
\end{enumerate}
For a diagonal matrix, one has
\begin{equation*}
  \exp (
\begin{pmatrix}
    t_1 &  &  \\
    & \dotsb  &  \\
    &  & t_n
  \end{pmatrix}
)
  = 
\begin{pmatrix}
    \exp(t_1) &  &  \\
    & \dotsb  &  \\
    &  & \exp(t_n)
  \end{pmatrix}
.
\end{equation*}
For a basic nilpotent Jordan block $N$,
given in the case (say) $n = 4$ by
\begin{equation*}
  N = 
\begin{pmatrix}
    & 1 &  & \\
    &  & 1 & \\
    &  & & 1 \\
    & & & 
  \end{pmatrix}
,
\end{equation*}
one has $N^k = 0$ for all $k \geq n$,
hence the series defining $\exp(N)$ is finite.
The series defining $\log(1 + t N)$ is also finite for any $t
\in \mathbf{k}$.
\begin{exercise}
 Using the above facts and Jordan decomposition,
show that
$\exp : M_n(\mathbb{C}) \rightarrow \GL_n(\mathbb{C})$ is
surjective. 
(The corresponding assertion over the reals is false
for several reasons to be discussed in due course.)
\end{exercise}
The exponential map is very rarely injective (away from the
origin);
for example,
$\exp(2 \pi i) = 1$,
and
\begin{equation*}
  \exp (\theta 
\begin{pmatrix}
     & 1 \\
    -1 & 
  \end{pmatrix}
)
  = 
\begin{pmatrix}
    \cos \theta  &  \sin \theta  \\
    - \sin \theta  & \cos \theta 
  \end{pmatrix}
.
\end{equation*}
Some other good examples to keep in mind are
\begin{equation*}
  \exp (t 
\begin{pmatrix}
     & 1 \\
    1 & 
  \end{pmatrix}
)
  = 
\begin{pmatrix}
    \cosh t  &  \sinh t  \\
    \sinh t  & \cosh t 
  \end{pmatrix}
\end{equation*}
and
\begin{equation*}
  \exp 
\begin{pmatrix}
    0 & x & z \\
     & 0 & y \\
     &  & 0
   \end{pmatrix}
   =
   \begin{pmatrix}
     1 & x & z + x y/2 \\
      & 1 & y \\
      &  & 1
   \end{pmatrix}
.
 \end{equation*}

\subsection{One-parameter subgroups\label{sec:one-param-subgps}}
\label{sec:org964bb98}
Let $G$ be a Lie group.  Let $\mathfrak{g} := \Lie(G)$
denote its Lie algebra.
\begin{definition}
  By a \emph{one-parameter subgroup} of
  $G$, we shall mean a morphism of Lie groups
  $\Phi : \mathbf{k} \rightarrow G$.
\end{definition}

\begin{remark}
~
  \begin{enumerate}
  \item Note the standard but slightly misleading terminology: a
    ``one-parameter subgroup'' $\Phi$ is not a subgroup;
    its image $\image(\Phi) \leq G$ a subgroup,
    but even if $\Phi$ is injective, the datum of $\Phi$ contains
    strictly more information than that of its image.
    For example, 
    the one-parameter subgroups $\Phi(t) := t$
    and $\Phi(t) := 2 t$ in the additive group $G = (\mathbf{k},+)$ have the same image.
  \item Note also that a one-parameter subgroup $\Phi$ of $G$ is, in
    particular, a curve (in the sense of
    \S\ref{sec:diff-geom-tangent-spaces}) with basepoint the
    identity element $e \in G$.  Its initial velocity $\Phi '(0)$
    is an element of $\mathfrak{g}$.
  \end{enumerate}
\end{remark}

\begin{example}\label{example:one-param-subgroups-gln}
  For $v \in M_n(\mathbf{k})$, the discussion of
  \S\ref{sec:matrix-exp} shows that the map
  $\Phi_v : \mathbf{k} \rightarrow \GL_n(\mathbf{k})$ given by
  $\Phi_v(t) := \exp(t v)$ is a one-parameter subgroup of the
  Lie group $\GL_n(\mathbf{k})$
  with initial velocity $\Phi_v'(0) = v$.
  Moreover, the map $M_n(\mathbf{k}) \ni v \mapsto \Phi_v(1) = \exp(v) \in \GL_n(\mathbf{k})$ is smooth.
\end{example}

For general $G$, a one-parameter subgroup $\Phi$ satisfies the
identity $\Phi(s+t) = \Phi(s) \Phi(t) = \Phi(t) \Phi(s)$.
Applying $\frac{d}{ d s}|_{s=0}$ to this identity gives the
differential equation\footnote{
  If the definition
  of products such as
  $\Phi'(0) \Phi(t)$ is unclear,
  one should either consult
  \S\ref{sec:translate-tangent-spaces-gp-elts}
  or (better)
  assume
  that $G$ is a linear Lie and interpret such products as
  as being given by matrix multiplication.
}
\begin{equation}\label{eq:one-param-subgp-diffeq}
  \Phi '(t)= \Phi '(0) \Phi(t) = \Phi(t) \Phi'(0).
\end{equation}
By the initial condition $\Phi(0) = 1$
and general uniqueness theorem for ODE's (\S\ref{sec:diffeq}),
it follows that $\Phi$ is determined uniquely
by its initial velocity $\Phi'(0) \in \mathfrak{g}$.
To put it another way, for each element $v \in \mathfrak{g}$,
there is \emph{at most one} one-parameter
subgroup $\Phi_v$ of $G$ with initial velocity $v$.
Conversely,
we will now show that such a one-parameter subgroup actually
exists, and moreover, that its values $\Phi_v(t)$ vary smoothly
with $v$.
For illustration, we explain a few different ways to establish existence:
\begin{enumerate}
\item 
  In the special case $G = \GL_n(\mathbf{k})$,
  one can just take $\Phi_v(t) := \exp(t v)$.
\item If $G$ is a \emph{linear} Lie group, that is to say, if it
  is a Lie subgroup of $\GL_n(\mathbf{k})$ for some $n$, so that
  $\mathfrak{g} \leq M_n(\mathbf{k})$, then it turns out that one can
  again take $\Phi_v(t) := \exp(t v)$
  where $\exp : M_n(\mathbf{k}) \rightarrow \GL_n(\mathbf{k})$
  is 
  as defined above.
  What requires proof here
  is the following:
  \begin{lemma}
    Let $G$ be a Lie subgroup of $\GL_n(\mathbf{k})$.
    Then $\exp(\mathfrak{g}) \subseteq G$.
  \end{lemma}
  \begin{proof}
    Let $x \in \mathfrak{g}$, and let
    $\gamma : \mathbf{k} \xdashrightarrow{} G$ be any curve with
    $\gamma '(0) = x$.  (Such a curve exists, more-or-less by
    definition of the tangent space; see
    \S\ref{sec:diff-geom-tangent-spaces} and especially Remark
    \ref{rmk:tangent-vectors-via-curves}.)  Since
    $G \leq \GL_n(\mathbf{k})$ and 
    $\gamma(t) = 1 + t v + o(t)$ as $t \rightarrow 0$, we may
    take for $t$ small enough the logarithm of $\gamma(t)$,
    which then satisfies
    $\log \gamma (t) = t v + o(t)$.  Taking
    $t := 1/n$ with $n \in \mathbb{Z}$ tending off to $\infty$
    gives $n \log \gamma(1/n) = n ( v/n + o(1/n)) = v + o(1)$.
    Exponentiating, one obtains $\gamma(1/n)^n = \exp(v + o(1))$.
    Hence $\exp(v) = \lim_{n \rightarrow \infty} \gamma(1/n)^n$.
    Since $\gamma$ is a curve in $G$ and $G$ is a group, we have
    $\gamma(1/n)^n \in G$ for all $n$.  Since $G$ is closed (see
    \S\ref{sec:lie-subgroups-are-closed}), it follows that
    $\exp(v) \in G$, as required.
  \end{proof}
\item For general $G$, we can appeal to existence theorems for
  ODE's (see \S\ref{sec:diffeq})  to produce a curve
  $\gamma : \mathbf{k} \xdashrightarrow{} G$ satisfying
  \begin{equation}\label{eq:curve-in-G-used-to-produce-exp}
    \gamma(0) = 1 \in G,
    \quad \gamma'(t) = \gamma(t) v
  \end{equation}
  for all $t$ in the domain of $\gamma$.  
  Moreover, the values of $\gamma$ vary smoothly
  with the initial data $v$.  A priori, the domain
  of $\gamma$ might be quite small, but we can now use the group
  structure on $G$, as follows, to enlarge it to all of
  $\mathbf{k}$.  To that end, it suffices to show
  that each solution
  $\gamma$ to
  \eqref{eq:curve-in-G-used-to-produce-exp} 
  on some
  ball\footnote{$\mathbf{k} = \mathbb{R}$, ``ball'' means
    ``interval'', of course} $B$ with the center the origin 
  can be extended to a solution domain the enlarged ball $2 B$ of twice the
  radius of $B$; iterating this, one eventually obtains a solution on all of $\mathbf{k}$.
  (Many variations on this argument are also possible.)
  \begin{enumerate}
  \item For $s \in B$, the curve
    $\gamma_s(t) := \gamma(s)^{-1} \gamma(s + t)$ is defined
    whenever $s+t \in B$ and satisfies the
    same initial condition $\gamma_s(0) = 1$ and differential
    equation
    $\gamma_s'(t) = \gamma(s)^{-1} \gamma'(s + t) = \gamma(s)^{-1}
    \gamma(s+t) v = \gamma_s(t) v$
    as $\gamma(t)$ does.
    By the uniqueness theorem cited above,
    we deduce that
    $\gamma_s(t) = \gamma(t)$
    and hence that 
    \begin{equation}\label{eq:curve-is-local-hom-on-B}
      \gamma(s+t) = \gamma(s) \gamma(t) \text{ provided that }s,t, s+t
      \in B.
    \end{equation}
  \item For any $s \in B$, denote by $\delta_s$ the curve in $G$
    with domain $B + s := \{t : t - s \in B\}$ given by
    $\delta_s(t) := \gamma(t-s) \gamma(s)$.
    By differentiating
    the condition \eqref{eq:curve-is-local-hom-on-B} , one sees
    that $\delta_s$ defines a solution to
    \eqref{eq:curve-in-G-used-to-produce-exp} on its domain.  For
    any two $s_1,s_2 \in B$, the identity
    \eqref{eq:curve-is-local-hom-on-B} implies that
    $\delta_{s_1} = \gamma = \delta_{s_2}$ on the neighborhood
    $B + s_1 \cap B \cap B + s_2$ of the origin, hence by the
    uniqueness theorem cited above also that
    $\delta_{s_1} = \delta_{s_2}$ on $B+ s_1 \cap B + s_2$.  
    By the smoothness of the group operation in
    \eqref{eq:curve-is-local-hom-on-B},
    we see that the values of $\delta_s$ still vary smoothly with $v$.
  \item Since
    $\cup \{B + s : s \in B\} = 2 B$, we can patch together the solutions given in the previous
    step to obtain a well-defined curve
    $\tilde{\gamma} : 2 B \rightarrow G$ given for $t \in B+s$ by
    $\tilde{\gamma}(t) := \delta_s(t - s)$.
    This curve solves \eqref{eq:curve-in-G-used-to-produce-exp},
    and extends $\gamma$, as required.
  \end{enumerate}
\end{enumerate}

To summarize the above discussion, we have the following:
\begin{theorem}\label{thm:classification-smooth-1-param-subgroups}
  Let $G$ be a Lie group with Lie algebra
  $\mathfrak{g} := \Lie(G)$.
  For each $x \in \mathfrak{g}$ there exists a unique one-parameter
  subgroup $\Phi_x$ of $G$ for which $\Phi_x'(0) = x$.
  Moreover,
  the map $x \mapsto \Phi_x(1)$ is smooth.
\end{theorem}

\subsection{Definition and basic properties of exponential map}
\label{sec:org1d85865}
\begin{definition}
  Let $G$ be a Lie group
  with Lie algebra $\mathfrak{g}$.
  The \emph{exponential map}
  $\exp : \mathfrak{g} \rightarrow G$
  is defined by $\exp(x) := \Phi_x(1)$,
  where $\Phi_x$ denotes the unique one-parameter
  subgroup of $G$ having initial velocity $\Phi_x'(0) = x$.
\end{definition}
The notation is consistent with that discussed in
\S\ref{sec:matrix-exp}.
We also have the following immediate consequences of \S\ref{sec:one-param-subgps}:
\begin{theorem}[Lie's first theorem]\label{thm:exp-local-diffeo}
  For a Lie group $G$ with Lie algebra $\mathfrak{g}$,
  the exponential map $\exp : \mathfrak{g} \rightarrow G$
  has the following properties:
  \begin{enumerate}
  \item $\exp : \mathfrak{g} \rightarrow G$ is smooth
  \item The derivative
    $T_0 \exp : \mathfrak{g} \rightarrow \mathfrak{g}$
    is the identity transformation,
    or equivalently,
    $\frac{d}{d t} \exp(t x) = x$ for all $x \in \mathfrak{g}$.
  \item $\exp : \mathfrak{g} \rightarrow G$
    is a local diffeomorphism at $0$,
    i.e.,
    there are open $0 \in U \subseteq \mathfrak{g}$
    and $1 \in V \subseteq G$
    so that $\exp : U \rightarrow V$
    is a diffeomorphism.
  \item For any $X \in \mathfrak{g}$, the map
    $\mathbf{k} \ni t \mapsto \exp(t x)$ is the unique
    one-parameter subgroup of $G$ with initial velocity $x$.
  \item
    Let $\gamma$ be any curve in $G$ with basepoint given by the
    identity.
    Set $X := \gamma '(0) \in \mathfrak{g}$.
    Then
    $\exp(X) = \lim_{n \rightarrow \infty} \gamma(1/n)^n$.
  \end{enumerate}
\end{theorem}
(The proof of the final assertion proceeds
exactly as in the case of linear Lie groups
now that we have the logarithm map on general Lie groups at our
disposal.)

Here is another key consequence:
\begin{corollary}\label{cor:connected-component-generated-by-exp}
  $\exp(\mathfrak{g})$ generates the connected component $G^0$ of $G$.
  In particular, if $G$ is connected, then $G$ is generated by $\exp(\mathfrak{g})$.
\end{corollary}
\begin{proof}
  We saw earlier that $G^0$ is generated by any neighborhood
  of the identity,
  and saw just now that $\exp$ is a local diffeomorphism
  at $0$;
  in particular, $\exp(\mathfrak{g})$ contains a neighborhood
  of the identity in $G$.
\end{proof}
\begin{remark}\label{rmk:exp-need-not-surject}
  Even if $G$ is connected, it need not be the case that
  $\exp(\mathfrak{g}) = G$.
  For example, one can show that
  \begin{equation*}
    \begin{pmatrix}
      -1/2 &  \\
       & -2
    \end{pmatrix}
 \notin \exp(\mathfrak{s} \mathfrak{l} _2(\mathbb{R})).
  \end{equation*}
  It is a non-obvious fact (which we might conceivably see later
  in the course)
  that if $G$ is a compact connected  Lie group,
  then $\exp(\mathfrak{g}) = G$.
\end{remark}

\subsection{Application to detecting invariance by a connected Lie group\label{sec:appl-inv-by-connected}}
\label{sec:org4b91333}
In lecture, we explained how the connectedness of $\SO(n)$
and Lie's first theorem
imply that a smooth function
$f : \mathbb{R}^n \rightarrow \mathbb{R}$
is rotation-invariant (i.e., $f(x)$ depends only upon $|x|$)
if and only if it satisfies the finite system
of homogeneous linear differential equations
\begin{equation}\label{eqn:pdes-characterizing-rotation-invariance}
  x_j \frac{\partial }{\partial x_i} f(x)
  = 
  x_i \frac{\partial }{\partial x_j} f(x)
  \quad \text{ for all }
  1 \leq i < j \leq n.
\end{equation}
This is computationally useful; for instance, it applies when $f$ is a polynomial of large degree in many variables.
This is not an earth-shaking fact, and it can probably be proved
directly in various ways, but the proof we will give here
illustrates in a simple way a rather fundamental technique of Lie theory.

To summarize the proof, we noted that the first condition
is visibly equivalent to
\begin{equation}\label{eq:group-invariance-son}
    f(g x) = f(x) \text{ for all } g \in \SO(n), x \in \mathbb{R}^n
\end{equation}
while the second is visibly equivalent to
\begin{equation}\label{eq:lie-invariance-basis-son}
    X f = 0 \text{ for all } X \in \mathcal{B}
\end{equation}
where for $X \in \mathfrak{g}$
we set
\begin{equation*}
  X f(x) := \frac{d}{d \eps} f(\exp(-\eps  X) x) |_{\eps=0}
\end{equation*}
and
where
$\mathcal{B}$ is the basis of $\so(n)$
given by
\begin{equation*}
  \mathcal{B} := \{X_{i j} : 1 \leq i < j \leq n\}
\end{equation*}
where
\begin{equation*}
  X_{i j} := E_{i j} - E_{j i}
\end{equation*}
where
$E_{i j}$ has a $1$ in the $(i,j)$ entry and $0$'s elsewhere;
for instance,
when $n = 3$,
a basis for $\so(3)$ is given by
\begin{equation*}
  X_{12}
  \begin{pmatrix}
    & 1 &  \\
    -1 &  &  \\
    &  & 
  \end{pmatrix}
,
  \quad 
  X_{13}
  \begin{pmatrix}
    &  & 1\\
    &  &  \\
    -1 &  & 
  \end{pmatrix}
,
  \quad 
  X_{23}
  \begin{pmatrix}
    &  & \\
    &  & 1 \\
    & -1  & 
  \end{pmatrix}
.
\end{equation*}
To relate \eqref{eqn:pdes-characterizing-rotation-invariance}
to \eqref{eq:lie-invariance-basis-son}
we used that
\begin{equation*}
  X_{i j} x =
  x_j e_i - x_i e_j
\end{equation*}
and hence that
\begin{equation*}
  f(x \exp(-\eps X_{i j}) x)
  = f(x  -  \eps  (x_j e_i - x_i e_j) + O(\eps ^2))
\end{equation*}
to compute that
\begin{equation*}
  X_{i j} f(x) = -
  (x_j \frac{\partial }{\partial x_i}
  - x_i \frac{\partial }{\partial x_j}
  ) f(x).
\end{equation*}
Each of the following conditions is visible equivalent to the next:
\begin{enumerate}
\item The condition
  \eqref{eq:group-invariance-son}.
\item The condition
  \eqref{eq:group-invariance-son}
  but restricted to $g$ in a generating set for $G$.
\item
   The condition
   \eqref{eq:group-invariance-son}
   for $g \in \exp(\mathfrak{g})$.
   (Here we use Lie's theorem and the connectedness of $G$.)
 \item The condition that
   \begin{equation*}
     f(\exp(-t X) x)
   \end{equation*}
   be independent of $t$ for all $x \in \mathbb{R}^n$ and all $X
   \in \mathfrak{g}$.
 \item The condition that
   \begin{equation}\label{eqn:some-derivative-bla-blab-alsdfsd-}
     \frac{d}{d t} f(\exp(-t X) x) = 0
   \end{equation}
   for all $x \in \mathbb{R}^n$ and all $X
   \in \mathfrak{g}$.
 \item The condition
  that $X f = 0$ for all $X \in \mathfrak{g}$.
   To relate this to the previous condition,
   we used the following key calculation:
   \begin{align*}
     \frac{d}{d t} f(\exp(-t X) x)
     &= 
       \frac{d}{d \eps} f(\exp(-(t+\eps) X) x)|_{\eps=0}
       \\
     &= 
       \frac{d}{d \eps} f(\exp(- \eps X) \exp(-t X) x)|_{\eps=0}
       \\
     &= 
     X f(\exp(-t X) x).
   \end{align*}
   Thus if \eqref{eqn:some-derivative-bla-blab-alsdfsd-} holds,
   then $X f (\exp(-t X) x) = 0$ for all $t,X,x$,
   and in particular for $t = 0$,
   giving $X f(x) = 0$ and thus $X f = 0$.
   Conversely,
   if $X f = 0$,
   then in particular $X f(\exp(-t X) x) = 0$ for all $t,X,x$;
   the above calculation then implies that
   \eqref{eqn:some-derivative-bla-blab-alsdfsd-} holds.
\item The condition
  \eqref{eq:lie-invariance-basis-son}.
  (Here we used that $X \mapsto X f$ is linear to reduce from
  testing 
  all $X \in \mathfrak{g}$
  to testing those in the basis $\mathcal{B}$.)
\end{enumerate}
\begin{remark}
  It's not too hard to show that
  $\exp : \so(n) \rightarrow \SO(n)$ is actually surjective (in
  contrast to the general case mentioned in Remark
  \ref{rmk:exp-need-not-surject}), so we didn't \emph{really}
  need Lie's theorem in the above argument, the point of which
  was to illustrate a technique in a simple case.
\end{remark}

\subsection{Connected Lie subgroups are determined by their Lie algebras}
\label{sec:org51d5203}
\begin{theorem}
  Let $G$ be a Lie group.
  Then connected Lie subgroups of $G$ are classified by their
  Lie algebra: if $H_1, H_2$ are two connected Lie subgroups of
  $G$ for which $\Lie(H_1) = \Lie(H_2)$, then $H_1 = H_2$.
\end{theorem}
\begin{proof}
  We know by Exercise
  \ref{exercise:connected-topological-group-generated-by-any-neighborhood}
  that $H_i$ is generated
  by any neighborhood of the identity;
  by Theorem \ref{thm:exp-local-diffeo},
  it follows that $H_i$
  is generated by $\exp(\Lie(H_i))$.
  Since $H_1,H_2$ are generated by the same set,
  they are equal.
\end{proof}

\subsection{The exponential map commutes with morphisms\label{sec:exp-commutes-with-morphisms}}
\label{sec:org45e312b}
\begin{theorem}
  The exponential map commutes with Lie group morphisms:
  If $f : G \rightarrow H$ is a morphism of Lie groups
  and $x \in \Lie(G)$ is given,
  then
  $f(\exp (x)) = \exp(d f(x))$, where $d f := T_e f : \mathfrak{g} \rightarrow \mathfrak{h}$.
\end{theorem}
For the sake of illustration, we record a few proofs.
\begin{proof}
[Proof \#1]
  It suffices to show that
  $f(\exp(t x)) = \exp(d f(t x))$ for all $t$.
  But now both sides, viewed as functions of $t$,
  are one-parameter subgroups
  of $H$
  with inital velocity
  $d f(x) = T_e f(x)$:
  indeed, we have
  \begin{equation*}
  f(\exp(t x))
  = f(1 + t x + o(t))
  = 1 + t \,d f(x)  + o(t)
  \end{equation*}
  and similarly
  \begin{equation*}
  \exp(d f(t x)) = 1 + t \, d f(x) + o(t).
  \end{equation*}
  By the uniqueness given in Theorem
  \ref{thm:classification-smooth-1-param-subgroups},
  we conclude.
\end{proof}
\begin{proof}
[Proof \#2]
  Let $\gamma$ be any curve on $G$ with
  basepoint $\gamma(0) = e$ and initial velocity $\gamma '(0) =
  x$.
  The curve $f \circ \gamma$ on $H$ then has basepoint $e$
  and initial velocity $(f \circ \gamma )'(0) = d f(x)$.
  By the final assertion of Theorem \ref{thm:exp-local-diffeo},
  we have
  \begin{equation*}
\exp (x) = \lim_{n \rightarrow \infty} \gamma(1/n)^n
\end{equation*}
  and similarly $\exp(d f(x)) = \lim_{n \rightarrow \infty}
  f(\gamma(1/n))^n$.
  Since $f$ is a continuous group homomorphism, it follows that
  \begin{equation*}
  f(\exp(x))
  = f( \lim_{n \rightarrow \infty} \gamma(1/n)^n)
  = \lim_{n \rightarrow \infty} f(\gamma(1/n))^n
  = \exp(d f(x)),
  \end{equation*}
  as required.
\end{proof}

\begin{example}
  Set $f := \det : \GL_n(\mathbf{k}) \rightarrow
  \mathbf{k}^\times$.
  Since $\det(1 + \eps X) = 1 + \eps \trace(X) + O(\eps^2)$,
  we have $d f = \trace : M_n(\mathbf{k}) \rightarrow
  \mathbf{k}$.
  The theorem then implies for $X \in M_n(\mathbf{k})$ that
  \begin{equation*}
    \det(\exp(X)) = f(\exp(X))
    = \exp(d f(X)) = \exp(\trace(X)),
  \end{equation*}
  which can also be seen directly via Jordan decomposition.
\end{example}

\subsection{Morphisms out of a connected Lie group are determined by their differentials}
\label{sec:org84c10f5}
\begin{theorem}
  Let $G$ be a connected Lie group
  and $H$ any Lie group.
  Then morphisms $f : G \rightarrow H$
  are determined by their differentials
  $d f : \mathfrak{g} \rightarrow \mathfrak{h}$.
\end{theorem}
\begin{proof}
  Since $G$ is connected,
  it is generated by a neighborhood
  of the identity, hence in particular
  by $\exp(\mathfrak{g})$.
  Since $f$ is a homomorphism,
  it is thus determined
  by the quantities $\exp(X)$ for all $X \in \mathfrak{g}$.
  But by the result of \S\ref{sec:exp-commutes-with-morphisms},
  we have
  $f(\exp(X)) = \exp(d f(X))$.
  Hence $f$ is determined by $f$.
\end{proof}

\section{Putting the ``algebra'' in ``Lie algebra''}
\label{sec:org03a946d}
\subsection{The commutator of small group elements}
\label{sec:org3a6e4da}
Let $G$ be a Lie subgroup of $\GL_n(\mathbf{k})$.
Consider a pair of elements $X,Y \in \mathfrak{g} := \Lie(G)$
and a corresponding pair of curves $\xi, \eta : \mathbf{k}
\xdashrightarrow{} G$
with basepoints $\xi(0) = \eta(0) = 1$
and initial velocities $\xi ' (0) = X,
\eta '(0) = Y$.
For example:
\begin{enumerate}
\item One can always take $\xi(s) := \exp(s X),
\eta(t) := \exp(t Y)$.
\item If $G = \GL_n(\mathbf{k})$,
  one could also take $\xi(s) := 1 + s X, \eta(t) := 1 + t Y$.
\end{enumerate}
We can then consider, for small enough $s,t \in \mathbf{k}$,
the commutator
\begin{equation*}
  \Gamma(s,t) :=
  (\xi(s),\eta(t))
  :=
  \xi(s) \eta(t) \xi(s)^{-1} \eta(t)^{-1}.
\end{equation*}
From the basepoint condition
we have
\begin{equation*}
  \Gamma(0,0) = \Gamma(s,0) = \Gamma(0,t) = 1,
\end{equation*}
so every term in the Taylor expansion of $\Gamma(s,t) - 1$
is divisible by $s t$.
We now determine the coefficient of $s t$,
or equivalently,
the derivative $\partial_{s=0} \partial_{t=0} \Gamma(s,t)$.
To compute this,
we write
\begin{equation*}
  \xi(s) \eta(t) = \Gamma(s,t) \eta(t) \xi(s)
\end{equation*}
and differentiate both sides,
first with respect to $s$ at $s=0$,
giving
\begin{equation*}
  X \eta(t) = \partial_{s=0} \Gamma(s,t) \eta(t) \xi(0)
  + \Gamma(0,t) \eta(t) X
  =
  \partial_{s=0} \Gamma(s,t) \eta(t)
  + \eta(t) X
\end{equation*}
and then with respect to $t$ at $t=0$,
giving after analogous simplifications that
\begin{equation*}
  X Y = \partial_{s=0} \partial_{t=0} \Gamma(s,t)
  + Y X,
\end{equation*}
whence
\begin{equation*}
  \Gamma(s,t) = X Y - Y X.
\end{equation*}

\subsection{The Lie bracket as an infinitesimal commutator}
\label{sec:org166886b}
For a general Lie group $G$,
we define the commutator bracket $[,] : \mathfrak{g}  \otimes
\mathfrak{g} \rightarrow \mathfrak{g}$
by setting
\begin{equation*}
  [X,Y] := \partial_{s=0}
  \partial_{t=0} \Gamma(s,t),
\end{equation*}
with notation as in the previous section.
To interpret this properly,
we have
\begin{equation*}
  \partial_{t=0}
  \Gamma(s,t)
  =
  \partial_{t=0}
  \xi(s)
  \eta(t)
  \xi(s)^{-1}
  \eta(t)^{-1}
\end{equation*}
This is the initial velocity of a curve
passing through the identity element of $G$ at time $t=0$,
hence it makes sense to regard it as an element of
$\mathfrak{g}$.
Thus
\begin{equation*}
  s \mapsto \partial_{t=0} \Gamma(s,t)
\end{equation*}
defines a curve in $\mathfrak{g}$.
Hence its $s$-derivative at $s=0$
defines an element of $\mathfrak{g}$.
Similar arguments show that $[X,Y]$ is independent of the choice of $\xi$,
$\eta$; alternatively, one could always take
$\xi(s) := \exp(s X), \eta(t) := \exp(t Y)$
in the definition, but it's occasionally convenient
to make other choices.

The bracket $[,]$
on the Lie algebra $\mathfrak{g}$ of a Lie group $G$
has the following properties:
\begin{enumerate}
\item $[,]$ is bilinear.  This is immediate from the definition
\item $[X,X] = 0$.  This is again immediate from the definition.
  It follows that
  $[X+Y,X+Y] = [X,X]  + [X,Y] + [Y,X] + [Y,Y]
  = [X,Y] + [Y,X]$,
  hence that
  \begin{equation}\label{eq:lie-bracket-alternating}
    [X,Y] = - [Y,X].
  \end{equation}
\item It satisfies the Jacobi identity
  \begin{equation}\label{eq:jacobi-identity}
    [X,[Y,Z]]
    +
    [Y,[Z,X]]
    +
    [Z,[X,Y]]
    = 0
  \end{equation}
  which, thanks to \eqref{eq:lie-bracket-alternating},
  can be put in the equivalent forms
  \begin{equation*}
    [X,[Y,Z]]
    =[[X,Y],Z]
    + [Y,[X,Z]]
  \end{equation*}
  or
  \begin{equation*}
    [[X,Y],Z]=
    [[X,Z],Y]
    + [X,[Y,Z]].
  \end{equation*}
  In the linear case $G \leq \GL_n(\mathbf{k})$,
  one can prove the Jacobi identity
  by expanding everything out using the identity $[X,Y] = X Y -
  Y X$.
  In general,
  they follow
  from the associativity of the group law in $G$
  in the form
  \begin{equation*}
    g h = (g h g^{-1}) g
  \end{equation*}
  together with some artful use of the chain rule.  We do not
  give the details here; we promise instead that a couple more
  ``conceptual'' derivations of the Jacobi identity will be
  given later.
\end{enumerate}

\subsection{Lie algebras}
\label{sec:org0f4ac0a}
\begin{definition}
  A \emph{Lie algebra} $L$ over $\mathbf{k}$ is a
  vector space
  equipped with a bilinear form $[,] : L \otimes L \rightarrow
  L$
  satisfying the properties mentioned in the previous section.
\end{definition}

Here are the basic examples:
\begin{enumerate}
\item We've seen (modulo verification
  of the Jacobi identity in general)
  that for any Lie group $G$,
  what we've already called the ``Lie algebra''
  $\mathfrak{g} := \Lie(G)$ of $G$ is in fact
  a Lie algebra in the above sense
  when equipped with the commutator bracket as we've defined it.
\item Any associative algebra $A$ over $\mathbf{k}$
  becomes a Lie algebra when equipped with the bracket
  $[x,y] := x y - y x$.
  A notable example is when $A = \End(V)$ for a vector space
  $V$.
  If $V = \mathbf{k}^n$, then of course $A = \End(V) =
  M_n(\mathbf{k})$.
\item If $L_1$ is a Lie algebra
  and $L_2 \leq L_1$ is a vector subspace
  with the property that $[x,y] \in L_2$ whenever $x,y \in L_2$,
  then $L_2$ is a Lie algebra when equipped with the commutator
  bracket induced from that on $L_1$;
  it is then (fittingly) called a \emph{Lie subalgebra}
  of $L_1$.
\item Given an algebra $A$,
  the
  space $\Der(A)$
  of
  \emph{derivation} of $A$
  (i.e., $\mathbf{k}$ -linear maps $D  : A \rightarrow A$
  satisfying $D(x \cdot y) = D x \cdot y + x \cdot D y$)
  is a Lie algebra.
  (Exercise: check this.)
  It is a Lie subalgebra of $\End(A)$.

  When $A$ is finite-dimensional,
  one can show that $\Aut(A)$ is a Lie group with Lie algebra
  $\Der(A)$,
  hence that this example is a special case of the first one.
  But what we've said here applies (usefully) also
  when $A$ is infinite-dimensional; see below.
\end{enumerate}

Every finite-dimensional example is already of the above form:
\begin{theorem}
[Ado]
  Let $L$ be a finite-dimensional Lie algebra.
  Then $L$ is isomorphic to a Lie subalgebra of $\End(V)$
  for some finite-dimensional vector space $V$.
\end{theorem}
The proof of this innocent-sounding theorem is not egregiously
difficult, but does seem to require most of the basic structure
theory of Lie algebras, and so will not be proved now.
However, it may aid intuition to know up front
that one can always think of any finite-dimensional Lie algebra
as a Lie subalgebra of some matrix algebra.

A special case of the final example mentioned above
is when $A = C^\infty(M)$
for a manifold $M$.
In that case,
it is known that
\begin{equation*}
  \Der(C^\infty(M))
  \cong \operatorname{Vect}(M)
\end{equation*}
where $\operatorname{Vect}(M)$ denotes the space of vector fields
on $M$,
i.e.,
smooth assignments
\begin{equation*}
  X : M \rightarrow T M := \sqcup_{p \in M} T_p M
\end{equation*}
satisfying $X_p := X(p) \in T_p M$ for all $p \in M$.
Such a vector field induces a derivation
by the rule: for $f \in C^\infty(M)$,
the image $X f \in C^\infty(M)$ under $X \in \operatorname{Vect}(X)$
is defined to be
\begin{equation*}
  (X f)(p) := (T_p f)(X_p),
\end{equation*}
i.e., ``the directional derivative of $f$ at $p$ in the direction
of the tangent vector $X_p$.''


\begin{remark}
  \label{rmk:}
  It can be instructive to check for some simple examples
  of linear Lie groups $G \leq \GL_n(\mathbf{k})$
  with Lie algebra $\mathfrak{g} \leq M_n(\mathbf{k})$
  that the bracket $[,]$ does in fact preserve $\mathfrak{g}$
  (as it must).
  For $X,Y \in \slLie_n(\mathbf{k})$
  we have $\trace([X,Y]) = 0$; indeed, the trace of any commutator
  is zero.
  For $X,Y \in \mathfrak{o}_n(\mathbf{k})$,
  so that $X + X^t = 0, Y + Y^t = 0$,
  we have $[X,Y]^t = (X Y)^t - (Y X)^t
  = Y^t X^t - X^t Y^t
  = (-Y)(-X) - (-X)(-Y)
  = -[X,Y]$,
  hence $[X,Y] \in \mathfrak{o}_n(\mathbf{k})$.
\end{remark}

\subsection{The Lie bracket commutes with differentials of morphisms\label{sec:diff-morphism-is-morphism}}
\label{sec:org96fd5c4}
Let $f : G \rightarrow H$ be a morphism of Lie groups.
Then for $X, Y \in \mathfrak{g}$,
one has
\begin{align*}
  d f ([X,Y])
  &=
    d f( \partial_{s=0} \partial_{t=0} (e^{s X}, e^{t Y}))
  &&
  \text{ (definition of $[,]$) }
  \\ 
  &=
    \partial_{s=0}  d f( \partial_{t=0} (e^{s X}, e^{t Y}))
    && \text{ ($d f$ is linear) }
    \\
  &=
    \partial_{s=0}  \partial_{t=0}
    f(  (e^{s X}, e^{t Y}))
    && \text{ (definition of $d f$) }
    \\
  &=
    \partial_{s=0}  \partial_{t=0}
    (f(e^{s X}), f(e^{t Y}))
    && \text{ ($f$ is a homomorphism) }
    \\
  &=
    \partial_{s=0}  \partial_{t=0}
    (e^{d f(s X)}, e^{t \, d f(s Y)})
  && \text{ ($f \circ \exp = \exp \circ d f$)}
     \\
  &=
    \partial_{s=0}  \partial_{t=0}
    (e^{s \, d f(X)}, e^{t \, d f(Y)})
  && \text{ ($d f$ is linear)}
     \\
  &=
    [d f(X), d f(Y)]
  && \text{ (definition of $[,]$)}.
\end{align*}
Thus $d f : \mathfrak{g} \rightarrow \mathfrak{h}$ is a morphismo f Lie algebras.
\section{How pretend that every Lie group is a matrix group and survive\label{sec:pretend-lie-groups-are-matrix-groups}}
\label{sec:org7275cd0}
(TODO: rewrite this section.)
For many arguments it is convenient to assume that
a Lie group $G$ is a matrix group, i.e., embeds in $\GL_n(\mathbb{R})$,
so that its Lie algebra embeds in $M_n(\mathbb{R})$.
Then lots of stuff simplifies
(in a non-serious way) because we can just regard
everything as a matrix and not worry about
which tangent space stuff belongs to, etc.
Not every Lie group is a matrix group,
but they are all close enough to being matrix groups (e.g., up
to covering homomorphisms
to be discussed later) that nothing really bad goes wrong
if one pretends that they are.
For example, it was convenient in class today to pretend that $G$
was a matrix Lie group when discussing the proof of Maurer--Cartan equations.

However, there is a rigorous
trick by which one can always
treat a Lie group $G$ as if it were a matrix group
by embedding it in the space $\GL(C^\infty(G))$
of linear automorphisms
of the (typically infinite-dimensional)
vector space $C^\infty(G)$.
The fact that $\GL(C^\infty(G))$
is not itself a Lie group doesn't matter much in practice.
More precisely, one defines an injective homomorphism
\begin{equation*}
  G \hookrightarrow \GL(C^\infty(G))
\end{equation*}
as follows:
we identify each $g \in G$ with the element of $\Aut(C^\infty(G))$
that sends a smooth function $\varphi \in C^\infty(G)$
to the new function $g \varphi \in C^\infty(G)$
given by right translation:
for $x \in G$,
\begin{equation*}
  g \varphi(x) := \varphi(x g).
\end{equation*}
(This is an action: $(g_1 g_2) \varphi = g_1 (g_2 \varphi)$.)
It makes sense to differentiate this action of $G$ element-wise.
We obtain in this way induces a morphism
$X \hookrightarrow \End(C^\infty(G))$,
whose image actually lies in
an easily characterized
subspace of
$\Der(C^\infty(G))$; more on that later.
The action of $X \in \mathfrak{g}$ on $\varphi \in C^\infty(G)$
is given by
\begin{equation*}
  X \varphi(x) := \partial_{t=0}
  \varphi(x \exp(t X)).
\end{equation*}
(This is a Lie algebra representation:
$[X,Y] \varphi = X Y \varphi - Y X \varphi$.)
In this way, one can make perfectly rigorous sense
of identities such as
\begin{equation*}
  [X,Y] = X Y - Y X
\end{equation*}
or
\begin{equation*}
  \Ad(g) X = g X g^{-1}
\end{equation*}
even when $G$ is not a matrix Lie group:
for instance, the products $X Y$ in the above expression
are just the compositions taking place inside
$\End(C^\infty(G))$.

\section{Something about representations, mostly \texorpdfstring{$\SL_2$}{SL2}}
\label{sec:orgf4f0402}
\label{sec:reps-sl2}
\subsection{Some preliminaries}
\label{sec:org2880980}
We have spoken so far in the course quite a bit about
$\GL_n(\mathbf{k})$ and its Lie algebra $M_n(\mathbf{k})$.
More abstractly, one can work with any finite-dimensional
vector space $V$ over $\mathbf{k}$.
Then $\GL(V)$ is a Lie group over $\mathbf{k}$ with Lie algebra $\End(V)$.
If $V = \mathbf{k}^n$,
then $\GL(V) = \GL_n(\mathbf{k})$ and $\End(V) =
M_n(\mathbf{k})$.

When $\mathbf{k} = \mathbb{C}$,
we can regard $\GL(V)$ either as a complex Lie group or as a
real Lie group.

\subsection{Definition}
\label{sec:org959bbb9}
\begin{definition}
  Let $\mathbf{k} = \mathbb{R}$ or $\mathbb{C}$.
  For us, a \emph{representation}
  of a Lie group $G$
  is a pair $(V,R)$,
  where
  \begin{itemize}
  \item $V$ is a finite-dimensional vector space and
  \item $R : G \rightarrow \GL(V)$ is a morphism of Lie groups over $\mathbf{k}$.
  \end{itemize}
  We allow the possibility
  that $\mathbf{k} = \mathbb{R}$
  and $V$ is a complex vector space
  but regarded as a real vector space ($\mathbb{C}^n \cong \mathbb{R}^{
    2n}$).
  When we wish to be more specific, we might introduce
  the following terminology:
  \begin{enumerate}
  \item Let $G$ be a real Lie group.
    A \emph{real representation} of $G$ is a morphism
    of real Lie groups
    $R : G \rightarrow \GL(V)$ (i.e.,
    an infinitely-real-differentiable group homomorphism).
  \item Let $G$ be a real Lie group.
    A \emph{complex representation} of $G$ is a morphism
    of real Lie groups
    $R : G \rightarrow \GL(V)$
    (i.e.,
    an infinitely-real-differentiable group homomorphism).
  \item Let $G$ be a complex Lie group.
    A \emph{holomorphic representation} of $G$ is a morphism
    of complex Lie groups
    $R : G \rightarrow \GL(V)$
    (i.e.,
    a complex-differentiable group homomorphism).
  \end{enumerate}
  One can also regard a complex Lie group as a real Lie group
  and consider its representations in that sense.
  % Even when $We allow for the possibility that $V$ is a \emph{complex}
  % One might more verbosely refer to such a pair
  % as a \emph{finite-dimensional complex representation}
  % when $\mathbf{k} = \mathbb{R}$
  % and as a \emph{finite-dimensional holomorphic representation}
  % when $\mathbf{k} = \mathbb{C}$.
  % Note that when $\mathbf{k} = \mathbb{R}$,
  % we regard $\GL(V)$ as a real Lie group even though $V$ is a complex vector space.

  A \emph{representation}
  of a Lie algebra $\mathfrak{g}$
  is likewise a pair $(V,\rho)$,
  where $V$ is as above
  and
  $\rho : \mathfrak{g} \rightarrow \End(V)$
  is a morphism of Lie algebras over $\mathbf{k}$.
  We can also speak of real or complex representations
  of Lie algebras, or of holomorphic representations of complex
  Lie algebras.

  The action of a representation
  $R : G \rightarrow \GL(V)$
  is often abbreviated $g v := R(g) v$
  and likewise
  that of
  $\rho  : \mathfrak{g}  \rightarrow \End(V)$
  by $X v := \rho(X) v$.

  Given two representations
  $R_1 : G \rightarrow \GL(V_1)$
  and
  $R_2 : G \rightarrow \GL(V_2)$,
  a \emph{morphism of representations}
  or \emph{equivariant map}
  $\Phi : V_1 \rightarrow V_2$
  is a linear map that commutes
  with the action,
  i.e., so that
  $\Phi(R_1(g) v) = R_2(g) \Phi(v)$
  for all $g \in G$, $v \in V$;
  one defines similarly the analogous notion for
  $\mathfrak{g}$-representations.
  An \emph{isomorphism of representations}
  or \emph{equivariant isomorphism}
  is a morphism with a two-sided inverse
  (equivalently, a bijective morphism),
  and two representations are said to be \emph{isomorphic}
  if there is an isomorphism between them.
\end{definition}
By what we've seen above,
a representation $R : G \rightarrow \GL(V)$ of a Lie group $G$
induces a representation
\begin{equation*}
d R : \mathfrak{g} \rightarrow \End(V)
\end{equation*}
of its Lie algebra
$\mathfrak{g}$,
given explicitly by for $X \in \mathfrak{g}$ by
\begin{equation*}
X v := d R(X) v :=\frac{d}{ d t} R(\exp(t X)) v |_{t=0}.
\end{equation*}
\begin{example}\label{ex:polynomial-reps-linear-groups}
  Let $G := \GL_n(\mathbf{k})$.
  Let $V := \mathbb{C}[x_1,\dotsc,x_n]_{(d)}$ be the space of homogeneous polynomials
  of degree $d$ in the variables $x_1,\dotsc,x_n$.
  One then has a representation
  $R : G \rightarrow \GL(V)$
  sending $g \in G$
  to the element $R(g) \in \GL(V)$
  that acts on a polynomial $\phi \in V$
  by the formula
  \begin{equation*}
    g \phi(x) :=
    (R(g) \phi)(x) :=
    \phi(x g),
  \end{equation*}
  where $x = (x_1,\dotsc,x_n)$ is regarded as an $n$-tuple of
  variables
  and $x g$ denotes the right multiplication of the matrix $g$
  against
  the row vector $x$.
  This is already an interesting representation.
  The differential
  $d R : \mathfrak{g} \rightarrow \End(V)$
  is given on the standard basis elements $E_{i j}$ of $\mathfrak{g} = \gl_n(\mathbf{k})$
  \begin{equation*}
  E_{i j} \phi(x) := (d R(E_{i j}) \phi )(x)
  = x_i \frac{\partial }{\partial_{x_j}} \phi(x).
  \end{equation*}
  (To see this,
  note that
  $x E_{i j} = x_i e_j$
  and thus $\phi(x(1 + \eps E_{i j}))
  = \phi(x) + x_i \frac{\partial \phi}{\partial x_j}(x) \eps + O(\eps^2)$.)
  The same definition makes sense and similar
  considerations apply more generally when $G$ is any subgroup of
  $\GL_n(\mathbf{k})$.
\end{example}

\subsection{Matrix multiplication}
\label{sec:org601ff7d}
Let $R : G \rightarrow \GL(V)$ be a representation
of a Lie group $G$.
Fixing a basis $(v_i)$ for $V$,
one can express a representation of $G$ in matrix form
\begin{equation*}
  R(g) = (R_{i j}(g))_{i,j},
\end{equation*}
where $R_{i j}(g)$ denotes the coefficient of
the basis element $v_i$
in $R(g) v_j$.
It's a fact of life that pretty much every special function
of mathematics or physics
is of the form $R_{i j}(g)$.
Identities such as the consequence
\begin{equation*}
  \sum_j R_{i j}(g) R_{j k}(h)
  = R_{i k}(g h)
\end{equation*}
of the homomorphism property
$R(g) R(h) = R(g h)$
can be of use.
For example,
let $G := \mathbb{R}, V := \mathbb{R}^2$,
\begin{equation*}
  R : G \rightarrow \GL_2(\mathbb{R})
\end{equation*}
\begin{equation*}
  R(\theta) := 
\begin{pmatrix}
    \cos \theta  & \sin \theta  \\
    - \sin \theta  & \cos \theta 
  \end{pmatrix}
.
\end{equation*}
Then
\begin{equation*}
  \cos(\theta + \phi)
  = R_{1 1}(\theta + \phi)
  = R_{1 1}(\theta) R_{1 1}(\phi)
  + R_{1 2}(\theta) R_{2 1}(\phi)
  = \cos(\theta) \cos(\phi)
  - \sin(\theta) \sin(\phi),
\end{equation*}
which makes for a nice way to remember
addition laws for trigonometric functions.

\subsection{Invariant subspaces and irreducibility\label{sec:stability-subspaces}}
\label{sec:orgfac8ca7}
\begin{definition}
  Let $G$  be a Lie group,
  and let $R : G \rightarrow \GL(V)$ be a finite-dimensional
  representation of $G$.
  A subspace $W$ of $V$ is said to be \emph{invariant} (or \emph{stable} or \emph{$G$-invariant} or \emph{$G$-stable})
  if $R(g) W \subseteq  W$ for all $g \in G$.

  Similarly, given a representation
  $\rho : \mathfrak{g} \rightarrow \End(V)$ of a Lie algebra
  $\mathfrak{g}$, we say that a subspace $W$ of $V$ is
  \emph{invariant} (or \emph{stable} or
  \emph{$\mathfrak{g}$-invariant} or
  \emph{$\mathfrak{g}$-stable}) if $\rho(X) W \subseteq W$ for
  all $X \in \mathfrak{g}$.

  We say that a representation $(R,V)$ of a Lie group
  or a representation $(\rho,V)$ of a Lie algebra
  is \emph{irreducible}
  if $V \neq \{0\}$
  and if $V$ has no nonzero proper invariant subspaces
  (i.e., none other than $\{0\}$ and $V$).
  Otherwise, it is said to be \emph{reducible}.
\end{definition}
\begin{exercise}
  Let $G$ be a Lie group
  and $R : G \rightarrow \GL(V)$ an $n$-dimensional
  representation.
  Fix
  a basis of $V$ and hence an identification
  $V := \mathbb{C}^n$.
  Let $m$ be an integer satisfying $0 < m < n$,
  and let $W := \mathbb{C}^m$
  regarded as
  a subspace of $V$ via the standard inclusion
  $(x_1,\dotsc,x_m) \mapsto (x_1,\dotsc,x_m,0,\dotsc,0)$.
  Denote by $P_m(V)$ the subgroup of $\GL(V)$ given by
  matrices of the form
  $
\begin{pmatrix}
    a & b \\
    0 & d
  \end{pmatrix}
$,
  where $a$ is an $m \times m$ matrix,
  $b$ is an $m \times (n-m)$ matrix,
  and $d$ is an $(n-m) \times (n-m)$ matrix.
  Show that the following are equivalent:
  \begin{enumerate}
  \item $R$ is reducible.
  \item
    There exist $0 < m < n$ and $\gamma \in \GL(V)$
    so that $R(G) \subseteq \gamma P_m(V) \gamma^{-1}$.
  \end{enumerate}
\end{exercise}

\begin{theorem}\label{thm:equivalences-irreducibility}
  Let $G$ be a Lie group,
  and let $R: G \rightarrow \GL(V)$ be a finite-dimensional
  representation of $G$.
  \begin{enumerate}
  \item Any $G$-invariant subspace of $V$ is also $\mathfrak{g}$-invariant.
  \item If $G$ is connected,
    then any $\mathfrak{g}$-invariant subspace of $V$ is also
    $G$-stable.
  \item If $G$ is connected, then $V$ is irreducible if and only
    if it is nonzero and contains no proper
    $\mathfrak{g}$-stable subspaces.
  \end{enumerate}
\end{theorem}
\begin{proof}
  If $W \leq V$ is $G$-invariant,
  then for each $X \in \mathfrak{g}$
  and $v \in W$
  and $t \in \mathbb{R}$,
  we have
  \begin{equation*}
    \frac{R(\exp(t X)) v - v}{t} \in W
  \end{equation*}
  (because $W$ is a vector space),
  hence upon differentiating that
  \begin{equation*}
    d R(X) v \in V
  \end{equation*}
  (because $W$ is closed).
  This shows that $W$ is $\mathfrak{g}$-invariant.
  Conversely,
  if $W \leq V$ is $\mathfrak{g}$-invariant and $G$ is
  connected,
  then
  \begin{equation*}
    R(\exp(t X)) v
    = \exp(t \, d R(X)) v
    = \sum_{n \geq 0}
    \frac{t^n}{n!}
    d R(X)^n v
    \in W,
  \end{equation*}
  hence $W$ is $\exp(\mathfrak{g})$-invariant,
  hence (because $G$ is connected
  and thus generated by $\exp(\mathfrak{g})$)
  $W$ is $G$-invariant.
  Etc.
\end{proof}

\subsection{Polynomial representations of \texorpdfstring{$\SL_2(\mathbb{C})$}{SL2(C)}}
\label{sec:orgb2dd983}
\label{sec:reps-sl2-C}
Here we specialize Example
\ref{ex:polynomial-reps-linear-groups} to
$G = \SL_2(\mathbb{C})$.
Let $W_n$ denote the space of homogeneous polynomials
$\phi \in \mathbb{C}[x,y]$
of degree $n$.
Then $W_n$ is an $(n+1)$-dimensional vector space
with basis given by the monomials
$x^n, x^{n-1} y, \dotsc, x y^{n-1}, y^n$.
As motivation, one can check that
for $g = 
\begin{pmatrix}
  \cos \theta  &  \sin \theta  \\
  - \sin \theta  &  \cos \theta 
\end{pmatrix}
$,
the matrix coefficients $R_{ij}(g)$
of this representation
with respect to the above basis
give the classical spherical polynomials (e.g.,
when $n$ is even, the coefficient of $x^n y^n$
in $R(g) x^n y^n$ is essentially the Legendre polynomial
$P_{n/2}(\cos \theta)$).

By specializing
the calculation of Example
\ref{ex:polynomial-reps-linear-groups},
we see that the basis elements
\begin{equation*}
  X := 
\begin{pmatrix}
    & 1 \\
    & 
  \end{pmatrix}
,
  \quad
  Y := 
\begin{pmatrix}
    &  \\
    1 & 
  \end{pmatrix}
,
  \quad
  H := 
\begin{pmatrix}
    1 &  \\
    & -1
  \end{pmatrix}
\end{equation*}
of $\mathfrak{g}$
act by
\begin{equation*}
  d R(X) =
  x \partial_y,
  \quad
  d R(Y)
  = y \partial_x,
  \quad
  d R(H)
  = x \partial_x - y \partial_y.
\end{equation*}
Their effects on the basis elements
is thus given by
$X x^n = 0,
Y y^n = 0$
and in all other cases by
\begin{equation*}
  X x^{n-k} y^k
  = k x^{n-k+1} y^{k-1},
  \quad 
  Y x^{n-k} y^k
  = (n-k) x^{n-k-1} y^{k+1},
  \quad 
  H x^{n-k} y^k
  = (n- 2 k) x^{n-k} y^{k}.
\end{equation*}
In lecture,
we drew a picture
in which the basis elements
$y^n, x y^{n-1} \dotsc, x^n y, x^n$ were lined up from left to right
and indicated by circles in which we indicated their
$H$-eigenvalues
$-n, -n+2, \dotsc, n-2, n$.
The action of $X$ may be depicted
\begin{equation*}
  y^n \xrightarrow{n}
  x y^{n-1} \xrightarrow{n-1}
  x^2 y^{n-2} \xrightarrow{n-2}
  \dotsb \xrightarrow{2}
  x^{n-1} y
  \xrightarrow{1}
  x^n
  \rightarrow 0.
\end{equation*}
The action of $Y$ may be depicted
\begin{equation*}
  0
  \leftarrow
  y^n \xleftarrow{1}
  x y^{n-1} \xleftarrow{2}
  x^2 y^{n-2} \xleftarrow{3}
  \dotsb \xleftarrow{n-1}
  x^{n-1} y
  \xleftarrow{n}
  x^n.
\end{equation*}
Explicitly, when $n = 3$,
we may represent the various actions
with respect to the basis
$x^3, x^2 y, x y^2, y^3$
by
\begin{equation*}
  d R(X)
  = 
\begin{pmatrix}
    & 1 & & \\
    &  & 2 & \\
    &  & & 3 \\
    & & &
  \end{pmatrix}
,
\end{equation*}
\begin{equation*}
  d R(Y)
  = 
\begin{pmatrix}
    &  & & \\
    3 &  &  & \\
    & 2 &  & \\
    & & 1 &
  \end{pmatrix}
,
\end{equation*}
\begin{equation*}
  d R(H)
  = 
\begin{pmatrix}
    3 &  & & \\
    &  1 &  & \\
    &  & -1  & \\
    & &  & -3
  \end{pmatrix}
.
\end{equation*}
As we saw in Homework \ref{hw:sl2-rep-verify-commutator},
the relation
\begin{equation*}
  [X,Y] = H
\end{equation*}
implies
that
\begin{equation*}
  [d R(X), d R(Y)] = d R(H).
\end{equation*}
It is an instructive exercise
to verify this directly
from as many perspective as possible
(e.g., by direct inspection of the action,
by staring at the action on basis vectors using the
graph-theoretic
depiction described above,
by explicitly computing the commutators of the above $4 \times
4$ matrices, etc.).

\begin{theorem}
  $W_n$ is an irreducible representation of $G = \SL_2(\mathbb{C})$.
\end{theorem}
By Theorem \ref{thm:equivalences-irreducibility},
it is equivalent to show that $W_n$ is irreducible
as a representation of $\mathfrak{g} = \slLie_2(\mathbb{C})$.
There are a couple ways to show this.
Firstly, given any nonzero invariant
subspace $W$ of $W_n$
and take a nonzero element $v \in W$,
then it follows from the above description
of the action that
there is a $k \geq 0$
for which $X^{k+1} v = 0$,
and moreover, that if $k$ is the smallest integer
with this property, then $X^k v$ is a nonzero
multiple of $x^n$;
then $Y^m X^k v$ is a nonzero
multiple of $x^{n-k} y^k$.
Since $W$ is invariant, it contains
$Y^m X^k v$,
hence contains all the basis elements for $W_n$,
and so $W = W_n$, i.e., $W_n$ is irreducible.

Another way to structure part of the argument is to use the
following
elementary consequence of the invertibility of the Vandermonde
determinant:
\begin{lemma}
 If $V$ is a representation of $\mathfrak{g}$
  and $W$ is an invariant subspaces
  and $v \in W$ is a vector that may be expressed
  as a sum $v = v_1 + \dotsb + v_n$
  where $H v_i = \lambda_i v_i$
  for some $\lambda_i \in \mathbb{C}$
  with $\lambda_i \neq \lambda_j$ whenever $i \neq j$,
  then
  each $v_i$ also belongs to $W$.
\end{lemma}
This shows that any invariant subspace
$W$ of $W_n$
contains the components
of each of its vectors wrt the standard basis,
and one can then argue as above to get all the basis elements.

\subsection{Classifying finite-dimensional irreducible representations of \texorpdfstring{$\SL_2(\mathbb{C})$}{SL2(C)}}
\label{sec:orgbd19a70}
One cares to do this because it shows up all over the place (in
studying special functions, in classifying Lie groups and Lie
algebras, in studying representations of other Lie groups thanks
to the various ways that $\SL_2(\mathbb{C})$ may be embedded in
them, in quantum mechanics, Hodge theory, etc.)

\begin{theorem}\label{thm:classify-irreps-sl2}
  Let $V$ be any finite-dimensional irreducible
  representation of $G = \SL_2(\mathbb{C})$.
  Then $V$ is isomorphic
  to one of the representations $W_n$
  considered in the previous section for some $n \geq 0$.
\end{theorem}
By arguing
as in the proof of Lemma
\ref{thm:equivalences-irreducibility},
it suffices to show
this for $\mathfrak{g}$-representations
instead of $G$-representations,
which makes the problem a bit easier.

\begin{lemma}\label{lem:linear-transformations-have-eigenvectors}
  Let $T$ be a linear transformation on a nonzero finite-dimensional
  complex vector space $V$.
  Then $T$ has an eigenvector,
  i.e., a nonzero vector $v \in V$
  so that $T v = \lambda v$ for some $\lambda \in \mathbb{C}$.
\end{lemma}
\begin{proof}
  The characteristic polynomial $\det(x - T)$
  is monic of degree $\dim(V) \geq 1$,
  hence has a root $\lambda$;
  then $\det(\lambda - T) = 0$,
  so $T - \lambda$ is non-invertible,
  so $\ker(T - \lambda) \neq 0$,
  i.e., $T$ has an eigenvector.
\end{proof}

\begin{definition}
  Let $V$ be a representation of $\mathfrak{g} =
  \slLie_2(\mathbb{C})$, and let $\lambda \in \mathbb{C}$.
  We say that a nonzero vector $v \in V$ has \emph{weight
    $\lambda$}
  if $v$ is an eigenvector
  for $H$ with eigenvalue $\lambda$,
  i.e.,
  $H v = \lambda v$.
\end{definition}
\begin{example}
  The vector $x^{n-k} y^k \in W_n$ has
  weight $n-2k$.
\end{example}

\begin{remark}
  In what follows,
  we write (for instance) $H X$
  as an abbreviation for
  $d R(H) d R(X)$;
  this differs from the matrix product of
  $H$ and $X$, which we shall have no occasion to refer to.
\end{remark}

\begin{lemma}\label{lem:raising-ops-etc}
  Suppose $v \in V$ as above
  has weight $\lambda$.
  Then $X v$ has weight $\lambda + 2$
  and $Y v$ has weight $\lambda - 2$.
\end{lemma}
\begin{proof}
  We will use that
  \begin{equation*}
    [H,X] = 2 X,
    \quad [H,Y] = - 2 Y.
    \end{equation*}
  We have
  \begin{align*}
    H X v &= (H X - X H) v + X H v \\
          &= [H,X] v + X H v
            \\
          &= 2 X v + X (\lambda  v)
            \\
          &= (\lambda + 2)X v,
  \end{align*}
  and similarly
  $H Y v = (\lambda - 2) Y v$.
\end{proof}

\begin{lemma}
  Let $V$ be a finite-dimensional representation
  of $\mathfrak{g} = \slLie_2(\mathbb{C})$.
  Then there is a nonzero $v \in V$
  and $\lambda \in \mathbb{C}$
  so that
  \begin{equation*}
    H v = \lambda v,
  \end{equation*}
  \begin{equation*}
    X v = 0.
  \end{equation*}
\end{lemma}
\begin{proof}
  By Lemma \ref{lem:linear-transformations-have-eigenvectors},
  there exists some nonzero $u \in V$
  with some weight $\mu \in \mathbb{C}$.
  The vectors $X^k u$ have weight $\mu + 2k$.
  Since $V$ is finite-dimensional,
  $H$ has only finitely many eigenvalues,
  so we have $X^{k+1} u = 0$ for large enough $k$.
  Choosing $k$ minimal with this property
  and taking $v := X^k u$
  gives
  $H v = (\mu + 2 k) v$
  and $X v = 0$, as required.
\end{proof}
\begin{remark}
  Although Lemma \ref{lem:raising-ops-etc} is basically trivial,
  it is one of the most frequently
  applied calculations in Lie theory, and deserves careful study.
\end{remark}

To prove Theorem \ref{thm:classify-irreps-sl2},
we now take for $V$ any irreducible finite-dimensional
representation
of $\mathfrak{g} = \slLie_2(\mathbb{C})$
and let $v_0 \in V$ be a nonzero element
satisfying
\begin{equation*}
  H v = \lambda_0 v,
  \quad X v = 0.
\end{equation*}
Such a vector exists by the previous lemma.
For $k \geq 0$,
set
\begin{equation*}
  v_k := Y^k v_0.
\end{equation*}
Then $v_k$ has weight $\lambda_0 - 2k$,
so the various $v_k$ are all linearly independent.
Let $W := \oplus \mathbb{C} v_k$ be the span of the $v_k$.
We claim that $W$ is $\mathfrak{g}$-invariant.
To that end, it suffices by the linearity
of the action to show
for each basis element $Z \in \{H,X,Y\}$ of $\mathfrak{g}$
and basis element $v_k$ of $W$
that $Z v_k \in W$.
Clearly
\begin{equation*}
  H v_k = (\lambda_0 - 2 k) v_k \in W
\end{equation*}
and
\begin{equation*}
  Y v_k = v_{k+1} \in W.
\end{equation*}
We now verify by induction on $k$ that
\begin{equation*}
  X v_k = c_k v_{k-1} \in W
\end{equation*}
with $c_k := k(\lambda_0 - k + 1)$
and (by convention) $v_{-1} := 0$.
When $k = 0$, this is clear.
For $k \geq 0$,
it follows by our inductive hypothesis
and using the trick $X Y = (X Y - Y X) + Y X$
as in  the proof of Lemma \ref{lem:raising-ops-etc}
that
\begin{align*}
  X v_{k+1}
  &=
    X Y v_k
  \\
  &=
    [X,Y] v_k + Y X v_k
  \\
  &=
    H v_k + Y c_k v_{k-1}
  \\
  &= (\lambda - 2k + c_k) v_k.
\end{align*}
We conclude by
checking that $c_{k+1} =\lambda - 2k + c_k$.

Since $W$ is nonzero and $\mathfrak{g}$-invariant
and since $V$ is assumed irreducible,
we must have $W = V$.
Since $V$ is finite-dimensional,
we have $v_{n+1} = 0$ for some $n$.
Choosing $n$ minimal with this property
implies that
\begin{equation*}
  v_n \neq 0
\end{equation*}
and
\begin{equation*}
  v_{n+1} = 0
\end{equation*}
whence
\begin{equation*}
  0 = X v_{n+1}
  = c_{n+1} v_n
  = (n+1)(\lambda_0 - n) v_n.
\end{equation*}
Since $n+1 \neq 0$ and $v_n \neq 0$,
it follows that $\lambda_0 = n$.

In summary, we  have shown that $V$ has the basis
$v_0,\dotsc,v_n$
on which the action is given by
\begin{align*}
  H v_k &= (n - 2k ) v_k,
          \\
  Y v_k &= v_{k+1},
          \\
  X v_k &= k(n-k+1) v_{k-1}.
\end{align*}
But it is easy to check that
$W_n$ has the basis $w_0,\dotsc,w_n$
with $w_k := Y^k x^n$
on which  the action is described in the same way.
Thus the linear map $V \rightarrow W_n$
extending $v_k \mapsto w_k$
is an isomorphism of $\mathfrak{g}$-representations,
as required.

\subsection{Complete reducibility\label{sec:compl-red}}
\label{sec:orgf4dbdd0}
It is interesting
to ask whether one can classify
\emph{all} finite-dimensional
representations of a group such as $\SL_2(\mathbb{C})$
rather than just the irreducible ones
as done in
\S\ref{sec:reps-sl2-C}.
For example, can we break arbitrary representations up  into
sums of irreducible ones?
We can also ask the same question
for more general Lie groups $G$:
how can we understand general representations
$R : G \rightarrow \GL(V)$
in terms of the irreducible ones?
This was a basic question of late 19th century mathematics
known nowadays as classical invariant theory;
the representations of interest were as in Example \ref{ex:polynomial-reps-linear-groups}.

\begin{definition}
  Let $G$ be any Lie group
  and $R : G \rightarrow \GL(V)$ a finite-dimensional
  representation.
  We say that $V$ is \emph{completely reducible}
  if there are irreducible invariant subspaces $V_1, \dotsc, V_n$ of $V$
  so that $V = V_1 \oplus \dotsb \oplus V_n$.
\end{definition}
\begin{definition}
  Similarly, let $\mathfrak{g}$ be a Lie algebra
  and $\rho : \mathfrak{g}  \rightarrow \End(V)$ a finite-dimensional
  representation.
  We say that $V$ is \emph{completely reducible}
  if there are irreducible invariant subspaces $V_1, \dotsc, V_n$ of $V$
  so that $V = V_1 \oplus \dotsb \oplus V_n$.
\end{definition}

\begin{lemma}\label{lem:complete-irreducible-gp-vs-alg}
  Let $G$ be a connected Lie group,
  let $R : G \rightarrow \GL(V)$ a finite-dimensional
  representation,
  and let $d R : \mathfrak{g} \rightarrow \End(V)$
  be the induced representation of the Lie algebra
  $\mathfrak{g}$ of $G$.
  Then $V$ is completely reducible
  as a representation of $G$
  if and only if $V$ is completely reducible
  as a representation of $\mathfrak{g}$.
\end{lemma}
\begin{proof}
  Indeed, the invariant subspaces of $G$ and $\mathfrak{g}$
  are the same, thanks to Lemma \ref{thm:equivalences-irreducibility}.
\end{proof}

\begin{example}
  The zero representation $V := \{0\}$ is completely reducible (take $n := 0$).
  Any irreducible representation is completely reducible (take $n := 1, V_1 := V$).
\end{example}
\begin{lemma}\label{lem:compl-red-vs-complements}
  Let $R : G \rightarrow \GL(V)$ be a finite-dimensional
  representation.
  The following are equivalent:
  \begin{enumerate}
  \item $V$ is completely reducible.
  \item Every invariant subspace $W$ of $V$ has an invariant
    complement $W'$.
  \end{enumerate}
\end{lemma}
Recall here
that a subspace $W' \leq V$ is said to be a \emph{complement}
of a subspace $W \leq V$ if
$V = W \oplus W'$, that is to say, if every
$v \in V$ may be expressed
as $v = w + w'$ for some unique $w \in W, w' \in W'$.

\begin{example}\label{example:invariant-inner-product-implies-complete-irreducibility}
  Suppose that $R : G \rightarrow \GL(V)$
  has the property that there is an inner product
  $\langle , \rangle$ on $V$
  that is invariant by $G$ in the sense that
  \begin{equation}\label{eq:invariant-inner-product}
    \langle R(g) u, R(g) v  \rangle = \langle u,v \rangle
  \end{equation}
  for all $g \in G$ and all $u, v \in V$.
  (In other words, after choosing an orthonormal basis of $V$ and using that basis to identify $V \cong
  \mathbb{C}^n$,
  we are assuming that $R(G)$ is contained in the unitary group $\U(n)$.)
  Then every invariant subspace $W$ has an invariant complement $W'$:
  one can just take for $W'$ the \emph{orthogonal complement}
  \begin{equation*}
    W^\perp := \{v \in V : \langle v,w \rangle = 0 \text{ for
 all }
w \in W\}
  \end{equation*}
  of
  $W$.
  It follows from the definition of an inner product that the orthogonal complement is in fact a complement;
  what needs to be checked
  is that it is invariant.
  Thus, let $v \in W^\perp$ and $g \in G$;
  we want to check that $R(g) v \in W^\perp$,
  i.e., that $\langle R(g) v, w \rangle = 0$
  for all $w \in W$.
  But \eqref{eq:invariant-inner-product} implise that
  \begin{equation*}
    \langle R(g) v, w \rangle = \langle R(g^{-1}) R(g) v,
    R(g^{-1}) w \rangle
    = \langle v, R(g^{-1}) w \rangle,
  \end{equation*}
  and the invariance of $W$ implies that $R(g^{-1}) \in w$,
  hence that
  $\langle v, R(g^{-1}) w \rangle= 0$,
  as required.
\end{example}

We turn to the proof of Lemma \ref{lem:compl-red-vs-complements}, which we split into two parts.
\begin{proof}
[Existence of invariant complements implies complete reducibility]
  Assume first that every invariant subspace of $V$ has an
  invariant complement;
  we aim then to show that $V$ is completely reducible.
  (In following this argument,
  it may be helpful to pretend
  that
  we are in the setting
  of Example \ref{example:invariant-inner-product-implies-complete-irreducibility}.)
  If $V = \{0\}$, then we are done.
  Since $\dim(V) < \infty$,
  there exists a minimal nonzero invariant subspace $V_1$ of
  $V$.
  If $V_1 = V$, then we are done.
  Otherwise, let $V_1'$ be an invariant complement of $V_1$;
  by  assumption, $V_1' \neq 0$
  and 
\begin{equation*}
V = V_1 \oplus V_1'.
\end{equation*}
  Let $V_2$ be a minimal nonzero invariant subspace
  of $V_1'$.
  If $V_2 = V_1'$, then $V = V_1 \oplus V_2$
  is a sum of invariant irreducible subspaces,
  so we are done.
  If not, let $W_2 \leq V$ be an invariant complement to $V_2$,
  and let $V_2' := W_2 \cap V_1 \leq V_1'$ be its intersection
  with $V_1'$, which is then invariant
  and satisfies $V_1' = V_2 \oplus V_2'$ (check this; it's easy),
  hence 
\begin{equation*}
V = V_1 \oplus V_2 \oplus V_2'.
\end{equation*}
  By assumption, $W_2$ is nonzero, hence $V_2'$ is nonzero.
  Let $V_3$ be a minimal nonzero invariant subspace of $V_2'$.
  If $V_3 = V_2'$, then we are done as before.
  If not, let $V_3' := V_2' \cap W_3 \leq V_2'$ be the intersection
  with $V_2'$ of some invariant complement $W_3' \leq V$ of $V_2'$; then, as before,
  \begin{equation*}
    V = V_1 \oplus V_2 \oplus V_3 \oplus V_3'.
  \end{equation*}
  Proceed as above, and invoke that $\dim(V) < \infty$
  to know that the process must eventually terminate.

  (The argument just presented is ``obvious''
  and fairly natural,
  but somewhat suboptimal.
  I'll leave it as an exercise for the interested reader
  to make it ``slicker''
  by considering in the first step an invariant subspace
  $W$ of $V$ that is maximal with respect
  to the property of being a direct sum of irreducible invariant
  subspaces,
  and deriving a contradiction if $W \neq V$.
  This ``slicker'' formulation of the argument has certain
  advantages;
  for instance, it works without any fuss in the
  infinite-dimensional setting.)
\end{proof}
\begin{proof}
[Complete reducibility implies  existence of invariant complements]
  Okay, now let's show that if $V$ is completely reducible, then
  every invariant subspace $W$ of $V$ has an invariant
  complement $W'$.
  Thus, suppose we can write
  $V = V_1 \oplus \dotsb \oplus V_n$
  as a sum of irreducible invariant subspaces.
  Let $I$ be a subset of $\{1,\dotsc,n\}$  that is maximal
  with respect to the property
  that
  \begin{equation*}
    W
    \cap (\oplus_{i \in I} V_i) = \{0\}.
  \end{equation*}
  (Note that the empty set satisfies this property,
  and there are only finitely many subsets, so such an $I$ exists.)
  Set $W' := \oplus_{i \in I} V_i$.
  We claim that $V = W \oplus W'$.
  By construction, we have $W \cap W' = \{0\}$,
  so it suffices to show that $V = W + W'$.
  To that end, it suffices to show for each
  $j \in \{1,\dotsc,n\}$ that
  \begin{equation}\label{eq:each-simple-compoennt-is-in-sum}
    V_j \subseteq W + W'.
  \end{equation}
  If $j \in I$, then $V_j \subseteq W'$,
  so \ref{eq:each-simple-compoennt-is-in-sum} holds,
  so suppose $j \notin I$.
  If
  \eqref{eq:each-simple-compoennt-is-in-sum}
  fails, then $V_j \cap (W + W')$ is a proper invariant subspace of $V_j$,
  hence
  \begin{equation*}
V_j \cap (W + W') = 0,
  \end{equation*}
  or in other words,
  there is no nontrivial solution to the equation
  $v = w + w'$ with $v \in V_j$, $w \in W, w' \in W'$,
  or equivalently (upon replacing $w,v$ by their negatives),
  there is no nontrivial solution
  to the equation
  $w = w' + v$ with $v \in V_j$, $w \in W, w' \in W'$,
  i.e.,
  \begin{equation*}
    W \cap (W' \oplus V_j) = 0,
  \end{equation*}
  which says that
  $W \cap (\oplus_{i \in I \cup \{j\}} V_i) = 0$,
  contradicting the assumed maximality of $I$.
\end{proof}
\begin{remark}
  The natural setting for Lemma \ref{lem:compl-red-vs-complements} is the theory
  of semisimple modules over a ring.
\end{remark}

\begin{example}[A representation that is not completely reducible]\label{example:defining-representation-of-borel-is-not-completely-reducible}
  Consider the Lie subgroup $P$ of $\SL_2(\mathbb{C})$
  consisting of matricse of the form
  $
\begin{pmatrix}
    a & b \\
    0 & d
  \end{pmatrix}
$.
  Let $R : P \rightarrow \GL(V)$ be the standard representation
  of $P$ on $V := \mathbb{C}^2$,
  thus
  \begin{equation*}
    R(
\begin{pmatrix}
      a & b \\
      & d
    \end{pmatrix}
)
    \begin{pmatrix}
      x  \\
      y        
    \end{pmatrix}
    :=
    \begin{pmatrix}
      a x + b  \\
      d y        
    \end{pmatrix}
.
  \end{equation*}
  Then
  $V$ is not completely reducible.
  Indeed, consider the subspace
  $W := \mathbb{C} e_1 \leq V$
  consisting of column vectors of the form
  \begin{equation*}
    \begin{pmatrix}
      x  \\
      0
    \end{pmatrix}
.
  \end{equation*}
  It is easy to check that $W$ is invariant,
  and that every complement $W'$ of $W$ has the form
  \begin{equation*}
    W' = \left\{ 
\begin{pmatrix}
        c y  \\
        y
      \end{pmatrix}
 : y \in \mathbb{C}  \right\}
  \end{equation*}
  for some $c \in \mathbb{C}$.
  But it is equally clear that $W'$ is not invariant;
  for instance, one has
  \begin{equation*}
    \begin{pmatrix}
      1 & 1 \\
       & 1
     \end{pmatrix}
 \in P,
     \quad 
    \begin{pmatrix}
      c  \\
      1
    \end{pmatrix}
 \in W,
    \quad 
    \begin{pmatrix}
      1 & 1 \\
       & 1
     \end{pmatrix}
    \begin{pmatrix}
      c  \\
      1
    \end{pmatrix}
    = 
\begin{pmatrix}
      c+1  \\
      1      
    \end{pmatrix}
 \notin  W.
  \end{equation*}
  Thus not all representations are completely reducible.

  One gets more general examples of this sort by replacing $P$
  by the stabilizer $P$  of any nontrivial flag of vector spaces
  $0 = V_0 \subset V_1 \subset \dotsb \subset V_r =
  \mathbb{C}^n$.  
\end{example}

\begin{example}\label{boring-examples-of-non-reductivity}
  The representation
  \begin{equation*}
    \mathbb{R} \rightarrow \GL_2(\mathbb{R})
  \end{equation*}
  \begin{equation*}
    x \mapsto 
\begin{pmatrix}
      1 & x \\
       & 1
    \end{pmatrix}
  \end{equation*}
  is not completely reducible,
  for the same reason as in the previous example.
  Similarly for the representation $\mathbb{C}  \rightarrow
  \GL_2(\mathbb{C})$
  defined by the same formula.
  Likewise for the representation
  \begin{equation*}
    \mathbb{R}^\times \rightarrow \GL_2(\mathbb{R})
  \end{equation*}
  \begin{equation*}
    x \mapsto 
\begin{pmatrix}
      1 & \log|x| \\
       & 1
    \end{pmatrix}
.
  \end{equation*}
  Likewise for the representation
  \begin{equation*}
    \GL_n(\mathbb{R}) \rightarrow \GL_2(\mathbb{R})
  \end{equation*}
  \begin{equation*}
    g \mapsto 
\begin{pmatrix}
      1 & \log|\det(g)| \\
       & 1
    \end{pmatrix}
.
  \end{equation*}
\end{example}


\begin{definition}
  Let $G$ be a Lie group.
  We say that $G$ is \emph{linearly reductive}
  if every finite-dimensional representation
  of $G$ is completely reducible.
\end{definition}

\begin{example}\label{rmk:lie-vs-alg}
  We have seen that the groups $P$ from 
  Example
  \ref{example:defining-representation-of-borel-is-not-completely-reducible}
  are \emph{not} linearly reductive.
  Similarly,
  we see from Example \ref{boring-examples-of-non-reductivity}
  that the real Lie groups $\mathbb{R}$ and
  $\GL_n(\mathbb{R})$
  and the complex Lie group $\mathbb{C}$ are not linearly
  reductive.

  A minor caution regarding terminology: We are working here in
  the category of Lie groups over the field $\mathbf{k} =
  \mathbb{R}$ or $\mathbb{C}$.
  When we speak of any property of a Lie group, it matters
  which we field
  we regard it as being defined over.
  For example, we will show eventually that 
  $\mathbb{C}^\times$,
  regarded as a complex Lie group
  is linearly reductive.
  However, if we instead regard $\mathbb{C}^\times$ as a
  real Lie group,
  then it is not linearly reductive:
  the representation
  \begin{equation*}
    \mathbb{C}^\times \ni z
    \mapsto 
\begin{pmatrix}
      1 & \log |z| \\
       & 1
    \end{pmatrix}
  \end{equation*}
  is not completely reducible.
  That representation is not smooth in the complex
  sense (i.e., is not holomorphic), and so does not define
  a representation of $\mathbb{C}^\times$ when we regard
  it as a complex Lie group.

  Similarly, although we are working 
  in this course in the category of Lie groups,
  we could instead work in the
  category of \emph{algebraic groups}, which are obtained by
  replacing manifolds with solution spaces to polynomial
  equations (called \emph{varieties}) and replacing smooth maps
  between manifolds with maps described by polynomials (called
  \emph{morphisms of varieties}).  The group
  $\GL_n(\mathbb{R})$ can be regarded either as a Lie group or
  as an algebraic group.  In the category of Lie groups, it is
  not linearly reductive.  But the counter-example we gave
  involved the logarithm function, which is not algebraic.  It
  turns out that when $\GL_n(\mathbb{R})$ is regarded as an
  algebraic group, it is linearly reductive.  This
  means that one can't construct non-completely-reducible
  representations of $\GL_n(\mathbb{R})$ using only polynomials;
  to put it another way, it turns out that any representation
  $R : \GL_n(\mathbb{R}) \rightarrow \GL_N(\mathbb{R})$ whose
  matrix coefficients $R_{i j}(g)$ are polynomial functions of
  the coordinates $g_{k l}$ is completely reducible.
\end{example}

\subsection{Linear reductivity of compact groups\label{sec:linear-reductivity-compact-groups}}
\label{sec:org7d9bd5a}
\begin{theorem}[Maschke]\label{thm:maschke-finite-groups}
  Any finite group $G$ is linearly reductive.
\end{theorem}
\begin{proof}
  Let $V$ be a complex vector space,
  and let $R : G \rightarrow \GL(V)$ be a representation.
  We wish to show that $V$ is completely reducible.
  There are a couple ways to phrase the argument; I'll record
  both
  for the sake of variety.

  First, by Lemma \ref{lem:compl-red-vs-complements}
  and Example
  \ref{example:invariant-inner-product-implies-complete-irreducibility},
  it will suffice to show that there exists
  a invariant inner product on $V$.
  To show this, let $\langle , \rangle_0$ be any inner product
  on $V$
  (just fix a linear isomorphism $V \cong \mathbb{C}^n$ and take
  the standard one),
  and then define  the averaged inner product $\langle ,
  \rangle$
  by the formula
  \begin{equation*}
    \langle u, v \rangle := \frac{1}{|G|}
    \sum_{g \in G}
    \langle R(g) u, R(g) v \rangle_0.
  \end{equation*}
  It is then easy to check that $\langle , \rangle$ is the
  required an invariant inner product.

  We now phrase the argument another way
  by making more direct use
  of the criterion of Lemma \ref{lem:compl-red-vs-complements}.
  It will suffice to show that each invariant subspace
  $W$ of $V$ has an invariant complement.
  To that end, it will suffice to construct an \emph{equivariant
    projection} $p : V \rightarrow W$.
  (A \emph{projection} from a vector space
  $V$ to a subspace $W$ is a linear map $p : V \rightarrow W$ whose
  restriction to $W$ is the identity map.
  A linear map $p$ between representations is said to be
  \emph{equivariant}
  if it is a morphism of representations, i.e.,
  if $p(g v) = g p(v)$ for all $v \in V, g \in G$.)
  Assuming we have constructed such a projection,
  we may take $W' := \ker(p)$.
  Since $p$ is a projection, we then have
  \begin{equation*}
    V = W \oplus W'.
  \end{equation*}
  On the other hand, since $p$ is equivariant,
  its kernel $W'$ is invariant.
  We thereby obtain the required invariant complement of $W$,
  assuming the existence
  of an equivariant projection.

  To produce an equivariant projection,
  start with any projection $\phi_0 : V \rightarrow W$
  (e.g., by taking a basis $e_1,\dotsc,e_d$ for $W$, extending
  it to a basis $e_1,\dotsc,e_n$ for $V$,
  and defining $\phi_0(e_j)$ to be $e_j$ if $j \leq d$ and $0$
  otherwise)
  and define the average $\phi : V \rightarrow W$ by
  \begin{equation*}
    \phi(v) := \frac{1}{|G|} \sum_{g \in G}
    R(g) \phi_0(R(g)^{-1} v).
  \end{equation*}
  Then it is easy to check that $\phi$ is still a projection,
  and also that $\phi$ is equivariant:
  for $h \in G$,
  one obtains using the change of variables $g \mapsto h g$
  on $G$ and the homomorphism property of representations
  that
  \begin{align*}
    \phi(R(h) v)
    &=
     \frac{1}{|G|} \sum_{g \in G}
      R(g) \phi_0(R(g)^{-1} R(h) v)
      \\
    &=
     \frac{1}{|G|} \sum_{g \in G}
      R(g) \phi_0(R(h^{-1} g)^{-1} v)
      \\
    &=
     \frac{1}{|G|} \sum_{g \in G}
      R(h g) \phi_0(R(g)^{-1} v)
      \\
    &=
      R(h) \phi(v),
  \end{align*}
  so $\phi$ is equivariant, as required.
\end{proof}

\begin{theorem}\label{thm:compact-implies-lin-red}
  Any compact Lie group $G$ is linearly reductive.
  More generally, if $G$ is a compact topological group
  and $R : G \rightarrow \GL(V)$ is any continuous
  finite-dimensional
  representation, then $R$ is completely reducible.
\end{theorem}
\begin{proof}
  We will use the following fact, whose proof is sketched in
  \S\ref{sec:inv-measures}, \S\ref{sec:inv-measure-lie},
  \S\ref{sec:haar-compact-gp-via-avg}:
  there is a unique Radon probability measure $\mu$
  which is left and right invariant under $G$ in the sense that
  $\mu(E g) = \mu(g E) = \mu(E)$ for all Borel subsets $E
  \subseteq G$ and $g \in G$,
  or equivalently,
  \begin{equation*}
    \int_{g \in G} f(g) \, d \mu(g)
    =
    \int_{g \in G} f(h g) \, d \mu(g)
    =
    \int_{g \in G} f(g h) \, d \mu(g)
  \end{equation*}
  for all $h \in G$ and all continuous functions $f : G
  \rightarrow \mathbb{C}$.
  For example, if $G$ is a finite group, one can take for $\mu$
  the normalized
  counting measure.
  We may then argue exactly
  as in either of the proofs of Theorem
  \eqref{thm:maschke-finite-groups}
  by replacing averaging over the group with averaging with
  respect to $\mu$,
  i.e., taking
    \begin{equation*}
    \langle u, v \rangle :=
    \int_{g \in G}
    \langle R(g) u, R(g) v \rangle_0
    \, d \mu(g)
  \end{equation*}
  or
    \begin{equation*}
    \phi(v) :=  \int_{g \in G}
    R(g) \phi_0(R(g)^{-1} v) \, d \mu(g)
  \end{equation*}
  instead of what we did above.
\end{proof}

The tools of
\S\ref{sec:unitary-trick}
will give us a number of examples
of linearly reductive complex Lie groups
($\GL_n(\mathbb{C})$, $\SL_n(\mathbb{C})$, $\SO_n(\mathbb{C})$, etc.)
and also linearly reductive real Lie groups
($\SL_n(\mathbb{R})$,
$\SO(p,q)^0$ for $(p,q) \neq (1,1)$)
in addition to the compact groups
($\U(n),
\SO(n)$, etc.) already covered above.
The following result was established in the above, and is of
independent interest:
\begin{theorem}
  Let $R : G \rightarrow \GL(V)$ be a finite-dimensional
  representation
  of a compact group $G$.
  Then there exists an invariant inner product on $V$.
\end{theorem}

The proofs given above were self-contained
except that we
punted the existence of the Haar measure $\mu$ to
\S\ref{sec:inv-measures}.
Here we record a self-contained way
(learned from Onishchik--Vinberg)
to ``get around''
constructing such a $\mu$.
(I put ``get around'' in quotes because
the ideas here are similar to
those used
in \S\ref{sec:haar-compact-gp-via-avg}
to construct such a $\mu$; however, they are somewhat simpler in the present context.)
% \begin{definition}
%   An \emph{affine space} $S$ is a principal homogeneous
%   space for a vector space $V$.
%   This means that $S$ is a set equipped with a map
%   $\tau : S \times V \rightarrow S$,
%   abbreviated typically to $s+v := \tau(s,v)$ for $s \in S, v
%   \in V$,
%   with the properties:
%   \begin{enumerate}
%   \item $s + 0 = s$.
%   \item $(s+v_1) + v_2 = s + (v_1 + v_2)$.
%   \item For any $s_0, s \in S$ there exists a unique
%     $v \in V$ so that $s = s_0 + v$.
%   \end{enumerate}
%   By choosing a basepoint $s_0 \in S$ one may thus identify
%   identify
%   For example, if $V$ is a vector subspace
% \end{definition}
\begin{theorem}\label{thm:fixed-point-theorem-for-affine-actions-of-compact-groups}
  Let $S$ be a finite-dimensional vector space, let $G$ be a
  compact group, and let $\alpha : G \rightarrow \GL(S)$ be a
  representation.  Let $M \subseteq S$ be a nonempty convex
  $G$-invariant subset.  Then $M$ contains a fixed point of $G$.
\end{theorem}
\begin{proof}
  We reduce first to the case that $M$ is
  bounded by replacing $M$ as necessary by the convex hull
  of some orbit of $G$ on $M$; such an orbit is compact because of the compactness of $G$.
  The idea is then that $M$ has a well-defined ``center of mass''
  which, being canonically defined, will turn out to be $G$-invariant.

  Turning to details, let $W \leq S$ denote the span of all differences of pairs of
  elements
  of $M$.
  Let $P$ denote the smallest plane containing $M$,
  or equivalently,
  the union of all lines in $S$ containing at least two elements
  of $M$.
  Then $P$ is a coset of $W$.
  Since $M$ is $G$-invariant and the associations
  $M \mapsto W,P$ are canonical,
  we know that $W$ and $P$ are $G$-invariant, too.
  The vector space $W$ has a Lebesgue measure
  $\mu$.
  For $g \in G$,
  let $J(g)$ denote the Jacobian of the linear transformation
  of $W$ induced by $R(g)$.
  Then $J : G \rightarrow \mathbb{R}_+^\times$
  is a homomorphism.
  Since $G$ is compact, $J$ is trivial.
  Thus the Lebesgue measure $\mu$ is $G$-invariant.
  By fixing a basepoint on $P$,
  we can transfer $\mu$ to a measure $\nu$ on $P$,
  which is again $G$-invariant.
  The \emph{center of mass}
  \begin{equation*}
  c(M)
  :=
  \frac{\int_{x \in M} x \, d \nu(x)}{\nu(M)}
  \end{equation*}
  of $M$ then makes sense (because $M$ is bounded) and is
  $G$-invariant (because $\nu$ is $G$-invariant).
  Since $M$ is convex,
  we have moreover that
  $c(M)$ belongs to $M$:
  this is clear if $M$ is a point,
  while otherwise the interior $M^0$ of $M$ (as defined using the topology on $P$)
  is nonempty,
  so if $c(M) \notin M$, then
  (by the separating hyperplane theorem,
  i.e., Hahn--Banach in finite dimensions)
  there is an affine-linear function
  $\ell : S \rightarrow \mathbb{R}$ satisfying $\ell(M^0)
  \subseteq \mathbb{R}_{>0}$
  but $\ell(c(M)) = 0$,
  which leads to a contradiction upon applying $\ell$ to the
  definition of $c(M)$.
  Thus $c(M) \in M$ gives the required
  $G$-fixed point.
\end{proof}
\begin{example}\label{example:fixed-point-thm-gives-equivariant-projections}
  Let $G$ be a compact group,
  let $R : G \rightarrow \GL(V)$
  be a finite-dimensional representation,
  let $W \leq V$ be an invariant subspace,
  and let $S$
  denote the set of all linear maps $V \rightarrow W$
  and $M \subseteq S$ the subset consisting of projections $\phi
  : V  \rightarrow W$.
  Then $G$ acts on $S$ by the rule $g \cdot \phi :=  R(g) \circ \phi \circ R(g)^{-1}$,
  and $M$ is a nonempty convex $G$-invariant subset.
  A projection $\phi \in M$ is equivariant if and only
  if it is a fixed point for this action,
  so Theorem
  \ref{thm:fixed-point-theorem-for-affine-actions-of-compact-groups}
  tells us that an equivariant projection exists.
  Using this, we can complete the proof of Theorem
  \ref{thm:compact-implies-lin-red}
  without directly establishing the existence of $\mu$.
\end{example}

\begin{example}
  Let $G$ be a compact group, let $R : G \rightarrow \GL(V)$ be
  a finite-dimensional representation, let $S$ be the space of
  hermitian forms on $V$, and let $M \subseteq S$ be the subset
  of positive definite hermitian forms (i.e., inner products).
  Then $G$ acts on $S$ by
  $(g \cdot B)(v_1,v_2) := B(R(g)^{-1} v_1, R(g)^{-1} v_2)$, and
  the subspace $M$ is convex, nonempty and $G$-invariant.  An
  inner product $B \in M$ is $G$-invariant in the present sense
  if and only if it is invariant in the sense defined above, so
  Theorem
  \ref{thm:fixed-point-theorem-for-affine-actions-of-compact-groups}
  tells us that an invariant inner product exists.
\end{example}

\subsection{Constructing new representations from old ones}
\label{sec:orgfb8c0e1}
\subsubsection{Direct sum}
\label{sec:org3c4515a}
Given a pair of representations $V_1, V_2$ of a Lie group $G$,
we get a representation on their direct sum $V_1 \oplus V_2$
by
\begin{equation*}
  g (v_1 \oplus v_2) := g v_1 \oplus g v_2.
\end{equation*}
Similarly for representations of a Lie algebra.  The two constructions are compatible under differentiation.

\subsubsection{Tensor product}
\label{sec:orgb0fcad8}
Given a pair of representations $V_1, V_2$ of a Lie group $G$,
we get a representation on their tensor product $V_1 \otimes V_2$
by
\begin{equation*}
  g (v_1 \otimes v_2) := g v_1 \otimes g v_2.
\end{equation*}
If $V_1, V_2$ are instead representations
of a Lie algebra $\mathfrak{g}$,
then the natural action to take on their tensor product is
\begin{equation*}
  X (v_1 \otimes v_2) := X v_1 \otimes v_2 + v_1 \otimes X v_2.
\end{equation*}
One of the homework problems for this week is to check that this
in fact defines a Lie algebra representation.
We checked in class that if we differentiate
the first action, we get the second:
\begin{equation*}
  \frac{d}{d t} (\exp(t X) v_1 \otimes \exp(t X) v_2)|_{t=0}
  = X v_1 \otimes v_2 + v_1 \otimes X v_2.
\end{equation*}

\subsubsection{Symmetric power representations\label{sec:constructing-sym-power}}
\label{sec:org4d627e9}
Given a complex vector space $V$
and $t \in \mathbb{Z}_{\geq 0}$,
the \emph{symmetric power} $\Sym^t(V)$
is the space of homogeneous polynomials of degree $t$
on the dual space $V^*$.
Each element of $V$ may be identified
with a linear polynomial on $V^*$.
Any element of $\Sym^t(V)$ may be written
as a finite linear combination
of monomials $v_1 \dotsb v_t$ with each $v_j \in V$.
If $e_1,\dotsc,e_n$ is a basis for $V$,
then the polynomials
$e_{i_1} \dotsb e_{i_t}$ taken over all indices satisfying
$1 \leq i_1 \leq \dotsb \leq i_t \leq n$
form a basis for $\Sym^t(V)$.

Given a representation $R : G \rightarrow \GL(V)$
of a Lie group $G$,
one obtains a symmetric power representation
$\Sym^n(R) : G \rightarrow \GL(\Sym^n(V))$
by setting: for $v_1,\dotsc,v_t \in V$,
\begin{equation*}
  (\Sym^n(R)(g)) v_1 \dotsb v_t
  := (R(g) v_1) \dotsb (R(g) v_t).
\end{equation*}
(See Wikipedia or Google for more details on this construction.)

\subsection{Characters}
\label{sec:orgc5f3d81}
For each representation $V$ of
$\mathfrak{g} := \slLie_2(\mathbb{C})$,
define the \emph{character} of $V$
to be the Laurent polynomial
\begin{equation*}
  \operatorname{ch}(V) \in  A := \mathbb{Z}[z,z^{-1}]
\end{equation*}
given by
\begin{equation*}
  \operatorname{ch}(V) :=
  \sum_{m \in \mathbb{Z}}
  (\dim V[m]) z^m,
\end{equation*}
where
$V[m] := \{v \in V : H v = m v\}$
with $H = \left(
  \begin{smallmatrix}
    1&\\
    &-1
  \end{smallmatrix}
\right) \in \mathfrak{g}$ as before.
(By this point in lecture,
we saw that the action of $H$ on any finite-dimensional
representation
is diagonalizable,
so $V = \oplus V[m]$.)
For example,
for the irreducible representations $W_m$ ($m \in \mathbb{Z}_{\geq 0}$)
considered in lecture,
we have
$\operatorname{ch}(W_m) = \sum_{-m \leq j \leq m : j \equiv m(2)} z^m
= z^m + z^{m-2} + \dotsb + z^{-m}$.
Such functions are \emph{symmetric}
(i.e., invariant under $z \mapsto z^{-1}$.)
The \emph{Weyl denominator}
is the element
\begin{equation*}
  D := z - z^{-1} \in A.
\end{equation*}
It is the simplest example of an \emph{anti-symmetric}
element of $A$.
One has
\begin{equation*}
  D \cdot \operatorname{ch}(W_m)
  = z^{m+1} - z^{-(m+1)}.
\end{equation*}
In other words,
\emph{as $V$ traverses the set of isomorphism
  classes of irreducible
  representations,
  $D \cdot \operatorname{ch}(V)$ traverses the ``obvious''
  basis for the space of anti-symmetric elements of $A$}.

For a finite-dimensional representation $R : G \rightarrow \GL(V)$ of $G :=
\SL_2(\mathbb{C})$,
one has
\begin{equation*}
  \operatorname{ch}(V)|_{z=e^{i \theta}}
  =
  \trace (R(
\begin{pmatrix}
    e^{i \theta} &  \\
     & e^{-i \theta}
  \end{pmatrix}
)).
\end{equation*}

The character satisfies
\begin{equation*}
  \operatorname{ch}(V_1 \oplus V_2) = \operatorname{ch}(V_1) + \operatorname{ch}(V_2),
\end{equation*}
\begin{equation*}
  \operatorname{ch}(V_1 \otimes V_2) = \operatorname{ch}(V_1) \operatorname{ch}(V_2).
\end{equation*}
This is easily seen by taking a basis
$e_1,\dotsc,e_m$ of $H$-eigenvectors
for $V_1$
and a basis
$f_1,\dotsc,f_m$ of $H$-eigenvectors
for $V_2$
and using that $e_1,\dotsc,e_m,f_1,\dotsc,f_m$
is then a basis of $H$-eigenvectors for $V_1 \oplus V_2$
while
$e_i \otimes f_j$ give a basis of $H$-eigenvectors
for $V_1 \otimes V_2$.

If we're given a representation $V$ of $\mathfrak{g}$,
we know that we can decompose
\begin{equation*}
  V \cong  \oplus_{m \geq 0} W_m^{\mu(m)}
\end{equation*}
for some multiplicities $\mu(m) \geq 0$.
We can determine the multiplicities easily if we know the
character
of $V$,
and particularly easily if we multiply first by the Weyl
denominator:
we have
\begin{equation*}
  D \cdot \operatorname{ch}(V)
  = \sum_{m \geq 0} \mu(m) D \cdot \operatorname{ch}(W_m)
  = \sum_{m \geq 0}
  \mu(m)
  (z^{m+1} - z^{-m-1}),
\end{equation*}
so we can read off the multiplicity $\mu(m)$
of $W_m$
inside $V$
as the coefficient of $z^{m+1}$ in the anti-symmetric Laurent
polynomial
$D \cdot \operatorname{ch}(V)$.
For example, in this way (or in others) we can easily
derive the \emph{Clebsh--Gordon decomposition}
\begin{equation*}
  W_m \otimes W_n \cong
  W_{m+n} \oplus W_{m+n-2} \oplus \dotsb \oplus W_{|m-n|}
  = \oplus_{
    \substack{
      |m-n| \leq j \leq m+n :  \\
      j \equiv m+n (2)
    }
  } W_j
\end{equation*}
from the polynomial identity: for $m \geq n$ (say),
\begin{equation*}
  (z^{m+1} - z^{-m-1}) (z^n + z^{n-2} + \dotsb + z^{-n})
  =
  \sum _{\substack{
      m-n \leq j \leq m + n : \\
      j \equiv m+n(2)
    }
  }
  (z^{j + 1} - z^{-j-1}).
\end{equation*}
We can make this more explicit, e.g.,
the isomorphism
\begin{equation*}
  W_2 \oplus W_0 \cong W_1 \otimes W_1
\end{equation*}
can be given by identifying
$W_2 = \mathbb{C}[z,w]_{(2)}$
and $W_0 = \mathbb{C}$
and $W_1 = \mathbb{C}[x,y]_{(1)}$ (where a subscripted $(n)$
denotes homogeneous elements of order $n$)
and the map
\begin{equation*}
  W_0 \rightarrow W_1 \otimes W_1
\end{equation*}
is given by
\begin{equation*}
  1 \mapsto (x \otimes y - y \otimes x)/2
\end{equation*}
and the map
\begin{equation*}
  W_2 \rightarrow W_1 \otimes W_1
\end{equation*}
by
\begin{equation*}
  z^2 \mapsto x \otimes x,
\end{equation*}
\begin{equation*}
  w^2 \mapsto y \otimes y,
\end{equation*}
\begin{equation*}
  z w \mapsto (x \otimes y + y \otimes x)/2.
\end{equation*}
See any introductory textbook on quantum mechanics
for more on the importance of these sorts of decompositions in
physics.

\section{The unitary trick\label{sec:unitary-trick}}
\label{sec:orgb4bc044}
\begin{definition}
  Let $\mathfrak{g}$ be a complex Lie algebra.
  Let $\mathfrak{h}$ be a real Lie algebra.
  We say that
  \begin{itemize}
  \item   $\mathfrak{h}$ is a \emph{real form} of
    $\mathfrak{g}$, or equivalently, that
  \item $\mathfrak{g}$ is the \emph{complexification} of $\mathfrak{h}$,
  \end{itemize}
  if
  $\mathfrak{h}$ is (isomorphic to)
  a real Lie subalgebra
  of $\mathfrak{g}$
  for which
  \begin{equation*}
    \mathfrak{g} = \mathfrak{h} \oplus i \mathfrak{h},
  \end{equation*}
  or equivalently, for which the natural map
  $\mathfrak{h} \otimes_\mathbb{R} \mathbb{C} \rightarrow
  \mathfrak{g}$
  is an isomorphism.
  In other words, every $z \in \mathfrak{g}$
  may be expressed uniquely as $x + i y$
  with $x,y \in \mathfrak{h}$.
\end{definition}
\begin{example}
  $\slLie_n(\mathbb{R})$
  and
  $\su(n)$
  are real forms of
  $\slLie_n(\mathbb{C})$;
  $\so(n)$
  is a real form of
  $\so_n(\mathbb{C})$.
  $\gl_n(\mathbb{R})$
  and
  $\mathfrak{u}(n)$
  are real forms of
  $\gl_n(\mathbb{C})$.
  A bit less obviously,
  $\so(p,q)$
  is (isomorphic to) a real form of
  $\so_n(\mathbb{C})$
  (try to check this!).
\end{example}

\begin{lemma}\label{lem:real-froms-same-reps-1}
  Let $L$ be any complex Lie algebra.
  Let $\mathfrak{g}$ be a complex Lie algebra
  and $\mathfrak{h}$ a real form of $\mathfrak{g}$.
  Then the natural restriction map
  \begin{equation*}
    \left\{
      \begin{gathered}
        \text{morphisms of complex Lie algebras} \\
        \Phi : \mathfrak{g} \rightarrow L
      \end{gathered}
    \right\}
    \rightarrow 
    \left\{
      \begin{gathered}
        \text{morphisms of real Lie algebras} \\
        \phi : \mathfrak{h} \rightarrow L
      \end{gathered}
    \right\}
  \end{equation*}
  is a bijective.
\end{lemma}
\begin{proof}
  Given $\Phi$, one defines $\phi$ by restriction.
  Given $\phi$, one defines $\Phi(x + i y) := \phi(x) + i
  \phi(y)$.
  One checks that the associations $\phi \mapsto \Phi$
  and $\Phi \mapsto \phi$ are mutually inverse, and that one
  defines a Lie algebra morphism (over the relevant field)
  if and only if the other does.  (One could alternatively take the conclusion
  of this lemma as the \emph{definition} of real form/complexification, as in the functorial characterization of tensor product.)
\end{proof}

\begin{example}\label{ex:complex-vs-real-reps}
  Suppose $L = \End(V)$ for a complex vector space $V$.
  Then Lemma \ref{lem:real-froms-same-reps-1}
  says that
  for a complex Lie algebra $\mathfrak{g}$ with real  form
  $\mathfrak{h}$,
  the following sets are in natural bijection:
  \begin{enumerate}
  \item 
    holomorphic representations
    $\rho : \mathfrak{g} \rightarrow \End(V)$.
  \item 
    representations
    $\rho_0 : \mathfrak{h} \rightarrow \End(V)$.
  \end{enumerate}
  Moreover, the invariant subspaces $W$ for $\rho$ and $\rho_0$
  are the same.  In particular, $\rho$ is irreducible if and
  only if $\rho_0$ is irreducible, and also $\rho$ is completely
  reducible if and only if $\rho_0$ is completely reducible.
\end{example}


\begin{definition}
  Let $G$ be a connected complex Lie group.
  A \emph{real form} of $G$ is a connected
  real Lie subgroup $H \leq G$
  with the property that
  the Lie algebra $\mathfrak{h}$ of $H$ is a real form
  of the Lie algebra $\mathfrak{g}$ of $G$.
\end{definition}


\begin{example}
  $\GL_n(\mathbb{R})$ and $\U(n)$
  are real forms of
  $\GL_n(\mathbb{C})$.
  $\SL_n(\mathbb{R})$ and $\SU(n)$
  are real forms of
  $\SL_n(\mathbb{C})$.
  $\SO(n)$ and $\SO(p,q)^0$ ($p + q = n$)
  are real forms of
  $\SO_n(\mathbb{C})$.
\end{example}
\begin{theorem}\label{thm:real-form-implies-linearly-reductive}
  Let $G$ be a connected complex Lie group.
  Suppose that $G$ has a compact (connected) real form.
  Then $G$ is linearly reductive.
\end{theorem}
\begin{proof}
  Let $H$ be a compact (connected) real form of $G$, and denote Lie algebras
  in the usual way.  Let $R : G \rightarrow \GL(V)$ be a
  finite-dimensional representation of $G$.
  We wish to show that $V$ is completely reducible under $G$.
  We have seen in
  Lemma \ref{lem:complete-irreducible-gp-vs-alg} that $V$ is
  completely reducible under $G$ if and only if it is under
  $\mathfrak{g}$, and likewise that $V$ is completely reducible
  under $H$ if and only if it is under $\mathfrak{h}$.  By Lemma
  \ref{lem:real-froms-same-reps-1}, we know that $V$ is
  completely reducible under $\mathfrak{g}$ if and only if it is
  under $\mathfrak{h}$.  In summary, $V$ is completely reducible
  under any one of $G,\mathfrak{g},\mathfrak{h},H$ if and only
  if it is under all of them.
  Since compact Lie groups as linearly reductive
  (Theorem \ref{thm:compact-implies-lin-red}),
  we know that $V$ is completely reducible under $H$,
  so we are done.
\end{proof}

\begin{example}
  It follows that the groups
  $\GL_n(\mathbb{C})$ (e.g., $\mathbb{C}^\times$)
  $\SL_n(\mathbb{C})$,
  $\SO_n(\mathbb{C})$
  are linearly reductive.
\end{example}

\begin{remark}
  Let $G$ be a connected complex Lie group
  which has a compact (connected) real form,
  then we have seen that $G$, as well as its compact (connected) real form, is linearly reductive.
  However, it need not be the case that every
  real form $H$ of $G$ is linearly reductive;
  consider for instance the case $G = \mathbb{C}^\times,
  H = \mathbb{R}^\times$.
  (This is another example where things become nicer
  by working in the category of algebraic groups;
  compare with Remark \ref{rmk:lie-vs-alg}.)
\end{remark}

In the proof of Theorem
\ref{thm:real-form-implies-linearly-reductive}, the notion of
``completely reducible'' relevant for the representation $V$ of
$\mathfrak{h}$
is that there exist no invariant complex subspaces.
One can ask what happens if one works instead with invariant
real subspaces:
\begin{exercise}
  Let $V$ be a complex vector space
  and $\End(V)$
  the Lie algebra of $\mathbb{C}$-linear endomorphisms of $V$.
  Let $\mathfrak{h}$ be a real Lie algebra
  and $\rho : \mathfrak{h} \rightarrow \End(V)$
  a morphism.

  Let $W \leq V$ be a real subspace (i.e., a subspace
  of the real vector space underlying $V$).
  \begin{enumerate}
  \item   Show that if $W$ is invariant or irreducible
    (under the action by $\mathfrak{h}$),
  then so is $i W$.
\item
  Suppose that $W$ is invariant and irreducible.
  Show that either
  \begin{enumerate}
  \item  $W = i W$,
    in which case $W$ is an invariant irreducible complex subspace, or
  \item
    $W \cap i W = \{0\}$,
    in which case $W + i W$ is an invariant irreducible complex subspace.
  \end{enumerate}
  Deduce that if $V$ decomposes
  as a direct sum of invariant irreducible
  real subspaces,
  then it also decomposes
  as a direct sum of invariant irreducible
  complex subspaces.
  \end{enumerate}
\end{exercise}


Let us work out the complexification of $H := \SO(p,q)^0$.
Recall that
$\SO(p,q) = \{g \in \SL_{p+q}(\mathbb{R}) : g I_{p,q} g^t =
I_{p,q}\}$
where
$I_{p,q} := \diag(1,\dotsc,1,-1,\dotsc,-1)$,
thus
\begin{equation*}
  \mathfrak{h} = \{X \in \SL_{p+q}(\mathbb{R}) :
  X I_{p,q} + I_{p,q} X^t = 0 \}
  = \left\{ 
\begin{pmatrix}
      A & B \\
      C & D
    \end{pmatrix}
 \in M_{p+q}(\mathbb{R})
    : A^t = - A, D^t = - D, B^t = C \right\}.
\end{equation*}
The complexification is given by
\begin{equation*}
  \mathfrak{h} \otimes_{\mathbb{R}} \mathbb{C}
  \cong 
  \mathfrak{h}  \oplus i \mathfrak{h}
  =
   \left\{ 
\begin{pmatrix}
      A & B \\
      C & D
    \end{pmatrix}
 \in M_{p+q}(\mathbb{C}) : A^t = - A, D^t = -
    D, B^t= C 
  \right\} \subseteq M_{p+q}(\mathbb{C}).
\end{equation*}
If $\eps := \diag(i,\dotsc,i,1,\dotsc,1)$,
then we can check easily that
\begin{equation*}
  \eps (\mathfrak{h} \oplus i \mathfrak{h}) \eps^{-1}
  =
  \so_{p+q}(\mathbb{C}).
\end{equation*}
Thus the complexification of $\so(p,q)$
is isomorphic to $\so_{p+q}(\mathbb{C})$.
Similarly,
$\SO(p,q)^0$ is (isomorphic to)
a real form of $\SO_{p+q}(\mathbb{C})$;
just consider
\begin{equation*}
  \eps \SO(p,q)^0 \eps^{-1}
  \leq \SO_{p+q}(\mathbb{C}) \leq \SL_{p+q}(\mathbb{C}).
\end{equation*}

To cook up some more interesting examples,
let
\begin{equation*}
  \mathbb{H} :=
  \left\{ 
\begin{pmatrix}
      z & w \\
      -\overline{w} &  \overline{z}
    \end{pmatrix}
 : z,w \in \mathbb{C}  \right\}
  \subseteq M_2(\mathbb{C})
\end{equation*}
denote Hamilton's quaternion algebra over the reals.
(It is an associative algebra with center $\mathbb{R}$
and of dimension $4$ over its center.)
There is a natural involution $x \mapsto x^*$ on $\mathbb{H}$,
given by
$x^* := \overline{x}^t$,
or equivalently,
\begin{equation*}
  \begin{pmatrix}
    a & b \\
    c & d
  \end{pmatrix}
^*
  =
  \begin{pmatrix}
    d & -b \\
    -c & a
  \end{pmatrix}
.
\end{equation*}
Then
$\mathbb{H}^\times$ is a Lie group and
\begin{equation*}
  \Lie(\mathbb{H}^\times) = \mathbb{H}.
\end{equation*}
The space $M_n(\mathbb{H})$
of $n \times n$ matrices with quaternionic entries
can be regarded as a subspace of
$M_{2 n}(\mathbb{C})$.
Set
\begin{equation*}
  \SL_n(\mathbb{H}) :=
  M_n(\mathbb{H}) \cap \SL_{2n}(\mathbb{C})
\end{equation*}
and
\begin{equation*}
  \U_n(\mathbb{H})
  :=
  \{ g \in M_n(\mathbb{H}) : g \overline{g^*} = 1 \}.
\end{equation*}
Recall that $\SU(p,q) = \{g \in \SL_{p+q}(\mathbb{C}) :
g I_{p,q} \overline{g}^t = I_{p,q}
\}$.

\begin{exercise}\label{exe:complexifications}
  Determine the complexifications of
  the Lie algebras of the following real Lie groups:
  \begin{enumerate}
  \item $\SL_n(\mathbb{H})$
  \item $\SU(p,q)$
  \item $\U_m(\mathbb{H})$
  \end{enumerate}
  (Each is isomorphic to a classical complex Lie group
  we're already familiar with.)
\end{exercise}

It turns out that with this exercise and the examples given
previously,
we've found all of the real forms of the complex Lie algebras
$\slLie_n(\mathbb{C}), \so_n(\mathbb{C})$.

\begin{exercise}
  Let $\mathfrak{h} := \slLie_n(\mathbb{C})$,
  but regarded as a real Lie algebra rather than a complex one.
  (Concretely, we can think of $\mathfrak{h}$ as a real Lie
  subalgebra
  of $\slLie_{2 n}(\mathbb{R})$.)
  Show that one has an isomorphism of complex Lie algebras
  $\mathfrak{h} \otimes_{\mathbb{R}} \mathbb{C}
  \cong \slLie_n(\mathbb{C}) \oplus  \slLie_n(\mathbb{C})$.
\end{exercise}

\begin{remark}\label{rmk:real-forms-vs-involutions}
  Let $\mathfrak{h}$ be a real form
  of a complex Lie algebra $\mathfrak{g}$.
  Then $\mathfrak{h}$ is the fixed point set of the automorphism
  $\sigma : \mathfrak{g} \rightarrow \mathfrak{g}$
  given by
  identifying $\mathfrak{g}$ with $\mathfrak{h}
  \otimes_{\mathbb{R}} \mathbb{C}$
  and requiring that
  for $X \in \mathfrak{h}$ and $z \in \mathbb{C}$,
  one has
  $\sigma(X \otimes z) := X \otimes \overline{z}$;
  in other words,
  if we identify
  $\mathfrak{g}$ with $\mathfrak{h} \oplus i \mathfrak{h}$,
  then $\sigma(x + i y) := x - i y$ for $x,y \in \mathfrak{h}$.
  Then $\sigma$ is an involution on $\mathfrak{g}$ (i.e.,
  $\sigma^2$ is the identity transformation);
  moreover, $\sigma$ is a Lie
  algebra
  automorphism that is anti-linear with respect to scalar
  multiplication
  by complex numbers,
  i.e., for all $c \in \mathbb{C}$, $Z \in \mathfrak{g}$,
  $\sigma(c Z) = \overline{c} \sigma(Z)$,
  and satisfies $\sigma^2 = 1$.

  Conversely, given an anti-linear involution
  $\sigma : \mathfrak{g} \rightarrow \mathfrak{g}$, we claim
  that its fixed point subspace
  $\mathfrak{h} : \{X \in \mathfrak{g} : \sigma(X) = X\}$ is a
  real form of $\mathfrak{g}$.
  Well,
  since $\sigma$ is a real Lie algebra automorphism,
  we know at least that
  $\mathfrak{h}$ is a real Lie subalgebra.
  Observe that $i \mathfrak{h} = \{X \in \mathfrak{g} :
  \sigma(X) = - X\}$.
  Since $\mathfrak{g}$ is the sum of the $+1$ and $-1$
  eigenspaces
  of $\sigma$,
  we deduce that $\mathfrak{g} = \mathfrak{h} \oplus i
  \mathfrak{h}$.
  Hence $\mathfrak{h}$ is a real form.

  %and their isomorphism classes in terms of the conjugacy
  %classes
  %of such involutions.
\end{remark}

\section{Ad and ad}
\label{sec:orga45c298}

\subsection{Basic definitions}
\label{sec:org7eda827}
When one learns basic group theory (say of finite groups),
one studies groups $G$ acting on sets $X$.
A particularly important action is the conjugation action
of $G$ on itself,
given by $(g,x) \mapsto g x g^{-1}$.
The orbits for this action are the conjugacy classes in $G$.
Much nontrivial information about $G$ can be extracted
from a careful study of the conjugation action of $G$ on itself.
For example, the Sylow theorems are proved in this way.

When $G$ is a Lie group, one can again consider the conjugation
action of $G$ on itself,
but it turns out to be more useful to differentiate this action
a bit, so that tools from linear algebra become at our disposal.
\begin{definition}
  Given a Lie group $G$
  with Lie algebra $\mathfrak{g}$,
  the \emph{adjoint representation} of $G$ is the map
  \begin{equation*}
    \Ad  : G \rightarrow \GL(\mathfrak{g})
  \end{equation*}
  is defined by
  \begin{equation*}
    \Ad(g) X := g X g^{-1},
  \end{equation*}
  where the RHS may be interpreted in various ways:
  \begin{enumerate}
  \item If $G$ is a subgroup of $\GL_n(\mathbf{k})$,
    then $\mathfrak{g}$ is a subalgebra of $M_n(\mathbf{k})$,
    and so we can interpret $g X g^{-1}$ as a product of
    matrices.
  \item
    We can use the trick of
    \S\ref{sec:pretend-lie-groups-are-matrix-groups}
    (``pretending that every Lie group is a matrix Lie group'')
    to embed both $G$ and $\mathfrak{g}$ inside
    $\End(C^\infty(G))$; in that optic,
    the product $g X g^{-1}$ is given by composition.
  \item
    (I don't recommend spending too much time studying
    this interpretation.)
    In general,
    for $g \in G$
    and
    $x_0 \in G$
    and $X \in T_{x_0}(G)$,
    we can define $g X \in T_{g x_0}(G)$ to be the image of $X$ under the
    differential
    of left translation by $g$,
    i.e.,:
    if $\psi : G \rightarrow G$ is the map
    $\psi(x) := g x$,
    then
    $g X := (T_{x_0} \psi)(X)$
    where
    $T_{x_0} \psi : T_{x_0}(G)  \rightarrow T_{g x_0}(G)$ is the
    derivative.
    We can similarly define
    $X g \in T_{x_0 g}(G)$ using right translations.
    This makes sense in particular when $x_0 = e$ is the
    identity element, so that $T_{e} G = \mathfrak{g}$;
    then for $X \in \mathfrak{g}$
    we have $g X \in T_g G$ and thus $(g X) g^{-1} \in
    \mathfrak{g}$.
    Alternatively, we may first form
    $X g^{-1} \in T_{g^{-1}} G$ and then $g (X g^{-1}) \in
    \mathfrak{g}$.
    The two answers are the same because left and right
    translations
    commute  with one another.  (See \S\ref{sec:translate-tangent-spaces-gp-elts} for related discussion.)
  \item
    For any $g \in G$ and $X \in \mathfrak{g}$,
    the map
    $\mathbf{k} \ni t \mapsto g \exp(t X) g^{-1}$
    is a one-parameter subgroup,
    so its initial velocity is an element of the Lie
    algebra:
    \begin{equation*}
      g X g^{-1} = \partial_{t=0}
      g \exp(t X) g^{-1}.
    \end{equation*}
  \end{enumerate}
  It is clear that $\Ad : G \rightarrow \GL(\mathfrak{g})$ is a
  morphism of Lie groups.

  Note that if $G$ is a real Lie
  group, then $\Ad$ is a real representation, not a complex
  representation of the sort that we have primarily been
  considering.  If $G$ is a complex Lie group, then $\Ad$ is a
  holomorphic representation.
\end{definition}

\begin{exercise}\label{exercise:conjugation-and-Ad-are-intertwined-by-exponential}
  Let $g \in G, X \in \mathfrak{g}$.
  Show that
  \begin{equation*}
    g \exp(X) g^{-1}
    =
    \exp(\Ad(g) X).
  \end{equation*}
  Hint: one can either
  \begin{enumerate}
  \item appeal to uniqueness of one-parameter subgroups
    (\S\ref{sec:one-param-subgps}), or
  \item apply the result of
    \S\ref{sec:exp-commutes-with-morphisms}
    to $f : G \rightarrow G$ given by
    $f(a) := g a g^{-1}$.
  \end{enumerate}
\end{exercise}

Since the Lie algebra $\mathfrak{g}$  of a Lie group is a vector
space,
we may form its linear dual $\mathfrak{g}^* :=
\Hom_{\mathbf{k}}(\mathfrak{g},\mathbf{k})$.
From some perspectives (which
we might discuss eventually),
the following action is better behaved than the adjoint action:
\begin{definition}
  The \emph{coadjoint representation} of a Lie group $G$
  is the map
  \begin{equation*}
    \Ad^* : G \rightarrow \GL(\mathfrak{g}^*)
  \end{equation*}
  given for $g \in G, \lambda \in \mathfrak{g}^*, X \in
  \mathfrak{g}$
  by
  \begin{equation*}
    (\Ad^*(g) \lambda)(X) := \lambda(\Ad(g)^{-1} X).
  \end{equation*}
\end{definition}
\begin{exercise}
  Check that $\Ad^*$ is a representation,
  but that it wouldn't be in general
  had we omitted the inverse in the definition.
\end{exercise}

\begin{definition}
  Given any Lie algebra $\mathfrak{g}$
  with Lie bracket $\mathfrak{g}$,
  we can define
  \begin{equation*}
    \ad : \mathfrak{g} \rightarrow \End(\mathfrak{g})
  \end{equation*}
  by the formula
  \begin{equation*}
    (\ad(X))(Y) := [X,Y].
  \end{equation*}
  We usually abbreviate the LHS to $\ad(X) Y$.
  We might sometimes also abbreviate $\ad_X := \ad(X)$,
  so that $\ad_X Y = [X,Y]$.
  Recall that
  the Jacobi identity may be written
  in the following equivalent ways:
  \begin{equation}\label{eq:ad-is-a-morphism}
    [[X,Y],Z] = [[X,Z],Y] + [X,[Y,Z]],
  \end{equation}
  \begin{equation}\label{eq:ad-X-is-a-derivation}
    [X,[Y,Z]] = [[X,Y],Z] + [Y,[X,Z]].
  \end{equation}
  The first identity \eqref{eq:ad-is-a-morphism}
  may be interpreted as saying that $\ad : \mathfrak{g}
  \rightarrow \End(\mathfrak{g})$
  is a morphism of Lie algebras,
  that is to say,
  that
  \begin{equation*}
    \ad([X,Y]) = [\ad(X),\ad(Y)],
  \end{equation*}
  since indeed
  $\ad([X,Y]) Z = [[X,Y],Z]$
  and
  $[\ad(X),\ad(Y)] Z
  = \ad(X) \ad(Y) Z - \ad(Y) \ad(X) Z
  = \ad(X) [Y,Z] - \ad(Y) [X,Z]
  = [X,[Y,Z]] - [Y,[X,Z]]$.
  The second identity may be interpreted
  as saying that
  that $\ad_X$ is a derivation for each $X \in \mathfrak{g}$,
  i.e.,
  that
  \begin{equation*}
    \ad_X [Y,Z] = [\ad_X Y, Z] + [Y,\ad_X Z],
  \end{equation*}
  i.e., that
  $\ad(\mathfrak{g}) \subseteq \Der(\mathfrak{g})$.
  So in summary, $\ad$ defines a morphism of Lie algebras
  \begin{equation*}
    \ad : \mathfrak{g} \rightarrow \Der(\mathfrak{g}).
  \end{equation*}
  One can already get \emph{huge} mileage out
  of this simple statement (see the tricky problem on this week's homework).
   Note in particular that it contains
  two different interpretations of the Jacobi identity.
\end{definition}

\subsection{Relationship between \texorpdfstring{$\Ad$}{Ad} and \texorpdfstring{$\ad$}{ad}}
\label{sec:orgeaeefa5}
The above definitions are related as follows:
\begin{lemma}\label{lem:dAd-is-ad}
  Let $G$ be a Lie group with Lie algebra $\mathfrak{g}$.
  Then the differential $d \Ad : \mathfrak{g}
  \rightarrow \End(\mathfrak{g})$
  of the morphism of Lie groups
  $\Ad : G \rightarrow \GL(\mathfrak{g})$
  is the morphism of Lie algebras
  $\ad : \mathfrak{g} \rightarrow \End(\mathfrak{g})$,
  that is to say, $d \Ad = \ad$,
  or more verbosely,
  for any $X ,Y \in \mathfrak{g}$,
  \begin{equation*}
    \partial_{s=0}
    \Ad(\exp(s X)) Y = \ad(X) Y.
  \end{equation*}
\end{lemma}
\begin{proof}
  The RHS is $\ad(X) Y = [X,Y]$.
  We expand out its definition and compute,
  obtaining
  \begin{align*}
    [X,Y]
    &:=
      \partial_{s=0} \partial_{t=0} (e^{s X}, e^{t Y})
      \\
    &=
      \partial_{s=0} \partial_{t=0}  e^{s X} e^{t Y} e^{-s X} e^{-t Y}
      \\
    &=
      \partial_{s=0} (e^{s X} Y e^{-s X}  - Y )
      \\
    &=
      \partial_{s=0} \Ad(e^{s X}) Y,
  \end{align*}
  as required.
\end{proof}

\subsection{Interpretation of the Jacobi identity}
\label{sec:orgeedd74d}
\begin{remark}
  Let $G$ be a Lie group.
  When we defined its Lie algebra $\mathfrak{g}$
  and defined the Lie bracket $[,]$,
  we \emph{promised} that the Jacobi identity
  followed from the associativity of the group law on $G$,
  but didn't prove it.
  We can now give a rigorous and fairly conceptual proof:
  Recall that we proved
  that any morphism $f : G \rightarrow H$
  of Lie groups
  induces a morphism $d f : \mathfrak{g} \rightarrow
  \mathfrak{h}$ of Lie algebras.
  Applying this fact with $H := \GL(\mathfrak{g})$
  to the adjoint representation $f := \Ad$
  implies that $\ad = d \Ad$ is a morphism of Lie algebras.
  But we saw above that this last assertion is equivalent
  to the Jacobi identity.
  (Exercise: check carefully that we haven't used circular reasoning here.)
\end{remark}

\subsection{\texorpdfstring{$\Ad, \ad$}{Ad, ad} are intertwined by the exponential map\label{sec:Ad-ad-intertwined-exp}}
\label{sec:orgcb8cdaf}
Let $G$ be a Lie group with Lie algebra $\mathfrak{g}$.
One has maps
\begin{equation*}
  \exp : \mathfrak{g} \rightarrow G
\end{equation*}
and also a map
\begin{equation*}
  \exp : \End(\mathfrak{g}) \rightarrow \GL(\mathfrak{g})
\end{equation*}
(the matrix exponential).
The adjoint maps defined above  are intertwined
by these exponential maps,
that is to say,
$\Ad \circ \exp = \exp \circ \ad$,
or more verbosely,
for each $X \in \mathfrak{g}$,
\begin{equation*}
  \Ad(\exp(X)) = \exp(\ad(X))
  :=
  \sum_{n = 0}^{\infty} \frac{\ad(X)^n}{n!},
\end{equation*}
or even more verbosely,
for each $X,Y \in \mathfrak{g}$,
\begin{equation*}
  \exp(X) Y \exp(-X)
  =\Ad(\exp(X)) Y
  =
  \sum_{n=0}^{\infty}
  \frac{\ad(X)^n Y}{n!}
  =
  \sum_{n=0}^{\infty}
  \frac{[X,[X,\dotsc,[X,Y]]]}{n!}
\end{equation*}
where there are $n$ copies of $X$ in the iterated commutator
on the RHS.
One can prove this by writing the LHS and RHS
as $\Phi_1(1)$ and $\Phi_2(1)$ respectively,
where $\Phi_1(t) := \Ad(\exp(t X))$ and $\Phi_2(t) := \exp(\ad(t
X))
= \exp(t \ad(X))$ are one-parameter subgroups
in $\GL(\mathfrak{g})$;
by  Lemma \ref{lem:dAd-is-ad},
we have $\Phi_1'(0) = d \Ad(X) = \ad(X) = \Phi_2'(0)$,
hence by the uniqueness of one-parameter subgroups
the two sides coincide.
This identity is already not entirely obvious in the matrix
case $G = \GL_n(\mathbf{k})$;
it is instructive to verify it directly in that case.

\subsection{Some low-rank exceptional isomorphisms induced by the adjoint representation\label{sec:low-rank-exceptional-isomorphisms}}
\label{sec:orgcb7a112}
The adjoint representation $\Ad$
of the groups $\SL_2(\mathbb{C}), \SU(2), \SL_2(\mathbb{R})$
induce the exceptional isomorphisms
\begin{equation*}
  \PSL_2(\mathbb{C}) := \SL_2(\mathbb{C}) / \{\pm 1\}
  \cong \SO_3(\mathbb{C}),
\end{equation*}
\begin{equation*}
  \SU(2) / \{\pm 1\}
  \cong \SO(3),
\end{equation*}
\begin{equation*}
  \PSL_2(\mathbb{R}) := \SL_2(\mathbb{R}) / \{\pm 1\}
  \cong \SO(1,2)^0.
\end{equation*}
We explained this in detail in class for the first two examples
and left the third as an exercise.
The adjoint representations $\ad$ of the corresponding Lie
algebras
likewise induce
isomorphisms
\begin{equation*}
  \slLie_2(\mathbb{C}) \cong \so_3(\mathbb{C}),
\end{equation*}
\begin{equation*}
  \su(2) \cong \so(3),
\end{equation*}
\begin{equation*}
  \slLie_2(\mathbb{R}) \cong \so(1,2).
\end{equation*}
(Note that $\Lie(\PSL_2(\mathbb{C})) = \Lie(\SL_2(\mathbb{C}))$,
etc.,
because $\{\pm 1\}$ is discrete.)

We spent some time in class introducing some terminology for
interpreting
the above isomorphisms in a natural way.
\begin{definition}
  Let $\mathbf{k}$ be a field, perhaps of characteristic $\neq
  2$.
  A (non-degenerate) \emph{quadratic space $V$ over $\mathbf{k}$}
  is a pair $V = (V,Q)$, where
  \begin{enumerate}
  \item $V$ is a finite-dimensional $\mathbf{k}$-vector space,
    and
  \item $Q$ is a map $Q : V \rightarrow \mathbf{k}$ for which
    the map $B := B_Q : V \times V \rightarrow \mathbf{k}$
    defined by $B(x,y) := Q(x+y) - Q(x) - Q(y)$ has the
    properties:
    \begin{enumerate}
    \item $B$ is \emph{bilinear}, i.e.,
      $B(a_1 x_1 + a_2 x_2, b_1 y_1 + b_2 y_2)
      = \sum_{i=1,2} \sum_{j=1,2} a_i b_j B_Q(x_i,y_j)$
      for all $a_i,b_j \in \mathbf{k}$
      and $x_i,y_j \in V$;
    \item $B$ is \emph{non-degenerate}
      in the sense that
      for each nonzero $x \in V$ there exists a nonzero $y \in
      V$
      so that $B(x,y) \neq 0$; equivalently,
      the map $x \mapsto B(x,\cdot)$ defines a linear isomorphism
      from $V$ to its linear dual $V^*$.
    \end{enumerate}
  \end{enumerate}
  A \emph{morphism} of quadratic spaces
  $f : (V_1,Q_1) \rightarrow (V_2,Q_2)$
  is a linear map $f : V_1 \rightarrow V_2$
  so that $Q_2 \circ f = Q_1$.
  Two such quadratic spaces are thus \emph{isomorphic}
  if there exists a linear isomorphism
  $f : V_1 \rightarrow V_2$
  satisfying $Q_2 \circ f = Q_1$.
\end{definition}
\begin{example}
  If $\mathbf{k} = \mathbb{C}$ and $n \in \mathbb{Z}_{\geq 0}$,
  then $V := \mathbb{C}^n$
  with $Q_{n}(x) := \sum_{i=1}^n x_i^2$
  is a quadratic space, called the \emph{standard
    $n$-dimensional
    quadratic space over $\mathbb{C}$}.
\end{example}
\begin{example}
  If $\mathbf{k} = \mathbb{R}$ and $p,q \in \mathbb{Z}_{\geq 0}$,
  then $V := \mathbb{R}^{p+q}$
  with $Q_{p,q}(x) := \sum_{i=1}^p x_i^2 - \sum_{j=1}^q x_{p+j}^2$
  is a quadratic space,
  called
  the \emph{standard
    quadratic space over $\mathbb{R}$ of signature $(p,q)$}.
\end{example}
\begin{theorem}
  Over $\mathbf{k} = \mathbb{R}$ or $\mathbb{C}$,
  every quadratic space is isomorphic to one of the above examples.
\end{theorem}
\begin{definition}
  Given a quadratic space $V = (V,Q)$ over $\mathbf{k}$ (take
  $\mathbf{k} = \mathbb{R}$ or $\mathbb{C}$ for the purposes
  of this course, although the construction applies more generally)
  we may define its \emph{orthogonal group}
  $\O(V) = \{g \in \GL(V) : Q(g v) = Q(v) \text{ for all } v \in
  V\}$
  and \emph{special orthogonal group}
  $\SO(V) = \O(V) \cap \SL(V)$.
\end{definition}
If two quadratic spaces are isomorphism, it is clear that their
(special)
orthogonal groups are likewise isomorphic.
The above  definition thus introduces
no new groups beyond
the examples
$\O_n(\mathbb{C})$, $\O(n)$, $\O(p,q)$,
$\SO_n(\mathbb{C})$, $\SO(n)$, $\SO(p,q)$
that we have already seen, 
but it is sometimes convenient to be able to refer
to them in a coordinate-free manner.

\begin{example}
  Over $\mathbf{k} = \mathbb{C}$,
  the space $\slLie_2(\mathbb{C})$
  with the quadratic form $\det$ is a quadratic space.
  In fact, the linear map
  $j : \mathbb{C}^3 \rightarrow \slLie_2(\mathbb{C})$
  given by
  \begin{equation}\label{eq:coordinates-for-exceptional-isomorphism-sl2}
        (x,y,z)
    \mapsto 
\begin{pmatrix}
      i x & y + i z \\
      -y + i z & -i x
    \end{pmatrix}
    \in \slLie_2(\mathbb{C})
  \end{equation}
  satisfies $\det(j(x,y,z)) = x^2 + y^2 + z^2$,
  and hence induces an explicit isomorphism of quadratic spaces
  \begin{equation*}
    (\mathbb{C}^3, Q_3)
    \cong (\slLie_2(\mathbb{C}),\det).
  \end{equation*}
  Thus, in particular,
  \begin{equation*}
    \SO(\slLie_2(\mathbb{C}),\det) \cong \SO_3(\mathbb{C}).
  \end{equation*}
\end{example}
\begin{example}
  Over $\mathbf{k} = \mathbb{R}$,
  the space $\su(2)$
  with the quadratic form $\det$ is a quadratic space.
  In fact, the linear map
  $j : \mathbb{R}^3 \rightarrow \su(2)$
  given by \eqref{eq:coordinates-for-exceptional-isomorphism-sl2}
  satisfies $\det(j(x,y,z)) = x^2 + y^2 + z^2$,
  and hence induces an explicit isomorphism of quadratic spaces
  \begin{equation*}
    \SO(\su(2),\det) \cong \SO(3).
  \end{equation*}
\end{example}
One of the homework problems
for this week is to work out something similar
for $\slLie_2(\mathbb{R})$.

Consider now the adjoint map
\begin{equation*}
  \Ad : \SL_2(\mathbb{C}) \rightarrow \GL(\slLie_2(\mathbb{C})).
\end{equation*}
Since $\det(\Ad(g) X) = \det(X)$
for all $g \in \SL_2(\mathbb{C})$ and $X \in
\slLie_2(\mathbb{C})$,
we have in fact
\begin{equation*}
  \Ad (\SL_2(\mathbb{C})) \subseteq
  \O(\slLie_2(\mathbb{C}),\det) \cong \O_3(\mathbb{C}).
  \end{equation*}
Since (as we have shown)
$\SL_2(\mathbb{C})$ is connected,
so is its image under $\Ad$,
thus
in fact
\begin{equation*}
  \Ad (\SL_2(\mathbb{C})) \subseteq \SO_3(\mathbb{C}).
\end{equation*}
Similarly
\begin{equation*}
  \ad(\slLie_2(\mathbb{C}))
  \subseteq \so(\slLie_2(\mathbb{C}),\det) \cong \so_3(\mathbb{C}).
\end{equation*}
We may check easily that $\ker(\Ad) = \{\pm 1\}$
and $\ker(\ad) = \{0\}$;
from this and a dimensionality check it follows that $\ad$ is a
linear isomorphism
and hence (using that 
$\SO_3(\mathbb{C})$
is connected and that the exponential map has the properties
that it has)
that
$\Ad : \SL_2(\mathbb{C}) \rightarrow \SO_3(\mathbb{C})$ is
surjective.
We thereby obtain an isomorphism
\begin{equation*}
  \SL_2(\mathbb{C}) / \{\pm 1\} \cong \SO_3(\mathbb{C}).
\end{equation*}
Similar arguments give the other isomorphisms claimed above.
The homework problems for this week give some other
interpretations.

\begin{remark}
  One can check that the inverse isomorphism
  $\so_3(\mathbb{C}) \xrightarrow{\cong } \slLie_2(\mathbb{C})$
  is not of the form $d f$
  for some  morphism $f$ of Lie groups
  $\SO_3(\mathbb{C}) \rightarrow \slLie_2(\mathbb{C})$;
  we shall return to this point later.
\end{remark}

Using the above exceptional isomorphisms, together with the fact
that $-1 \in \SL_2(\mathbb{C})$ acts on the irreducible
representation $W_n$ by the sign $(-1)^n$, we deduce that the
irreducible representations of $\SO_3(\mathbb{C})$ are given by
the $W_{2 n}$ for all $n \geq 0$; the action of
$g \in \SO_3(\mathbb{C})$ on $W_{2 n}$ is defined to be
$R(\tilde{g})$ for some preimage
$\tilde{g} \in \SL_2(\mathbb{C})$.
We should see next time a bit more explicitly how this goes.

\section{Maurer--Cartan equations and applications}
\label{sec:org6e14ee2}
\subsection{The equations\label{sec:cartan-maurer-eqn}}
\label{sec:org61f279c}
Let $G$ be a Lie group and a smooth map
\begin{equation*}
  g : \mathbf{k}^2 \xdashrightarrow{} G.
\end{equation*}
We can think of $g$ as a smooth parametrized surface, or as a family of (possibly disconnected) curves $t \mapsto g(t,s)$ indexed by a deformation parameter $s$.  The velocity of the curve with parameter $s$ may be described by the smooth map
\begin{equation*}
  \xi : \mathbf{k}^2 \xdashrightarrow{} \mathfrak{g}
\end{equation*}
characterized by the equation
\begin{equation*}
  \frac{\partial g}{\partial t} = g \xi.
\end{equation*}
Similarly, for a fixed time $t$, the velocity of the deformation is quantified by the map
\begin{equation*}
  \eta : \mathbf{k}^2 \xdashrightarrow{}\mathfrak{g}
\end{equation*}
characterized by
\begin{equation*}
  \frac{\partial g}{\partial s} = g \eta.
\end{equation*}
Since $g$ is smooth, we have
\begin{equation*}
  \frac{\partial ^2 g }{\partial s \partial t} = \frac{\partial ^2 g }{\partial t \partial s}.
\end{equation*}
Equating the two expressions obtained by expanding out the LHS and RHS and making use of the definition of the Lie bracket give what is known as the Maurer--Cartan equation.

For the remaining discussion, we either assume that $G$ is a matrix Lie group (so that everything is a matrix for us to multiply willy-nilly) or use the trick of \S\ref{sec:pretend-lie-groups-are-matrix-groups}.  Then
\begin{equation*}
  \frac{\partial ^2 g }{\partial s \partial t} = \frac{\partial (g \xi) }{\partial s} = g \eta \xi + g \frac{\partial \xi }{ \partial s},
\end{equation*}
while
\begin{equation*}
  \frac{\partial ^2 g }{\partial t \partial s} = \frac{\partial (g \eta) }{\partial t} = g \xi \eta + g \frac{\partial \eta }{ \partial t}.
\end{equation*}
Equating the two and using the key relation
\begin{equation*}
  g \eta \xi - g \xi \eta = g [\eta,\xi]
\end{equation*}
now gives
\begin{equation}\label{eq:cartan-maurer}
  \frac{\partial \eta }{\partial t} - \frac{\partial \xi }{\partial s}
  = [\eta,\xi],
\end{equation}
which we may expand a bit more verbosely as
\begin{equation}\label{eq:cartan-maurer-expanded}
  \frac{\partial}{\partial t}
  (g^{-1} \frac{\partial g}{\partial s})
  - \frac{\partial}{\partial s}
  (g^{-1} \frac{\partial g}{\partial t})
  =
  [g^{-1} \frac{\partial g}{\partial s},
  g^{-1} \frac{\partial g}{\partial t}].
\end{equation}

One can rewrite everything we've said here in the language of differential geometry in terms of derivatives of a certain natural $\mathfrak{g}$-valued $1$-form on $G$; see Google for details.


% TODO: remove this, or make it make sense.  To make sense of the above calculation for general $G$, it seems best to introduce the $\mathfrak{g}$-valued $1$-form $\theta$ on $G$ characterized by
% \begin{enumerate}
% \item $\theta$ is left-invariant, and
% \item $\theta_e : \mathfrak{g} \rightarrow \mathfrak{g}$ is the identity map.
% \end{enumerate}
% For $X,Y \in \mathfrak{g}$ with corresponding left-invariant vector fields $X_*, Y_*$ on $G$, the functions $\langle \theta, X_* \rangle$ and $\langle \theta, Y_* \rangle$ are left-invariant, hence constant, so we have $X_* \langle \theta, Y_* \rangle = Y_* \langle \theta, X_* \rangle = 0$; it follows that
% \begin{equation*}
%   \langle d \theta, X_* \otimes Y_* \rangle = - \langle \theta, [X_*, Y_*] \rangle = - \langle \theta, [X, Y]_* \rangle =: - \langle \frac{1}{2} [\theta,\theta], X_* \otimes Y_* \rangle
% \end{equation*}
% where in general
% \begin{equation*}
%   [\theta_1,\theta_2](X \otimes Y) := [\theta_1(X),\theta_2(Y)] - [\theta_1(Y),\theta_2(X)],
% \end{equation*}
% so that
% \begin{equation*}
%   \frac{1}{2} [\theta,\theta](X \otimes Y) := [\theta(X),\theta(Y)].
% \end{equation*}
% Thus we have
% \begin{equation*}
%   d \theta + \frac{1}{2} [\theta,\theta] = 0
% \end{equation*}
% when tested against left-invariant vector fields, hence in general.

% The pullback $g^* \theta$ is then a $\mathfrak{g}$-valued $2$-form on $\dom(g) \subseteq \mathbf{k}^2$, so we can pair it against the vector fields $\partial_s, \partial_t$ on $\mathbf{k}^2$.  We find that
% \begin{equation*}
%   \langle g^* \theta, \partial_t \rangle = \langle \theta, \frac{\partial g}{\partial t} \rangle(g) = g^{-1} \frac{\partial g}{\partial t}
% \end{equation*}
% as functions on $\dom(g) \subseteq \mathbf{k}^2$, and also that
% \begin{equation*}
%   \eta = \langle g^* \theta, \partial_s \rangle
% \end{equation*}
% and
% \begin{equation*}
%   \xi = \langle g^* \theta, \partial_t \rangle,
% \end{equation*}
% hence
% \begin{equation*}
%   \langle d(g^* \theta), \partial_t \otimes \partial_s \rangle = \partial_t \langle g^* \theta, \partial_s \rangle - \partial_s \langle g^* \theta, \partial_t \rangle = \frac{\partial \eta }{ \partial t} - \frac{\partial \xi }{ \partial s}.
% \end{equation*}
% On the other hand, $d(g^* \theta) = g^* d \theta = - \frac{1}{2} g^* [\theta,\theta]$, so the above equals
% \begin{equation*}
%   - \langle g^* \frac{[\theta,\theta]}{2}, \partial_t \otimes \partial_s \rangle = - [ \langle \theta, \frac{\partial g}{\partial t} \rangle(g), \langle \theta, \frac{\partial g}{\partial s} \rangle(g) ] = [g^{-1} \frac{\partial g}{\partial s}, g^{-1} \frac{\partial g}{\partial t}],
% \end{equation*}
% as required.

\subsection{Lifting morphisms of Lie algebras}
\label{sec:org6f964c7}
\begin{theorem}
  Let $G,H$ be Lie groups.  Consider the natural map $j : \Hom(G,H) \rightarrow \Hom(\mathfrak{g},\mathfrak{h})$ given by $f \mapsto d f$.
  \begin{enumerate}
  \item If $G$ is connected, then $j$ is injective.
  \item If $G$ is simply-connected, then $j$ is surjective.
  \end{enumerate}
\end{theorem}
We have already shown the first part.  It remains to show when $G$ is simply-connected that for each morphism $\phi : \mathfrak{g} \rightarrow \mathfrak{h}$ of Lie algebras that there is a morphism $f : G \rightarrow H$ of Lie groups so that $d f = \phi$.

To construct $f$, start with an element $x \in G$.  Since $G$ is connected, we can find a smooth curve $\gamma$ in $G$ with $\gamma(0) = e, \gamma(1) = x$.  This curve will satisfy a differential equation
\begin{equation*}
  \frac{\partial \gamma }{\partial t} = \gamma \xi
\end{equation*}
for some $\xi : \mathbb{R} \xdashrightarrow{} \mathfrak{g}$.  We take $\phi(\xi) := \phi \circ \xi : \mathbb{R} \xdashrightarrow{} \mathfrak{h}$ and consider the curve $\delta : \mathbb{R} \xdashrightarrow{} H$ satisfying the initial condition $\delta(0) = e$ and the differential equation
\begin{equation*}
  \frac{\partial \delta }{\partial t} = \delta \phi(\xi);
\end{equation*}
this can always be solved, by an argument similar to that used to construct the exponential map.  We now attempt to define
\begin{equation*}
  f(x) := \delta(1).
\end{equation*}
The issue is that this definition is not obviously independent of the choice of path $\gamma$.

However, since $G$ is \emph{simply-connected}, we can join any two such paths $\gamma_0, \gamma_1$ by a smooth homotopy $g : \mathbb{R}^2 \xdashrightarrow{} G$ satisfying
\begin{equation*}
  g(t,0) = \gamma_0(t), \quad g(t,1) = \gamma_1(t), \quad g(0,s) = e, \quad g(1,s) = x.
\end{equation*}
It will again satisfy a differential equation
\begin{equation*}
  \frac{\partial g}{\partial t} = g \xi
\end{equation*}
now for some $\xi : \mathbb{R}^2 \xdashrightarrow{} \mathfrak{g}$.  We can take the composition $\phi(\xi) : \mathbb{R}^2 \xdashrightarrow{} \mathfrak{h}$ and solve for a function $h : \mathbb{R}^2 \xdashrightarrow{} H$ satisfying the initial condition $h(0,s) = e$ and the differential equation
\begin{equation}\label{eq:velocity-h-path}
  \frac{\partial h}{\partial t}
  = h \phi(\xi).
\end{equation}
Then $h(t,0) = \delta_0(t)$ and $h(t,1) = \delta_1(t)$ where $\delta_0,\delta_1$ are attached to $\gamma_0,\gamma_1$ as above.  Our aim is to show that $\delta_0(1) = \delta_1(1)$.  To that end, it will suffice to show that
\begin{equation}\label{eq:initial-cond-h-at-1}
  \text{$h(1,s)$ is independent of $s$.}
\end{equation}
We can express what we are given and what we want to show more succinctly in terms of the deformation velocities $\eta : \mathbb{R}^2 \xdashrightarrow{} \mathfrak{g}$ of $g$ and $\zeta : \mathbb{R}^2 \xdashrightarrow{} \mathfrak{h}$ characterized by
\begin{equation}\label{eq:velocity-g-deformation}
  \frac{\partial g}{\partial s}
  = g \eta
\end{equation}
and
\begin{equation}\label{eq:velocity-h-deformation}
  \frac{\partial h}{\partial s}
  = g \zeta.
\end{equation}
With this notation, what we are given is that
\begin{equation}\label{eq:given-eta-at-s-equals-1}
  \eta(1,s) = 0
\end{equation}
and what we want to show is that
\begin{equation}\label{eq:given-zeta-at-s-equals-1}
  \zeta(1,s) = 0.
\end{equation}
The required implication will follow in a slightly stronger form if we can show that
\begin{equation}\label{eq:deformation-velocity-preserved}
  \zeta = \phi(\eta).
\end{equation}
We are given the compatible inital conditions
\begin{equation}\label{eq:deformation-velocity-preserved-initial-conditions}
  \zeta(0,s) = 0 = \phi(\eta)(0,s).
\end{equation}
The Maurer--Cartan equation now gives
\begin{equation}\label{eq:claimed-double-partial-identity}
  \frac{\partial \eta }{ \partial t } - \frac{\partial \xi
  }{ \partial s}
  = [\eta,\xi].
\end{equation}
and
\begin{equation*}
  \frac{\partial \zeta }{ \partial t } - \frac{\partial \phi(\xi) }{ \partial s} = [\zeta,\phi(\xi)].
\end{equation*}
By applying $\phi$ to \eqref{eq:claimed-double-partial-identity} and using that $\phi$ preserves brackets, we obtain
\begin{equation*}
  \frac{\partial \phi(\eta) }{ \partial t } - \frac{\partial \phi(\xi) }{ \partial s} = [\phi(\eta),\phi(\xi)].
\end{equation*}
Thus for each $s$, the functions $t \mapsto \zeta(t,s)$ and $t \mapsto \phi(\eta)(t,s)$ satisfy the same initial conditions at $t = 0$ and the same differential equation.  The required identity \eqref{eq:deformation-velocity-preserved} follows finally from the basic uniqueness theorem for ODEs.

We've shown that the map $f : G \rightarrow H$ is well-defined, i.e., independent of the choice of path.
\begin{exercise}
  Show that $f$ is a group homomorphism, i.e., $f(g_1 g_2) = f(g_1) f(g_2)$.  [Hint: use the uniqueness of paths involved in the definition of $f$.]
\end{exercise}
It's also not hard to verify that $f$ is smooth.
\section{The universal covering group}
\label{sec:org1f9aec4}
We record the basic definitions and results from class (in the order essentially opposite to that in which they were presented)

\begin{definition}
  Let $p_1 : X_1 \rightarrow Y$ and $p_2 : X_2 \rightarrow Y$ be a pair of maps.  A map $f : X_1 \rightarrow X_2$ will be said to \emph{commute with $p_1$ and $p_2$} if the only conceivable condition relating the three maps is satisfied: $p_1 = p_2 \circ f$.
\end{definition}

\begin{definition}
  Let $Z$ be a manifold and let $Y$ be a connected manifold.  The \emph{trivial fiber bundle over $Y$ with fiber $Z$} is the map $\pr_1 : Y \times Z \rightarrow Y$ given by taking the first coordinate.  More generally, a \emph{trivial fiber bundle over $Y$ with fiber $Z$} is a smooth map of manifolds $p : X \rightarrow Y$ such that there exists a diffeomorphism $\iota : X \rightarrow Y \times Z$ commuting with $p$ and $\pr_1$.
\end{definition}

\begin{definition}
  Let $Z$ be a manifold and let $Y$ be a connected manifold.  A \emph{locally trivial fiber bundle over $Y$ with fiber $Z$} is a map $p : X \rightarrow Y$, where $X$ is a manifold and $p$ is a smooth map with the property that each element of $Y$ is contained in an open neighborhood $U$ for which the induced map $p : p^{-1}(U) \rightarrow U$ is a trivial fiber bundle over $U$ with fiber $Z$.
\end{definition}

\begin{definition}\label{defn:cover}
  Let $X,Y$ be connected manifolds.  A \emph{cover} $p : X \rightarrow Y$ is a locally trivial fiber bundle with discrete fiber.  This means more concretely that every element of $Y$ has an open neighborhood $U$ so that $p^{-1}(U)$ is a disjoint union of open subsets $V_\alpha$ with the property that $p : V_\alpha \rightarrow U$ is a diffeomorphism.  (A picture involving a ``stack of pancakes'' is appropriate here.)
\end{definition}
\begin{remark}
 One can speak about covers of much more general topological spaces; we won't ened such notions here. 
\end{remark}

\begin{example}
  The natural map $p : \mathbb{R} \rightarrow \mathbb{R}/\mathbb{Z} \cong U(1) \cong S^1$ is a non-trivial locally trivial fiber bundle over the circle $S^1$ with fiber $\mathbb{Z}$.
\end{example}

\begin{definition}
  A morphism $f : G \rightarrow H$ between connected Lie groups is a \emph{covering morphism} if it is a morphism of Lie groups that is also a cover in the above sense.
\end{definition}

\begin{exercise}\label{exercise:covering-morphisms}
  The following are equivalent for a morphism $f : G \rightarrow H$ between connected Lie groups:
  \begin{enumerate}
  \item $f$ is a cover in the sense of Definition \ref{defn:cover}, i.e., a locally trivial fiber bundle.
  \item $\ker(f)$ is discrete and $f$ is onto.
  \item $f$ is a local homeomorphism.
  \item $d f : \mathfrak{g} \rightarrow \mathfrak{h}$ is an isomorphism.
  \end{enumerate}
  (In all cases, $f$ is surjective.)
\end{exercise}

\begin{lemma}\label{lemma:discrete-norma-implies-central}
  If $N$ is a discrete normal subgroup of a connected Lie group $G$, then $N$ is contained in the center of $G$.

  In particular, the kernel of any covering morphism is a discrete subgroup of the center
\end{lemma}
\begin{proof}
  Let $n \in N$.  To show that $n$ belongs to the center of $G$, we must verify that the set $S := \{g n g^{-1} : g \in G \} \subseteq N$ satisfies $S = \{n\}$.  Indeed, $S$ is a subset of the \emph{discrete} topological space $N$ that contains $n$, but $S$ is the continuous image under the map $g \mapsto g n g^{-1}$ of the \emph{connected} topological space $G$, so $S$ is connected and thus $S = \{n\}$.
\end{proof}

The following key result classifies connected Lie groups as quotients of simply-connected Lie groups by discrete central subgroups:
\begin{theorem}
  Let $G$ be a connected Lie group.  Then there exists a simply-connected (connected) Lie group $\tilde{G}$ and a covering morphism $p : \tilde{G} \rightarrow G$.  The kernel $N$ of $p$ is a discrete subgroup of the center of $G$.  One has $\pi_1(G) \cong N$.  The pair $(\tilde{G},N)$ is unique up to isomorphism.
\end{theorem}
By combining with the result from last lecture, together with the (not proved yet and not obvious) fact that every finite-dimensional Lie algebra arises as the Lie algbebra of some Lie group, we obtain:
\begin{corollary}
  The category of simply-connected Lie groups is equivalent to the category of finite-dimensional Lie algebras, that is to say:
  \begin{itemize}
  \item Every simply-connected Lie group has a finite-dimensional Lie algebra, and every finite-dimensional Lie algebra arises (up to isomorphism) in this way.
  \item If $G,H$ are simply-connected Lie groups with Lie algebras $\mathfrak{g},\mathfrak{h}$, then the map $f \mapsto d f$ induces a bijection $\Hom(G,H) = \Hom(\mathfrak{g},\mathfrak{h})$.  In particular, $G \cong H$ if and only if $\mathfrak{g} \cong \mathfrak{h}$.
  \end{itemize}
\end{corollary}
In lecture we gave the following examples:
\begin{enumerate}
\item $G = \mathbb{R}/\mathbb{Z} \cong \U(1) \cong S^1$, $\tilde{G} = \mathbb{R}$, $N = \mathbb{Z}$
\item $G = \SO(3), \tilde{G} = \SU(2), N = \{\pm 1\}$.
\item $G = \mathbb{C}^\times, \tilde{G} = \mathbb{C}, p = \exp : \tilde{G} \rightarrow G$, $N = 2 \pi i \mathbb{Z}$
\end{enumerate}
We gave a few other similar examples.  See the homework for further examples.

The proof of the theorem relies on the construction of the universal cover $p : \tilde{X} \rightarrow X$ of a connected manifold $X$, which is a cover in the sense of Definition \ref{defn:cover} with the property: for any cover $q : \tilde{Y} \rightarrow Y$ and any smooth map $f : X \rightarrow Y$ and any $x \in X, y \in Y, \tilde{x} \in \tilde{X}, \tilde{y} \in \tilde{Y}$ satisfying the compatibility conditions $x = p(\tilde{x})$ and $y = f(x) = q(\tilde{y})$ there exists a unique smooth map $\tilde{f} : \tilde{X} \rightarrow \tilde{Y}$ such that
\begin{enumerate}
\item $\tilde{f}$ lifts $f$ in the sense that $q \circ \tilde{f} = p \circ f$, and
\item $\tilde{f}(\tilde{x}) = \tilde{y}$.
\end{enumerate}
The construction of $p : \tilde{X} \rightarrow X$ (which is not terribly important for our purposes) is as follows: One fixes a basepoint $x_0 \in X$ and defines $\tilde{X}$ as a set to be the set of all pairs $(x,[\gamma])$, where $x \in X$ and $[\gamma]$ is a homotopy class of smooth paths $\gamma : [0,1] \rightarrow X$ with $\gamma(0) = x_0$ and $\gamma(1) = x$.  The map $p : \tilde{X} \rightarrow X$ is given by $p((x,[\gamma])) := x$.  We must now define a smooth structure on $\tilde{X}$ and verify that it is indeed a locally trivial fiber with discrete fiber and that the universal property is satisfied.  We will content ourselves here to define the smooth structure; the verifications are then routine.  Given a point $x_1 \in X$, there is a small open neighborhood $U$ of $x_1$ that is diffeomorphic to an open Euclidean ball $B$.  For each $x \in U$, each homotopy class $[\gamma]$ of paths $\gamma$ from $x_0$ to $x$ may be factored uniquely as $[\delta_x \circ \gamma_1]$, where $\delta_x$ is given by the straight line from $x$ to $x_1$ (under the identification $U \cong B$) and $[\gamma_1]$ traverses the set $A$ of homotopy classes of paths $\gamma_1$ from $x_0$ to $x_1$.  We obtain in this way an identification of sets $p^{-1}(U) = \bigsqcup_{\alpha \in A} V_\alpha$, where each $V_\alpha$ identifies naturally with $B$.  We use this identification to define the smooth structure on $p^{-1}(U)$.

Given a connected Lie group $G$, one obtains first a cover of manifolds $p : \tilde{G} \rightarrow G$.  To define the group structure on $\tilde{G}$, one first selects an arbitrary preimage $\tilde{e} \in \tilde{G}$ of the identity element $e \in G$.  Using the universal property, one obtains unique lifts $\tilde{m} : \tilde{G} \times \tilde{G} \rightarrow \tilde{G}$ and $\tilde{i} : \tilde{G} \rightarrow \tilde{G}$ of the multiplication and inversion maps $m : G \times G \rightarrow G$ and $i : G \rightarrow G$ satisfying $\tilde{m}(\tilde{e},\tilde{e}) = \tilde{e}$ and $\tilde{i}(\tilde{e}) = \tilde{e}$.  One can now check that the usual axioms (associativity, smoothness, etc.)  are satisfied, and that $p : \tilde{G} \rightarrow G$ is a covering morphism.  The kernel $N$ of $p$ is obviously discrete, so by Lemma \ref{lemma:discrete-norma-implies-central}, it is central.

One can also check, using the universal property, that $\pi_1(G) \cong N$: The isomorphism $j : \pi_1(G) \rightarrow N$ is obtained by defined by taking a homotopy class $[\gamma]$ of loops $\gamma$ in $G$ with basepoint $e$, using the universal property to lift them uniquely to paths $\tilde{\gamma} : \tilde{G} \rightarrow \tilde{G}$ with $\tilde{\gamma}(0) = \tilde{e}$, and setting $j([\gamma]) := \tilde{\gamma}(1)$.  This is well-defined.  Conversely, given $n \in N$, we can take a path $\tilde{\gamma}$ in $\tilde{G}$ from $\tilde{e}$ to $n \in N$ and project it down via $p$ to obtain a loop $\gamma$ in $G$ based at the identity element $e \in G$.  The maps $\pi_1(G) \rightarrow N$ and $N \rightarrow \pi_1(G)$ constructed in this way are mutually inverse group homomorphisms.

For more details, I recommend consulting the first reference listed on the course homepage.

\section{Quotients of Lie groups}
\label{sec:org1288635}
\begin{theorem}
  Let $G$ be a Lie group and $H$ a Lie subgroup.  There is a unique smooth structure on the set $G/H$ so that the natural map $G \rightarrow G/H$ is a submersion, or equivalently, a quotient map in the smooth category.  If moreover $H$ is a normal subgroup, then $G/H$ is naturally a Lie group and $G \rightarrow G/H$ is a surjective morphism of Lie groups.  Moreover, the map $G \rightarrow G/H$ is a locally trivial fiber bundle with fiber $H$ (and also what is known as a ``principal $H$-bundle'').  Moreover, if the Lie group $G$ acts transitively on a smooth manifold $X$ and if $H$ is the stabilizer in $G$ of some point $x \in X$, then the natural map $G/H \rightarrow X$ given by $g H \mapsto gx$ is a diffeomorphism.
\end{theorem}
The final assertion that $G/H \cong X$ is related to Exercise \ref{exercise-topological-groups-quotient-map-homeomorphism}; its proof in the present setting requires the second-countability assumption on our manifolds.  We sketched the construction of the smooth structure on $G/H$ in some detail in lecture, leaving the verification of the properties as an exercise; see the first reference on the course webpage for more details.

The argument here is very similar (basically ``dual to'') that concerning submanifolds given by Theorem \ref{thm:characterize-submanifold-smooth-structure}.  In this analogy, ``immersion'' is to ``submanifold'' as ``submersion'' is to ``quotient manifold.''

By an argument dual to that of Corollary \eqref{prop:smoothness-preserved-codomain-pass-to-submfld}, we know that a smooth surjection $p : X \rightarrow Y$ is a submersion if and only if it is a \emph{quotient map} in the category of smooth manifolds, that is to say, if and only if smooth maps of manifolds $Y \rightarrow Z$ are in natural bijection with smooth maps $X \rightarrow Z$ that factor set-theoretically through $p$.  (This is easy to see in local coordinates, using the local description of submersions as surjective linear maps.)  The proof of uniqueness of smooth structure on $G/H$ is then dual to that given in the proof of Theorem \ref{thm:characterize-submanifold-smooth-structure}.

For existence, one first takes a small enough submanifold $S$ of $G$ that is transversal to $H$ at the identity element (draw a picture).  By the inverse function theorem, the multiplication map $\mu : S \times H \rightarrow G$ is then a local diffeomorphism at the identity, and is in particular injective in a neighborhood of the identity.  (In a bit more detail: the transversality assumption says that that the derivative $T_{(e,e)} \mu : T_e S \times \mathfrak{h} \rightarrow \mathfrak{g}$ is an isomorphism, hence by the inverse function theorem, it is a diffeomorphism in a neighborhood of the origin.)

It follows that if $S$ is small enough, then $\mu$ is actually a diffeomorphism onto its image: for else we may (as explained in more detail in lecture) find arbitrarily small distinct $s_1,s_2 \in S$ for which $s_1^{-1} s_2 \in H$, contrary to the local injectivity of $\mu$.  In particular, after shrinking $S$ as necessary, the map of sets $\pi : S \rightarrow G/H$ is injective.  Denote by $U \subseteq G/H$ the image of $S$ and by $g U$ the image of the translate $g S$ by an element $g \in G$.  We equip $g U$ with the smooth structure transferred from $g S$.  Then $\pi$ is a submersion over $g U$.  The sets $g U$ cover $G/H$, and the smooth structures on their overlaps are compatible thanks to the uniqueness established before (compare with the proof of Theorem \ref{thm:characterize-submanifold-smooth-structure}).  One can check using the definition of quotient maps that the natural map $G \times G/H \rightarrow G/H$ is smooth, and that if $H$ is normal, then the induced multiplication map on $G/H$ is smooth, so $G/H$ is naturally a Lie group.

\section{Homotopy exact sequence}
\label{sec:org245e5dc}
If $G,H$ are Lie groups with $G$ connected, one has an exact sequence
\begin{equation*}
  \pi_2(G/H) \rightarrow \pi_1(H) \rightarrow \pi_1(G) \rightarrow \pi_1(G/H) \xrightarrow{\delta} H/H^0 \rightarrow 0
\end{equation*}
where $\delta$ sends the homotopy class $[\gamma]$ of a loop $\gamma : [0,1] \rightarrow H$ based at the identity element to $\delta([\gamma]) := \tilde{\gamma}(1)^{-1}$, where $\tilde{\gamma} : [0,1] \rightarrow G$ is the unique lift of $\gamma$ to a path in $G$ satisfying $\tilde{\gamma}(0) = e$.

\begin{corollary}
  If $\pi_1(G/H) = \pi_2(G/H) = 0$, then $\pi_1(G) \cong \pi_1(H)$.
\end{corollary}

\begin{corollary}
  If $\pi_1(G) =0$, then $\pi_1(G/H) \cong H/H^0$.
\end{corollary}

We explained how this may be used to compute inductively the fundamental groups of the classical groups; see the first reference on the course webpage for more details.

\section{Cartan decomposition\label{sec:cartan-decmop}}
\label{sec:org167c08b}
Let $G$ be a real Lie subgroup of $\GL_N(\mathbb{C})$ with the property that $\Theta(G) = G$, where $\Theta : \GL_N(\mathbb{C}) \rightarrow \GL_N(\mathbb{C})$ is the involution given by inverse conjugate transpose:
\begin{equation*}
  \Theta(g) := {}^t \overline{g}^{-1}.
\end{equation*}
Then the set
\begin{equation*}
  K := \{g \in G : \Theta(g) = g \} = \U(N) \cap G
\end{equation*}
of elements in $G$ fixed by $\Theta$, or equivalently, belonging to the unitary rgoup, may be shown to be a real Lie subgroup with Lie algebra
\begin{equation*}
  \mathfrak{k} = \{X \in \mathfrak{g} : \theta(X) = X\}
\end{equation*}
where $\theta := d \Theta$ is given by
\begin{equation*}
  \theta(X) := - \overline{X}^t.
\end{equation*}
Set
\begin{equation*}
  \mathfrak{p} := \{X \in \mathfrak{g} : \theta(X) = - X\}.
\end{equation*}
Then $\mathfrak{g} = \mathfrak{k} \oplus \mathfrak{p}$.  In words, $\mathfrak{k}$ consists of the skew-hermitian elements of $\mathfrak{g}$, while $\mathfrak{p}$ consists of the hermitian elements of $\mathfrak{g}$; the assumption $\Theta(G) = G$ implies also that $\theta(\mathfrak{g}) = \mathfrak{g}$, hence (easily) that such a decomposition exists.
\begin{example}\label{example:cartan-decmop-complex-matrices}
  If $G = \GL_n(\mathbb{C})$, then $K = \U(n)$ and $\mathfrak{p}$ consists of all hermitian matrices and $\exp(\mathfrak{p})$ consists of all positive-definite hermitian matrices.

  In particular, if $n =1$, then $G = \mathbb{C}^\times, K = U(1)$, and $\mathfrak{p}$ consists of all $1 \times 1$ real matrices.
\end{example}
\begin{example}\label{example:cartan-decmop-real-matrices}
  If $G = \GL_n(\mathbb{R})$, then $K = \O(n)$ and $\mathfrak{p}$ consists of all symmetric matrices and $\exp(\mathfrak{p})$ consists of all positive-definite symmetric matrices.
\end{example}
\begin{example}
  One of the homework problems this week is to verify that if $G = \O(p,q)$, then $K = \O(p) \times \O(q)$.
\end{example}

\begin{definition}
  We say that $G$ is (real) \emph{algebraic} if it may be defined inside $\GL_N(\mathbb{C})$ by a system of polynomial equations in the real and imaginary parts of group elements and their inverses.  (Every example we've seen has this property.)
\end{definition}

\begin{theorem}[Cartan decomposition]\label{thm:cartan-decomp}
  Let $G,K$ be as above.  Assume that $G$ is algebraic.  Then the natural map
  \begin{equation*}
    K \times \mathfrak{p} \rightarrow G
  \end{equation*}
  \begin{equation*}
    (k,Y) \mapsto k \exp(Y)
  \end{equation*}
  is a diffeomorphism.
\end{theorem}

\begin{remark}
  If $G$ has finitely many connected components, then the assumption that $G$ is algebraic turns out to hold automatically in this setup (TODO: double-check this), but we probably won't have time to prove this.  In any event, it's true in all of the examples we've seen.
\end{remark}

\begin{example}\label{example:polar-decomp}
  In the context of Example \ref{example:cartan-decmop-complex-matrices}, this amounts to the ``polar decomposition'' of an invertible complex matrix $g$ as a product of a unitary matrix $k$ and a positive-definite hermitian matrix $\exp(Y)$.  In the special case $n = 1$, this is just the polar decomposition of a nonzero complex numbers as $e^{i \theta} r$ (writing $r = e^y, y \in \mathbb{R}$).
\end{example}

\begin{corollary}
  Let $G,K$ be as above.  Then $K$ is a deformation retract of $G$.  In particular, $\pi_i(G) \cong \pi_i(K)$ for all $i \geq 0$.
\end{corollary}
(One of the homework problems involves applying this last corollary in the special case $i=0$ to relate the connected components of $G$ and $K$.)

\begin{remark}
  \label{rmk:}
  This corollary ``explains'' some of the ``coincidences'' such as
  \begin{equation*}
    \pi_1(\SL_2(\mathbb{R})) \cong \mathbb{Z} \cong \pi_1(\SO(2)),
  \end{equation*}
  \begin{equation*}
    \pi_1(\SL_n(\mathbb{R})) \cong \mathbb{Z}/2 \cong \pi_1(\SO(n)) \text{ for } n \geq 3,
  \end{equation*}
  \begin{equation*}
    \pi_1(\SU(n)) \cong \{0\} \cong \pi_1(\SL_n(\mathbb{C})),
  \end{equation*}
  \begin{equation*}
    \pi_1(\U(n)) \cong \mathbb{Z} \cong \pi_1(\GL_n(\mathbb{C})),
  \end{equation*}
  \begin{equation*}
    \pi_1(\SO_n(\mathbb{C})) \cong \pi_1(\SO(n))
  \end{equation*}
  that we observed empirically at the beginning of the lecture.
\end{remark}

\begin{remark}
  Let $G$ be a connected complex Lie group, and suppose it has a compact real form $K$, so that $\mathfrak{g} \cong \mathfrak{k} \oplus i \mathfrak{k}$.  The point of this remark is to indicate briefly (as is evident in all examples) why the Cartan decomposition should always apply to $G$ and $K$.  It turns out (as we'll show later in the course) that we may always realize $K$ as a subgroup of $\U(N)$ for some $N$.  We can then realize $G$ as a subgroup of $\GL_N(\mathbb{C})$ in such a way that $\mathfrak{g} = \mathfrak{k} \oplus i \mathfrak{k}$; given what we've seen in the course, we can verify this already in the special case $G$ is simply-connected (by lifting the inclusion map $\mathfrak{g} \hookrightarrow \mathfrak{g} \mathfrak{l}_N(\mathbb{C})$), and by the end of the course we should also be able to reduce the general case to that special one.  Then $K = G \cap \U(N)$.
\end{remark}



We briefly indicate the proof of Theorem \ref{thm:cartan-decomp}.  One basically takes the proof of the special case (see Example \ref{example:polar-decomp}) concerning polar decomposition on $\GL_N(\mathbb{C})$ (perhaps seen in a linear algebra course?)  and checks that it descends to $G$.  So, let's see.  There are a few things to check.
\begin{enumerate}
\item The map $K \times \mathfrak{p} \ni (k,Y) \mapsto k \exp(Y) \in G$ is bijective.
\item The map $Y \mapsto \exp(Y)$ is a diffeomorphism onto its image.
\end{enumerate}
Using the first two assertions and the inverse function theorem, one gets that the map $(k,Y) \mapsto k \exp(Y)$ is itself a diffeomorphism onto its image.  To verify the second assertion, we should first compute the differential of the exponential map (a result of independent interest).  The answer is that for any $X,Y \in \mathfrak{g}$,
\begin{equation}\label{eq:diff-exp}
  \exp(-X) \frac{d}{d t} \exp(X + t Y)|_{t=0}
  =
  \Psi(\ad_X) Y
\end{equation}
where
\begin{equation*}
  \Psi(z) := \sum_{n=1}^{\infty} \frac{(-z)^{n-1}}{n!}  = \frac{1 - \exp(-z)}{z}.
\end{equation*}
This is obtained by applying Homework \ref{hw:diff-exp} to the map $f : \mathbb{R} \rightarrow \mathfrak{g}$ given by $f(t) := X + t Y$; one then has $f(0) = X$ and $f'(0) = Y$, and so \eqref{eq:diff-exp} follows from Homework \ref{hw:diff-exp}.  It follows from the second description of $\Psi$ that $(d \exp)_X$ is injective provided that $\ad_X$ has no eigenvalues of the form $2 \pi i k$ with $k$ a nonzero integer.  Since hermitian matrices have \emph{real} eigenvalues (under the standard representation as well as the adjoint representation), it follows in particular that $\exp : \mathfrak{p} \rightarrow \GL_N(\mathbb{C})$ is everywhere regular.  Finally, we observe that every positive hermitian matrix $g$ may be written uniquely as the exponential of a symmetric matrix: any such matrix is diagonal with respect to some basis and has positive real entries on the diagonal, etc.  In summary, $\exp : \mathfrak{p} \rightarrow \GL_N(\mathbb{C})$ is a diffeomorphism onto its image, as required.

All that remains now is the bijectivity.  We verify first the injectivity, which will serve as useful motivation.  Suppose that $g = k \exp(Y)$.  Since $\Theta(k) = k$ and $\Theta(\exp(Y)) = \exp(\theta(Y)) = \exp(-Y) = \exp(Y)^{-1}$ and $\Theta$ is a homomorphism, it follows that $g^{-1} \Theta(g) = \exp(2 Y)$.  Since $\exp : \mathfrak{p} \rightarrow \exp(\mathfrak{p})$ is bijective, it follows that $Y$ is uniquely determined by $g$, hence so is $k = g \exp(-Y)$.

We turn finally to surjectivity.  Given $g \in G \leq \GL_N(\mathbb{C})$, we can verify directly that $\Theta(g)^{-1} \Theta(g)$ is a positive definite hermitian matrix, and so we can define $Y \in \gl_N(\mathbb{C})$ to be the unique hermitian matrix for which $\exp(2 Y) = \Theta(g)^{-1} \Theta(g)$.  It is then not hard to verify that $k := g \exp(-Y)$ is unitary, i.e., $\Theta(k) = k$.  This gives the required decomposition in the Lie group $\GL_N(\mathbb{C})$; the problem is to show that in fact $k \in K$ and $Y \in \mathfrak{p}$, or equivalently, that $k \in G$ and $Y \in \mathfrak{g}$.  Since $\exp(\mathfrak{g}) \subseteq G$ and because of the way $k$ was defined, it will suffice to verify that $Y \in \mathfrak{g}$.  What we know (from our assumptions $g \in G$ and $\Theta(G) = G$) is that $\exp(2 Y) \in G$.  Since $G$ is a group, we can raise the last assertion to any integer power $t \in \mathbb{Z}$ to see that $\exp(2 t Y) \in G$.  Since $Y$ is hermitian, we may choose a basis with respect to which it is diagonal and suppose that $Y = \diag(y_1,\dotsc,y_N)$ for some real numbers $y_1,\dotsc,y_N \in \mathbb{R}$.  Since $G$ was assumed to be algebraic (defined by polynomial equations), the condition $\exp(2 t Y) \in G$ is then a system of polynomial equations involving the positive real numbers $\exp(2 t y_1), \dotsc, \exp(2 t y_N)$.  Since this polynomial system is satisfied for all integers $t$, one can show (see below) that it is satisfied also for all real numbers $t$.  Thus $\exp(2 t Y) \in G$ for all $t \in \mathbb{R}$.  By differentiating this last fact we deduce as required that $Y \in \mathfrak{g}$.

For completeness, we record the algebraic fact that we used in the proof:
\begin{exercise}
  Let $a_1,\dotsc,a_n \in \mathbb{R}$ and let $F \in \mathbb{R}[x_1,\dotsc,x_n]$ be a polynomial satisfying
  \begin{equation}\label{eq:condition-on-F-vanishes-at-integers-exp-stuff}
    F(e^{a_1 t},\dotsc,e^{a_n t})
    = 0
  \end{equation}
  for all $t \in \mathbb{Z}$.  Show that \eqref{eq:condition-on-F-vanishes-at-integers-exp-stuff} holds also for all $t \in \mathbb{R}$.
\end{exercise}
(We apply this with $n := 2 N$ and $(a_1,\dotsc,a_n) := (y_1,\dotsc,y_N,-y_1,\dotsc,-y_N)$.)

\section{BCHD}
\label{sec:orgc67fbf4}
Let $G_1, G_2$ be Lie groups.
\begin{definition}
  We say that $G_1,G_2$ are \emph{locally isomorphic} if there are open neighborhoods $U_i \subseteq G_i$ of the identity elements and a diffeomorphism $f : U_1 \xrightarrow{\cong } U_2$ so that whenever $x,y,x y \in U_1$, one has $f(xy)=f(x)f(y)$.  In that case, $f$ is said to be (guess!) a \emph{local isomorphism}.
\end{definition}
\begin{example}\label{example:local-isom}
  ~
  \begin{enumerate}
  \item Any covering homomorphism $f : G_1 \rightarrow G_2$ induces a local isomorphism.
  \item If $G_1$ is the connected component of $G_2$, then the inclusion $G_1 \hookrightarrow G_2$ defines a local isomorphism.
  \item The relation of being locally isomorphic is obviously an equivalence relation, i.e., is reflexive and transitive.  In verifying this it is convenient to note that one can always shrink the subsets $U_1,U_2$ suitably.
  \end{enumerate}
\end{example}
\begin{theorem}\label{thm:local-isom-iff-lie-alg-isom}
  $G_1,G_2$ are locally isomorphic if and only if $\mathfrak{g}_1,\mathfrak{g}_2$ are isomorphic.
\end{theorem}
\begin{proof}
  The forward direction is easy: given $f : U_1 \rightarrow U_2$ with inverse $f^{-1} : U_2 \rightarrow U_1$ as in the definition of ``locally isomorphic,'' the differentials at the identity $d f : \mathfrak{g}_1 \rightarrow \mathfrak{g}_2$ and $d (f^{-1}) : \mathfrak{g}_2 \rightarrow \mathfrak{g}_1$ define morphisms of Lie algebras (by the same proof as in \S\ref{sec:diff-morphism-is-morphism}) which are mutually inverse.

  The converse direction is more subtle, but follows from what we have seen already.  Namely, let $G_1^0$ denote the connected component and $\widetilde{G_1^0}$ its simply-connected covering group.  Then $\mathfrak{g}_1 = \Lie(G_1) = \Lie(G_1^0) = \Lie(\widetilde{G_1^0})$.  Let $\phi : \mathfrak{g}_1 \rightarrow \mathfrak{g}_2$ be an an isomorphism.  Then $\phi$ is of the form $d f$ for some $f : \widetilde{G_1^0} \rightarrow G_2$.  Since $\phi$ is an isomorphism, we know by (for instance) Exercise \ref{exercise:covering-morphisms} that $f$ is a covering morphism.  By Example \ref{example:local-isom}, it follows that $G_1$ is locally isomorphic to $G_1^0$ which is in turn locally isomorphic to $\widetilde{G_1^0}$ which is finally (via $f$) locally isomorphic to $G_2$, as required.
\end{proof}

It is natural to ask for a more ``local'' proof of Theorem \ref{thm:local-isom-iff-lie-alg-isom} that does not require topological considerations or global constructions involving universal covers, etc.  The BCHD formula gives such a proof.  To motivate that, recall that $\exp : \mathfrak{g} \rightarrow G$ is a local diffeomorphism near the origin; for this reason, it makes sense to define for small enough $x,y \in \mathfrak{g}$ the quantity
\begin{equation*}
  x \ast y := \log(\exp(x) \exp(y)).
\end{equation*}
(We explained this in somewhat more detail in class.)  Thus $x \ast y$ is the group law on $G$ expressed in local coordinates defined via the exponential map.  One has identities like
\begin{equation*}
  x \ast (y \ast z) = (x \ast y) \ast z
\end{equation*}
whenever all involved quantities make sense (e.g., whenever $x,y,z$ are all small enough).  We also have
\begin{equation*}
  x \ast (-x) = 0,
\end{equation*}
etc.  If $G$ is abelian, then $x \ast y = x + y$.  In general, it is somewhat more complicated.  (See the homework problems this week for some examples where it isn't too complicated.)
\begin{exercise}\label{exercise:star-product-commutes-with-differentials-of-morphisms}
  If $\phi : \mathfrak{g} \rightarrow \mathfrak{h}$ is the differential of some morphism of Lie groups $G \rightarrow H$ and $x,y \in \mathfrak{g}$ are small enough, then
  \begin{equation*}
    \phi(x) \ast \phi(y) = \phi(x \ast y).
  \end{equation*}
\end{exercise}

Suppose now temporarily that $G = \GL_n(\mathbb{R})$, so that $\mathfrak{g} = M_n(\mathbb{R})$.  Using the series expansions $\log(z) = \sum_{m \geq 1} (-1)^{m-1} (z-1)^m/m$ and $\exp(x) = \sum_{p \geq 0} x^p/p!$, we obtain
\begin{equation*}
  \exp(x) \exp(y) - 1 = \sum _{\substack{
      p,q \geq 0: \\
      (p,q) \neq (0,0) } } \frac{x^p y^q}{p! q!}
\end{equation*}
and thus
\begin{equation}\label{eq:BCHD-naive}
  x \ast y
  = \sum_{m \geq 1}
  \frac{(-1)^{m-1}}{m}
  \sum _{\substack{
      p_1,q_1,\dotsc,p_m,q_m \geq 0  \\  
      (p_j,q_j) \neq (0,0)
    }
  }
  \frac{x^{p_1} y^{q_1} \dotsb x^{p_m} y^{q_m}}{
    p_1! q_1! \dotsb p_m! q_m!}.
\end{equation}
For $n \geq 1$, let $z_n$ denote the $n$th homogeneous component of the sum on the RHS, so that
\begin{equation}\label{eq:bchd-homogeneous-components}
  x \ast y = \sum_{n \geq 1} z_n.
\end{equation}
If we play around for a bit (as done in class), we find quickly that
\begin{equation*}
  z_1 = x + y,
\end{equation*}
\begin{equation*}
  z_2 = \frac{1}{2} [xy]
\end{equation*}
where $[x y] := [x,y]$.  The verification of this involved a ``miraculous'' coincidence of the shape
\begin{equation*}
  \left( \frac{x^2}{2} + x y + \frac{y^2}{2} \right) - \frac{1}{2} (x^2 + x y + y x + y^2) = \frac{x y - y x}{2}.
\end{equation*}
We indicated that with more work one can show that
\begin{equation*}
  z_3 = \frac{1}{12} ([x [x y]] + [y [y x]])
\end{equation*}
and
\begin{equation*}
  z_4 = \frac{1}{24} [y[x[yx]]].
\end{equation*}
\begin{theorem}[BCH ``formula'']\label{thm:BCH}
  Let $G$ be a Lie group with Lie algebra $\mathfrak{g}$.  Then for small enough $x,y \in \mathfrak{g}$, the identity \eqref{eq:bchd-homogeneous-components} holds for some degree $n$ Lie polynomial $z_n$ in $x,y$ (i.e., $z_n$ is a linear combination of iterated $n$-fold commutators as above).
\end{theorem}
There is a more explicit version of this:
\begin{theorem}[BCHD formula]\label{thm:BCHD}
  Let $G$ be a Lie group with Lie algebra $\mathfrak{g}$.  Then for small enough $x,y \in \mathfrak{g}$, the identity \eqref{eq:bchd-homogeneous-components} holds with
  \begin{equation}\label{eq:BCHD}
    z_n
    = \frac{1}{n}
    \sum_{m \geq 1}
    \frac{(-1)^{m-1}}{m}
    \sum _{\substack{
        p_1,q_1,\dotsc,p_m,q_m \geq 0  \\  
        (p_j,q_j) \neq (0,0) \\
        p_1+q_1+\dotsb+p_m+q_m=n
      }
    }
    \frac{[x^{p_1} y^{q_1} \dotsb x^{p_m} y^{q_m}]}{
      p_1! q_1! \dotsb p_m! q_m!}
  \end{equation}
  where, for instance,
  \begin{equation*}
 [x^3 y^2 x^4 y^1] := [x[x[x[y[y[x[x[x[x y]]]]]].
  \end{equation*}
\end{theorem}
The similarity between \eqref{eq:BCHD-naive} and \eqref{eq:BCHD} is no coincidence:
\begin{exercise}
  Convince yourself that the problem involving commutators on Homework \ref{hw:all-about-Ad} (known as something like \emph{Dynkin's lemma}) allows one to deduce Theorem \ref{thm:BCHD} from Theorem \ref{thm:BCH}.
\end{exercise}

To prove Theorem \ref{thm:BCH}, define
\begin{equation*}
  f(t) := x \ast t y
\end{equation*}
for small $t \in \mathbb{R}$.  Then $f$ is smooth, and
\begin{equation*}
  x \ast y = f(1) = f(0) + \int_{t=0}^1 f'(t) \, d t.
\end{equation*}
Since $\exp f(t) = \exp(x) \exp(t y)$ we have $\partial_t \exp f(t) = f(t) y$ and thus by Homework \ref{hw:diff-exp},
\begin{equation*}
  y = \exp(-f(t)) \partial_t \exp(f(t)) = \Psi(\ad_{f(t)}) f'(t),
\end{equation*}
where
\begin{equation*}
  \Psi(z) = \sum_{m=1}^{\infty} \frac{(-z)^{m-1}}{m!}  = \frac{1 - \exp(-z)}{z}.
\end{equation*}
We form the inverse power series
\begin{equation*}
  \Psi(z)^{-1} = \frac{z}{1 - \exp(-z)} = 1 + \frac{z}{2} + \dotsb.
\end{equation*}
We then have
\begin{equation*}
  f'(t) = \Psi(\ad_{f(t)})^{-1} y.
\end{equation*}
Using Exercise \ref{exercise:star-product-commutes-with-differentials-of-morphisms}, we have
\begin{equation*}
  \ad_{f(t)} = \ad_{x} \ast \ad_{t y} = \log(e^{\ad(x)} e^{t \ad(y)})),
\end{equation*}
hence
\begin{equation*}
  \Psi(\ad_{f(t)})^{-1} = \psi(e^{\ad(x)} e^{t \ad(y)})
\end{equation*}
where
\begin{equation*}
  \psi(w) := \Psi(\log(w))^{-1} = \frac{w \log w}{w - 1} = 1 + (w-1)/2 + \dotsb
\end{equation*}
(whose coefficients are what are called Bernoulli numbers).  In summary,
\begin{equation*}
  x \ast y = x + \int_{t=0}^1 \psi(e^{\ad(x)} e^{t \ad(y)}) y \, d t.
\end{equation*}
We can now expand the integrand out into a power series and integrate term-by-term; we then obviously get an analytic expression of the required form.  (In fact, it's not too hard to push this analysis a bit further to derive \eqref{eq:BCHD} directly, without using Dynkin's trick; just expand everything out.)
\begin{remark}
  The most ``conceptual'' perspective on the BCH theorem in its qualitative form may be found in Serre's book on Lie algebras and Lie groups in one of the final chapters on Lie algebras; see also around p72 of the book by Onischik--Vinberg--Gourbatsevich.
\end{remark}

Note that Theorem \ref{thm:local-isom-iff-lie-alg-isom} follows directly from the BCHD formula: if $x,y \in \mathfrak{g}_i$ are small enough then the product $\exp(x) \exp(y)$ is determined entirely by the Lie bracket on $\mathfrak{g}_i$, so an isomorphism $\mathfrak{g}_1 \cong \mathfrak{g}_2$ obviously lifts to a local isomorphism between neighborhoods of the identity elements in $G_1,G_2$.

There is a lot more to say about the BCHD formula; a taste is given on the homework set for this lecture.  For some problems it may help to note that for any fixed norm $|.|$ on $\mathfrak{g}$ and $x,y$ small enough, one has
\begin{equation*}
  x \ast y = O(|x|\,|y|).
\end{equation*}

Next lecture we should state some further consequences of BCHD.

\section{Some more ways to produce and detect Lie groups}
\label{sec:orgdccc169}
\subsection{Summary}
\label{sec:org1d479ae}
Recall that we have called a subgroup $H$ of a Lie group $G$ a \emph{Lie subgroup} if it is a submanifold, and that for this to hold, it suffices to verify that $H$ is locally a submanifold near the identity element of $G$.  Checking this condition over and over again eventually becomes tedious, so we ask for some more systematic ways to detect it.  Here are a few, to be developed in detail throughout this section:
\begin{enumerate}
\item It was observed in \S\ref{sec:lie-subgroups-are-closed} that Lie subgroups are automatically closed.  Much more interestingly and perhaps surpririnsgly, the converse is true over $k = \mathbb{R}$: any closed subgroup $H$ of a \emph{real} Lie group $G$ is automatically a Lie subgroup (Theorem \ref{thm:closed-implies-lie}).  This is very powerful, and implies most of the criteria discussed below.

  This criterion does not apply directly to complex Lie groups: for instance a real Lie subgroup of a complex Lie group is seldom a complex Lie subgroup (think $\mathbb{R} \hookrightarrow \mathbb{C}$ or $\GL_n(\mathbb{R}) \hookrightarrow \GL_n(\mathbb{C})$).  But it's not hard to verify (e.g., by inspecting the proof of what we are talking about) that if $H$ is a real Lie subgroup of a complex Lie group $G$ with the property that $\mathfrak{h}$ is a complex vector space of $\mathfrak{g}$, then $H$ is a complex Lie subgroup of $G$.  A fairly good rule of thumb is that if a subgroup $H$ of a complex Lie group $G$ has the properties
  \begin{enumerate}
  \item $H$ is closed, and
  \item the definition of $H$ does not make reference to the real numbers, complex conjugation or similar ``non-holomorphic'' notions,
  \end{enumerate}
  then $H$ is probably a complex Lie subgroup.

  Some of the methods used to prove the criteria to be given below are of independent interest, even in the real case, because they give convenient ways to compute Lie algebras in many common situations.
\item ``Stabilizers'' of any sort (of points in a manifold, of vectors in a representation, etc.)  are, in practice, obviously closed, hence are Lie subgroups by the previous item.  Moreover,their Lie algebras tend to be ``the obvious thing.''  Many subgroups can be somehow interpreted as stabilizers:
  \begin{enumerate}
  \item kernels of morphisms of Lie groups,
  \item stabilizers of subspaces in representations,
  \item intersections of Lie subgroups,
  \item etc.
  \end{enumerate}
\item An interesting result that does not follow from the above criteria is that in a \emph{simply-connected} Lie group $G$, the commutator subgroup $G' := [G,G]$ is a Lie subgroup.  This conclusion fails in general, although it remains true that the commutator subgroup is an \emph{immersed} Lie subgroup.
\item Given a Lie subgroup $H$ of a Lie group $G$, one can naturally construct the quotient manifold $G/H$; if $H$ is normal, then $G/H$ is also a Lie group.
\end{enumerate}

\subsection{Closed subgroups of real Lie groups}
\label{sec:orgd737096}
\subsubsection{Statement of the key result}
\label{sec:org018f0b5}
\begin{theorem}\label{thm:closed-implies-lie}
  Let $G$ be a real Lie group, and let $H \subseteq G$ be a subset.  The following are equivalent:
  \begin{enumerate}
  \item $H$ is a closed subgroup of $G$, that is to say,
    \begin{enumerate}
    \item $H$ is a closed subset of $G$, and
    \item $e \in H$, and $h^{-1}, h_1 h_2 \in H$ whenever $h,h_1,h_2 \in H$.
    \end{enumerate}
  \item $H$ is a Lie subgroup of $G$, that is to say,
    \begin{enumerate}
    \item $H$ is a submanifold of $G$, and
    \item $e \in H$, and $h^{-1}, h_1 h_2 \in H$ whenever $h, h_1,h_2 \in H$.
    \end{enumerate}
  \end{enumerate}
\end{theorem}

\subsubsection{A toy example}
\label{sec:org3cf56ab}
One can already illustrate the basic idea behind Theorem \ref{thm:closed-implies-lie} in the case $G = \mathbb{R}$, where it amounts to the following:
\begin{theorem}
  Let $H$ be a closed subgroup of the real line $\mathbb{R}$.  Then exactly one of the following possibilities occur:
  \begin{itemize}
  \item $H = \mathbb{R}$, or
  \item $H$ is discrete, or equivalently, one has $(-\eps,\eps) \cap H = \{0\}$ for some $\eps > 0$.
  \end{itemize}
\end{theorem}
\begin{proof}
  If the second possibility does not occur, then we can find a sequence $x_n \in H$ with $x_n \rightarrow 0$ and $x_n \neq 0$.  Let $x \in \mathbb{R}$ be given.  We can find a sequence of reals $c_n$ so that $c_n x_n \rightarrow x$; for instance, one can take $c_n = x/x_n$.  But since $x_n \rightarrow 0$, the conclusion $c_n x_n \rightarrow x$ is unaffected by rounding $c_n$ to the nearest integer.  We can thus find a sequence of \emph{integers} $c_n$ so that $c_n x_n \rightarrow x$.  Since $H$ is a subgroup, we then have $c_n x_n \in H$ for all $n$.  Since $H$ is closed, it follows that $x \in H$.  Since $x$ was arbitrary, we conclude as required that $H = \mathbb{R}$.
\end{proof}

\subsubsection{Proof of the key result}
\label{sec:org5574bc9}
\begin{proof}
[Proof of Theorem \ref{thm:closed-implies-lie}]
  Thanks to ??? and ???, all we need to show is that if $H$ is a closed subgroup of $G$, then $H$ is locally a submanifold of $G$ at the identity.  Let $\mathfrak{g}$ denote the Lie algebra of $\mathfrak{g}$.  Set
  \begin{equation*}
    \mathfrak{h} := \left\{ x \in \mathfrak{g} : \exists c_n \in \mathbb{R}, x_n \in \mathfrak{g} \cap \exp^{-1}(H) \text{ so that } x_n \rightarrow 0, c_n x_n \rightarrow x \right\}.
  \end{equation*}
  Two quick remarks before continuing with the proof:
  \begin{itemize}
  \item If we somehow knew already that $H$ were a Lie subgroup, then $\mathfrak{h}$ would of course be its Lie algebra, as any $x_n$ in the definition would belong to $\mathfrak{h}$ for $n$ large enough.
  \item A good ``enemy scenario'' to keep in mind is when $G = (\mathbb{R} /\mathbb{Z})^2$ and $H$ is the image of the map $x \mapsto (x, \alpha x)$, where $\alpha \in \mathbb{R} - \mathbb{Q}$.  Then $H$ is a subgroup (indeed, an immersed Lie subgroup), but fails to be closed.  The set $\mathfrak{h}$ defined as above is all of $\mathfrak{g}$, and so has nothing to do with the Lie algebra of $H$.  The argument to follow will need to rule out this scenario.
  \end{itemize}

  We show now that
  \begin{center}
    $\mathfrak{h}$ is a vector subspace of $\mathfrak{g}$ for which $\exp(\mathfrak{h}) \subseteq H$.
  \end{center}
  \begin{enumerate}
  \item It is clear that $\mathfrak{h}$ is stable under scalar multiplication.
  \item Let $x \in \mathfrak{h}$, so that $x = \lim c_n x_n$ for some $c_n, x_n$ as in the definition of $\mathfrak{h}$.  The condition $x_n \rightarrow 0$ implies that rounding $c_n$ to the nearest integer does not affect the condition $c_n x_n \rightarrow x$, so we may assume without loss of generality that $c_n \in \mathbb{Z}$.  Since $H$ is a closed subgroup, we then have $\exp (x) = \lim \exp(x_n)^{c_n} \in H$.
  \item Let $x,y \in \mathfrak{h}$.  For $n$ large enough, set $z_n := \log(\exp(x/n) \exp(y/n))$.  By the previous item, $\exp(z_n) = \exp(x/n) \exp(y/n) \in H$.  As we've seen earlier in the course (during the discussion of the exponential map; what we need here also follows easily from BCH), we have $z_n = x/n + y/n + O(1/n^2)$,
    % From Lemma ???, and the calculation $\frac{d }{d t}\exp(t x) \exp(t y)|_{t=0} = x + y$, the curve $\zeta(t) := \log(\exp(t x) \exp(t y))$ satisfies $n \zeta(1/n) \rightarrow x+y$ as $n \rightarrow \infty$.  From the previous two points, the quantities $z_n := \zeta(1/n)$
    hence $z_n \rightarrow 0$ and $n z_n \rightarrow x + y$.  Therefore $x + y \in \mathfrak{h}$.
  \end{enumerate}

  Let $\mathfrak{h} ' \leq \mathfrak{g}$ be any vector space complement to $\mathfrak{h}$, so that $\mathfrak{g} = \mathfrak{h} \oplus \mathfrak{h} '$.  The map $\mathfrak{g} = \mathfrak{h} \oplus \mathfrak{h} ' \ni (v,w) \mapsto \exp(v) \exp(w)$ has derivative $1$ at the origin, hence is a local diffeomorphism.  There is thus a small open neighborhood $U$ of the identity element in $G$ and a smooth chart
  \begin{equation*}
    (\alpha,\alpha ' ) : U \rightarrow \mathfrak{h} \oplus \mathfrak{h}'
  \end{equation*}
  characterized by the identity
  \begin{equation*}
g = \exp(\alpha(g)) \exp(\alpha'(g)) \text{ for all }g \in U.
\end{equation*}
 We claim that if $U$ is small enough, then
  \begin{equation*}
U \cap H = \{g \in U : \alpha'(g) = 0\}.
\end{equation*}
 This shows that $H$ is locally a submanifold of $G$ at the identity, as required.

  To prove the claim, note first that if $\alpha '(g) = 0$, then $g = \exp(\alpha(g)) \in \exp(\mathfrak{h}) \subseteq H$.  This establish one inclusion.

  Conversely, if the reverse inclusion fails for arbitrarily small $U$, then we can find a sequence $h_n \in U \cap H$ with $h_n \rightarrow 1$ so that $\alpha '(h_n) \neq 0$.  Set 
\begin{equation*}
x_n := \alpha '(h_n).
\end{equation*}
 Since $\exp(x_n)$ belongs to the group generated by $h \in H$ and $\exp(\alpha(h_n)) \in \exp(\mathfrak{h}) \subseteq H$, it belongs to $H$, and so
  \begin{equation*}
    x_n \in \mathfrak{h} ' \cap \exp^{-1}(H), \quad x_n \rightarrow 0, x_n \neq 0.
  \end{equation*}
  By passing to a subsequence, we have $|x_n|^{-1} x_n \rightarrow x$ for some nonzero $x \in \mathfrak{h} '$, but then also $x \in \mathfrak{h}$; since $\mathfrak{h} \cap \mathfrak{h} ' = 0$, we obtain the required contradiction.
\end{proof}

A simple corollary that already illustrates the basic idea is the following:
\begin{corollary}\label{cor:classification-vector-subspaces}
  Let $H$ be a closed subgroup of $\mathbb{R}^n$.  Then there is a vector space $V \leq \mathbb{R}^n$ and an open neighborhood $0 \in U \subseteq \mathbb{R}^n$ so that $H \cap U = V \cap U$.
\end{corollary}
\begin{proof}
  We take $G := \mathbb{R}^n$ and note that $\mathfrak{g} = \mathbb{R}^n$ and that the exponential map $\mathfrak{g} \rightarrow G$ is the identity.  The conclusion then follows from Theorem \ref{thm:closed-implies-lie}.
\end{proof}

\begin{exercise}
  Write down a direct proof of Corollary \ref{cor:classification-vector-subspaces}.  (The proof of Theorem \ref{thm:closed-implies-lie} simplifies a bit in this special case while retaining its basic flavor; it is instructive to work out exactly how it simplifies.)
\end{exercise}

\subsection{Stabilizers\label{sec:detect-lie-stabilizers}}
\label{sec:orgca48d81}
\subsubsection{The basic result\label{sec:stab-basic-result}}
\label{sec:org9ced8a6}
Let $M$ be a manifold and $G$ a Lie group acting on $M$, i.e., equipped with a smooth map $G \times M \rightarrow M$ satisfying the usual requirements of an action (see Definition \ref{defn:lie-group}).  Let $x \in M$.  Consider the orbit map $\alpha : G \rightarrow M$ given by $\alpha(g) := g x$.  A crucial property of this map is that it has \emph{constant rank}.  Indeed, its rank at $g_2$ is the same as its rank at $g_1 g_2$ thanks to the identity $\alpha(g_1 g_2) = \alpha(g_1) \alpha(g_2)$ and the fact that $\alpha(g_1)$ is a diffeomorphism.  Using the constant rank theorem from multivariable calculus, it follows that $\alpha$ is linearizable in a neighborhood of any point of $G$, and in particular, near the identity element.  It follows that
\begin{equation*}
  \Stab_G(x) = \{g \in G : \alpha(g) = x\}
\end{equation*}
is a submanifold of $G$, hence a Lie subgroup, with Lie algebra
\begin{equation*}
  \stab_\mathfrak{g}(x) := \{X \in \mathfrak{g} : d \alpha(X) = 0\}.
\end{equation*}

\subsubsection{Application to kernels}
\label{sec:org93667ef}
If $f : G \rightarrow H$ is a morphism of Lie groups, then we may regard $G$ as acting on $H$ via $g \cdot x := f(g) x$.  The stabilizer of the identity element of $H$ under this action is then the kernel of $f$, so by the result of the previous section,
\begin{equation*}
  \ker(f) = \{g \in G : f(g) = e\}
\end{equation*}
is a Lie subgroup of $G$ with Lie algebra
\begin{equation*}
  \ker(d f) : \{X \in \mathfrak{g} : d f (X) = 0 \}.
\end{equation*}

\subsubsection{Application to preimages\label{sec:stab-appl-preimages}}
\label{sec:org79033ad}
Given a morphism $f : G \rightarrow H$ of Lie groups and a Lie subgroup $H_1$ of $H$, the preimage $f^{-1}(H_1)$ may be interpreted as the stabilizer in $G$ of the identity element in the quotient manifold $H/H_1$ under the action afforded by $f$.  Thus $f^{-1}(H_1)$ is a Lie subgroup of $G$ with Lie algebra $(d f)^{-1}(\mathfrak{h}_1) \leq \mathfrak{g}$.

\subsubsection{Application to intersections}
\label{sec:org60c2f88}
If $H_1, H_2$ are Lie subgroups of a Lie group $G$, then $H_1 \cap H_2$ is the preimage of $H_2$ under the inclusion map $H_1 \hookrightarrow G$, and is thus itself a Lie subgroup.

\subsubsection{Application to stabilizers of vectors or subspaces in representations}
\label{sec:orgc9b1def}
Let $G$ be a Lie group and $R : G \rightarrow \GL(V)$ a representation.  For $v \in V$, we know by \S\ref{sec:stab-basic-result} that $\Stab_G(v)$ is a Lie subgroup with Lie algebra $\stab_G(v)$.  For a subspace $U$ of $V$, we can apply the considerations of \S\ref{sec:stab-basic-result} to a suitable Grassmannian manifold (consisting of subspaces of $V$ of given dimension) to see that
\begin{equation}\label{eq:defn-stabilizer-subspace-subgroup}
  \{g \in G : R(g) U \subseteq U\}
\end{equation}
is a Lie subgroup of $G$ with Lie algebra
\begin{equation*}
  \{X \in \mathfrak{g} : d R(X) U \subseteq U \}.
\end{equation*}
Alternatively, we can note that $\{g \in \GL(V) : g U \subseteq U\}$ is a Lie subgroup of $\GL(V)$ (consisting of block upper-triangular matrices); it follows then from from the discussion of \S\ref{sec:stab-appl-preimages} that its preimage \eqref{eq:defn-stabilizer-subspace-subgroup} is a Lie subgroup of $G$ with Lie algebra as indicated.

\subsection{Commutator subgroups}
\label{sec:orge98ecbb}
For a general Lie group $G$, the subgroup $G' := [G,G]$ of commutators need not be closed, hence need not be a Lie subgroup.  But if $G$ is \emph{simply-connected}, then $G'$ is indeed a Lie subgroup.  To see this, denote by $\mathfrak{g}$ the Lie algebra of $G$ and by $\mathfrak{g}' := [\mathfrak{g},\mathfrak{g}]$ the subalgebra generated by the commutators.  Then $\mathfrak{g} / \mathfrak{g} '$ is an abelian Lie algebra, hence is the Lie algebra of a vector space $V$.  Since $G$ is simply-connected, the natural Lie algebra morphism $d f : \mathfrak{g} \rightarrow \mathfrak{g} / \mathfrak{g} '$ lifts to a Lie group morphism $f : G \rightarrow V$.  Then $\ker(f)$ is a Lie subgroup with Lie algebra $\mathfrak{g} '$; moreover, it is clear that $\ker(f) \supseteq G'$.  In the opposite direction, we can play around with commutators of paths near the identity in $G$ and the inverse function theorem to see that $G'$ contains a neighborhood of the identity in $\ker(f)$.  It follows that $G'$ and $\ker(f)$ coincide near the identity.  In particular, $G'$ is a Lie subgroup.

In fact, since $G/\ker(f) \cong V$ is a vector space, it follows from the short exact sequence $\dotsb \rightarrow \pi_1(G/\ker(f)) \rightarrow \pi_0(\ker(f)) \rightarrow 0$ that $\pi_0(\ker(f)) = 0$, i.e., that $\ker(f)$ is connected.  So we actually have $G' = \ker(f)$.

\section{Immersed Lie subgroups}
\label{sec:org1df5b23}
We've described thus far a fair bit of the basic Lie-theoretic dictionary: simply-connected Lie groups correspond to Lie algebras, etc.  We've also seen that for a Lie group $G$, the connected Lie subgroups $H$ of $G$ are determined by their Lie algebras $\mathfrak{h} \leq \mathfrak{g}$.  It's natural to ask which $\mathfrak{h}$ arise in this way.  The subtlety of the problem can be seen by considering simple examples such as $G = \mathbb{R}^2$ and $G = (\mathbb{R}/\mathbb{Z})^2$, as in lecture.

An easier question is to ask whether arbitrary Lie subalgebras $\mathfrak{h} \leq \mathfrak{g}$ of the Lie algebra $\mathfrak{g}$ of a Lie group $G$ correspond to ``something'' involving $G$.  The answer is that they are in natural bijection with \emph{connected immersed Lie subgroups} $H$ of $G$ (recall Definition \ref{defn:immersed-lie-subgroup}, which I've gone back in the notes and modified since we started the course in order to make things work here).  By definition, the latter are abstract subgroups $H$ of $G$ with the property that there exists a manifold structure on $H$ with respect to which $H$ is a Lie group and so that the inclusion $H \rightarrow G$ is an injective (immersive) morphism of Lie groups.

To put it a bit more verbosely: a subset $H$ of a Lie group $G$ is an immersed Lie subgroup if there exists a Lie group $\hat{H}$ and an injective immersion $\iota : \hat{H} \rightarrow G$ with image $H$.  In that case $d \iota : \Lie(\hat{H}) \rightarrow \mathfrak{g}$ is an injective morphism of Lie algebras whose image $\mathfrak{h}$ we define to be the Lie algebra of $H$.  This gives one direction of the above correspondence.

The reverse direction is more subtle: given a subalgebra $\mathfrak{h}$ of $\mathfrak{g}$, one takes for $H$ the subgroup generated by the image of $\mathfrak{h}$ under the exponential map.  One then attempts to define a manifold structure on $H$ to be that generated for small open $0 \in U \subseteq \mathfrak{h}$ and $h \in H$ by the charts $h \exp(U) \ni h \exp(X) \mapsto X \in U$.

Another part of the correspondence is that the Lie group structure on any immersed Lie subgroup $H \leq G$ is uniquely determined by the subset $H$.

See Chapter 1, Sections 2.4 and 5.3 of the first reference on the course page for more details.

\section{Simple Lie groups}
\label{sec:org0fe80c3}
Recall that a subset $H$ of an abstract group $G$ is called
\begin{itemize}
\item a \emph{subgroup} if $e \in H$ and $h_1,h_2 \in H \implies h_1 h_2 \in H$ and $h \in H \implies h^{-1} \in H$, and is in that case called
\item \emph{normal} if $g H g^{-1} \subseteq H$ for all $g \in G$,
\item \emph{trivial} if $H = \{1\}$,
\item \emph{proper} if $H \neq G$,
\end{itemize}
and that $G$ is called \emph{simple} if it has no nontrivial proper normal subgroups.  For Lie theory, a slightly modified definition turns out to be convenient.
\begin{definition}
  Let $\mathfrak{g}$ be a Lie algebra, thus $\mathfrak{g}$ is a vector space (over $\mathbf{k} = \mathbb{R}$ or $\mathbb{C}$, say) equipped with a bracket operation $[,]$ that is bilinear, antisymmetric, and satisfies the Jacobi identity.  A vector subspace $\mathfrak{h}$ of $\mathfrak{g}$ is called
  \begin{itemize}
  \item a \emph{subalgebra} if $[\mathfrak{h},\mathfrak{h}] \subseteq \mathfrak{h}$, and is
  \item an \emph{ideal} if $[\mathfrak{g},\mathfrak{h}] \subseteq \mathfrak{h}$.
  \end{itemize}
  Here $[\mathfrak{g},\mathfrak{h}]$ denotes the span of the commutators $[x,y]$ with $x \in \mathfrak{g}, y \in \mathfrak{h}$.  It is clear that an ideal is a subalgebra.

  We denote the relationship that $\mathfrak{h}$ is an ideal of $\mathfrak{g}$ symbolically by $\mathfrak{h} \triangleleft \mathfrak{g}$.
\end{definition}
Henceforth denote by $G$ a \emph{connected} Lie group and by $\mathfrak{g}$ its Lie algebra.  Recall that there is a natural bijection
\begin{equation*}
  \{ \text{ subalgebras } \mathfrak{h} \text{ of } \mathfrak{g} \} \cong \{ \text{ connected virtual Lie subgroups $H$ of $G$ } \}
\end{equation*}
given in the forward direction by $\mathfrak{h} \mapsto H := \langle \exp_G(\mathfrak{h}) \rangle$.
\begin{exercise}\label{exe:normal-vs-ideal}
  Let $H$ be a connected virtual Lie subgroup of $G$ with Lie algebra $\mathfrak{h}$.  The following are equivalent:
  \begin{enumerate}
  \item $H$ is a normal subgroup of $G$.
  \item $\mathfrak{h}$ is an ideal in $\mathfrak{g}$.
  \end{enumerate}
 [Use the standard differentiation/exponentiation technique.]
\end{exercise}
\begin{definition}
  $\mathfrak{g}$ is \emph{abelian} if $[\mathfrak{g},\mathfrak{g}]=0$.
\end{definition}
\begin{exercise}\label{exe:abelian-lie-alg-lie-gp}
  $G$ is abelian if and only if $\mathfrak{g}$ is abelian.  [Use the standard differentiation/exponentiation technique.]
\end{exercise}

\begin{definition}
  $\mathfrak{g}$ is \emph{simple} if it is non-abelian and has no nontrivial proper ideals.
\end{definition}


\begin{definition}
  $G$ is \emph{simple} if it is non-abelian it has no nontrivial proper normal connected virtual Lie subgroups.
\end{definition}

\begin{lemma}
  The following are equivalent:
  \begin{enumerate}
  \item[(i)] $G$ is simple.
  \item[(ii)] $\mathfrak{g}$ is simple.
  \item[(iii)] Every proper normal subgroup of $G$ is discrete.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The equivalence of (i) and (ii) is immediate from Exercise \ref{exe:normal-vs-ideal}.  It is clear that (iii) implies (i): if every proper normal subgroup of $G$ is discrete, and if $H$ is a proper normal connected virtual Lie subgroup, then $H$ is in particular a proper normal subgroup, hence is discrete; since $H$ is then discrete and connected, it is trivial, and since $H$ was arbitrary we conclude that $G$ is simple.

  The interesting implication is thus that (i) and (ii) imply (iii).  To see that, let $K$ be a normal subgroup of $G$ that is not discrete; we wish to show that $K = G$.  The closure $\overline{K}$ of $K$ is (by continuity) a closed normal subgroup of $G$.  Since $K$ is not discrete, neither is $\overline{K}$, hence neither is the connected component $\overline{K}^0$.  Since $\overline{K}^0$ is a characteristic subgroup of $\overline{K}$, it is a closed normal subgroup of $G$, hence a non-discrete normal Lie subgroup of $G$; since $G$ is simple, the only possibility is that $\overline{K}^0 = G$, hence $\overline{K} = G$.

  In summary, $K$ is dense in $G$.  We claim that there is $k \in K$ and $X \in \mathfrak{g}$ so that $\Ad(k) X \neq X$.  If not, then it would follow by continuity that $\Ad(G)$ is trivial, hence that $\mathfrak{g}$ is abelian, contrary to our assumption that $\mathfrak{g}$ is simple, hence non-abelian.

  Consider the curve $\gamma(t) := (k,\exp_G(t X))$, where $(,)$ denotes the commutator.  We then have $\gamma (0) = e$, $\gamma(t) \in K$ for all $t$, and $Y := \gamma '(0) = \Ad(k) X - X \neq 0$.  Since $\mathfrak{g}$ is simple, its center $\mathfrak{z}(\mathfrak{g}) = \{ Z \in \mathfrak{g} : [Z,\mathfrak{g}] = 0\}$ is trivial (for else its center would be a nontrivial ideal, hence $\mathfrak{g}$ would coincide with its center, i.e., $\mathfrak{g}$ would be abelian; contradiction).  In particular, $[Y,\mathfrak{g}] \neq 0$.  It follows by the standard differentiate/exponentiate technique that the subspace $\mathfrak{a}$ of $\mathfrak{g}$ spanned by $\Ad(G) Y$ is a nonzero ideal (check this).  Since $\mathfrak{g}$ is simple, it follows that $\mathfrak{a} = \mathfrak{g}$.  We can thus find $g_1,\dotsc,g_n \in G$, where $n = \dim(G)$, so that the elements $\Ad(g_1) \gamma'(0),\dotsc,\Ad(g_n) \gamma '(0)$ span $\mathfrak{g}$.  Also, the curves $t \mapsto \Ad(g_j) \exp(t X)$ all lie in $K$, since $\exp(tX) \in K$ and $K$ is normal.  The map
  \begin{equation*}
    (t_1,\dotsc,t_n) \mapsto (\Ad(g_1) \exp(t_1 X) ) \dotsb (\Ad(g_n) \exp(t_n X) )
  \end{equation*}
  then has differntial at $(0,\dotsc,0)$ given by an invertible linear map, hence (by the inverse function theorem) defines a local diffeomorphism; since its image lies in $K$, we deduce that $K$ contains a neighborhood of the identity in $G$, and since $G$ is connected, it follows that $K = G$, as required.
\end{proof}

Thus apart from excluding abelian examples and possible discrete normal subgroups, the notions of a connected Lie group being simple as an abstract group or simple as a Lie group are the same.

In the rest of the lecture, we described which classical Lie groups/algebras are simple and what the isomorphisms between them are.  This will be discussed in subsequent lectures.

\section{Simplicity of the special linear group\label{sec:simplicity-sln}}
\label{sec:org0a0c43a}

\subsection{Some linear algebra\label{sec:some-lin-alg}}
\label{sec:orge7b7a5c}
Let $V$ be a complex vector space (not necessarily finite-dimensional, for now).  Given an operator $x \in \End(V)$ and $\lambda \in \mathbb{C}$, we may define the \emph{eigenspace}
\begin{equation*}
  V^\lambda := \{v \in V : x v = \lambda v\}.
\end{equation*}
\begin{lemma}\label{lem:lin-indep-eigenspaces}
  The spaces $V^{\lambda}$ are linearly independent, that is to say, if $n \geq 1$ and $\lambda_1,\dotsc,\lambda_n$ are distinct complex numbers and $v_1 \in V^{\lambda_1}, \dotsc, v_n \in V^{\lambda_n}$ satisfy $v_1 + \dotsb + v_n = 0$, then $v_1 = \dotsb = v_n = 0$.
\end{lemma}
\begin{proof}
  We induct on $n$.  When $n = 1$, the required conclusion is clear: if $v_1 \in V^{\lambda_1}$ satisfies $v_1 = 0$, then certainly $v_1 = 0$.  Suppose now that $n \geq 2$, and let $v_1, \in V^{\lambda_1}, \dotsc,v_n \in V^{\lambda_n}$ with $v_1 + \dotsb + v_n = 0$.  Then certainly
  \begin{equation*}
    0 = \lambda_n(v_1 + \dotsb + v_n)
  \end{equation*}
  and also
  \begin{equation*}
    0 = x (v_1 + \dotsb + v_n) = \lambda_1 v_1 + \dotsb + \lambda_n v_n,
  \end{equation*}
  hence upon taking differences,
  \begin{equation*}
    (\lambda_1 - \lambda_n) v_1 + (\lambda_2 - \lambda_n) v_2 + \dotsb + (\lambda_{n-1} - \lambda_n) v_{n-1} = 0.
  \end{equation*}
  By our inductive hypothesis, $(\lambda_j - \lambda_n) v_j = 0$ for $j = 1,\dotsc,n-1$.  Since $\lambda_j \neq \lambda_n$, it follows that $v_1 = \dotsb = v_{n-1} = 0$ and hence also that $v_n = 0$, as required.
\end{proof}

Thus the sum $\sum_{\lambda \in \mathbb{C}} V^\lambda$ is in fact a direct sum $\oplus V^\lambda$.

\begin{definition}
  An operator $x \in \End(V)$ is \emph{semisimple} (or \emph{diagonalizable}, or \emph{completely reducible}; depending upon my mood I alternate between the various terminologies) if
  \begin{equation*}
    V = \oplus_{\lambda \in \mathbb{C}} V^\lambda.
  \end{equation*}
\end{definition}

Assume henceforth that $V$ is \emph{finite-dimensional}.

\begin{definition}
  For $x \in \End(V)$, we say that a subspace $W \leq V$ is $x$-invariant if $x W \subseteq W$.
\end{definition}

\begin{exercise}\label{exe:characterizations-of-semisimplicity-of-a-boring-old-matrix}
  The following are equivalent:
  \begin{enumerate}
  \item $x$ is semisimple.
  \item There is a basis of $V$ with respect to which $x$ is represented by a diagonal matrix.
  \item In the Jordan decomposition of $x$ as a sum of Jordan blocks Jordan blocks
    \begin{equation*}
      \begin{pmatrix}
        \lambda  & 1 &  &  & \\
           & \lambda   & 1 & & \\
           & & \dotsb   & \dotsb & \\
           & & &  \lambda    &  1\\
           & & &      &  \lambda
      \end{pmatrix}
,
    \end{equation*}
    only $1 \times 1$ blocks appear.
  \item The characteristic polynomial and minimal polynomial of $x$ are the same.
  \item Every $x$-invariant subspace $W \leq V$ has an $x$-invariant complement $W'$, i.e., a subspace $W' \leq V$ for which $V = W \oplus W'$.
  \item The minimal polynomial of $x$ is squarefree, i.e., of the form $(X - a_1) \dotsb (X - a_r)$ for some \emph{distinct} complex numbers $a_1,\dotsc,a_r$.
  \item There exists a squarefree polynomial that annihilates $x$.
  \end{enumerate}
  (1) iff (2): take as the basis for $V$ a union of arbitrary bases of $V^\lambda$.  (2) iff (3): clear.  (3) iff (4): clear.  (1) iff (5): argue as in the proof of Lemma \ref{lem:compl-red-vs-complements}.  It is clear that (4), (6) and (7) are equivalent.
\end{exercise}

\begin{exercise}\label{exercise:restriction-of-semisimple-is-semisimple}
  Suppose $x \in \End(V)$ is semisimple.
  \begin{enumerate}
  \item For each eigenvalue $\lambda$ of $x$, let $W^\lambda$ be a subspace of the $\lambda$-eigenspace $V^\lambda$ of $x$.  Verify that $\oplus_{\lambda} W^\lambda$ is an $x$-invariant subspace of $V$.
  \item Let $W \leq V$ be an $x$-invariant subspace.  Show that $W$ is of the form $\oplus_{\lambda} W^\lambda$, where $W^\lambda = W \cap V^\lambda$ is a subspace of $V^\lambda$.  [Hint: one can either appeal to the previous lemma or argue as in the proof of Lemma \ref{lem:lin-indep-eigenspaces}; other arguments are probably also possible.]
  \item Let $W \leq V$ be an $x$-invariant subspace.
  \item Show that $x|_W$ is semisimple.
  \end{enumerate}
\end{exercise}


\begin{theorem}\label{thm:commuting-semisimple-operators}
  Let $x_1,\dotsc,x_n \in \End(V)$ be semisimple commuting elements.  Then there is a basis of $V$ with respect to which all of $x_1,\dotsc,x_n$ are diagonalizable.
\end{theorem}
\begin{proof}
  We induct on $n$.  For $n = 1$, everything is clear, so suppose $n \geq 2$.  Let $V = \oplus V^\lambda$ be the decomposition of $V$ into eigenspaces for $x_1$.  For $j \in \{2..n\}$, we claim that $x_j V^\lambda \subseteq V^\lambda$.  Indeed, for $v \in V^\lambda$, we have $x_1 x_j v = x_j x_1 v = x_j \lambda v = \lambda x_j v$, hence $x_j v \in V^\lambda$, as required.  By Exercise \ref{exercise:restriction-of-semisimple-is-semisimple}, the restrictions to $V^\lambda$ of the operators $x_2,\dotsc,x_n$ are semisimple (and commuting), hence by our inductive hypothesis may be simultaneously diagonalized; we now conclude by taking a union over $\lambda$ of bases of $V^\lambda$ with respect to which the $x_1,\dotsc,x_n$ are diagonalized.
\end{proof}

\subsection{Recap on \texorpdfstring{$\slLie_2(\mathbb{C})$}{sl2C}}
\label{sec:org77c4374}
Recall that $\mathfrak{g} = \slLie_2(\mathbb{C})$ has the basis elements
\begin{equation*}
  H = 
\begin{pmatrix}
    1 &  \\
      & -1
  \end{pmatrix}
,
  \quad
  X = 
\begin{pmatrix}
    & 1 \\
    & 
  \end{pmatrix}
,
  \quad
  Y = 
\begin{pmatrix}
    &  \\
    1 & 
  \end{pmatrix}
\end{equation*}
which satisfy the relations
\begin{equation*}
 [X,Y] = H, \quad \ad(H) X = 2 X, \quad \ad(H) Y = -2 Y.
\end{equation*}
We can view
\begin{equation*}
  \mathfrak{h} = \mathbb{C} X \oplus \mathbb{C} H \oplus \mathbb{C} Y
\end{equation*}
as the eigenspace decomposition of $\ad(H)$ with eigenvalues $2, 0, -2$.

\begin{theorem}
  Let $\rho : \mathfrak{g} \rightarrow \End(V)$ be a finite-dimensional representation.  Then $\rho(H)$ is semisimple.
\end{theorem}
\begin{proof}
  Since $V$ decomposes as a direct sum of the irreducible representations $W_m$, we may assume $V = W_m$.  Then the elements $x^m, x^{m-1} y, \dotsc, y^m$ give a basis of $V$ with respect to which $\rho(H)$ is diagonal.
\end{proof}

We record for future reference that the element
\begin{equation}\label{eq:weyl-element-via-exponentials-sl2}
  w := e^{-X} e^Y e^{-X} = 
\begin{pmatrix}
    & -1 \\
    1 & 
  \end{pmatrix}
 \in \SL_2(\mathbb{C})
\end{equation}
has the property that
\begin{equation}\label{eq:weyl-element-via-expoenntials-sl2-acting-as-reflection}
  Ad(w) H = - H.
\end{equation}

\subsection{Reformulation in terms of representations of abelian Lie algebras}
\label{sec:org67e3030}
In this section all Lie algebras are finite-dimensional over the complex numbers and all vector spaces are complex and finite-dimensional.  Recall that a Lie algebra $\mathfrak{h}$ is \emph{abelian} if $[\mathfrak{h},\mathfrak{h}] = 0$.

Let $\mathfrak{h}$ be an abelian Lie algebra and $\rho : \mathfrak{h} \rightarrow \End(V)$ a representation.  Let $\mathfrak{h}^* := \Hom(\mathfrak{h},\mathbb{C})$ denote the dual vector space.  For $\lambda \in \mathfrak{h}^*$, define the \emph{eigenspace}
\begin{equation*}
  V^\lambda := \{v \in V : H v = \lambda v \text{ for all }H \in \mathfrak{h} \}
\end{equation*}
where we abbreviate $H v := \rho(H) v$.
\begin{lemma}
  The spaces $V^\lambda$ are linearly independent, i.e., $\sum_{\lambda \in \mathbb{C}} V^\lambda = \oplus_{\lambda \in \mathbb{C}} V^\lambda$.
\end{lemma}
\begin{proof}
  Same proof as Lemma \ref{lem:lin-indep-eigenspaces}.
\end{proof}

\begin{lemma}
  Let $\mathfrak{h}$ be an abelian Lie algebra and $\rho : \mathfrak{h} \rightarrow \End(V)$ a representation.  The following are equivalent:
  \begin{enumerate}
  \item $\rho(H)$ is semisimple for all $H \in \mathfrak{h}$.
  \item There is a basis of $V$ with respect to which every element of $\rho(\mathfrak{h})$ is diagonal.
  \item $V = \oplus V^\lambda$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  Take a basis $H_1,\dotsc,H_n$ and consider $x_1 := \rho(H_1), \dotsc, x_n := \rho(H_n) \in \End(V)$.  If each $x_j$ is semisimple, then Theorem \ref{thm:commuting-semisimple-operators} tells us that there is a basis with respect to which they are (and hence every element of $\rho(\mathfrak{h})$ is) diagonal.  The converse and the equivalence with $V = \oplus V^\lambda$ are left to the reader.
\end{proof}
\begin{definition}
  We say that a representation $\rho : \mathfrak{h} \rightarrow \End(V)$ of an abelian Lie algebra $\mathfrak{h}$ is \emph{semisimple} if teh equivalent conditions of the previous lemma are satisfied.
\end{definition}

\begin{definition}
  Let $\rho : \mathfrak{h} \rightarrow \End(V)$ be a representation of an abelian Lie algebra $\mathfrak{h}$.  A \emph{weight} of $\rho$ is an element $\lambda \in \mathfrak{h}^*$ for which $V^\lambda \neq 0$.  The space $V^\lambda$ is then called the \emph{weight space of weight $\lambda$}.  (The definition is most interesting when $\rho$ is semisimple, so that $V = \oplus V^\lambda$.)
\end{definition}

\begin{example}
  Let $V = \mathbb{C}^n$, let $\mathfrak{h}$ be the space of diagonal matrices in $M_n(\mathbb{C}) = \End(V)$, and let $\rho : \mathfrak{h} \rightarrow \End(V)$ be the identity map.  Then the weights of $\rho$ are the functionals $\lambda_1,\dotsc,\lambda_n : \mathfrak{h} \rightarrow \mathbb{C}$ giving diagonal coordinates on $\mathfrak{h}$, i.e.,
  \begin{equation*}
    H =
    \begin{pmatrix}
      \lambda_1(H) &  &  \\
             & \dotsb  &  \\
             &  & \lambda_n(H)
    \end{pmatrix}
.
  \end{equation*}
  The corresponding weight spaces $V^{\lambda_j}$ are the one-dimensional subspaces $\mathbb{C} e_j$ spanned by the standard basis elements $e_1,\dotsc,e_n$ of $\mathbb{C}^n$.
\end{example}


\subsection{Roots of an abelian subalgebra\label{sec:roots-of-abelian-subalgebra}}
\label{sec:orgccb748c}
Let $\mathfrak{g}$ be a Lie algebra (always finite-dimensional and over the complex numbers, for the present purposes) and $\mathfrak{h} \leq \mathfrak{g}$ an abelian subalgebra.

Recall that adjoint representation $\ad : \mathfrak{g} \rightarrow \End(\mathfrak{g})$ given by $\ad(X) Y := [X,Y]$.  We restrict this to $\mathfrak{h}$ to obtain a representation
\begin{equation*}
  \ad : \mathfrak{h} \rightarrow \End(\mathfrak{g}).
\end{equation*}
The set $R$ of \emph{roots} of the pair $(\mathfrak{g},\mathfrak{h})$ is defined to be the set of nonzero weights of this representation.  (We might more verbosely write $R = R(\mathfrak{g},\mathfrak{h})$ when we wish to make explicit the dependence.)  In other words, for $\lambda \in \mathfrak{h}^*$, set
\begin{equation*}
  \mathfrak{g}^\lambda := \left\{ X \in \mathfrak{g} : [H,X] = \lambda(H) X \text{ for all } H \in \mathfrak{h} \right\}.
\end{equation*}
Then
\begin{equation}
  R = \{\alpha \in \mathfrak{h}^* - \{0\} : \mathfrak{g}^\alpha
  \neq 0 \}.
\end{equation}
For $\alpha \in R$, we call $\mathfrak{g}^\alpha$ the corresponding \emph{root space}.

\begin{exercise}
  Write $0 \in \mathfrak{h}^*$.  Then
  \begin{equation*}
    \mathfrak{h} \subseteq \mathfrak{g}^0.
  \end{equation*}
\end{exercise}

Here is a very useful general observation:
\begin{lemma}\label{lem:commutators-of-root-spaces}
  For any $\alpha,\beta \in \mathfrak{h}^*$, one has
  \begin{equation*}
 [\mathfrak{g}^\alpha, \mathfrak{g}^\beta] \subseteq \mathfrak{g}^{\alpha + \beta}.
  \end{equation*}
\end{lemma}
\begin{proof}
 { \bf It is probably easier to work this out as an exercise than to read the proof.  }
  It suffices to show for all $X \in \mathfrak{g}^\alpha$ and $Y \in \mathfrak{g}^\beta$ that $[X,Y] \in \mathfrak{g}^{\alpha + \beta}$, i.e., that for all $H \in \mathfrak{h}$, one has $\ad(H)[X,Y] = (\alpha + \beta)(H) [X,Y]$.  This follows from the Jacobi identity in the form
  \begin{equation}
    \ad(H) [X,Y] = [\ad(H) X, Y] + [X,\ad(H) Y],
  \end{equation}
  like so:
  \begin{equation}
    \ad(H)[X,Y] = [\alpha(H) X, Y] = [X,\beta(H) Y]
    = (\alpha(H) + \beta(H))[X,Y]
    = (\alpha + \beta)(H)[X,Y].
  \end{equation}
\end{proof}
More generally:
\begin{lemma}\label{lem:general-commutation-root-spaces-on-weight-spaces}
  Let $\rho : \mathfrak{g} \rightarrow \End(V)$ be any linear representation.  For $\lambda \in \mathfrak{h}^*$, set $V^\lambda := \{v \in V : \rho(H) v = \lambda(H) v \text{ for all } v \in V\}$.  Let $\alpha, \lambda \in \mathfrak{h}^*$.  Then
  \begin{equation*}
    \rho(\mathfrak{g}^\alpha) V^\lambda \subseteq V^{\alpha + \lambda}.
  \end{equation*}
\end{lemma}
\begin{proof}
  This is essentially the same proof as the previous lemma, but with the adjoint representation replaced by $\rho$; it should also remind the reader of stuff we did awhile ago involving raising/lowering operators acting on representations of $\slLie_2(\mathbb{C})$:

  { \bf It is probably easier to work this out as an exercise than to read the proof.  } Let $X \in \mathfrak{g}^\alpha$ and $v \in V^\lambda$.  To establish the required membership $\rho(X) v \in V^{\alpha + \lambda}$, we must verify for each $H \in \mathfrak{h}$ that $\rho(H) \rho(X) v = (\alpha + \lambda)(H) \rho(X) v$.  Indeed,
  \begin{equation*}
    \rho(H) \rho(X) = \rho([H,X]) + \rho(X) \rho(H).
  \end{equation*}
  Since $[H,X] = \alpha(H) X$ and $\rho$ is linear, we have
  \begin{equation*}
    \rho([H,X]) v = \rho(\alpha(H) X) v = \alpha(H) \rho(X) v.
  \end{equation*}
  Since $\rho(H) v = \lambda(H) v$ by assumption and $\rho(X)$ is linear, we have
  \begin{equation*}
    \rho(X) \rho(H) v = \rho(X) \lambda(H) v = \lambda(H) \rho(X) v.
  \end{equation*}
  Summing up, we obtain
  \begin{equation*}
    \rho(H) \rho(X) = \alpha(H) \rho(X) v + \lambda(H) \rho(X) v,
  \end{equation*}
  which simplifies to give the required identity.
\end{proof}

In other words, weight spaces $V^\lambda$ are permuted by root spaces $\mathfrak{g}^\alpha$ and root spaces $\mathfrak{g}^\alpha, \mathfrak{g}^\beta$ interact nicely with respect to the commutator.  Note that Lemma \ref{lem:commutators-of-root-spaces} is the specialization of \ref{lem:general-commutation-root-spaces-on-weight-spaces} to the adjoint representation.

\subsection{The roots of the special linear Lie algebra\label{sec:roots-sln}}
\label{sec:orgca2a787}
Let $\mathfrak{g} := \slLie_n(\mathbb{C})$.  Let $\mathfrak{h} \leq \mathfrak{g}$ be the subspace of diagonal matrices.  One has $\dim(\mathfrak{h}) = n-1$.  Every element $H \in \mathfrak{h}$ is of the form
\begin{equation}\label{eqn:elements-cartan-slNC}
  H =
  \begin{pmatrix}
    \lambda_1(H) &  &  \\
           & \dotsb  &  \\
           &  & \lambda_n(H)
  \end{pmatrix}
\end{equation}
with $\lambda_1(H) + \dotsb + \lambda_n(H) = 0$.  The functionals $\lambda_1,\dotsc,\lambda_n \in \mathfrak{h}^*$ defined by \eqref{eqn:elements-cartan-slNC} span $\mathfrak{h}^*$ and satisfy the relation $\lambda_1 + \dotsb + \lambda_n = 0$.

For $j,k \in \{1..n\}$, let $E_{j k} \in M_n(\mathbb{C})$ denote the elementary matrix with entries $(E_{j k})_{m n} := \delta_{j m} \delta_{k n}$.  If $j \neq k$, then $E_{j k} \in \mathfrak{g}$.  One has in general $E_{j j} - E_{k k} \in \mathfrak{g}$, but $E_{j j} \neq \mathfrak{g}$ (by the trace condition).  In general,
\begin{equation}\label{eq:commutators-of-elementary-matrices}
 [E_{i j}, E_{k l}]
  = \delta_{j k} E_{i l}
  - \delta_{i l} E_{k j}.
\end{equation}

For $H \in \mathfrak{h}$ and any $j,k$, we compute directly that $H E_{j k} = \lambda_j(H) E_{j k}$ and $E_{j k} H = \lambda_k(H) E_{j k}$, hence
\begin{equation}
  \ad(H) E_{j k}
  = (\lambda_j(H) - \lambda_k(H)) E_{j k}.
\end{equation}
Thus for all $j \neq k$, we see that $\lambda_j - \lambda_k$ is a root.  Observe also that
\begin{equation}\label{eq:h-self-centralizing-slN}
  \mathfrak{g}^0 = \mathfrak{h};
\end{equation}
said another way, $\mathfrak{h}$ is a \emph{maximal} abelian subalgebra of $\mathfrak{g}$.  Since
\begin{equation}\label{eqn:decomp-into-roots-slNC}
  \mathfrak{g} = \mathfrak{h} \oplus (\oplus_{j \neq k}
  \mathbb{C} E_{j k})
\end{equation}
we see that
\begin{equation}\label{eq:roots-of-slN}
  R = \{\lambda_j - \lambda_k : j \neq k\}.
\end{equation}
We may rewrite \eqref{eqn:decomp-into-roots-slNC} in either of the forms
\begin{equation}\label{eqn:decomp-into-roots-slNC-2}
  \mathfrak{g} = \oplus_{\lambda \in \mathbb{C}}
  \mathfrak{g}^\lambda
\end{equation}
or
\begin{equation}\label{eqn:decomp-into-roots-slNC-3}
  \mathfrak{g} = \mathfrak{h} \oplus (\oplus_{\alpha \in R}
  \mathfrak{g}^\alpha).
\end{equation}
In particular:
\begin{lemma}\label{lem:adjoint-action-cartan-slNC-is-semisimple}
  $\ad : \mathfrak{h} \rightarrow \End(\mathfrak{g})$ is semisimple.
\end{lemma}
\begin{lemma}\label{lem:roots-span}
  The set $R$ of roots spans $\mathfrak{h}^*$.
\end{lemma}
\begin{proof}
  We must verify that if $H \in \mathfrak{h}$ satisfies $\alpha(H) = 0$ for all $\alpha \in R$, then $H = 0$.  Indeed, given such an $H$, we have (by taking $\alpha = \lambda_j - \lambda_k$) that $\lambda_j(H) = \lambda_k(H)$ for all $i \neq j$, hence all the entries of $H$ are the same; since $H$ has trace zero, it follows as required that $H = 0$.
\end{proof}
For $\alpha = \lambda_j - \lambda_k \in R$, the corresponding root space is
\begin{equation*}
  \mathfrak{g}^{\alpha} = \mathbb{C} X_\alpha \text{ where } X_\alpha := E_{j k}.
\end{equation*}
Note that
\begin{equation}\label{eq:negative-roots-are-roots}
  \alpha \in R \implies - \alpha \in R
\end{equation}
since indeed $-\alpha = \lambda_k - \lambda_j$ for $\alpha = \lambda_j - \lambda_k$, but that
\begin{equation*}
\text{ $n \alpha \notin R$ for any $n \in \mathbb{Z} - \{\pm 1\}$.}
\end{equation*}
Define $Y_\alpha := E_{k j} \in \mathfrak{g}^{-\alpha}$; by \eqref{eq:commutators-of-elementary-matrices}, the element
\begin{equation*}
  H_\alpha := [X_\alpha,Y_\alpha]
\end{equation*}
is given explicitly by $H_\alpha = E_{j j} - E_{k k} \in \mathfrak{h}$ and satisfies
\begin{equation*}
  \alpha(H_\alpha) = 2
\end{equation*}
because $\alpha(H_\alpha) = (\lambda_j - \lambda_k)(E_{j j} - E_{k k}) = 1 - (-1) = 2$.  (One has here that $Y_{\alpha} = X_{-\alpha}$, but what matters most is that they both span the same one-dimensional space.)  One has
\begin{equation*}
 H_{-\alpha} = - H_\alpha \text{ for all } \alpha \in R.
\end{equation*}
 It is easy to see that
\begin{equation}\label{eq:coroots-span-cartan-slNC}
  \sum_{\alpha \in R} \mathbb{C} H_\alpha = \mathfrak{h},
\end{equation}
but that the sum is not direct for $n > 2$.
\begin{lemma}\label{lem:root-space-commutation-relations}
  Each $\mathfrak{g}^{\alpha}$ is one-dimensional.  For all $\alpha,\beta \in R$,
  \begin{equation*}
 [\mathfrak{g}^\alpha, \mathfrak{g}^\beta]
    = 
\begin{cases}
      \mathfrak{g}^{\alpha + \beta} & \text{ if } \alpha + \beta \in R \\
      \mathbb{C} H_\alpha  & \text{ if } \alpha + \beta = 0 \\
      0 & \text{ otherwise.}
    \end{cases}
  \end{equation*}
\end{lemma}
\begin{proof}
  This reduces to an explicit computation using \eqref{eq:commutators-of-elementary-matrices}.  It is instructive to note the consistency with the conclusion $[\mathfrak{g}^\alpha, \mathfrak{g}^\beta] \subseteq \mathfrak{g}^{\alpha + \beta}$ of Lemma \ref{lem:commutators-of-root-spaces} and the identity \eqref{eq:h-self-centralizing-slN}.
\end{proof}

Here are some basic consequences:
\begin{lemma}\label{lem:compositions-of-ad-stuff}
  \begin{enumerate}
  \item If $\alpha,\beta,\alpha+\beta$ are all roots, then the map
    \begin{equation*}
      \ad(X_\beta) : \mathfrak{g}^\alpha \rightarrow \mathfrak{g}^{\alpha+\beta}
    \end{equation*}
    is an isomorphism (of one-dimensional vector spaces).
  \item If $\alpha,\beta,\alpha-\beta$ are all roots, then the map
    \begin{equation*}
      \ad(Y_\beta) : \mathfrak{g}^\alpha \rightarrow \mathfrak{g}^{\alpha-\beta}
    \end{equation*}
    is an isomorphism (of one-dimensional vector spaces).
  \item For any roots $\alpha,\beta$, the composition
    \begin{equation*}
      \ad(X_\beta) \circ \ad(X_\alpha) : \mathfrak{g}^{-\alpha} \rightarrow \mathfrak{g}^{\beta}
    \end{equation*}
    is an isomorphism (of one-dimensional vector spaces) if and only if $\beta(H_\alpha) \neq 0$.
  \item For any roots $\alpha,\beta$, the composition
    \begin{equation*}
      \ad(Y_\beta) \circ \ad(Y_\alpha) : \mathfrak{g}^{\alpha} \rightarrow \mathfrak{g}^{-\beta}
    \end{equation*}
    is an isomorphism (of one-dimensional vector spaces) if and only if $\beta(H_\alpha) \neq 0$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The first two assertions are immediate from Lemma \ref{lem:root-space-commutation-relations}.  For the third assertion, we factor the map of interest as
  \begin{equation*}
    \mathfrak{g}^{-\alpha} \xrightarrow{\ad(X_\alpha)} \mathbb{C} H_\alpha \xrightarrow{\ad(X_\beta)} \rightarrow \mathfrak{g}^{\beta}
  \end{equation*}
  and observe (using Lemma \ref{lem:root-space-commutation-relations}) that $\ad(X_\alpha) : \mathfrak{g}^{-\alpha} \rightarrow \mathbb{C} H_\alpha$ is always an isomorphism of one-dimensional vector spaces and noting that $\ad(X_\beta) H_\alpha = -[H_\alpha,X_\beta] = - \beta(H_\alpha) X_\beta$, which vanishes if and only if $\beta(H_\alpha) = 0$.

  The proof of the fourth assertion is similar: we factor the map $\mathfrak{g}^{\alpha} \rightarrow \mathfrak{g}^{-\beta}$ in question as
  \begin{equation*}
    \mathfrak{g}^{\alpha} \xrightarrow{\ad(Y_\alpha)} \mathbb{C} H_\alpha \xrightarrow{\ad(Y_\beta)} \rightarrow \mathfrak{g}^{-\beta},
  \end{equation*}
  observe that the first map in this composition is always an automorphism, and then observe that the second map sends $H_\alpha$ to $[Y_\beta,H_\alpha] = - [H_\alpha,Y_\beta] = \beta(H_\alpha) Y_\beta$.
\end{proof}

One can check directly using \eqref{eq:roots-of-slN} that for all $\alpha,\beta \in R$,
\begin{equation}\label{eq:orthogonality-equivalence}
  \beta(H_\alpha) = 0
  \iff \alpha(H_\beta) = 0.
\end{equation}
\begin{definition}\label{defn:orthogonal-roots}
  Two roots $\alpha, \beta$ are called \emph{orthogonal} if either of the equivalent conditions \eqref{eq:orthogonality-equivalence} hold.  They are called \emph{non-orthogonal} if $\beta(H_\alpha) \neq 0$ (equivalently, $\alpha(H_\beta) \neq 0$).
\end{definition}
Explicitly, if $\alpha = \lambda_i - \lambda_j$ and $\beta = \lambda_k - \lambda_l$, then $\alpha,\beta$ are orthogonal if and only if $\{i,j\} \cap \{k,l\} = \emptyset$.

\begin{lemma}\label{lem:condition-for-chain-of-root-maps-not-to-vanish}
  Suppose given roots $\beta, \alpha_1, \dotsc, \alpha_r \in R$ with the property that $\beta + \alpha_1 + \dotsb + \alpha_r$ is a root and that for each $s = 1,2,\dotsc,r$, the partial sum $\beta + \alpha_1 + \dotsb + \alpha_s$ is either a root, or is zero, and if it is zero that the roots $\alpha_s, \alpha_{s+1}$ are non-orthogonal.  Then the compositions
  \begin{equation*}
    \mathfrak{g}^\beta \xrightarrow{X_{\alpha_1}} \mathfrak{g}^{\beta+\alpha_1} \xrightarrow{X_{\alpha_2}} \dotsb \xrightarrow{X_{\alpha_{r}}} \mathfrak{g}^{\beta+\alpha_r}
  \end{equation*}
  \begin{equation*}
    \mathfrak{g}^{\beta+\alpha_r} \xrightarrow{Y_{\alpha_r}} \dotsb \xrightarrow{Y_{\alpha_2}} \mathfrak{g}^{\beta+\alpha_1} \xrightarrow{Y_{\alpha_{1}}} \mathfrak{g}^{\beta}
  \end{equation*}
  (where we abbreviate $\xrightarrow{X}$ for $\xrightarrow{\ad(X)}$, etc) are isomorphisms of one-dimensional vector spaces.
\end{lemma}
\begin{proof}
  Follows immediately by repeated application of Lemma \ref{lem:compositions-of-ad-stuff}.
\end{proof}

\subsection{The simplicity of \(\slLie_n(\mathbb{C})\)}
\label{sec:org952f7d3}
Set $\mathfrak{g} := \slLie_n(\mathbb{C})$.  Let $\mathfrak{h}$ denote its diagonal subalgebra, and $R$ the set of roots for $(\mathfrak{g},\mathfrak{h})$.

\begin{lemma}\label{lemma:clean-way-to-get-simplicity}
  Set
  \begin{equation*}
\lambda_{\max} := \lambda_1 - \lambda_n \in R.
\end{equation*}
  Let $\beta \in R$.  Then there is a nonnegative integer $r \geq 0$ and roots $\alpha_1,\dotsc,\alpha_r$ so that
  \begin{equation*}
    \beta + \alpha_1 + \dotsb + \alpha_r = \lambda_{\max}
  \end{equation*}
  and so that for $s \in \{1..r-1\}$, the partial sum $\beta + \alpha_1 + \dotsb + \alpha_s$ is either a root, or zero, and if it is zero, then the roots
  \begin{equation}\label{eq:nonvanishing-required-for-simplicity}
    \alpha_{s+1}(H_{\alpha_s}) \neq 0,
    \quad 
    \alpha_{s}(H_{\alpha_{s+1}}) \neq 0,
  \end{equation}
  i.e., the roots $\alpha_s, \alpha_{s+1}$ are non-orthogonal in the sense of Definition \ref{defn:orthogonal-roots}.  (We can always take $r \leq 2$, but that doesn't matter so much.)
\end{lemma}
\begin{proof}
  Write $\beta = \lambda_j - \lambda_k$.  The proof is basically to stare at the diagrams
  \begin{equation*}
    \lambda_j - \lambda_k \xrightarrow{\lambda_k - \lambda_n} \lambda_j - \lambda_n \xrightarrow{\lambda_1 - \lambda_j} \lambda_1 - \lambda_n
  \end{equation*}
  and
  \begin{equation*}
    \lambda_j - \lambda_k \xrightarrow{\lambda_1 - \lambda_j} \lambda_1 - \lambda_k \xrightarrow{\lambda_k - \lambda_n} \lambda_1 - \lambda_n.
  \end{equation*}
  We omit $\xrightarrow{\mu}$ from the above diagrams if $\mu = 0$.  What needs to be checked is that for each $j,k$, at least one of the above diagrams has the property that whenever it looks like
  \begin{equation*}
    \lambda_j - \lambda_k \xrightarrow{\mu } 0 \xrightarrow{\nu}, \lambda_1 - \lambda_n,
  \end{equation*}
  the roots $\mu$ and $\nu$ are non-orthogonal, i.e., satisfy $\mu(H_\nu) \neq 0$.

  We turn to the details:
  \begin{itemize}
  \item If $k = n$ and $j = 1$, we take $r = 0$.
  \item If $k = n$ and $j > 1$, we take $r := 1$ and $\alpha_1 := \lambda_1 - \lambda_j \in R$; then $\beta + \alpha_1 = \lambda_{\max}$.
  \item If $k < n$ and $j = 1$, we take $r := 1$ and $\alpha_1 := \lambda_k - \lambda_n \in R$; then $\beta + \alpha_1 = \lambda_{\max}$.
  \item If $k < n$ and $1 < j < n$, we take $r := 2$ and $\alpha_1 := \lambda_k - \lambda_n \in R$ and $\alpha_2 := \lambda_1 - \lambda_j \in R$.  Then $\beta + \alpha_1 + \alpha_2 = \lambda_{\max}$.  The partial sum $\beta + \alpha_1 = \lambda_j - \lambda_n$ is always a root, since $j < n$.
  \item If $k < n$ and $j = n$, we take $r := 2$ and $\alpha_1 := \lambda_1 - \lambda_j \in R$ and $\alpha_2 := \lambda_k - \lambda_n \in R$.  Then $\beta + \alpha_1 + \alpha_2 = \lambda_{\max}$.  The partial sum $\beta + \alpha_1 = \lambda_1 - \lambda_k$ is either a root or zero; it is zero iff $k = 1$, in which case
    \begin{equation*}
      \alpha_2(H_{\alpha_1}) = (\lambda_k - \lambda_n)(E_{1 1} - E_{j j}) = 2 \neq 0,
    \end{equation*}
    \begin{equation*}
      \alpha_1(H_{\alpha_2}) = (\lambda_1 - \lambda_j)(E_{k k} - E_{n n}) = 2 \neq 0,
    \end{equation*}
    so the roots $\alpha_1,\alpha_2$ are non-orthogonal in that case, as required.
  \end{itemize}
\end{proof}

\begin{theorem}\label{thm:simplicity-slNC}
  $\mathfrak{g}$ is simple.
\end{theorem}
\begin{proof}
  In lecture we gave a more ``brute force'' proof; here we will clean it up a bit by appeal to Lemmas \ref{lem:compositions-of-ad-stuff} and \ref{lemma:clean-way-to-get-simplicity}.

  Let $\mathfrak{a} \leq \mathfrak{g}$ be a nonzero ideal, or equivalently, an $\ad(\mathfrak{g})$-invariant subspace.  We must show that $\mathfrak{a} = \mathfrak{g}$.

  Suppose first that $\mathfrak{a} \subseteq \mathfrak{h}$, and let $0 \neq H \in \mathfrak{a}$ be given.  Since the set $R$ of roots spans $\mathfrak{h}^*$ (Lemma \ref{lem:roots-span}), we can find $\alpha \in R$ so that $\alpha(H) \neq 0$.  But then $X_\alpha \in \mathfrak{g}^\alpha$ (see \S\ref{sec:roots-sln}) has the property that $[H,X_\alpha] = \alpha(H) X_\alpha$ is a nonzero element of $\mathfrak{g}^\alpha$.  On the other hand, we have $[H,X_\alpha] \in \mathfrak{a} \subseteq \mathfrak{h}$ because $\mathfrak{a}$ is an ideal.  Since $\mathfrak{h} \cap \mathfrak{g}^\alpha = 0$, we obtain the required contradiction.

  Suppose next that $\mathfrak{a}$ is not contained in $\mathfrak{h}$.  By the semisimplicity of $\ad : \mathfrak{h} \rightarrow \End(\mathfrak{g})$ (see Lemma \ref{lem:adjoint-action-cartan-slNC-is-semisimple}) and the fact that $\ad(\mathfrak{h})$-semisimplicity is preserved upon passage to $\ad(\mathfrak{h})$-invariant subspaces (see Exercise \ref{exercise:restriction-of-semisimple-is-semisimple}), we have
  \begin{equation}
    \mathfrak{a} = (\mathfrak{a} \cap \mathfrak{h})
    \oplus (\oplus_{\alpha \in R}
    \mathfrak{a} \cap \mathfrak{g}^\alpha ).
  \end{equation}
  Since $\mathfrak{a}$ is not contained in $\mathfrak{h}$, there is some $\beta \in R$ for which have $\mathfrak{a}$ intersects and hence contains the one-dimensional space $\mathfrak{g}^\beta$.

  We claim now with notation as in Lemma \ref{lemma:clean-way-to-get-simplicity} that $\mathfrak{g}^{\lambda_{\max}} \subseteq \mathfrak{a}$.  To see this, let us write
  \begin{equation}\label{eq:from-beta-to-largest-root}
    \beta + \alpha_1 + \dotsb + \alpha_r
    = \lambda_{\max}
  \end{equation}
  as in Lemma \ref{lemma:clean-way-to-get-simplicity}.  We know that $\mathfrak{a}$ is an ideal, i.e., is $\ad(\mathfrak{g})$-invariant, and that $\mathfrak{a}$ contains $\mathfrak{g}^\beta$, so it will suffice to show that the composition
  \begin{equation}
    \mathfrak{g}^\beta \xrightarrow{X_{\alpha_1}}
    \mathfrak{g}^{\beta+\alpha_1}
    \xrightarrow{X_{\alpha_2}} \dotsb
    \xrightarrow{X_{\alpha_{r-1}}}
    \mathfrak{g}^{\beta+\alpha_1+\dotsb+\alpha_{r-1}}
    \xrightarrow{X_{\alpha_r}}
    \mathfrak{g}^{\lambda_{\max}}
  \end{equation}
  is an isomorphism (because then $\mathfrak{a} \supseteq \mathfrak{g}^{\lambda_{\max}}$), which follows from Lemma \ref{lem:condition-for-chain-of-root-maps-not-to-vanish}.


  % Let $s \in \{1..r\}$.  Since $X_\beta \in \mathfrak{a}$ and $\mathfrak{a}$ is an ideal, we have $[X_\beta, X_{\alpha_1}] \in \mathfrak{a}$.  We will use Lemma \ref{lem:root-space-commutation-relations} in nearly every step of the following arguments.  We see that either
  % \begin{itemize}
  % \item $\beta + \alpha_1$ is a root, in which case $[X_{\beta}, X_{\alpha_1}]$ spans the one-dimensional root space $\mathfrak{g}^{\beta + \alpha_1}$ and hence $\mathfrak{a}$ contains $X_{\beta + \alpha_1}$; or
  % \item $\beta + \alpha_1 = 0$, in which case $[X_\beta, X_{\alpha_1}] = - [X_{\alpha_1}, X_{\beta}]$ must be a nonzero multiple of $H_{\alpha_1}$, and so $[[X_{\beta}, X_{\alpha_1}], X_{\alpha_2}]$ is a nonzero multiple of $[H_{\alpha_1}, X_{\alpha_2}] = \alpha_2(H_{\alpha_1}) X_{\alpha_2}$; by the setup of Lemma \ref{lem:root-space-commutation-relations}, we have $\alpha_2(H_{\alpha_1}) \neq 0$, hence $\mathfrak{a}$ contains the nonzero element $[[X_{\beta}, X_{\alpha_1}], X_{\alpha_2}]$ of $\mathfrak{g}^{\beta+\alpha_1+\alpha_2}$ and thus contains $X_{\beta+\alpha_1+\alpha_2}$.
  % \end{itemize}
  % Continuing in this way, we verify inductively that $\mathfrak{a}$ contains $X_{\beta + \alpha_1 + \dotsb + \alpha_s}$ for all $s=1,2,\dotsc,r$, hence that $\mathfrak{a}$ contains $\lambda_{\max}$, as required.

  Now let $\beta \in R$ be arbitrary; we will show that $\mathfrak{a} \supseteq \mathfrak{g}^\beta$.  We again write \eqref{eq:from-beta-to-largest-root} as in Lemma \ref{lemma:clean-way-to-get-simplicity}.  Since we already know that $\mathfrak{a} \supseteq \mathfrak{g}^{\lambda_{\max}}$ and since $\mathfrak{a}$ is $\ad(\mathfrak{g})$-stable, it will suffice to show that the composition
  \begin{equation}
    \mathfrak{g}^{\lambda_{\max}}
    \xrightarrow{Y_{\alpha_r}}
    \mathfrak{g}^{\beta+\alpha_1+\dotsb+\alpha_{r-1}}
    \xrightarrow{Y_{\alpha_{r-1}}}
    \dotsb
    \xrightarrow{Y_{\alpha_2}}
    \mathfrak{g}^{\beta+\alpha_1}
    \xrightarrow{Y_{\alpha_1}}
    \mathfrak{g}^\beta
  \end{equation}
  is an isomorphism, which follows again from \eqref{eq:nonvanishing-required-for-simplicity} and Lemma \ref{lem:compositions-of-ad-stuff}.


  % By a similar argument using the Similarly, By reversing the above reasoning, we see that $\mathfrak{a}$ contains $X_\beta$ for all $\beta \in R$: Write \eqref{eq:from-beta-to-largest-root} as above.  We have seen that $\mathfrak{a}$ contains $X_{\lambda_{\max}}$.  Since $\mathfrak{a}$ is an ideal, it also contains $[X_{-\alpha_r}, X_{\lambda_{\max}}]$.
  % \begin{itemize}
  % \item If $\beta + \alpha_1 + \dotsb + \alpha_{r-1}$ is a root, then $[Y_{\alpha_r}, X_{\lambda_{\max}}]$ is a nonzero element of the corresponding root space, while
  % \item if it is zero, then it is a nonzero multiple of $H_{\alpha_r}$ (by Lemma \ref{lem:root-space-commutation-relations}), and thus $[Y_{\alpha_{r-1}}, [Y_{\alpha_r}, X_{\lambda_{\max}}]$ is a nonzero multiple of $[Y_{\alpha_{r-1}}, H_{\alpha_r}] =-[H_{\alpha_r}, Y_{\alpha_{r-1}}] = \alpha_{r-1}(H_{\alpha_r}) Y_{\alpha_{r-1}}$.
  % \end{itemize}
  % Continuing in this way as above, we verify by reverse induction on $s = r-1,\dotsc,0$ that $\mathfrak{a}$ contains $X_{\beta + \alpha_1 + \dotsc + \alpha_s}$ and hence contains $\beta$, as claimed.

  In summary, we have seen that $\mathfrak{a}$ contains $\mathfrak{g}^\alpha$ and hence $X_\alpha$ for all $\alpha \in R$; since $\mathfrak{a}$ is an ideal, it contains also $[X_\alpha,Y_\alpha] = H_\alpha$.  Since the $H_\alpha$ span $\mathfrak{h}$ (see \eqref{eq:coroots-span-cartan-slNC} and surrounding), we have $\mathfrak{a} \supseteq \mathfrak{h}$.  Thus $\mathfrak{a} \supseteq \mathfrak{h} \oplus (\oplus_{\alpha \in R} \mathfrak{g}^\alpha)$, i.e., $\mathfrak{a} =\mathfrak{g}$, as required.
\end{proof}

% We can use Lemma \ref{lemma:clean-way-to-get-simplicity} to give a cleaner proof of the key claim \eqref{eq:gamma-equals-R} involved in the proof of Theorem \ref{thm:simplicity-slNC}.  Namely, let $\beta \in R$.  Write
% \begin{equation*}
%   \lambda^{\max} = \beta + \alpha_1 + \dotsb + \alpha_r
% \end{equation*}
% as in the conclusion of Lemma \ref{lemma:clean-way-to-get-simplicity}.  We claim that
% \begin{equation}\label{eq:key-equivalence-involving-Gamma}
%   \beta \in \Gamma \iff \lambda^{\max} \in \Gamma.
% \end{equation}
% From this and the nonemptyness of $\Gamma$, it follows that $\Gamma = R$.

% \begin{theorem}\label{thm:simplicity-slNC}
%   $\mathfrak{g}$ is simple.
% \end{theorem}
% \begin{proof}
%   Let $\mathfrak{a} \leq \mathfrak{g}$ be a nonzero ideal, or equivalently, an $\ad(\mathfrak{g})$-invariant subspace.  We must show that $\mathfrak{a} = \mathfrak{g}$.

%   Suppose first that $\mathfrak{a} \subseteq \mathfrak{h}$, and let $0 \neq H \in \mathfrak{a}$ be given.  Since the set $R$ of roots spans $\mathfrak{h}^*$ (Lemma \ref{lem:roots-span}), we can find $\alpha \in R$ so that $\alpha(H) \neq 0$.  But then the nonzero element $X_\alpha \in \mathfrak{g}^\alpha$ constructed above has the property that $[H,X_\alpha] = \alpha(H) X_\alpha$ is a nonzero element of $\mathfrak{g}^\alpha$.  On the other hand, we have $[H,X_\alpha] \in \mathfrak{a} \subseteq \mathfrak{h}$ because $\mathfrak{a}$ is an ideal.  Since $\mathfrak{h} \cap \mathfrak{g}^\alpha = 0$, we obtain the required contradiction.

%   Suppose next that $\mathfrak{a}$ is not contained in $\mathfrak{h}$.  By the semisimplicity of $\ad : \mathfrak{h} \rightarrow \End(\mathfrak{g})$ (see Lemma \ref{lem:adjoint-action-cartan-slNC-is-semisimple}) and the fact that $\ad(\mathfrak{h})$-semisimplicity is preserved upon passage to $\ad(\mathfrak{h})$-invariant subspaces (see Exercise \ref{exercise:restriction-of-semisimple-is-semisimple}), we have
%   \begin{equation}
%     \mathfrak{a} = (\mathfrak{a} \cap \mathfrak{h})
%     \oplus (\oplus_{\alpha \in R}
%     \mathfrak{a} \cap \mathfrak{g}^\alpha ).
%   \end{equation}
%   (This is not so hard to verify directly by playing around with the proof technique of Lemma \ref{lem:lin-indep-eigenspaces}; it might be instructive to attempt that.)  Since $\mathfrak{a}$ is not contained in $\mathfrak{h}$, there is some $\alpha \in R$ for which have $\mathfrak{a} \cap \mathfrak{g}^\alpha \neq 0$.  Since $\dim (\mathfrak{g}^\alpha) = 1$, we have in fact $\mathfrak{a} \supseteq \mathfrak{g}^\alpha$.
%   %   We observe first using \eqref{lem:root-space-commutation-relations} that \begin{equation}\label{eq:closure-of-gamma-under-adding-roots} \alpha \in \Gamma, \beta \in R, \alpha + \beta \in R \implies \alpha + \beta \in \Gamma \end{equation} and similarly (using that $\beta \in R \implies - \beta \in R$) that \begin{equation}\label{eq:closure-of-gamma-under-adding-roots-2} \alpha \in \Gamma, \beta \in R, \alpha - \beta \in R \implies \alpha - \beta \in \Gamma.  \end{equation}

%   %   Suppose first that $\beta = \lambda_j - \lambda_k$ with $j > k$.
% \end{proof}

% Now that we've given the proof, let's clean it up a bit and abstract away some of the details, as follows:

\subsection{The proof given in lecture}
\label{sec:orgd32f640}
Here are some notes indicating the more ``brute force'' approach to Theorem \ref{thm:simplicity-slNC} presented in lecture.  It may be instructive to compare this approach to that above (they differ primarily in notation).

With notation as in the proof of Theorem \ref{thm:commuting-semisimple-operators}, set 
\begin{equation*}
 \Gamma := \{\alpha \in R : \mathfrak{a} \cap \mathfrak{g}^\alpha \neq 0 \}.
\end{equation*}
Then $\mathfrak{a} \supseteq \oplus_{\alpha \in \Gamma} \mathfrak{g}^\alpha$.  The key step in the proof was to show that if $\Gamma$ is nonempty, then
\begin{equation}\label{eq:gamma-equals-R}
  \Gamma = R.
\end{equation}
To see this, let $\beta \in \Gamma$ be given.  Suppose that $\beta = \lambda_i - \lambda_j$ with $i > j$; a similar but slightly simpler argument applies if instead $i < j$.  We have
\begin{equation}
 [E_{i j}, E_{j n}]
  = 
\begin{cases}
    E_{i n} & \text{ if } i \neq n, \\
    E_{i n} - E_{j j} & \text{ if } i = n,
  \end{cases}
\end{equation}
and in either case it follows that
\begin{equation}
 [E_{1 i}, [E_{i j}, E_{j n}]]
  = E_{1 n}.
\end{equation}
Since $\mathfrak{a}$ is an ideal, we deduce that $E_{1 n} \in \mathfrak{a}$, i.e.,
\begin{equation}
  \lambda_1  - \lambda_n \in \Gamma.
\end{equation}

Now let $\alpha \in R$ be arbitrary, say $\alpha = \lambda_j - \lambda_k$.  If $j < k$, then we have
\begin{equation*}
 [E_{1 n}, E_{n k}] = E_{1 k},
\end{equation*}
hence $\lambda_1 - \lambda_k \in \Gamma$, and
\begin{equation*}
 [E_{j 1}, E_{1 k}]= E_{j k}
\end{equation*}
hence $\lambda_j - \lambda_k \in \Gamma$, as required.  Suppose instead that $j > k$.  If $k = 1$, then we have
\begin{equation*}
 [E_{n 1},E_{1 n}]
  = E_{n n} - E_{1 1}
\end{equation*}
and hence
\begin{equation*}
 [E_{n 1},[E_{n 1},E_{1 n}]]
  = - 2 E_{n 1},
\end{equation*}
hence $\lambda_n - \lambda_1 \in \Gamma$, as required.  In the remaining case that $j > k > 1$, we have
\begin{equation*}
 [E_{1 n}, E_{n k}]
  = E_{1 k}
\end{equation*}
and hence
\begin{equation*}
 [E_{j 1}, [E_{1 n}, E_{n k}]]
  = E_{j k},
\end{equation*}
hence $\lambda_j - \lambda_k \in \Gamma$, as required.

\section{Classification of the classical simple complex Lie algebras\label{sec:classify-classical-simple-algebras}}
\label{sec:orgdbcaaaf}

\subsection{Recap\label{sec:recap-classification-includes-better-defn-of-orthogonal-gp}}
\label{sec:org4260801}
We've seen in lecture that the Lie algebra $\slLie_n(\mathbb{C})$ is simple ($n \geq 2$), and on the homework that $\sp_{2n}(\mathbb{C})$ is simple ($n \geq 1$).  Similar arguments imply that $\so_{2n+1}(\mathbb{C})$ is simple for $n \geq 1$ and that $\so_{2n}(\mathbb{C})$ is simple for $n \geq 3$; the handout (\S\ref{sec:dynkin-diagrams-classical-examples}) from lecture (available now also on the course homepage) describes the root systems, and I'll leave it as an exercise to adapt the techniques used to prove the simplicity of $\slLie_n(\mathbb{C})$ and $\sp_{2n}(\mathbb{C})$ to the orthogonal case.  There is one trick in that case which is very handy.  Recall from \S\ref{sec:low-rank-exceptional-isomorphisms} the notion of a \emph{quadratic space} over $\mathbb{C}$, and that any $n$-dimensional quadratic space is isomorphic to the standard one.  It is convenient to equip $\mathbb{C}^{2 n}$ with the structure of a quadratic space for which the associated non-degenerate symmetric bilinear form $\langle , \rangle$ (denoted $B$ in \S\ref{sec:low-rank-exceptional-isomorphisms}) is given by
\begin{equation}
  \langle x,y \rangle
  = \sum_{i=1}^n
  (x_i y_{n+i} + x_{n+i} y_{i})
\end{equation}
and $\mathbb{C}^{2n+1}$ that for which
\begin{equation}
  \langle x,y \rangle
  = \sum_{i=1}^n
  (x_i y_{n+i} + x_{n+i} y_{i})
  + x_{2n+1} y_{2 n+1}.
\end{equation}
It is then easy to see that $\SO_{m}(\mathbb{C}) := \{g \in \SL_m(\mathbb{C}): \langle g x, g y \rangle \text{ for all } x,y \in \mathbb{C}^m\}$, when defined with respect to the above inner products, contains in the case $m = 2 n$ the diagonal subgroup $H = \{\diag(z_1,\dotsc,z_n,z_1^{-1},\dotsc,z_n^{-1}) : z_1,\dotsc,z_n \in \mathbb{C}^\times\}$ and in the case $m = 2 n+ 1$ the diagonal subgroup $H = \{\diag(z_1,\dotsc,z_n,z_1^{-1},\dotsc,z_n^{-1},1) : z_1,\dotsc,z_n \in \mathbb{C}^\times\}$ whose Lie algebras $\mathfrak{h}$ are given respectively by $\mathfrak{h} = \{\diag(Z_1,\dotsc,Z_n,-Z_1,\dotsc,-Z_n) : Z_1,\dotsc,Z_n \in \mathbb{C}\}$ and $\mathfrak{h} = \{\diag(Z_1,\dotsc,Z_n,-Z_1,\dotsc,-Z_n,0) : Z_1,\dotsc,Z_n \in \mathbb{C}\}$.  The Lie algebra of $\mathfrak{g}$ is described on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}) and will not be repeated here.

\subsection{Classical simple complex Lie algebras}
\label{sec:org617bb1c}
\begin{definition}
  By a \emph{classical simple complex Lie algebra} we shall mean a complex Lie algebra of one of the following forms:
  \begin{itemize}
  \item $A_n := \slLie_{n+1}(\mathbb{C})$ for $n \geq 1$,
  \item $B_n := \so_{2n+1}(\mathbb{C})$ for $n \geq 1$,
  \item $C_n := \sp_{2n}(\mathbb{C})$ for $n \geq 1$,
  \item $D_n := \so_{2n}(\mathbb{C})$ for $n \geq 3$.
  \end{itemize}
\end{definition}
\begin{remark}
  $D_2 \cong A_1 \times A_1$ is not simple (it is a direct sum of two simple Lie algebras).  $D_1 = \so_2(\mathbb{C})$ is not simple (it is abelian).
\end{remark}

Recall that our motivating goal for the past few lectures has been to prove the following theorem:
\begin{theorem}\label{thm:classify-complex-simple-classical}
  There are no isomorphisms between the classical simple complex Lie algebras except possibly those of the form
  \begin{equation}\label{eqn:exceptional-isoms-1}
    A_1 \cong B_1 \cong C_1,
  \end{equation}
  \begin{equation}\label{eqn:exceptional-isoms-2}
    B_2 \cong C_2,
  \end{equation}
  \begin{equation}\label{eqn:exceptional-isoms-3}
    A_3 \cong D_3.
  \end{equation}
\end{theorem}
\begin{remark}
  In fact, the isomorphisms \eqref{eqn:exceptional-isoms-1}, \eqref{eqn:exceptional-isoms-2} and \eqref{eqn:exceptional-isoms-3} all hold.  We have proven that $A_1 \cong B_1$.  It is immediate from the definition that $A_1 \cong C_1$.  We have not yet proven the exceptional isomorphisms \eqref{eqn:exceptional-isoms-2} and \eqref{eqn:exceptional-isoms-3}, but they exist, and are not inordinately complicated to establish.
\end{remark}
\begin{remark}
  We may reformulate Theorem \ref{thm:classify-complex-simple-classical} in terms of the simply-connected complex Lie groups having the indicated Lie algebras.
\end{remark}
\begin{remark}
  We record some motivation for caring about Theorem \ref{thm:classify-complex-simple-classical}.
  \begin{enumerate}
  \item It's interesting in its own right; it's natural to ask for a complete list of isomorphisms between some naturally occurring groups.
  \item The techniques involved in the proof (roots, weights, reflections, Dynkin diagrams, ...)  are very important in all of Lie theory and its applications in other fields of mathematics.  The specific groups involved are also universally important.  Our primary goal is really to introduce those techniques by application to a motivating problem.
  \item Theorem \ref{thm:classify-complex-simple-classical} is weaker than the full classification theorem for \emph{all} complex simple Lie algebras (not just the classical ones), which says that the above list is complete with five exceptions, denoted $G_2, F_4, E_6, E_7, E_8$.  That full classification is not inordinately difficult, but would probably take most of a semester to present properly, and it's very easy to get lost in the middle of it and lose the big picture.  On the other hand, we should be able to complete the proof of Theorem \ref{thm:classify-complex-simple-classical} in a couple lectures.
  \item It takes a lot of experience to gain intuition for working with roots, weights, reflections, etc.  It seems best to introduce them as explicitly as possible.  That way, many properties that would normally require laborious and unenlightening proofs can be discovered by inspection; one can then later learn proofs of such properties that apply more generally.
  \end{enumerate}
\end{remark}

\subsection{How to classify them (without worrying about why it works)\label{sec:how-to-classify}}
\label{sec:orgaf78e16}
We now outline the structure of the proof of Theorem \ref{thm:classify-complex-simple-classical}.  Let $\mathfrak{g}$ be a classical simple complex Lie algebra.  We will attach to $\mathfrak{g}$ a certain oriented multigraph, called a \emph{Dynkin diagram}.  (See the handout (\S\ref{sec:dynkin-diagrams-classical-examples}) for what these look like in all cases.  We explained in class how coincidences between ``small'' Dynkin diagrams explain all of the exceptional isomorphisms \eqref{eqn:exceptional-isoms-1}, \eqref{eqn:exceptional-isoms-2} and \eqref{eqn:exceptional-isoms-3}.)

The procedure by which we will attach the Dynkin diagram will involve several choices.  To make the proof of Theorem \ref{thm:classify-complex-simple-classical} rigorous, we will later have to go back and check that these choices did not affect the final result.

Let us note right away that because $\mathfrak{g}$ is simple, its center (equivalently, the kernel of $\ad : \mathfrak{g} \rightarrow \End(\mathfrak{g})$) is trivial.  Indeed, $\mathfrak{g}$ is non-abelian (by the definition of ``simple''), so the center $\mathfrak{z}$ of $\mathfrak{g}$ satisfies $\mathfrak{z} \neq \mathfrak{g}$.  On the other hand, the center is an ideal; since $\mathfrak{g}$ is simple, we must have $\mathfrak{z} = 0$.  In other words,
\begin{equation}\label{eq:adjoint-map-is-injective}
  \ad : \mathfrak{g} \rightarrow \End(\mathfrak{g})
  \text{ is injective.}
\end{equation}

\begin{enumerate}
\item First, introduce the following definition:
  \begin{definition}\label{defn:cartan-subalgebra-of-simple-lie-algebra}
    Let $\mathfrak{g}$ be a simple complex Lie algebra (the case that $\mathfrak{g}$ is ``classical'' is all we will use for now).  A \emph{Cartan subalgebra}\footnote{ What I've recorded here is not the standard definition of ``Cartan subalgebra,'' but is equivalent to a specialization of that definition, and is convenient for our immediate purposes; we may return to the more general notion later.  } of $\mathfrak{g}$ is a subalgebra $\mathfrak{h}$ of $\mathfrak{g}$ for which
    \begin{enumerate}
    \item $\mathfrak{h}$ is abelian,
    \item $\mathfrak{h}$ consists entirely of $\ad$-semisimple elements (that is to say, for each $X \in \mathfrak{h}$, the linear endomorphism $\ad_X \in \End(\mathfrak{h})$ is diagonalizable, i.e., admits a basis of eigenvectors), and
    \item $\mathfrak{h}$ is its own centralizer: if $X \in \mathfrak{g}$ satisfies $[X,H] = 0$ for all $H \in \mathfrak{h}$, then $X \in \mathfrak{h}$.
    \end{enumerate}
  \end{definition}
  For example, the subalgebra $\mathfrak{h}$ defined on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}) is a Cartan subalgebra, and it turns out that all other Cartan subalgebras are ``conjugate'' to it; we will explain this more in a bit.  The following definition thus depends only upon $\mathfrak{g}$, not upon $\mathfrak{h}$:
  \begin{definition}
    The \emph{rank} of $\mathfrak{g}$ is defined to the dimension of $\mathfrak{h}$.
  \end{definition}

  Let $R$ denote the set of roots for $\ad : \mathfrak{h} \rightarrow \End(\mathfrak{g})$, thus $R$ is the set of all nonzero $\alpha \in \mathfrak{h}^*$ for which the subspace $\mathfrak{g}^\alpha := \{X \in \mathfrak{g} : [H,X] = \alpha(H) X \text{ for all } H \in \mathfrak{h}\}$ of $\mathfrak{g}$ is nonzero.  We describe $R$ explicitly on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}).

  For example, for $\mathfrak{g} = \slLie_n(\mathbb{C})$, we can take for $\mathfrak{h}$ the standard diagonal subalgebra.  We saw in the lecture on the simplicity of $\mathfrak{g}$ that $\mathfrak{h}$ consists entirely of $\ad$-semisimple elements; indeed,
  \begin{equation}\label{eq:more-root-space-decmop-yippee}
    \mathfrak{g} = \mathfrak{h} \oplus (\oplus_{\alpha \in R} \mathfrak{g}^\alpha)
  \end{equation}
  where each space on the RHS is an eigenspace for $\ad(\mathfrak{h})$.  It is clear that $\mathfrak{h}$ is its own centralizer.  Indeed, suppose $Z \in \mathfrak{g}$ commutes with every element of $\mathfrak{h}$.  We can decompose $Z$ using \eqref{eq:more-root-space-decmop-yippee} as a sum $Z_0 + \sum_{\alpha \in R} Z_\alpha$, where $Z_0 \in \mathfrak{h}$ and $Z_\alpha \in \mathfrak{g}^\alpha$.  For each $H \in \mathfrak{h}$, we have $[H,Z] = 0$, by assumption; on the other hand,
  \begin{equation}\label{eq:eigenspace-decomp-relevant-fro-checking-maximality}
 [H,Z]
    = [H,Z_0] + \sum_{\alpha \in R} [H,Z_\alpha]
    = \sum_{\alpha \in R} \alpha(H) Z_\alpha.
  \end{equation}
  Since the $Z_\alpha$ are linearly independent of one another (as they belong to distinct root spaces), we must have $\alpha(H) Z_\alpha = 0$ for all $H \in \mathfrak{h}$.  Since each root $\alpha \in R$ is nonzero, we can find $H \in \mathfrak{h}$ so that $\alpha(H) \neq 0$, thus $Z_\alpha = 0$ for all $\alpha \in R$ and thus $Z = Z_0$ belongs to $\mathfrak{h}$.  Since $Z$ was arbitrary, we conclude that $\mathfrak{h} \subseteq \mathfrak{h} '$, as required.

  More generally, one can verify that the subaglebras $\mathfrak{h}$ defined on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}) in the cases $A_n,B_n,C_n,D_n$ are in fact Cartan subalgebras.  One sees also that $A_n, B_n, C_n, D_n$ have rank $n$.  This explains the indexing.

  We observe (by inspecting each family) that the set $R$ of roots for $(\mathfrak{h},\mathfrak{g})$ has the following properties (noted earlier for $\slLie_n(\mathbb{C})$ and $\spLie_{2n}(\mathbb{C})$):
  \begin{enumerate}
  \item For $\alpha \in R$, one has $\{n \in \mathbb{Z} : n \alpha \in R\} = \{\pm 1\}$.
  \item $\dim \mathfrak{g}^\alpha = 1$ for all $\alpha \in R$.
  \item Let $X_\alpha \in \mathfrak{g}^\alpha$ be nonzero, so that $\mathfrak{g}^\alpha = \mathbb{C} X_\alpha$.  There exists a unique $Y_\alpha \in \mathfrak{g}^{-\alpha}$ so that the element $H_\alpha \in \mathfrak{h}$ defined by $H_\alpha := [X_\alpha,Y_\alpha]$ satisfies $\alpha(H_\alpha) = 2$.
  \item For all $\alpha,\beta \in R$,
    \begin{equation*}
 [\mathfrak{g}^\alpha, \mathfrak{g}^\beta]
      = 
\begin{cases}
        \mathfrak{g}^{\alpha + \beta} & \text{ if } \alpha + \beta \in R \\
        \mathbb{C} H_\alpha  & \text{ if } \alpha + \beta = 0 \\
        0 & \text{ otherwise.}
      \end{cases}
    \end{equation*}
  \end{enumerate}
  Explicit choices for the $X_\alpha, Y_\alpha, H_\alpha$ in all cases are given on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}).

\item Next, we introduce the following definition:
  \begin{definition}
    A \emph{base} (or \emph{simple system} or \emph{system of simple roots}) $S \subseteq R$ is a subset with the following properties:
    \begin{enumerate}
    \item $S$ is a basis of $\mathfrak{h}^*$.
    \item For each $\beta \in R$, if one writes
      \begin{equation}\label{eq:decomp-in-terms-of-simple-roots}
        \beta = \sum_{\alpha \in S} c_\alpha \alpha
      \end{equation}
      with $c_\alpha \in \mathbb{C}$ (as one can, because $S$ is a basis), then the $c_\alpha$ are integers and all have the same sign (i.e., either $c_\alpha \geq 0$ for all $\alpha$ or $c_\alpha \leq 0$ for all $\alpha$).
    \end{enumerate}
  \end{definition}
  On the handout (\S\ref{sec:dynkin-diagrams-classical-examples}), an explicit choice of a simple system $S$ is given for each of the classical complex simple Lie algebras.  There are in fact many possible choices, and we will have to argue later that they are all ``sufficiently equivalent'' for the purposes of the construction to follow.

  The set of \emph{positive roots} (with respect to the given simple system $S$) is the set $R^+$ consisting of all $\beta \in R$ for which in the decomposition \eqref{eq:decomp-in-terms-of-simple-roots}, one has $c_\alpha \geq 0$ for all $\alpha \in S$.  The set $R^-$ of \emph{negative roots} is defined analogously.  One has $R = R^+ \sqcup R^-$ and $R^- = (-R^+) := \{- \alpha : \alpha \in R^+\}$.  It's worth going through the examples and seeing what $R^+$ looks like.  For example, in the case $\mathfrak{g} = \slLie_n(\mathbb{C})$ and for the standard choice of $S$ recorded on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}), $R^+$ consists of those $\alpha$ for which the corresponding root space $\mathfrak{g}^\alpha$ belongs to the space of strictly upper-triangular matrices; by contrast, the negative roots correspond to strictly lower-triangular root spaces.
\item One now writes down the \emph{Cartan matrix} $N = (\alpha(H_\beta))_{\alpha,\beta \in S}$.  This is straightforward, but it's worth working through all of the examples to make sure you understand it.  Two Cartan matrices are \emph{equivalent} if one is obtained from the other by relabeling the indices.  (There is no preferred ordering among elements of the finite set $S$, so this is a natural notion of equivalence.)
\item Finally, for convenience, one converts the Cartan matrix into a \emph{Dynkin diagram}.  This is a finite graph whose vertices are given by the elements of $S$.  (It is convenient in practice because most entries of the Cartan matrix turn out to be zero.)  Given distinct elements $\alpha, \beta \in S$, one sees by inspection that the ordered pair of integers $(\alpha(H_\beta), \beta(H_\alpha))$ is of the following form:
  \begin{enumerate}
  \item $(0,0)$: in this case we draw \emph{no} edges connecting $\alpha,\beta$.
  \item $(-1,-1)$: in this case we draw one undirected edge between $\alpha,\beta$.
  \item $(-2,-1)$: in this case we draw a double edge directed from $\alpha$ to $\beta$; see the handout
  \item $(-1,-2)$: in this case we draw a double edge directed from $\beta$ to $\alpha$; see the handout
  \item $(-3,-1)$: in this case we draw a triple edge directed from $\alpha$ to $\beta$; this case doesn't occur for classical Lie algebras (but does for the exceptional Lie algebra $G_2$)
  \item $(-1,-3)$: in this case we draw a triple edge directed from $\beta$ to $\alpha$; same comments apply.
  \end{enumerate}
  It is obvious that the Dynkin diagram determines the Cartan matrix (and vice-versa, of course).  Dynkin diagrams are nicer to work with because their equivalences are easier to spot.
\end{enumerate}

To complete the proof of Theorem \ref{thm:classify-complex-simple-classical}, we need to check that the Dynkin diagram (up to equivalence, i.e., relabeling of the vertices) is independent of the choice of Cartan subalgebra $\mathfrak{h}$ and simple system $S$ made in the above construction.  This will occupy the next several sections.

\begin{exercise}
  Show that if $\mathfrak{g} := \so_m(\mathbb{C})$ is defined using the \emph{standard} scalar product $\langle x,y \rangle := \sum_{i=1}^m x_i y_i$ on $\mathbb{C}^m$ (as opposed to that in \S\ref{sec:recap-classification-includes-better-defn-of-orthogonal-gp}), so that $\mathfrak{g} = \{X \in \slLie_m(\mathbb{C}) : X^t + X = 0\}$, then $\mathfrak{g}$ contains no nonzero diagonal elements, but that the following subalgebra $\mathfrak{h}$ of $\mathfrak{g}$ is a Cartan subalgebra (see Definition \ref{defn:cartan-subalgebra-of-simple-lie-algebra}): if $m = 2 n$, then
  \begin{equation*}
    \mathfrak{h} = \left\{
      \begin{pmatrix}
        0 & i \lambda_1  & & & & & & \\
        -i \lambda_1 & 0 & & & & & &  \\
          & & 0 & i \lambda_2  & & & &    \\
          & & -i \lambda_2 & 0  & & & &  \\
          & & & & \ddots &    & &    \\
          & & & &  & \ddots   & &  \\
          & & & & & & 0 & i \lambda_n    \\
          & & & & & & -i \lambda_n & 0
      \end{pmatrix}
,
    \right\}
  \end{equation*}
  while if $m = 2 n + 1$, then
  \begin{equation*}
    \mathfrak{h} = \left\{
      \begin{pmatrix}
        0 & i \lambda_1  & & & & & & & \\
        -i \lambda_1 & 0 & & & & & & &  \\
          & & 0 & i \lambda_2  & & & & &   \\
          & & -i \lambda_2 & 0  & & & &  & \\
          & & & & \ddots &    & &  &  \\
          & & & &  & \ddots   & & &  \\
          & & & & & & 0 & i \lambda_n &   \\
          & & & & & & -i \lambda_n & 0 & \\
          & & & & & &  &  & 0
      \end{pmatrix}
.
    \right\}
  \end{equation*}
\end{exercise}

\subsection{Dynkin diagrams of classical simple Lie algebras\label{sec:dynkin-diagrams-classical-examples}}
\label{sec:orga54764a}
As an exercise, reproduce the following in private.

$A_{n-1} : \SL_{n}(\mathbb{C}), \SU(n), \mathfrak{g} = \slLie_{n}(\mathbb{C}) = \{a \in M_n(\mathbb{C}): \trace(a) = 0\}$

$\eps_i := E_{i i}$,
\begin{equation*}
  \mathfrak{h} = \left\{ \sum_{i=1}^n a_i \eps_i : \sum a_i = 0 \right\} = \left\{ H =
    \begin{pmatrix}
      \lambda_1(H) &  &  \\
             & \dotsb  &  \\
             &  & \lambda_n(H)
    \end{pmatrix}
    : \lambda_1(H) + \dotsb + \lambda_n(H) = 0 \right\}
\end{equation*}
\begin{equation*}
  \mathfrak{h}^* = \frac{\mathbb{C} \lambda_1 \oplus \dotsb \oplus \mathbb{C} \lambda_n}{ \mathbb{C} (\lambda_1 + \dotsb + \lambda_n) }, \quad R = \left\{ \pm (\lambda_j - \lambda_k) : j < k \right\}
\end{equation*}
\begin{equation*}
  X_{\lambda_j - \lambda_k} = E_{j k}, \quad Y_{\lambda_j - \lambda_k} = E_{k j}, \quad H_{\lambda_j - \lambda_k} = \eps_j - \eps_k \quad (j \neq k)
\end{equation*}
\begin{equation*}
  S = \left\{ \lambda_1 - \lambda_2, \lambda_2 - \lambda_3, \dotsc, \lambda_{n-1} - \lambda_n \right\}, \quad R^+ = \left\{ (\lambda_j - \lambda_k) : j < k \right\}
\end{equation*}
\begin{equation*}
  N = (\alpha(H_\beta))_{\alpha,\beta \in S} =
  \begin{pmatrix}
    2 & -1 &  & & \\
    -1 & 2 & -1 & & \\
      & -1 & 2 & -1 & \\
      & & -1 & 2 & -1 \\
      & & & -1 & 2
  \end{pmatrix}
  \quad
  \begin{dynkin}
    \dynkinline{1}{0}{2}{0}; \dynkinline{2}{0}{3}{0}; \dynkinline{3}{0}{4}{0}; \dynkinline{4}{0}{5}{0} \foreach \x in {1,...,5} { \dynkindot{\x}{0} }
  \end{dynkin}
  \quad (A_5)
\end{equation*}
% sample calculations:
e.g., $(\lambda_{j+1} - \lambda_j)(H_{\lambda_{j} - \lambda_{j-1}}) = -1$ and $(\lambda_{j} - \lambda_{j-1})(H_{\lambda_{j+1} - \lambda_{j}}) =-1$

\hrule


$B_{n}: \Spin_{2n+1}(\mathbb{C}), \Spin(2n+1),$\footnote{ Defined via the scalar product $\langle x,y \rangle := \sum_{j=1}^n (x_j y_{n+j} + x_{n+j} y_j) + x_{2 n+1} y_{2n+1}$ }
\begin{equation*}
  \mathfrak{g} = \soLie_{2n+1}(\mathbb{C}) = \left\{ 
\begin{pmatrix}
    a & b  & x \\
    c & -a^t & y \\
    -y^t & - x^t & 0
  \end{pmatrix}
 \in M_{2n+1}(\mathbb{C}) :
  \begin{split}
    a,b,c \in M_{n \times n}(\mathbb{C}),
    \\
    x,y \in M_{n \times 1}(\mathbb{C}),
    b^t = -b, c^t = -c
  \end{split}
\right\}
\end{equation*}


$\eps_i := E_{i i} - E_{n+i,n+i}$,
\begin{equation*}
  \mathfrak{h} = \mathbb{C} \eps_1 \oplus \dotsb \oplus \mathbb{C} \eps_n = \left\{ H = 
\begin{pmatrix}
    \lambda_1(H) & 0 & 0 & 0 & 0 & 0 & 0  \\
    0 & \dotsb & 0 & 0 & 0 & 0 & 0  \\
    0 & 0 & \lambda_n(H) & 0 & 0 & 0 & 0  \\
    0 & 0 & 0 & -\lambda_1(H) & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & \dotsb & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 & -\lambda_n(H) & 0  \\
    0 & 0 & 0 & 0 & 0 & 0 & 0
  \end{pmatrix}
\right\}
\end{equation*}
\begin{equation*}
  \mathfrak{h}^* = \mathbb{C} \lambda_1 \oplus \dotsb \oplus \mathbb{C} \lambda_n, \quad R = \left\{ \pm (\lambda_j \pm \lambda_k) : j < k \right\} \sqcup \left\{ \pm \lambda_j \right\}
\end{equation*}
\begin{equation*}
  X_{\lambda_j - \lambda_k} = E_{j k} - E_{n+k,n+j}, \quad Y_{\lambda_j - \lambda_k} = E_{k j} - E_{n+j,n+k}, \quad H_{\lambda_j - \lambda_k} = \eps_j - \eps_k \quad (j \neq k)
\end{equation*}
\begin{equation*}
  X_{\lambda_j + \lambda_k} = E_{j,n+k} - E_{k,n+j}, \quad Y_{\lambda_j + \lambda_k} = - E_{n+j, k} + E_{n+k,j}, \quad H_{\lambda_j + \lambda_k} = \eps_j + \eps_k \quad (j < k)
\end{equation*}
\begin{equation*}
  X_{-\lambda_j - \lambda_k} = E_{n+j,k} - E_{n+k,j}, \quad Y_{-\lambda_j - \lambda_k} = -E_{j, n+k} + E_{k,n+j}, \quad H_{-\lambda_j - \lambda_k} = -\eps_j - \eps_k \quad (j < k)
\end{equation*}
\begin{equation*}
  X_{\lambda_j} = E_{j,2 n + 1} - E_{2n+1,n+j} \quad Y_{\lambda_j} = 2(E_{2 n + 1,j} - E_{n+j,2n+1}), \quad H_{\lambda_j} = 2 \eps_j
\end{equation*}
\begin{equation*}
  X_{-\lambda_j} = E_{2 n + 1,j} - E_{n+j,2n+1} \quad Y_{-\lambda_j} = 2(E_{j,2 n + 1} - E_{2n+1,n+j}), \quad H_{-\lambda_j} = -2 \eps_j
\end{equation*}
\begin{equation*}
  S = \left\{ \lambda_1 - \lambda_2, \lambda_2 - \lambda_3, \dotsc, \lambda_{n-1} - \lambda_n, \lambda_n \right\}, \quad R^+ = \left\{ (\lambda_j \pm \lambda_k) : j < k \right\} \sqcup \left\{ \lambda_j \right\}
\end{equation*}
\begin{equation*}
  N = (\alpha(H_\beta))_{\alpha,\beta \in S} =
  \begin{pmatrix}
    2 & -1 &  & & \\
    -1 & 2 & -1 & & \\
      & -1 & 2 & -1 & \\
      & & -1 & 2 & -2 \\
      & & & -1 & 2
  \end{pmatrix}
  \quad
  \begin{dynkin}
    \dynkinline{1}{0}{2}{0}; \dynkinline{2}{0}{3}{0}; \dynkinline{3}{0}{4}{0}; \dynkindoubleline{4}{0}{5}{0}; \foreach \x in {1,...,5} { \dynkindot{\x}{0} }
  \end{dynkin}
  \quad (B_5)
\end{equation*}
% sample calculations:
% \begin{equation*}
%   (\lambda_{j+1} - \lambda_j)(H_{\lambda_{j} - \lambda_{j-1}}) = -1, \quad (\lambda_{j} - \lambda_{j-1})(H_{\lambda_{j+1} - \lambda_{j}}) = -1,
% \end{equation*}
e.g., $(\lambda_{n})(H_{\lambda_{n-1} - \lambda_{n}}) = -1$ and $(\lambda_{n-1} - \lambda_n)(H_{\lambda_{n}}) = -2$.



\hrule

$C_{n}: \Sp_{2n}(\mathbb{C}), \Sp(2n), \quad \mathfrak{g} = \spLie_{2n}(\mathbb{C}) = \left\{ 
\begin{pmatrix}
  a & b \\
  c & -a^t
\end{pmatrix}
 \in M_{2n}(\mathbb{C}) : b^t = b, c^t = c
\right\}$,

$\eps_i := E_{i i} - E_{n+i,n+i}$,
\begin{equation*}
  \mathfrak{h} = \mathbb{C} \eps_1 \oplus \dotsb \oplus \mathbb{C} \eps_n = \left\{ H = 
\begin{pmatrix}
    \lambda_1(H) & 0 & 0 & 0 & 0 & 0 \\
    0 & \dotsb & 0 & 0 & 0 & 0 \\
    0 & 0 & \lambda_n(H) & 0 & 0 & 0 \\
    0 & 0 & 0 & -\lambda_1(H) & 0 & 0\\
    0 & 0 & 0 & 0 & \dotsb & 0 \\
    0 & 0 & 0 & 0 & 0 & -\lambda_n(H)
  \end{pmatrix}
\right\}
\end{equation*}
\begin{equation*}
  \mathfrak{h}^* = \mathbb{C} \lambda_1 \oplus \dotsb \oplus \mathbb{C} \lambda_n, \quad R = \left\{ \pm (\lambda_j \pm \lambda_k) : j < k \right\} \sqcup \left\{ \pm 2 \lambda_j \right\}
\end{equation*}
\begin{equation*}
  X_{\lambda_j - \lambda_k} = E_{j k} - E_{n+k,n+j}, \quad Y_{\lambda_j - \lambda_k} = E_{k j} - E_{n+j,n+k}, \quad H_{\lambda_j - \lambda_k} = \eps_j - \eps_k \quad (j \neq k)
\end{equation*}
\begin{equation*}
  X_{\lambda_j + \lambda_k} = E_{j,n+k} + E_{k,n+j}, \quad Y_{\lambda_j + \lambda_k} = E_{n+j, k} + E_{n+k,j}, \quad H_{\lambda_j + \lambda_k} = \eps_j + \eps_k \quad (j < k)
\end{equation*}
\begin{equation*}
  X_{-\lambda_j - \lambda_k} = E_{n+j,k} + E_{n+k,j}, \quad Y_{-\lambda_j - \lambda_k} = E_{j, n+k} + E_{k,n+j}, \quad H_{-\lambda_j - \lambda_k} = -\eps_j - \eps_k \quad (j < k)
\end{equation*}
\begin{equation*}
  X_{2 \lambda_j} = E_{j,n+j}, \quad Y_{2 \lambda_j} = E_{n+j,j}, \quad H_{2 \lambda_j} = \eps_j
\end{equation*}
\begin{equation*}
  X_{-2 \lambda_j} = E_{n+j,j}, \quad Y_{-2 \lambda_j} = E_{j,n+j}, \quad H_{-2 \lambda_j} = -\eps_j
\end{equation*}
\begin{equation*}
  S = \left\{ \lambda_1 - \lambda_2, \lambda_2 - \lambda_3, \dotsc, \lambda_{n-1} - \lambda_n, 2 \lambda_n \right\}, \quad R^+ = \left\{ (\lambda_j \pm \lambda_k) : j < k \right\} \sqcup \left\{ 2 \lambda_j \right\}
\end{equation*}
\begin{equation*}
  N = (\alpha(H_\beta))_{\alpha,\beta \in S} =
  \begin{pmatrix}
    2 & -1 &  & & \\
    -1 & 2 & -1 & & \\
      & -1 & 2 & -1 & \\
      & & -1 & 2 & -1 \\
      & & & -2 & 2
  \end{pmatrix}
  \quad
  \begin{dynkin}
    \dynkinline{1}{0}{2}{0}; \dynkinline{2}{0}{3}{0}; \dynkinline{3}{0}{4}{0}; \dynkindoubleline{5}{0}{4}{0}; \foreach \x in {1,...,5} { \dynkindot{\x}{0} }
  \end{dynkin}
  \quad (C_5)
\end{equation*}
e.g., $(2 \lambda_{n})(H_{\lambda_{n-1} - \lambda_{n}}) = -2$ and $(\lambda_{n-1} - \lambda_n)(H_{2 \lambda_{n}}) = -1$



\hrule


\begin{equation*}
D_{n}: \Spin_{2n}(\mathbb{C}), \Spin(2n),\footnote{ Defined via the scalar product $\langle x,y \rangle := \sum_{j=1}^n (x_j y_{n+j} + x_{n+j} y_j)$.  } \quad \mathfrak{g} = \soLie_{2n}(\mathbb{C}) = \left\{ 
\begin{pmatrix}
  a & b   \\
  c & -a^t
\end{pmatrix}
 \in M_{2n}(\mathbb{C}) :
b^t = -b, c^t = -c
\right\},
\end{equation*}
$\eps_i := E_{i i} - E_{n+i,n+i}$,
\begin{equation*}
  \mathfrak{h} = \mathbb{C} \eps_1 \oplus \dotsb \oplus \mathbb{C} \eps_n = \left\{ H = 
\begin{pmatrix}
    \lambda_1(H) & 0 & 0 & 0 & 0 & 0 \\
    0 & \dotsb & 0 & 0 & 0 & 0 \\
    0 & 0 & \lambda_n(H) & 0 & 0 & 0 \\
    0 & 0 & 0 & -\lambda_1(H) & 0 & 0\\
    0 & 0 & 0 & 0 & \dotsb & 0 \\
    0 & 0 & 0 & 0 & 0 & -\lambda_n(H)
  \end{pmatrix}
\right\}
\end{equation*}
\begin{equation*}
  \mathfrak{h}^* = \mathbb{C} \lambda_1 \oplus \dotsb \oplus \mathbb{C} \lambda_n, \quad R = \left\{ \pm (\lambda_j \pm \lambda_k) : j < k \right\}
\end{equation*}
\begin{equation*}
  X_{\lambda_j - \lambda_k} = E_{j k} - E_{n+k,n+j}, \quad Y_{\lambda_j - \lambda_k} = E_{k j} - E_{n+j,n+k}, \quad H_{\lambda_j - \lambda_k} = \eps_j - \eps_k \quad (j \neq k)
\end{equation*}
\begin{equation*}
  X_{\lambda_j + \lambda_k} = E_{j,n+k} - E_{k,n+j}, \quad Y_{\lambda_j + \lambda_k} = - E_{n+j, k} + E_{n+k,j}, \quad H_{\lambda_j + \lambda_k} = \eps_j + \eps_k \quad (j < k)
\end{equation*}
\begin{equation*}
  X_{-\lambda_j - \lambda_k} = E_{n+j,k} - E_{n+k,j}, \quad Y_{-\lambda_j - \lambda_k} = -E_{j, n+k} + E_{k,n+j}, \quad H_{-\lambda_j - \lambda_k} = -\eps_j - \eps_k \quad (j < k)
\end{equation*}
\begin{equation*}
  S = \left\{ \lambda_1 - \lambda_2, \lambda_2 - \lambda_3, \dotsc, \lambda_{n-2} - \lambda_{n-1}, \lambda_{n-1} - \lambda_n, \lambda_{n-1} + \lambda_n \right\}, \quad R^+ = \left\{ (\lambda_j \pm \lambda_k) : j < k \right\}
\end{equation*}
\begin{equation*}
  N = (\alpha(H_\beta))_{\alpha,\beta \in S} =
  \begin{pmatrix}
    2 & -1 &  & & \\
    -1 & 2 & -1 & & \\
      & -1 & 2 & -1 & -1 \\
      & & -1 & 2 & 0 \\
      & & -1 & 0 & 2
  \end{pmatrix}
  \quad
  \begin{dynkin}
    \foreach \x in {1,...,3} { \dynkindot{\x}{0} } \dynkindot{3.5}{.9} \dynkindot{3.5}{-.9} \dynkindot{1}{0} \dynkinline{1}{0}{2}{0} \dynkinline{2}{0}{3}{0} \dynkinline{3}{0}{3.5}{.9} \dynkinline{3}{0}{3.5}{-.9}
  \end{dynkin}
  \quad (D_5)
\end{equation*}
e.g., $(\lambda_{n-2} - \lambda_{n-1})(H_{\lambda_{n-1} - \lambda_{n}}) = (\lambda_{n-2} - \lambda_{n-1})(H_{\lambda_{n-1} + \lambda_{n}}) = (\lambda_{n-1} - \lambda_{n})(H_{\lambda_{n-2} - \lambda_{n-1}}) = (\lambda_{n-1} + \lambda_{n})(H_{\lambda_{n-2} - \lambda_{n-1}}) = -1$ and $(\lambda_{n-1} - \lambda_n)(H_{\lambda_{n-1} + \lambda_n}) = (\lambda_{n-1} + \lambda_n)(H_{\lambda_{n-1} - \lambda_n}) = 0$


\subsection{Classical algebras come with faithful representations and are cut out by anti-involutions\label{sec:classical-action}}
\label{sec:org395e74a}
Let $\mathfrak{g}$ be a classical simple complex Lie algebra.  Then $\mathfrak{g}$ comes equipped with a defining faithful representation $\mathfrak{g} \hookrightarrow \End(V)$, where $V = \mathbb{C}^{n+1}, \mathbb{C}^{2 n+1}, \mathbb{C}^{2 n}, \mathbb{C}^{2 n}$ according as $\mathfrak{g} = A_n, B_n, C_n, D_n$.

We now record a property of $\mathfrak{g}$ that will allow us to give ``ad hoc'' proofs of some assertions in the sections to follow.  Set $\slLie(V) := \{x \in \End(V) : \trace(x) = 0\}$.  There is a linear anti-involution $\sigma : \End(V) \rightarrow \End(V)$ for which
\begin{equation}\label{eqn:sigma-preserves-trace}
  \trace(\sigma(x)) = \trace(x) \text{ for all } x \in \End(V)
\end{equation}
and
\begin{equation}\label{eqn:g-is-what-sigma-anti-fixes}
  \mathfrak{g}
  = \{x \in \slLie(V) : \sigma(x) = -x\}.
\end{equation}
Namely:
\begin{enumerate}
\item If $\mathfrak{g} = A_n$, then we take for $\sigma$ the identity map.
\item If $\mathfrak{g} = B_n, C_n, D_n$, we take
  \begin{equation*}
    \sigma(x) := J^{-1} x^t J
  \end{equation*}
  where
  \begin{equation*}
    J =
    \begin{pmatrix}
      0_{n \times n} & 1_{n \times n} & 0_{n \times 1} \\
      1_{n \times n} & 0_{n \times n} & 0_{n \times 1} \\
      0_{1 \times n} & 0_{1 \times n} & 1_{1 \times 1}
    \end{pmatrix}
    \text{ if } \mathfrak{g} = B_n,
  \end{equation*}
  \begin{equation*}
    J =
    \begin{pmatrix}
      0_{n \times n} & 1_{n \times n} \\
      -1_{n \times n} & 0_{n \times n}
    \end{pmatrix}
    \text{ if } \mathfrak{g} = C_n,
  \end{equation*}
  \begin{equation*}
    J =
    \begin{pmatrix}
      0_{n \times n} & 1_{n \times n} \\
      1_{n \times n} & 0_{n \times n}
    \end{pmatrix}
    \text{ if } \mathfrak{g} = D_n,
  \end{equation*}
\end{enumerate}
By ``linear anti-involution'' we mean that
\begin{equation*}
  \sigma(a x+ b y) = a \sigma(x) + b \sigma(y), \quad \sigma(x y) = \sigma(y) \sigma(x), \quad \sigma(\sigma(x)) = x.
\end{equation*}

\subsection{Diagonalization in classical Lie algebras\label{sec:classical-lie-alg-diagonalization}}
\label{sec:org67896e3}
Let $\mathfrak{g}$ be a classical simple complex Lie algebra and let $g \hookrightarrow \End(V)$ be as in \S\ref{sec:classical-action}.  Let $G$ denote a simply-connected complex Lie group having Lie algebra $\mathfrak{g}$.  (Thus $G$ is one of $\SL_{n+1}(\mathbb{C}), \Spin_{2n+1}(\mathbb{C}), \Sp_{2n}(\mathbb{C}), \Spin_{2n}(\mathbb{C})$ according as we are in case $A_n,B_n,C_n,D_n$.)  Let $x \in \mathfrak{g}$.  We can think of it as a linear transformation $x : V \rightarrow V$.
\begin{lemma}
  If $x : V \rightarrow V$ is semisimple, then there exists $g \in G$ so that $\Ad(g) x \in \mathfrak{g} \subseteq \End(V)$ is diagonal, i.e., represented by a diagonal matrix with respect to the standard basis of $V$.
\end{lemma}
\begin{proof}
  Suppose first that $\mathfrak{g} = \slLie_n(\mathbb{C})$.  Let $\mathcal{B} = (v_1,\dotsc,v_n)$ be a basis of eigenvectors for $x$.  Let $g \in \GL_n(\mathbb{C})$ be the ``change of basis matrix'' from $\mathcal{B}$ to the standard basis.  Then $g x g^{-1}$ is diagonal.  This conclusion is unaffected by replacing $g$ with a scalar multiple; by doing so, we may arrange that $g$ belongs to $\SL_n(\mathbb{C})$.

  TODO (or Exercise): discussion of other cases.

  % Similar arguments apply in the other cases; the proofs are similar to that of the fact that one may diagonalize symmetric matrices by orthogonal transformations.  For example, consider the orthogonal case $\mathfrak{g} = \so_{2 n}(\mathbb{C})$.  Let $x \in \mathfrak{g}$, regarded as an operator $x : V \rightarrow V$.  It has some eigenvector $v$ with eigenvalue $\lambda$.  One has $\langle x v, v \rangle<++>

  % The proof of the standard theorem from linear algebra that one can diagonalize any self-adjoint operator via an orthogonal transformation applies tells us that we may diagonalize $x$ by an element of $g \in \O_m(\mathbb{C})$; we can then adjust by a suitable diagonal matrix with $\pm 1$ entries to get $g \in \SO_m(\mathbb{C})$ with $g x g^{-1}$ diagonal.  Similar arguments apply in the symplectic case.
\end{proof}

\subsection{Semisimplicity of elements of classical Lie algebras}
\label{sec:org00974ae}
Let $\mathfrak{g} \hookrightarrow \End(V)$ be as in \S\ref{sec:classical-action}.  We have seen (see \eqref{eq:adjoint-map-is-injective}) that $\ad : \mathfrak{g} \hookrightarrow \End(\mathfrak{g})$ is another faithful representation.  Given an element $x \in \mathfrak{g}$, it thus makes sense to compare properties of the linear transformation $x : V \rightarrow V$ with those of $\ad_x : \mathfrak{g} \rightarrow \mathfrak{g}$.  The following comparison will be of particular use (see \S\ref{sec:some-lin-alg} to refresh the terminology):
\begin{lemma}\label{lem:classical-simple-semisimple-iff-ad-semisimple}
  $x : V \rightarrow V$ is semisimple if and only if $\ad_x : \mathfrak{g} \rightarrow \mathfrak{g}$ is semisimple.
\end{lemma}
For the proof, it will be convenient to recall a standard fact from linear algebra:
\begin{theorem}\label{thm:jordan-decomp}
  Let $x \in \End(V)$ be a linear endomorphism of a finite-dimensional complex vector space $V$.  Then there exist unique $s,n \in \End(V)$ so that
  \begin{enumerate}
  \item $s$ is semisimple,
  \item $n$ is nilpotent, and
  \item $[s,n] = 0$.
  \end{enumerate}
  Moreover, there exist polynomials $S$ and $N$ (depending upon $x$) so that $S(0) = 0 = N(0)$ and $s = S(x)$ and $n = N(x)$ and so that $S,T$ are both odd (i.e., they are sums of monomials of odd degree).
\end{theorem}
\begin{proof}
  See for instance p40 of the book by Serre on the course reference.  The final condition is not stated there, but may be obtained by inspection of the proof.  (We note that the ``existence'' may be obtained by taking for $s$ the ``diagonal part'' and $n$ the ``off-diagonal part'' of the Jordan normal form of $x$.)
\end{proof}

We turn to the proof of Lemma \ref{lem:classical-simple-semisimple-iff-ad-semisimple} (which wasn't presented correctly in lecture).  Before embarking, we note that the same conclusion holds for any simple complex Lie algebra but with a slightly more complicated proof; since we are focusing on the classical case for now, we will freely make use of the assumption that $\mathfrak{g}$ is classical.  \emph{This proof is not particularly important}; I am including it here only because it is short and suffices for our present focused goal of classifying \emph{classical} simple complex Lie algebras.  (I'll also remark that it may be possible to check it by hand more simply than how I have argued here.)

We observe first that it is easy to see that any $x \in \mathfrak{g}$ for which $x :V \rightarrow V$ is semisimple has the property that $\ad_x : \mathfrak{g} \rightarrow \mathfrak{g}$ is semisimple: we may assume (by \S\ref{sec:classical-lie-alg-diagonalization}) that $x$ is diagonal, in which case the required conclusion is clear by the root space decomposition as computed explicitly in the handout (\S\ref{sec:dynkin-diagrams-classical-examples}).  It remains to establish the converse.

Let $\sigma$ be the anti-involution discussed in \S\ref{sec:classical-action} that defines $\mathfrak{g}$.  Let $x \in \mathfrak{g}$ be any element.  Consider its Jordan decomposition $x = s + n$ in $\End(V)$.  Write $s = S(x), n = N(x)$ as above.  Since $S,N$ are odd, we have $\sigma(S(x)) = S(\sigma(x)) = - S(x)$ and $\sigma(N(x)) = N(\sigma(x)) = - N(x)$, whence $s,n$ also belong to $\mathfrak{g}$.

We've seen already that as $s$ is semisimple, so is $\ad_s$.  Moreover, since $n$ is nilpotent, it's not hard to see that also $\ad_n$ is nilpotent: if $n^m = 0$ for some $m \in \mathbb{Z}_{\geq 1}$, then for any $y \in \mathfrak{g}$, we have $\ad_n^{2 m}(y) = [n,[n,\dotsc,[n,y]]] = 0$, since when we expand out in terms of monomials, we always have at least $m$ copies of $n$ occurring consecutievly.

Finally, since $\ad$ is a Lie algebra morphism, we have $[\ad_s,\ad_n] = \ad_{[s,n]} = \ad_0 = 0$.

In summary, $\ad_s, \ad_n$ satisfy we see that the decomposition $\ad_x = \ad_s + \ad_n$ satisfies the assumptions of Theorem \ref{thm:jordan-decomp}.

Assume finally that $\ad_x$ is semisimple.  Then (by the uniqueness assertion of Theorem \ref{thm:jordan-decomp}) we must have $\ad_x = \ad_s$ and $\ad_n = 0$; since $\ad$ is injective, we must have $n = 0$; therefore $x = s$ is semisimple, as required.

\subsection{Conjugacy of Cartan subalgebras}
\label{sec:orgf4e7f31}
Let $\mathfrak{g}$ be a classical simple complex Lie algebra.  Here's the key to showing the independence of the constructions given above with respect to the choice of $\mathfrak{h}$:
\begin{lemma}\label{lem:conj-cartan-classical-case}
  For any two Cartan subalgebras $\mathfrak{h}, \mathfrak{h} '$ of $\mathfrak{g}$, there exists $g \in G$ so that $\Ad(g) \mathfrak{h} = \mathfrak{h} '$.  (Here we can take for $G$ any Lie group having $\mathfrak{g}$ as its Lie algebra; the simply-connected one will do.  One could alternatively and more naturally take for $G$ the \emph{inner automorphism group} $\Int(\mathfrak{g}) := \langle \exp(\ad_X) : X \in \mathfrak{g} \rangle \leq \Aut(\mathfrak{g}) \leq \GL(\mathfrak{g})$ as defined in lecture.)
\end{lemma}
\begin{proof}
  Recall from \S\ref{sec:classical-action} the embedding $\mathfrak{g} \hookrightarrow \End(V)$.  We have seen in \S\ref{lem:classical-simple-semisimple-iff-ad-semisimple} that an element $x \in \mathfrak{g}$, regarded as a linear transformation $x : V \rightarrow V$, is semisimple if and only if $\ad_x : \mathfrak{g} \rightarrow \mathfrak{g}$ is semisimple.

  We turn to the proof.  It will suffice to consider the case that $\mathfrak{h}$ is the standard diagonal Cartan subalgebra and $\mathfrak{h} '$ is arbitrary.  Since the elements of $\mathfrak{h} '$ are all commuting and $\ad$-semisimple, we may simultaneously diagonalize their adjoint action as a sum of eigenspaces, i.e., we may write down a root space decomposition of $\mathfrak{h} '$; see \S\ref{sec:roots-of-abelian-subalgebra}, and recall that elements of $\mathfrak{h}'$ are $\ad$-semisimple).  Let $Z \in \mathfrak{h}'$ be any element with the property that $\alpha '(Z) \neq 0$ for all roots $\alpha '$ of $\mathfrak{h} '$.  (Such an element exists because each $\alpha '$ is nonzero, and so its kernel has codimension one, and a finite union of codimension one subspaces of a vector space over an infinite field is properly contained in that vector space.)  Then the only elements of $\mathfrak{g}$ that commute with $Z$ are those in $\mathfrak{h} '$ (compare with discussion surrounding \eqref{eq:eigenspace-decomp-relevant-fro-checking-maximality}).  By \S\ref{sec:classical-lie-alg-diagonalization}, there is an element $g \in G$ (the simply-connected complex Lie group having Lie algebra $\mathfrak{g}$) for which $g Z g^{-1} := \Ad(g) Z \in \mathfrak{h}$.  Thus every $H \in \mathfrak{h}$ commutes with $g Z g^{-1}$.  It follows that $Z$ commutes with $g^{-1} H g$ for all $H \in \mathfrak{h}$.  By the property of $Z$ just mentioned, it follows that $\Ad(g^{-1}) \mathfrak{h} \subseteq \mathfrak{h} '$, hence $\mathfrak{h} \subseteq \Ad(g) \mathfrak{h} '$.  By the maximality condition in the definitino of ``Cartan subalgebra,'' it follows that $\mathfrak{h} = \Ad(g) \mathfrak{h} '$, as required.
\end{proof}

\subsection{Interpretation of Cartan matrix in terms of inner products\label{sec:cartan-via-inner-products}}
\label{sec:org8b90669}
Let $\mathfrak{g}$ be one of $A_n, B_n, C_n, D_n$.  We can think of $\mathfrak{h}^*$ as a subspace of $\mathbb{C}^n$ by writing each $\lambda \in \mathfrak{h}^*$ in the form $\lambda = l_1 \lambda_1 + \dotsb + l_n \lambda_n$ and associating to $\lambda$ the element $(l_1,\dotsc,l_n) \in \mathbb{C}^n$.  In the case $\mathfrak{g} = A_n$, we have $\lambda_1 + \dotsb + \lambda_n = 0$, so there is some ambiguity in this assignment; we pin it down by requiring that $l_1 + \dotsb + l_n = 0$.  Using this assignment, we can define an inner product by the formula
\begin{equation*}
  (\lambda,\mu) := l_1 m_1 + \dotsb + l_n m_n \text{ if } \lambda = \sum l_i \lambda_i, \mu = \sum m_j \lambda_j.
\end{equation*}
By inspection of the formulas on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}), we have
\begin{equation*}
  \alpha(H_\beta) = 2 \frac{(\alpha,\beta)}{(\beta,\beta)}.
\end{equation*}
(We will explain this properly later; for now, we stick to the narrow goal of classifying the classical simple complex Lie algebras.)  Thus the Cartan matrix can be described in terms of inner products involving the simple roots.

\subsection{Independence with respect to the choice of simple system}
\label{sec:orgecfb737}
In the context of \S\ref{sec:how-to-classify}, suppose $S, S' \subseteq R$ are two simple systems.  We want to know that the Cartan matrices $N, N'$ that they define are equivalent (i.e., that they coincide after relabeling the indices).

We will do this as follows:
\begin{proposition}\label{prop:existence-of-weyl-element-relating-two-simple-systems}
  Let $S, S' \subseteq R$ be simple systems.  There is a linear transformation $w : \mathfrak{h}^* \rightarrow \mathfrak{h}^*$ that is orthogonal with respect to the pairing $(,)$ on $\mathfrak{h}^*$ defined in \S\ref{sec:cartan-via-inner-products} and for which $w S = S'$.
\end{proposition}
Assuming Proposition \ref{prop:existence-of-weyl-element-relating-two-simple-systems}, we may complete the proof of Theorem \ref{thm:classify-complex-simple-classical} as follows: We need to check that $N, N'$ are equivalent.  Let $\alpha , \beta \in S$.  Then
\begin{align*}
  \alpha(H_\beta)
  &=
    2 \frac{(\alpha,\beta)}{(\beta,\beta)}
  \\
  &=
    2 \frac{(w \alpha,w \beta)}{(w \beta,w\beta)}
  \\
  &=
    (w \alpha)(H_{w \beta}).
\end{align*}
Since $w \alpha$ traverses $S'$ as $\alpha$ traverses $S$, we deduce that $N$ and $N'$ are equivalent, as required.

To complete the proof of Theorem \ref{thm:classify-complex-simple-classical}, it remains only to prove Proposition \ref{prop:existence-of-weyl-element-relating-two-simple-systems}, i.e., to produce $w$.  We do so as follows.  For each $\alpha \in R$, let $s_\alpha : \mathfrak{h}^* \rightarrow \mathfrak{h}^*$ denote the linear transformation given by
\begin{equation*}
  s_\alpha \lambda := \lambda - \langle \lambda|\alpha \rangle \alpha,
\end{equation*}
where
\begin{equation*}
  \langle \lambda|\alpha \rangle := \lambda(H_\alpha) = 2 \frac{(\lambda,\alpha)}{(\alpha,\alpha)}.
\end{equation*}
Geometrically, $s_\alpha$ is a reflection with respect to the hyperplane orthogonal to $\alpha$.  It follows by inspection of the formulas on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}) that
\begin{equation}\label{eq:root-reflections-preserve-roots}
  s_\alpha(R) = R
  \text{ for all } \alpha \in R.
\end{equation}
We will see that this is an astonishingly powerful condition; we will explain it properly in due course.

The $s_\alpha$ may be described as follows, as detailed in lecture:
\begin{itemize}
\item If $\alpha = \lambda_j - \lambda_k$, then $s_\alpha : \lambda_j \mapsto \lambda_k, \lambda_k \mapsto \lambda_j$ (with all other $\lambda_i$ left fixed) Here and henceforth we assume that $j \neq k$.
\item If $\alpha = \lambda_j + \lambda_k$, then $s_\alpha : \lambda_j \mapsto - \lambda_k, \lambda_k \mapsto -\lambda_j$ (with all other $\lambda_i$ left fixed).
\item If $\alpha = \lambda_j$ or $2 \lambda_j$, then $s_\alpha : \lambda_j \mapsto -\lambda_j$ (with all other $\lambda_i$ left fixed).
\end{itemize}


\begin{definition}
  \label{defn:weyl-gp-alg}
  Let $W$ be the group generated by the root reflections $s_\alpha$ for $\alpha \in R$; it is called the \emph{Weyl group} and is finite, as it is a subgroup of the permutation group of the spanning set $R$ for $\mathfrak{h}^*$.  Since it is generated by reflections, it consists of orthogonal transformations.
\end{definition}
As we explained in lecture, it is described as follows:
\begin{itemize}
\item $A_n$: one has $|W| = n!$; for each permutation $j \mapsto j'$ of $(1,2,\dotsc,n)$, one has the element $w \in W$ given bye $\lambda_j \mapsto \lambda_{j'}$.
\item $B_n$: one has $|W| = 2^n n!$; for each permutation $j \mapsto j'$ of $(1,2,\dotsc,n)$ and collection of signs $\pm$ (indexed by $j$), one has the element $w \in W$ given by $\lambda_j \mapsto \pm \lambda_{j'}$.
\item $C_n$: same description as for $B_n$.
\item $D_n$: same description as for $B_n,C_n$, except require that the product of all signs be $+1$ (i.e., that the number of minus signs be even).
\end{itemize}



Now let $S \subseteq R$ be a simple system.  Let $\mathfrak{h}_\mathbb{R}^*$ denote the $\mathbb{R}$-span of $R$, or equivalently, the $\mathbb{R}$-span of the elements $\lambda_1,\dotsc,\lambda_n$; it is a real vector space of dimension $\dim_\mathbb{C}(\mathfrak{h}^*)$.
\begin{definition}\label{defn:partial-order-induced-by-simple-system}
  An element $\lambda \in h_\mathbb{R}^*$ is said to be \emph{$S$-nonnegative} (or simply \emph{nonnegative} when the simple system $S$ is clear by context), denoted $\lambda \geq 0$, if when we write $\lambda = \sum_{\alpha \in S} c_\alpha \alpha$ with each $c_\alpha \in \mathbb{R}$, then we actually have $c_\alpha \geq 0$ fro all $\alpha \in S$.

  Given $\lambda, \mu \in h_\mathbb{R}^*$, we say that $\lambda$ is \emph{$S$-higher than} $\mu$ (or simply \emph{higher than $\mu$} when $S$ is clear), denoted $\lambda \geq \mu$, if $\lambda - \mu$ is nonnegative.

  We write $\lambda > \mu$ if $\lambda \geq \mu$ and $\lambda \neq \mu$, etc.
\end{definition}
\begin{remark}\label{rmk-partial-order-but-can-compare-after-refelcting}
  Note that $\lambda \geq \mu$ defines a \emph{partial order} on $\mathfrak{h}_\mathbb{R}^*$: there are plenty of pairs of elements that are incomparable.  On the other hand, for any $\lambda \in \mathfrak{h}_\mathbb{R}^*$ and $\alpha \in R$, the elements $\lambda$ and $s_\alpha \lambda$ are always comparable: one has $\lambda \geq s_\alpha \lambda$ if and only if $\langle \lambda|\alpha \rangle \alpha \geq 0$.
\end{remark}
\begin{example}
  $R^+$ is precisely the set of $S$-nonnegative elements of $R$.
\end{example}
\begin{definition}\label{defn:dominant}
  An element $\lambda \in \mathfrak{h}_\mathbb{R}^*$ is said to be \emph{$S$-dominant} (or simply \emph{dominant}, when the simple system $S$ is clear by context) provided that any of the following evidently equivalent conditions are satisfied:
  \begin{enumerate}
  \item $\lambda(H_\alpha) \geq 0$ for all $\alpha \in S$.
  \item $(\lambda, \alpha) \geq 0$ for all $\alpha \in S$.
  \item $(\lambda, \alpha) \geq 0$ for all $\alpha \in R^+$.
  \item $\lambda(H_\alpha) \geq 0$ for all $\alpha \in R^+$.
  \item $(\lambda, \alpha) \leq 0$ for all $\alpha \in R^-$.
  \item $\lambda(H_\alpha) \leq 0$ for all $\alpha \in R^-$.
  \item $\lambda \geq s_\alpha \lambda$ for all $\alpha \in S$.
  \item $\lambda \geq s_\alpha \lambda$ for all $\alpha \in R$.
  \end{enumerate}
  [The following equivalences are clear: $(1) \iff (2), (3) \iff (4), (5) \iff (6)$.  We have $(2) \iff (3) \iff (5)$ by linearity of the inner product.  We have $(1) \iff (7)$ and $(4),(6) \iff (7)$ by definition of $s_\alpha$ and the partial relation ``$\geq$.'']
\end{definition}

\begin{definition}
  An element $\lambda \in \mathfrak{h}^*_\mathbb{R}$ is \emph{regular} if $(\lambda,\alpha) \neq 0$ for all $\alpha \in R$.
\end{definition}

It is easy to see that regular dominant elements exist: they are just those elements belonging to a suitable ``upper-right quadrant'' (TODO: explain better).


\begin{example}
  Supopse $S$ is the ``standard'' simple system described on the handout (\S\ref{sec:dynkin-diagrams-classical-examples}).  Then it's easy to see that the elements $\lambda = \sum l_i \lambda_i \in \mathfrak{h}^*_\mathbb{R}$ that are $S$-dominant are precisely those satisfying the following conditions in the respective cases:
  \begin{enumerate}
  \item[$(A_n)$] $l_1 \geq l_2 \geq l_3 \geq \dotsb \geq l_{n-1} \geq l_n$
  \item[$(B_n)$] $l_1 \geq l_2 \geq l_3 \geq \dotsb \geq l_{n-1} \geq l_n \geq 0$
  \item[$(C_n)$] $l_1 \geq l_2 \geq l_3 \geq \dotsb \geq l_{n-1} \geq l_n \geq 0$
  \item[$(D_n)$] $l_1 \geq l_2 \geq l_3 \geq \dotsb \geq l_{n-1} \geq |l_n|$
  \end{enumerate}
  The regular dominant elements are those for which every ``$\geq$'' is actually a strict inequality ``$>$.''
\end{example}

\begin{lemma}\label{lem:recover-S-from-dominant-element}
  Let $S$ be a simple system with associated set $R^+$ of positive roots.  Let $\lambda$ be regular and $S$-dominant.  Then $R^+ = \{\alpha \in R : (\alpha,\lambda) > 0\}$.  Moreover, $S$ is the set of elements $\alpha \in R^+$ that are \emph{indecomposable} in the sense that one cannot write $\alpha = \beta_1 + \dotsb + \beta_k$ for some $\beta_1,\dotsc,\beta_k \in R^+$ with $k \geq 2$.

  In particular, $S$ is determined by any regular $S$-dominant element $\lambda$.
\end{lemma}
\begin{proof}
  The first assertion is clear: if $\alpha \in R$ satisfies $(\alpha,\lambda) > 0$, then in the decomposition $\alpha = \sum_{\beta \in S} c_\beta \beta$ (where the $c_\beta$ are integers all of the same sign, and $(\beta,\lambda) > 0$) we deduce that each $c_\beta \geq 0$, etc.  The second assertion follows immediately from the definition of ``simple system.''
\end{proof}

% \begin{lemma}\label{lem:positive-roots-determine-the-base}
%   Let $S \subseteq R$ be a simple system.  Let $R^+ \subseteq R$ be the associated set of positive roots.  Then $R^+$ determines $S$: precisely, $S$ is the set of \emph{indecomposable} elements $\alpha \in R^+$, namely those that do not admit decompositions of the shape $\alpha = \beta + \gamma$ with $\beta, \gamma \in R^+$.
% \end{lemma}
% \begin{proof}
%   We show first that elements of $S$ are indecomposable.  Thus, let $\alpha \in S$, and suppose that we may write $\alpha = \beta +\gamma$ with $\beta,\gamma \in R^+$.  By staring at the expansions of $\beta, \gamma$ as linear combinations of elements of $S$ with nonnegative integral coefficients (not all zero), we obtain a contradiction.

%   Conversely, let us show that indecomposable elements of $R^+$ belong to $S$???


%   %   Indeed, it is clear that $S$ consists of those elements of $R^+$ that cannot be written as finite linear combinations with positive coefficients of elements
% \end{proof}

% \begin{lemma}\label{lem:regular-dominant-element-determines-positive-roots}
%   Let $S \subseteq R$ be a simple system.  Let $R^+ \subseteq R$ be the associated set of positive roots.  Let $\lambda \in \mathfrak{h}_\mathbb{R}^*$ be regular and $S$-dominant.  Then $\lambda$ determines $R^+$, namely
%   \begin{equation}
%     R^+
%     =
%     \{\alpha \in R :
%     (\alpha,\lambda)  > 0
%     \}
%     = 
%     \{\alpha \in R :
%     (\alpha,\lambda)  \geq 0
%     \}.
%   \end{equation}
% \end{lemma}
% \begin{proof}
%   The rightmost two sets are obviously the same by our assumption that $\lambda$ is regular.

%   If $\alpha \in R^+$, then
%   \begin{equation}\label{eq:decompose-yet-again-into-simple-roots}
%     \alpha = \sum_{\beta \in S} c_\beta  \beta
%   \end{equation}
%   with $c_\beta \geq 0$, hence $(\alpha,\lambda) = \sum c_\beta (\beta,\lambda) \geq 0$.

%   Conversely, if $\alpha \in R$ satisfies $(\alpha,\lambda) > 0$, then we know that the $c_\beta$ in the decomposition \eqref{eq:decompose-yet-again-into-simple-roots} are all $\geq 0$ (since a priori they must all have the same sign).
% \end{proof}


% \begin{corollary}\label{cor:simple-system-determined-by-dominant-elements}
%   A simple system $S$ is determined by (any one of) the regular $S$-dominant elements.
% \end{corollary}
% \begin{proof}
%   Apply Lemma \ref{lem:positive-roots-determine-the-base} and Lemma \ref{lem:regular-dominant-element-determines-positive-roots}.
% \end{proof}

We can now prove Proposition \ref{prop:existence-of-weyl-element-relating-two-simple-systems}.  Let $S_0, S_1$ be two simple systems; we want to show that there is $w \in W$ so that $w S_1 = S_0$.
% Since $W$ is a group, it will suffice to consider the case that $S_0$ is the ``standard'' simple system.
Let $\lambda$ be regular and $S_1$-dominant.  Choose $w \in W$ so that $w \lambda$ is \emph{maximal} with respect to the partial order given by Definition \ref{defn:partial-order-induced-by-simple-system} \emph{with respect to the simple system $S_0$}.  Thus, in paricular, $s_\alpha w \lambda \leq w \lambda$ for all $\alpha \in R$ (cf. Remark \ref{rmk-partial-order-but-can-compare-after-refelcting}).  Actually, we must have $s_\alpha w \lambda < w \lambda$: for if instead we had $s_\alpha w \lambda = w \lambda$, then we'd have $\lambda = w^{-1} s_\alpha w \lambda = s_{w^{-1} \alpha} \lambda$ and so $(w^{-1} \alpha, \lambda) = 0$, contrary to the assumption that $\lambda$ is regular.  It follows from the equivalence of the various conditions in Definition \ref{defn:dominant} that $w \lambda$ is $S_0$-dominant.

In summary, $\lambda$ is $S_1$-dominant and $w \lambda$ is $S_0$-dominant.  Using Lemma \ref{lem:recover-S-from-dominant-element}, it follows easily that $w S_1 = S_0$, as required.  TODO: explain more.
% Let $R_0^+, R_1^+$ denote the sets of positive roots with respect to the simple systems $S_0, S_1$.  Since $w$ is an orthogonal transformation, we deduce that for all $\alpha \in R_1^+$, one has $(w \alpha, \lambda) > 0$.  It follows from Lemma \ref{lem:recover-S-from-dominant-element} that $w R_1^+ \subseteq R_0^+$.  Since $R = R_1^+ \sqcup (-R_1^+) = R_0^+ \sqcup (- R_0^+) = (w R_1^+) \sqcup (-w R_1^+)$, we must have $w R_1^+ = R_0^+$.  We have that $w S_1$ is a simple system contained in $R_0^+$, hence elements of $w S_1$ are indecomposable in $R_0^+$; Lemma \ref{lem:recover-S-from-dominant-element} then implies that $w S_1 \subseteq S_0$ and hence (by comparing cardinalities) that $w S_1 = S_0$.  This completes the proof.

We record a few other facts of independent interest.  (Most of these assertions can be deduced by inspection for the ``standard'' simple system and then deduced for general simple systems from the fact that the Weyl group acts transitively on them; there are also more natural but lengthier proofs that apply more generally.)
\begin{enumerate}
\item Let $S$ be a simple system.  Then the Weyl group is generated by the root reflections $s_\alpha$, $\alpha \in S$.
\item A \emph{Weyl chamber} is a connected component $C$ of the set $\mathfrak{h}_\mathbb{R}^{\reg} := \{\lambda \in \mathfrak{h}_\mathbb{R}: (\alpha, \lambda ) \neq 0 \text{ for all } \alpha \in R\}$ of regular elements.  The set of simple systems $S$ is in natural ($W$-equivariant) bijection with the set of Weyl chambers:
  \begin{enumerate}
  \item Given $S$, one takes for $C$ the set $C := \{\lambda \in \mathfrak{h}_\mathbb{R} : (\alpha,\lambda) > 0 \text{ for all } \alpha \in S\}$ of regular $S$-dominant elements; that set is called (naturally) the \emph{$S$-dominant Weyl chamber}.  (The $S$-dominant Weyl chamber is, of course, a Weyl chamber: it is connected, or even path-connected by straight line segments; it is also maximal among connceted subsets, by (say) the intermediate value theorem.)
  \item Given $C$, one takes for $S$ the set of indecomposable elements in $R^+ := \{\alpha \in R : (\alpha,\lambda) > 0 \}$.
  \end{enumerate}
\item The Weyl group acts \emph{simply transitively} on the set of Weyl chambers, for each pair $S, S'$ of simple systems there exists a unique $w \in W$ for which $w S = S'$.  In particular, if $w \in W$ satisfies $w S = S$ for some simple system $S$, then $w = 1$.  (This follows from the argument given above, together with the empirical observation that the only $w \in W$ which stabilizes the ``standard'' Weyl chamber is $w = 1$.)
\item The Weyl group acts \emph{simply transitively} on the set of simple systems, for each pair $S, S'$ of simple systems there exists a unique $w \in W$ for which $w S = S'$.  In particular, if $w \in W$ satisfies $w S = S$ for some simple system $S$, then $w = 1$.  This follows from the previous few points.
\item Let $S$ be a simple system.  For each $\lambda \in \mathfrak{h}_\mathbb{R}^*$ there exists a \emph{unique} $w \in W$ so that $w \lambda$ is $S$-dominant.
\end{enumerate}

In lecture, we presented (some of) the above material in a slightly different order; namely, we first stated the bijection between simple systems and Weyl chambers (after working out enough examples to make it seem obvious).


\section{Why simple Lie algebras give rise to root systems}
\label{sec:orgbc4b3b3}

\subsection{Overview\label{sec:overview:simple-lie-alg-induce-root-systems}}
\label{sec:orgca704ad}
In the previous section, we explained how Dynkin diagrams may be used to classify the classical complex simple Lie algebras $A_n,B_n,C_n,D_n$.  That explanation involved a fair number of ``empirical observations:''
\begin{enumerate}
\item We observed (without ``explanation'') that Cartan subalgebras $\mathfrak{h}$ of $\mathfrak{g}$ exist and are unique.
\item We observed that the root spaces $\mathfrak{g}^\alpha$ of $\mathfrak{h}$ are one-dimensional and satisfy
  \begin{equation*}
    \alpha \in R \implies \{n \in \mathbb{Z} : n \alpha \in R\} = \{\pm 1\}.
  \end{equation*}
  We observed also that there exist $X_\alpha \in \mathfrak{g}^\alpha, Y_\alpha \in \mathfrak{g}^{-\alpha}$ and $H_\alpha \in \mathfrak{h}$ so that
  \begin{equation*}
 [X_\alpha,Y_\alpha] = H_\alpha
  \end{equation*}
  and
  \begin{equation*}
    \alpha(H_\alpha) = 2
  \end{equation*}
  and
  \begin{equation*}
 [\mathfrak{g}^\alpha, \mathfrak{g}^\beta]
    = 
\begin{cases}
      \mathfrak{g}^{\alpha+\beta} & \alpha + \beta \in R \\
      \mathbb{C} H_\alpha  & \alpha + \beta = 0 \\
      0 & \text{otherwise.}
    \end{cases}
  \end{equation*}
\item We observed the relation
  \begin{equation*}
    \langle \beta | \alpha \rangle := \beta(H_\alpha) = 2 \frac{(\beta,\alpha)}{(\alpha,\alpha)}
  \end{equation*}
  for any $\alpha, \beta \in R$.
\item We observed that the root reflections $s_\alpha : \mathfrak{h}^* \rightarrow \mathfrak{h}^*$ defined for $\alpha \in R$ by
  \begin{equation*}
    s_\alpha (\beta) := \beta - \langle \beta|\alpha \rangle \alpha,
  \end{equation*}
  satisfy $s_\alpha(R) = R$.
\end{enumerate}
We would like now to go back and ``explain'' the above observations a bit more ``conceptually.''  This will involve an application of some properties of representations of $\slLie_2(\mathbb{C})$ that we established long ago.

For the remainder of this section, ``Lie algebra'' always means ``over the complex numbers.''

\subsection{The basic theorem on Cartan subalgebras}
\label{sec:org60618cb}
Recall Definition \ref{defn:cartan-subalgebra-of-simple-lie-algebra}; it applies to any simple Lie algebra $\mathfrak{g}$.
\begin{theorem}\label{thm:cartan-subalgebras}
  \begin{enumerate}
  \item There exists a Cartan subalgebra $\mathfrak{h} \leq \mathfrak{g}$.
  \item Any two Cartan subalgebras $\mathfrak{h}, \mathfrak{h} '$ are conjugate in the sense that for any Lie group $G$ with $\Lie(G) = \mathfrak{g}$ (such as the inner automorphism group $G = \Int(\mathfrak{g})$ as defined in Lemma \ref{lem:conj-cartan-classical-case}), there exists $g \in G$ so that $\Ad(g) \mathfrak{h} ' = \mathfrak{h}$.
  \item There is a scalar product $(,)$ on $\mathfrak{g}$ (i.e., a non-degenerate symmetric bilinear form $\mathfrak{g} \otimes \mathfrak{g} \rightarrow \mathbb{C}$) and a real form $\mathfrak{h}_\mathbb{R} \leq \mathfrak{h}$ with the following properties:
    \begin{enumerate}
    \item The roots $\alpha$ of $\ad : \mathfrak{h} \rightarrow \End(\mathfrak{g})$ satisfy $\alpha(\mathfrak{h}_\mathbb{R}) \subseteq \mathbb{R}$.
    \item The restriction of $(,)$ to $\mathfrak{h}_\mathbb{R}$ is real-valued and positive-definite.
    \item $(,)$ is $\mathfrak{g}$-invariant, i.e., for all $x,y,z \in \mathfrak{g}$,
      \begin{equation*}
        ([z,x],y) + (x,[z,y]) = 0.
      \end{equation*}
      (Think of this condition as the ``$t = 0$ derivative'' of a condition like $(e^{t z} x, e^{t z} y) = (x,y)$.)
    \end{enumerate}
  \end{enumerate}
\end{theorem}

\begin{example}
  \begin{enumerate}
  \item One can take $\mathfrak{g} = \slLie_n(\mathbb{C})$, $\mathfrak{h} \leq \mathfrak{g}$ the diagonal subalgebra, $\mathfrak{h}_\mathbb{R} \leq \mathfrak{h}$ the real Lie subalgebra consisting of elements with real entries, and $(x,y) := \trace(x y)$ for $x,y \in \mathfrak{g}$.  Similar choices apply for all of the classical simple algebras.
  \item For any simple $\mathfrak{g}$, it turns out that one can take $(x,y) := \trace(\ad_x \ad_y)$ (which is called the \emph{Killing form}).
  \end{enumerate}
\end{example}

As noted earlier, Theorem \ref{thm:cartan-subalgebras} is easy to establish for the classical simple algebras.  We will not prove it in general for the following reasons:
\begin{enumerate}
\item The proof might take a couple weeks, and I think that it is not as interesting or useful as the other topics that I plan to cover in the remaining time.
\item The conclusion is not difficult in the primary examples of interest (the classical families).  We may thus interpret it as telling us that we might as well have \emph{defined} a simple Lie algebra to be a simple Lie algebra in the ordinary sense with the additional property that it possesses a Cartan subalgebra satisfying the above properties; such a definition would apply to the primary examples of interest, and the above theorem may be interpreted as giving a weaker condition under which it holds.
\end{enumerate}
A good reference for the proof of Theorem \ref{thm:cartan-subalgebras} is Chapter 3 of Serre's \emph{Complex semisimple Lie algebras}.

In the following sections, we will include in some hypotheses phrases like ``Let $\mathfrak{g}$ be a simple Lie algebra (that satisfies the Cartan subalgebra theorem).''  Theorem \ref{thm:cartan-subalgebras} says that the parenthetical hypothesis is unnecessary; we include it only to keep track of what we have actually proven in the course.

\subsection{Abstract root systems}
\label{sec:orgc2613cc}
\begin{definition}
  \label{defn:abstract-root-system}
  Let $V$ be a finite-dimensional real inner product space.  A \emph{root system} is a subset $R$ of $V$ such that
  \begin{enumerate}
  \item $R$ is finite;
  \item $R$ does not contain $0$;
  \item for any $\alpha,\beta \in R$, the quantity
    \begin{equation*}
      \langle \beta|\alpha \rangle := 2\frac{(\beta,\alpha)}{(\alpha,\alpha)}
    \end{equation*}
    is an integer;
  \item for any $\alpha \in R$, the map
    \begin{equation*}
      s_\alpha : V \rightarrow V
    \end{equation*}
    \begin{equation*}
      s_\alpha(\lambda) := \lambda - \langle \lambda | \alpha \rangle \alpha
    \end{equation*}
    satisfies $s_\alpha(R) = R$.
  \end{enumerate}
  We say that $R$ is \emph{reduced} if for all $\alpha \in R$,
  \begin{equation}\label{eq:reduced-axiom-1}
    \{n \in \mathbb{R} : n \alpha \in R\} = \{\pm 1\}.
  \end{equation}
  The notion of an \emph{isomorphism of root systems} is clear.\footnote{ A morphism is a map that preserves all relevant structure.  An isomorphism is a morphism with a two-sided inverse morphism.  }
\end{definition}

In \S\ref{sec:classify-classical-simple-algebras}, we saw several examples of root systems (without referring to them by that name).

\begin{example}\label{ex:disj-union-root-systems}
  Let $R_1 \subseteq V_1, R_2 \subseteq V_2$ be two root systems.  We may regard $V_1, V_2$ as being embedded in the direct sum inner product space $V := V_1 \oplus V_2$ by the maps $v_1 \mapsto (v_1,0)$, $v_2 \mapsto (0,v_2)$.  We may then define their \emph{disjoint union} $R_1 \cup R_2 \subset V$ to be the set of images of $R_1,R_2$ under such maps.  It is easily seen to define a root system.
\end{example}
\begin{definition}
  A root system $R$ is \emph{irreducible} if it is not isomorphic to a disjoint union of nonempty root systems.
\end{definition}

\subsection{Illustration of the root system axioms}
\label{sec:org44a29a8}
The root system axioms have a number of consequences; we illustrate a few of them here, referring to the second reference by Serre on the course homepage for details and further discussion.

As illustration, let us first verify that the axiom \eqref{eq:reduced-axiom-1} can be weakened (assuming the other axioms) to
\begin{equation}\label{eq:reduced-axiom-2}
  \alpha \in R \implies 2 \alpha \notin R.
\end{equation}
To that end, suppose $\alpha, c \alpha \in R$ for some nonzero scalar $c \in \mathbb{R}$.  Then the quantities
\begin{equation*}
  \langle c \alpha | \alpha \rangle = 2 c, \quad \langle \alpha | c \alpha \rangle = 2 c^{-1}
\end{equation*}
are both integers, hence
\begin{equation}\label{eqn:limited-possibilities-for-c-in-root-system}
  c \in \{\pm 1/2, \pm 1, \pm 2\}.
\end{equation}
It is clear that \eqref{eq:reduced-axiom-2} and \eqref{eqn:limited-possibilities-for-c-in-root-system} imply \eqref{eq:reduced-axiom-1}.

As our next illustration:
\begin{lemma}
  \label{lem:possibilities-for-inner-products-roots}
  Let $\alpha,\beta$ be non-proportional roots (i.e., elements of the given root system $R$ that are not multiples of one another).  Then the unordered pair of integers $\{\langle \alpha|\beta \rangle, \langle \beta|\alpha \rangle\}$ is of the form $\{0,0\}$ or $\{\eps, \eps n\}$ for some $\eps \in \{\pm 1\}$ and $n \in \{1,2,3\}$; in other words, it belongs to the following list:
  \begin{itemize}
  \item $\{0,0\}$
  \item $\{1,1\}$
  \item $\{1,2\}$
  \item $\{1,3\}$
  \item $\{-1,-1\}$
  \item $\{-1,-2\}$
  \item $\{-1,-3\}$
  \end{itemize}
\end{lemma}
\begin{proof}
  By elementary geometry, we have $\langle \alpha |\beta \rangle \langle \beta | \alpha \rangle = 4 \cos^2(\phi)$, where $\pm \phi$ denotes the angle between the vectors $\alpha,\beta$.  Since $\alpha,\beta$ are non-proportional, we have $\cos^2(\phi) < 1$.  If $\cos(\phi) = 0$, then $\alpha,\beta$ are orthogonal and so both quantities are zero.  Otherwise $\langle \alpha|\beta \rangle$ and $\langle \beta|\alpha \rangle$ are integers whose product belongs to $\{1,2,3\}$, for which the only possibilities are those listed.
\end{proof}

\begin{lemma}
  \label{lem:inner-products-between-roots-predict-other-roots}
  Suppose that $\alpha,\beta$ are non-proportional roots.
  \begin{itemize}
  \item If $(\alpha,\beta) < 0$, then $\alpha + \beta$ is a root.
  \item If $(\alpha,\beta) > 0$, then $\alpha - \beta$ is a root.
  \end{itemize}
\end{lemma}
\begin{proof}
  In the first case, we see from Lemma \ref{lem:possibilities-for-inner-products-roots} that after possibly swapping $\alpha$ and $\beta$, we have $\langle \beta |\alpha \rangle = -1$ (and $\langle \alpha | \beta \rangle = - n$ for some $n \in \{1,2,3\}$).  Then $s_\alpha(\beta) = \beta - \left\langle \beta | \alpha \right\rangle \alpha = \beta + \alpha$ is a root thanks to the axiom $s_\alpha(R) = R$.  A similar argument applies in the second case.
\end{proof}

We may define a \emph{base} (or \emph{simple system}) exactly as before to be a subset $S$ of $R$ that is a basis for the underlying inner product space $V$ with the property that for each $\alpha \in R$, the coefficients $c_\beta$ in the expansion $\alpha = \sum_{\beta \in S} c_\beta \beta$ either all belong to $\mathbb{Z}_{\geq 0}$ or all belong to $\mathbb{Z}_{\leq 0}$.  One can show directly from the root system axioms that bases exist and have the properties established previously for the classical families and on the homeworks; see the second reference by Serre on the course webpage for more details.  We note for now just that the observation from the homework that $(\alpha,\beta) \leq 0$ for $\alpha,\beta \in S$ follows from Lemma \ref{lem:inner-products-between-roots-predict-other-roots}: if otherwise $(\alpha,\beta) > 0$, then $\alpha - \beta$ would be a root, contrary to the defining property of the simple system $S$.

One can likewise define the Weyl group $W$ of a root system to be the subgroup of the orthogonal group of $V$ generated by the root reflections $s_\alpha$; the properties we established previously for the root systems arising from classical families can also be established directly from the root system axioms (see Serre for details).

Finally, one can attach to each reduced root system a Cartan matrix and a Dynkin diagram; the diagram turns out to be connected if and only if the root system is irreducible, and one can show (by elaborate application of the root system axioms) that all irreducible reduced root systems belong either to the classical families $A_n, B_n, C_n, D_n$ or belong to an exceptional set $\{G_2,F_2,E_6,E_7,E_8\}$.


\subsection{Simple Lie algebras give rise to root systems\label{sec:simple-lie-alg-give-roots}}
\label{sec:orgfacc967}
The ``unexplained observations'' recorded in \S\ref{sec:overview:simple-lie-alg-induce-root-systems} are all contained in the following result, which is our next target:
\begin{theorem}\label{thm:main-theorem-on-roots-of-simple-algebras}
  Let $\mathfrak{g}$ be a simple Lie algebra (that satisfies the Cartan subalgebra theorem).  Let $\mathfrak{h}$ be a Cartan subalgebra.  Let $R \subseteq \mathfrak{h}_\mathbb{R}^*$ be the set of roots for $\ad : \mathfrak{h} \rightarrow \End(\mathfrak{g})$.  Then $R$ is a reduced root system.  Moreover:
  \begin{enumerate}
  \item For each $\alpha \in R$, one has $\dim \mathfrak{g}^\alpha = 1$.
  \item For each $\alpha \in R$ there is a unique $H_\alpha \in \mathfrak{h}_\mathbb{R}$ with $\alpha(H_\alpha) = 2$ so that for each nonzero $X_\alpha \in \mathfrak{g}^\alpha$ there is a unique $Y_\alpha \in \mathfrak{g}^{-\alpha}$ so that $H_\alpha = [X_\alpha,Y_\alpha]$.
  \item One has
    \begin{equation*}
 [\mathfrak{g}^\alpha, \mathfrak{g}^\beta]
      = 
\begin{cases}
        \mathfrak{g}^{\alpha+\beta} & \alpha + \beta \in R \\
        \mathbb{C} H_\alpha  & \alpha + \beta = 0 \\
        0 & \text{otherwise.}
      \end{cases}
    \end{equation*}
  \end{enumerate}
\end{theorem}

\begin{remark}
  We will discuss the proof of Theorem \ref{thm:main-theorem-on-roots-of-simple-algebras} in detail for the following reasons:
  \begin{enumerate}
  \item Although the proof gives us nothing ``new'' for the classical families (we have already checked all of the conclusions by hand), it tells us \emph{why} they are true, gives a less computational explanation, etc.
  \item The techniques involved in the proof of Theorem \ref{thm:main-theorem-on-roots-of-simple-algebras} are of general use.
  \item The proof of Theorem \ref{thm:main-theorem-on-roots-of-simple-algebras} will give us the opportunity to apply some properties of representations of $\slLie_2(\mathbb{C})$ that we established earlier in the course.
  \end{enumerate}
\end{remark}

\begin{remark}
  One can also establish the following complements to Theorem \ref{thm:main-theorem-on-roots-of-simple-algebras}
  \begin{enumerate}
  \item The root systems arising from simple Lie algebras are irreducible.
  \item Two simple Lie algebras are isomorphic if and only if their associated root systems are isomorphic.
  \item Every irreducible reduced root system arises from a (unique) simple Lie algebra.
  \end{enumerate}
  We might discuss the first couple of these if we have time; see the second reference by Serre on the course homepage for further discussion of the final point.
\end{remark}

\subsection{Some stuff about scalar products and inner products\label{sec:some-stuff-about-scalar-products}}
\label{sec:org3ce571d}
Let $\mathfrak{g}$ be a simple Lie algebra (that satisfies the Cartan subalgebra theorem).  Let $\mathfrak{h}$ be a Cartan subalgebra, and let $(,)$ be an invariant scalar product on $\mathfrak{g}$ that is real-valued and positive-definite on $\mathfrak{h}_\mathbb{R}$.  Since $(,)$ is nondegenerate, it induces a linear isomorphism $\mathfrak{g} \rightarrow \mathfrak{g}^*$ given by $x \mapsto (x,\cdot)$.  We can thus transfer $(,)$ to an scalar product (also denoted $(,)$) on $\mathfrak{g}^*$ by requiring that $((x,\cdot),(y,\cdot)) = (x,y)$ for all $x,y \in \mathfrak{g}$.  We may restrict the scalar product $(,)$ on $\mathfrak{g}^*$ to
\begin{equation*}
  \mathfrak{h}_\mathbb{R}^* := \Hom_\mathbb{R}(\mathfrak{h}_\mathbb{R},\mathbb{R}) \cong \{\lambda \in \mathfrak{h}^* : \lambda(h_\mathbb{R}) \subseteq \mathbb{R}\}.
\end{equation*}
Since the scalar product that we started with on $\mathfrak{g}$ has positive-definite restriction to $\mathfrak{h}_\mathbb{R}$, we know also that the scalar product on $\mathfrak{g}^*$ that we just defined has positive-definite restriction to $\mathfrak{h}_\mathbb{R}^*$, hence defines an inner product on that space.

In what follows, we shall always regard $\mathfrak{h}_\mathbb{R}^*$ as an inner product space with respect to an inner product as constructed above.

\subsection{Some recap on \(\SL(2)\)}
\label{sec:org573a7f4}
Let's recall a few facts we learned long ago.  Recall the standard basis elements
\begin{equation*}
  H = 
\begin{pmatrix}
    1 &  \\
      & -1
  \end{pmatrix}
  ,
  \quad
  X = 
\begin{pmatrix}
    & 1 \\
    & 
  \end{pmatrix}
  ,
  \quad
  Y = 
\begin{pmatrix}
    &  \\
    1 & 
  \end{pmatrix}
\end{equation*}
of $\mathfrak{g} := \slLie_2(\mathbb{C})$, and that any finite-dimensional representation $V$ of $\mathfrak{g}$ breaks up into weight spaces $V = \oplus_{m \in \mathbb{C}} V[m]$ where $V[m] := \{v \in V : H v = m v\}$.  (In other words, $H$ acts semisimply in any finite-dimensional representation.  We proved this awhile ago.  Another quick proof: we can first decompose with respect to the action of the diagonal subgroup of $\SU(2)$, since the latter is compact; the Lie algebra of that subgroup is generated by $H$, so we get a decomposition with respect to $H$.)

Recall that the $m$ for which $V[m] \neq 0$ are called the \emph{weights} of $V$; the spaces $V[m]$ are then called \emph{weight spaces}.

The set of weights of the irreducible representations $W_m$ of dimension $m+1$ is
\begin{equation*}
  \{-m, -m+2,\dotsc,m-4,m-2,m\}.
\end{equation*}
The set of weights of a direct sum of several copies of $W_m$ is the union of the sets of weights of the $W_m$ that occur.  Since any such $V$ is isomorphic to such a finite direct sum, we know a lot about the set of weights.

\begin{lemma}\label{lem:sl2-recap-1}
  The weights are integers.  An integer $m$ is a weight if and only if $-m$ is a weight.
\end{lemma}
\begin{proof}
  This follows (among other ways) from the classification: $V$ is a finite direct sum of copies of the $W_m$, and each of those irreducible representations has the above property.
\end{proof}

\begin{lemma}\label{lem:sl2-recap-2}
  If the weights of $V$ all have the same parity (i.e., are all even or all odd), then the set of weights is of the form
  \begin{equation*}
    \{-m, -m + 2, \dotsc, m-4, m-2, m\}
  \end{equation*}
  for some $m \in \mathbb{Z}_{\geq 0}$.

  In that case, let $\ell$ be any weight of $V$.  Let $p,q \geq 0$ be the largest positive integers for which $\ell - 2 p$ and $\ell + 2 q$ are weights.  Then
  \begin{equation}
    \ell = p- q.
  \end{equation}
\end{lemma}
\begin{proof}
  The first assertion follows from the classification.  For the second assertion, we must have $\ell - 2 p = - m$ and $\ell + 2 q = m$, whence $2 \ell = 2 (p - q)$, as required.
\end{proof}

\begin{lemma}\label{lem:sl2-recap-3}
  If $m, m+2$ are weights of $V$, then the maps $X : V[m] \rightarrow V[m+2]$ and $Y : V[m+2] \rightarrow V[m]$ are not identically zero.

  % If $m$ is a positive weight (thus $m \in \mathbb{Z}_{\geq 1}$ and $V[m] \neq 0$), then $m-2$ is a weight, and the maps $Y : V[m] \rightarrow V[m-2]$ and $X : V[m-2] \rightarrow V[m]$ are nonzero.

  % Similarly, if $m$ is a negative weight (thus $m \in \mathbb{Z}_{\leq -1}$ and $V[m] \neq 0$), then $m+2$ is a weight, and the maps $X : V[m] \rightarrow V[m+2]$ and $Y : V[m+2] \rightarrow V[m]$ are nonzero.
\end{lemma}
\begin{proof}
  Again follows by reducing to the irreducible case and then inspecting.
\end{proof}

\begin{lemma}\label{lem:sl2-recap-4}
  Suppose $V$ has the following properties:
  \begin{enumerate}
  \item The weights of $V$ are all even.
  \item $V[0]$ is one-dimensional.
  \item $V[2]$ is nonzero.
  \item There exists a nonzero element $v \in V[2]$ such that $X v = 0$.
  \end{enumerate}
  Then the set of weights is $\{-2,0,2\}$ and each weight space is one-dimensional.
\end{lemma}
\begin{proof}
  The first three conditions tell us that we must have $V \cong W_m$ for some even integer $m \geq 2$.  The fourth condition implies that $m = 2$.
\end{proof}

\subsection{Proof of Theorem \ref{thm:main-theorem-on-roots-of-simple-algebras}}
\label{sec:orgfd608b2}
Let notation and assumptions be as in the statement of that theorem.  Let $(,)$ denote a $\mathfrak{g}$-invariant scalar product on $\mathfrak{g}$ whose restriction to $\mathfrak{h}_\mathbb{R}$ is real-valued and positive-definite.  (We will only use this final property of the inner product at one point in the proof to follow, and it could be avoided at the cost of a bit more work.)  As in \S\ref{sec:some-stuff-about-scalar-products}, let $(,)$ denote also the inner product induced on $\mathfrak{g}^*$ by duality, whose restriction to $\mathfrak{h}_\mathbb{R}^*$ is then real-valued and positive-definite.  (Revisit the examples of classical families.)  Since $\mathfrak{h}$ is a Cartan subalgebra, we have a root space decomposition
\begin{equation*}
  \mathfrak{g} = \mathfrak{h} \oplus (\oplus_{\alpha \in R} \mathfrak{g}^\alpha)
\end{equation*}
for some finite set $R \subseteq \mathfrak{h}_\mathbb{R}^* - \{0\}$ of roots.  Here $[H,X] = \alpha(H) X$ for all $H \in \mathfrak{h}, X \in \mathfrak{g}^\alpha$.

We verify first that
\begin{equation}
 [\mathfrak{g}^\alpha, \mathfrak{g}^\beta]
  \begin{cases}
    \subseteq \mathfrak{g}^{\alpha+\beta} & \text{ if } \alpha + \beta \in R \\
    \subseteq \mathfrak{h} & \text{ if } \alpha + \beta =0 \\
    = \{0\} & \text{otherwise.}
  \end{cases}
\end{equation}
This is the same verification we've done by now many times: for $x \in \mathfrak{g}^\alpha, y \in \mathfrak{g}^\beta, H \in \mathfrak{h}$, we have by the Jacobi identity
\begin{equation*}
 [H,[x,y]]
  = [[H, x],y] +[x,[H, y]] = \alpha(H) [x,y] + \beta(H) [x,y] = (\alpha + \beta)(H) [x,y],
\end{equation*}
giving what we want.

We show next that
\begin{equation}
  (\mathfrak{g}^\alpha, \mathfrak{g}^\beta) = 0 \text{ unless } \alpha + \beta = 0.
\end{equation}
Indeed, assume $\alpha + \beta \neq 0$.  We must show for $x \in \mathfrak{g}^\alpha, y \in \mathfrak{g}^\beta$ that $(x,y) = 0$.  Choose $H \in \mathfrak{h}$ so that $(\alpha + \beta)(z) \neq 0$.  By the $\mathfrak{g}$-invariance of $(,)$, we then have
\begin{equation*}
  0 = ([H,x],y) + (x,[H,y]) = (\alpha + \beta)(H) \cdot (x,y),
\end{equation*}
hence $(x,y) = 0$, as required.

As a consequence, we see that the decomposition
\begin{equation}
  \mathfrak{g} = \mathfrak{h} \oplus (\oplus_{\pm \alpha \in R}
  \mathfrak{g}^{\alpha} \oplus \mathfrak{g}^{-\alpha})
\end{equation}
is orthogonal with respect to $(,)$.  In particular:
\begin{enumerate}
\item $(,)$ has non-degenerate restriction to $\mathfrak{h} \times \mathfrak{h}$, although this follows already from our assumptions.
\item $(,)$ induces a duality between $\mathfrak{g}^\alpha$ and $\mathfrak{g}^{-\alpha}$.  In particular, $\alpha \in R$ if and only if $- \alpha \in R$.
\end{enumerate}

Let $\mathfrak{h}^* \ni \lambda \mapsto u_\lambda \in \mathfrak{h}$ denote the isomorphism induced by $(,)$, so that $\mu(u_\lambda) = \langle \lambda,\mu \rangle$ for all $\mu \in \mathfrak{h}^*$.  We claim next that
\begin{equation}\label{eqn:key-relation-inner-product-commutators}
 [x,y] = (x,y) u_\alpha.
\end{equation}
For the proof, let $H \in \mathfrak{h}$; since $(,)$ has nondegenerate restriction to $\mathfrak{h}$, it will suffice to verify that
\begin{equation*}
  (H,[x,y]) = (H,(x,y) u_\alpha).
\end{equation*}
Since the $\mathfrak{g}$-invariance gives $(H,[x,y]) = ([H,x],y) = \alpha(H) (x,y)$ and the linearity gives $(H,(x,y) u_\alpha) = (H,u_\alpha) (x,y) = \alpha(H) (x,y)$, we are done.

Note in particular that for $\alpha \in \mathfrak{h}_\mathbb{R}^* - \{0\}$, one has
\begin{equation*}
  0 < (\alpha,\alpha) = \alpha(u_\alpha).
\end{equation*}
Hence for each $\alpha \in R \subseteq \mathfrak{h}_\mathbb{R}^* - \{0\}$, it makes sense to define
\begin{equation*}
  H_\alpha := 2 \frac{u_\alpha}{(\alpha,\alpha)} \in \mathfrak{h}_\mathbb{R}.
\end{equation*}
With this definition, we then have
\begin{equation*}
  \alpha(H_\alpha) = 2.
\end{equation*}
Moreover, let us fix a nonzero element $X_\alpha \in \mathfrak{g}^\alpha$.  Since $\mathfrak{g}^{-\alpha}$ is in duality with $\mathfrak{g}^\alpha$, there then exists $Y_\alpha \in \mathfrak{g}^{-\alpha}$ so that $(X_\alpha,Y_\alpha) = 2 / (\alpha,\alpha)$.  By \eqref{eqn:key-relation-inner-product-commutators}, it follows that
\begin{equation*}
 [X_\alpha,Y_\alpha] = H_\alpha,
\end{equation*}
which is consistent with what we stipulated in the classical examples.

Now let $\mathfrak{s}_\alpha \leq \mathfrak{g}$ denote the three-dimensional vector subspace
\begin{equation*}
  \mathfrak{s}_\alpha := \mathbb{C} X_\alpha \oplus \mathbb{C} H_\alpha \oplus \mathbb{C} Y_\alpha.
\end{equation*}
Recall that $[X_\alpha,X_\alpha] = [Y_\alpha,Y_\alpha] = [H_\alpha,H_\alpha] = 0$ and $[H_\alpha,X_\alpha] = 2 X_\alpha$, $[H_\alpha,Y_\alpha] = -2 Y_\alpha$, $[X_\alpha,Y_\alpha] = H_\alpha$.  It follows that the map
\begin{equation*}
  \phi_\alpha : \slLie_2(\mathbb{C}) \rightarrow \mathfrak{s}_\alpha \subseteq \mathfrak{g}
\end{equation*}
given by sending the standard basis elements
\begin{equation*}
  H = 
\begin{pmatrix}
    1 &  \\
      & -1
  \end{pmatrix}
  ,
  \quad
  X = 
\begin{pmatrix}
    & 1 \\
    & 
  \end{pmatrix}
  ,
  \quad
  Y = 
\begin{pmatrix}
    &  \\
    1 & 
  \end{pmatrix}
\end{equation*}
in the evident way ($H,X,Y \mapsto H_\alpha,X_\alpha,Y_\alpha$) is an isomorphism of Lie algebras.

\begin{lemma}\label{lem:root-spaces-one-dimensional}
  Let $\alpha \in R$.  Then $\dim \mathfrak{g}^\alpha = 1$ and $\{n \in \mathbb{Z} : n \alpha \in R \} = \{\pm 1 \}$.  In particular, $2 \alpha \notin R$.
\end{lemma}
\begin{proof}
  Set
  \begin{equation*}
    V := \mathbb{C} H_\alpha \oplus (\oplus_{n \in \mathbb{Z}_{\neq 0}} \mathfrak{g}^{n \alpha}),
  \end{equation*}
  where by convention $\mathfrak{g}^{n \alpha} := 0$ if $n \alpha \notin R$.  Observe that $V$ is stable under $\mathfrak{s}_\alpha$; this follows from what was shown above, together with the observation that $H_{n \alpha} \in \mathbb{C} H_\alpha$ for all $n$.  We may thus regard $V$ as a representation of $\slLie_2(\mathbb{C})$ via the map $\phi_\alpha$.  Equivalently, the map $\rho : \slLie_2(\mathbb{C}) \rightarrow \End(V)$ is given for $x \in \slLie_2(\mathbb{C})$ and $v \in V$ by
  \begin{equation*}
    \rho(x) v := [\phi_\alpha(x),v].
  \end{equation*}
  The possibly nonzero weight spaces are $V[0] = \mathbb{C} H_\alpha$, which is one-dimensional, and $V[2 n] = \mathfrak{g}^{n \alpha}$ for $n \neq 0$.  We observe that $V[2]$ is nonzero (since it contains $X_\alpha$) and that there exists $v \in V[2]$ for which $\rho(X) v = 0$ (take $v := X_\alpha$ and use that $[X_\alpha,X_\alpha] = 0$).  Lemma \ref{lem:sl2-recap-4} applies, telling us that $V \cong W_2$.  The various conclusions follow from the description of the weight spaces of $W_2$.
\end{proof}

For $\alpha \in R$ and $\lambda \in \mathfrak{h}^*$, set
\begin{equation*}
  \langle \lambda|\alpha \rangle := \lambda(H_\alpha)
\end{equation*}
and define the root reflection
\begin{equation*}
  s_\alpha : \mathfrak{h}^* \rightarrow \mathfrak{h}^*
\end{equation*}
by
\begin{equation*}
  s_\alpha(\lambda) := \lambda - \langle \lambda|\alpha \rangle \alpha.
\end{equation*}
Observe that $s_\alpha(s_\alpha(\lambda)) = \lambda$.
\begin{lemma}
  \begin{equation*}
    \langle \lambda|\alpha \rangle = 2 \frac{(\lambda,\alpha)}{(\alpha,\alpha)}.
  \end{equation*}
\end{lemma}
\begin{proof}
  Well, by definition, we have
  \begin{equation*}
    u_\alpha = \frac{2}{(\alpha,\alpha)} H_\alpha.
  \end{equation*}
  If we apply $\lambda$ to both sides, we get
  \begin{equation*}
    (\lambda,\alpha) = \lambda(u_\alpha) = 2 \frac{\lambda(H_\alpha)}{(\alpha,\alpha)},
  \end{equation*}
  which rearranges to
  \begin{equation*}
    \lambda(H_\alpha) = 2 \frac{(\lambda,\alpha)}{(\alpha,\alpha)},
  \end{equation*}
  as required.
\end{proof}

\begin{lemma}\label{lem:alpha-string-thru-beta-description}
  Let $\alpha, \beta \in R$.  Then $\beta(H_\alpha) \in \mathbb{Z}$ and $s_\alpha(\beta) \in R$.  Thus $s_\alpha(R) = R$.

  Moreover, let $p,q \geq 0$ be the largest nonnegative integers for which $\beta - q \alpha$ and $\beta + p \alpha$ are roots.  Then $\beta + k \alpha$ is a root for all integers $k \in \{-q..p\}$, and we have $\beta(H_\alpha) = p- q$.
\end{lemma}
\begin{proof}
  We argue as in the proof of Lemma \ref{lem:root-spaces-one-dimensional}, but now with
  \begin{equation*}
    V := \oplus_{k \in \mathbb{Z}} \mathfrak{g}^{\beta + k \alpha},
  \end{equation*}
  regarded as an $\slLie_2(\mathbb{C})$-module via $\phi_\alpha$ as before.  The element $H \in \slLie_2(\mathbb{C})$ acts on $\mathfrak{g}^{\beta + k \alpha}$ by the eigenvalues $(\beta + k \alpha)(H_\alpha) = \beta(H_\alpha) + 2 k$; in particular, $\beta(H_\alpha)$ is an $H$-weight of $V$.  By Lemma \ref{lem:sl2-recap-2} (applied to $\ell := \beta(H_\alpha)$), we have $\beta(H_\alpha) = p - q \in \mathbb{Z}$.  The $H$-weight of $\mathfrak{g}^{\beta - \beta(H_\alpha) \alpha}$ is $\beta(H_\alpha) - 2 \beta(H_\alpha) = - \beta(H_\alpha)$, which shows that $\mathfrak{g}^{\beta - \beta(H_\alpha) \alpha} \neq 0$, or equivalently, that $s_\alpha(\beta) \in R$.
\end{proof}

\begin{lemma}
  Let $\alpha,\beta \in R$ such that $\alpha + \beta \in R$.  Then $[\mathfrak{g}^\alpha,\mathfrak{g}^\beta] = \mathfrak{g}^{\alpha+\beta}$.
\end{lemma}
\begin{proof}
  Since the root spaces are all one-dimensional, it suffices to show that the map $\ad_{X_\alpha} : \mathfrak{g}^\beta \rightarrow \mathfrak{g}^{\alpha+\beta}$ is nonzero.  This follows from Lemma \ref{lem:sl2-recap-3} upon taking $V = \oplus \mathfrak{g}^{\beta + k \alpha}$ as above.
\end{proof}

All assertions in Theorem \ref{thm:main-theorem-on-roots-of-simple-algebras} have now been established.  (The uniqueness of $Y_\alpha$ follows from the one-dimensionality of $\mathfrak{g}^{-\alpha}$.)


\begin{remark}
  Lemma \ref{lem:inner-products-between-roots-predict-other-roots} was proved using only the root system axioms.  It may be alternatively deduced ``directly'' from Lemma \ref{lem:alpha-string-thru-beta-description}.
\end{remark}

\section{Serre relations and applications}
\label{sec:org96d3081}

\subsection{Generators and relations for simple complex Lie algebras}
\label{sec:orgd6370a3}
Recall from \S\ref{sec:simple-lie-alg-give-roots} and following that one can\footnote{ We have only proved this for those that satisfy the Cartan subalgebra theorem.  } attach root systems to simple Lie algebras over $\mathbb{C}$.  We also mentioned briefly that root systems can be classified in terms of their Cartan matrices, or equivalently, their Dynkin diagrams.

Conversely, it turns out that one go the other direction: if the root systems of a pair $\mathfrak{g}_1, \mathfrak{g}_2$ of simple Lie algebras over $\mathbb{C}$ are isomorphic, then so are $\mathfrak{g}_1$ and $\mathfrak{g}_2$.  One can prove this fairly directly (see p184 of Onishchik--Vinberg), but a particularly convincing way to see it is via the following theorem of Serre:
\begin{theorem}\label{thm:serre-relations}
  Let $\mathfrak{h}$ be a Cartan subalgebra with associated root system $R$, let $S$ be any simple subsystem, and let $N = (N_{\alpha \beta})_{\alpha,\beta \in S}$ be the Cartan matrix (thus $N_{\alpha \beta} := \alpha(H_\beta)$, say).  Then $\mathfrak{g}$ is generated as a Lie algebra by the symbols $H_\alpha, X_\alpha, Y_\alpha$ ($\alpha \in S$) subject only to the relations: for all $\alpha,\beta \in S$,
  \begin{equation*}
 [H_\alpha,X_\beta] = N_{\alpha \beta} X_\beta, \quad [H_\alpha,Y_\beta] = N_{\alpha \beta} Y_\beta, \quad [X_\alpha,Y_\alpha] = H_\alpha;
  \end{equation*}
  for distinct $\alpha,\beta \in S$,
  \begin{equation*}
 [X_\alpha,Y_\beta] = 0, \quad \ad_{X_\alpha}^{-N_{\alpha \beta}+1}(X_\beta) = 0, \quad \ad_{Y_\alpha}^{-N_{\alpha \beta}+1}(Y_\beta) = 0.
  \end{equation*}
\end{theorem}
The meaning of this is hopefully clear by analogy to presentations and relations for groups; for a more precise statement, see either Onishchik--Vinberg or Serre.


Theorem \ref{thm:serre-relations} immediately implies that the isomorphism class of a simple Lie algebra over $\mathbb{C}$ depends only upon its Cartan matrix.  It has many other applications to be discussed shortly.

\subsection{Semisimple complex Lie algebras}
\label{sec:org051a3bb}
These play a central role in the theory.  They admit several equivalent definitions.  The most convenient one for our immediate purposes is the following:
\begin{definition}
  A Lie algebra $\mathfrak{g}$ over $\mathbb{C}$ is \emph{semisimple} if it is isomorphic to a finite direct sum of simple Lie algebras, or equivalently, if $\mathfrak{g}$ is the direct sum of some finite collection of simple ideals.
\end{definition}

We can define Cartan subalgebras $\mathfrak{h}$ of semisimple Lie algebras $\mathfrak{g}$ over $\mathbb{C}$ just as we did in the simple case.  Moreover, if $\mathfrak{g} = \oplus \mathfrak{g}_i$ with $\mathfrak{g}_i$ simple and containing a Cartan subalgebra $\mathfrak{h}_i$, then we can take $\mathfrak{h} = \oplus \mathfrak{h}_i$.  We can likewise associate root systems in the semisimple case just as in the simple case.  The only difference is that now the root systems we obtain are not irreducible; instead, they decompose as finite disjoint unions (in the sense of Example \ref{ex:disj-union-root-systems}) of irreducible root systems, corresponding to the decomposition of the semisimple Lie algebra as a finite direct sum of simple Lie algebras.

The bijection between:
\begin{itemize}
\item simple Lie algebras over $\mathbb{C}$
\item irreducible reduced root systems
\item connected Dynkin diagrams
\end{itemize}
induces one between:
\begin{itemize}
\item semisimple Lie algebras over $\mathbb{C}$
\item reduced root systems
\item Dynkin diagrams
\end{itemize}

The Serre relations apply just as well in the semisimple case.


There is a nontrivial equivalence which seems worth mentioning up front:
\begin{theorem}
  $\mathfrak{g}$ is semisimple if and only if $\mathfrak{g}$ contains no abelian ideals.
\end{theorem}
Thus being semisimple is in some sense the ``opposite'' of being abelian.  There are other nice criteria for checking semisimplicity of $\mathfrak{g}$ that one can read about in any of the course references (e.g., either by Serre).  One says that there should exist a bilinear form on $\mathfrak{g}$ that is $\ad_\mathfrak{g}$-invariant and non-degenerate.  For the classical examples, the trace form $(x,y) := trace(xy)$ is easily seen to have such properties.  Using this criterion, one can give ``another'' proof that (say) $\slLie_n(\mathbb{C})$ is simple (see \S\ref{sec:simplicity-sln}) by computing the Cartan matrix and checking that the Dynkin diagram is \emph{connected}.

\subsection{Reductive complex Lie algebras}
\label{sec:orgdfba908}
Here is another definition which admits many equivalent characterizations; we again give that which is more convenient for our immediate purposes.
\begin{definition}
  Let $(\mathfrak{g}_i)_{i \in I}$ be a family of Lie algebras.  The \emph{direct sum Lie algebra} is the direct sum vector space $\mathfrak{g} := \oplus_{i \in I} \mathfrak{g}_i$ equipped with the Lie bracket characterized by:
  \begin{itemize}
  \item for $i \in I$ and $x,y \in \mathfrak{g}_i \hookrightarrow \mathfrak{g}$, one has $[x,y]_{\mathfrak{g}} := [x,y]_{\mathfrak{g}_i}$;
  \item for $i \neq j \in I$ and $x \in \mathfrak{g}_i, y \in \mathfrak{g}_j$, one has $[x,y]_{\mathfrak{g}} := 0$.
  \end{itemize}
  It has the universal property: $\Hom(\oplus \mathfrak{g}_i, \mathfrak{h}) = \prod \Hom(\mathfrak{g}_i,\mathfrak{h})$ for all Lie algebras $\mathfrak{h}$.
\end{definition}
\begin{definition}
  A finite-dimensional Lie algebra $\mathfrak{g}$ over $\mathbb{C}$ is \emph{reductive} if it is a direct sum of an abelian Lie algebra and a semisimple Lie algebra.
\end{definition}
For example, $\gl_n(\mathbb{C})$ is reductive (it is the direct sum of $\sl_n(\mathbb{C})$ and the central subalgebra $\mathfrak{z}$ consisting of scalar matrices), but not semisimple (it contains the abelian ideal $\mathfrak{z}$).

Abelian Lie algebras over any field are classified by their dimension, so the classification of semisimple Lie algebras readily induces a classification of reductive Lie algebras.

Here's a handy and clarifying lemma, proved in lecture and left here as an exercise (ask me if it's unclear):
\begin{lemma}\label{lem:reductive-iff-reducible}
  A complex Lie algebra $\mathfrak{g}$ is reductive if and only if its adjoint representation $\ad : \mathfrak{g} \rightarrow \End(\mathfrak{g})$ is completely reducible.
\end{lemma}

We can define Cartan subalgebras $\mathfrak{h}$ of reductive Lie algebras $\mathfrak{g}$ just as we did in the semisimple case.  (They always contain the center $\mathfrak{z}$.)  We can also define the set $R$ of roots.  The only difference with the semisimple case is now that the roots $R$ need not span $\mathfrak{h}^*$.  For example, if $\mathfrak{g}$ is abelian, then $R = \emptyset$.

\subsection{Compact complex Lie groups}
\label{sec:orgdfd4c01}
We don't talk about these much.  There's a good reason:
\begin{theorem}
  Any compact connected complex Lie group $G$ is abelian.
\end{theorem}
\begin{proof}
  Consider $\Ad : G \rightarrow \GL(\mathfrak{g})$.  It is a holomorphic matrix-valued function.  Since $G$ is compact, it is bounded.  By Liouville's theorem, it must be constant.  But it preserves the identity, and so must be trivial.  Since $G$ is connected, we conclude that it must be abelian.
\end{proof}
Such $G$ typically go instead by the name ``abelian variety'' and have an interesting theory orthogonal to the primary aims of this course.

\subsection{Compact real Lie algebras}
\label{sec:orgc85f06b}
From now on, when I write ``compact Lie group,'' I mean ``compact real Lie group.''

Think of your favorite compact Lie group $K$ (e.g., $K = \U(n)$).  Consider its Lie algebra $\mathfrak{k}$.  How would you go about telling just from $\mathfrak{k}$ that $K$ was compact?
\begin{definition}
  Let $\mathfrak{k}$ be a real Lie algebra.  (Every Lie algebra here and for the rest of the course should be assumed finite-dimensional.)  We call $\mathfrak{k}$ \emph{compact} if it admits an $\ad(\mathfrak{k})$-invariant inner product, that is to say, a positive definite symmetric bilinear form $(,) : \mathfrak{k} \otimes \mathfrak{k} \rightarrow \mathbb{R}$ with the property that
  \begin{equation*}
    ([z,x],y) + (x,[z,y]) = 0
  \end{equation*}
  for all $x,y,z \in \mathfrak{k}$.
\end{definition}

\begin{lemma}
  Let $K$ be a compact Lie group.  Then $\mathfrak{k}$ is compact.
\end{lemma}
\begin{proof}
  Start with any inner product $(,)_0$ on $\mathfrak{k}$.  Average it under $\Ad(K)$ with respect to an invariant measure, as in the discussion of the unitary trick earlier in the course.  Call $(,)$ the averaged inner product so obtained (it is still an inner product).  Then $(,)$ is $\Ad(K)$-invariant, by construction.  By differentiating, we see that it is $\ad(\mathfrak{k})$-invariant, as required.
\end{proof}

\begin{example}\label{example:abelian-real-is-compact}
  If $\mathfrak{k}$ is an abelian real Lie algebra, then it is compact.  This is easy to see directly.  Alternatively, we can write $\mathfrak{k} = \mathbb{R}^n$ and apply the Lemma to $K = (\mathbb{R}/\mathbb{Z})^n$.
\end{example}

\subsection{Complex reductive vs. compact real Lie algebras}
\label{sec:org00eed09}
Recall our discussion of complexifications and real forms from \S\ref{sec:unitary-trick}.

We include the following mainly as a bridge from our discussion of complex simple Lie algebras to our next target (compact Lie groups).

\begin{theorem}
  \begin{enumerate}
  \item Let $\mathfrak{k}$ be a compact real Lie algebra.  Then its complexification $\mathfrak{k}_\mathbb{C} := \mathfrak{k} \otimes \mathbb{C}$ is a reductive complex Lie algebra.
  \item Let $\mathfrak{g}$ be a complex reductive Lie algebra.  Then it has a compact real form $\mathfrak{k}$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
  \item By Lemma \ref{lem:reductive-iff-reducible}, we have to show that the adjoint representation $\ad : \mathfrak{k} _\mathbb{C} \rightarrow \End(\mathfrak{k}_\mathbb{C})$ is completely reducible.  By linear algebra (cf. Example \ref{ex:complex-vs-real-reps}), it suffices to show that $\ad : \mathfrak{k} \rightarrow \End(\mathfrak{k})$ is completely reducible.  To that end, we make use of the existence of a $\mathfrak{k}$-invariant inner product and argue using orthogonal complements as in Example \ref{example:invariant-inner-product-implies-complete-irreducibility}.
  \item Note first that if result holds for $\mathfrak{g}_1$ and $\mathfrak{g}_2$, then it also holds for their direct sum (take $\mathfrak{k}_1 \oplus \mathfrak{k}_2 \subseteq \mathfrak{g}_1 \oplus \mathfrak{g}_2$).  Since $\mathfrak{g}$ is reductive, it suffices to consider separately the case that $\mathfrak{g}$ is abelian and the case that $\mathfrak{g}$ is semisimple (or indeed, simple).  The abelian case is easy (see Example \ref{example:abelian-real-is-compact}), so we focus henceforth on the semisimple case.

    Think of the prototypical example $\mathfrak{g} = \slLie_2(\mathbb{C})$.  How would one go about ``discovering'' the compact real form $\mathfrak{k} = \su(2)$?  Well, we have
    \begin{equation*}
      \mathfrak{k} = \{Z \in \mathfrak{g} : \sigma(Z) = Z\},
    \end{equation*}
    where $\sigma(Z) := -\overline{Z}^t$.  On the standard basis elements $X,Y,H$ this involution is given by
    \begin{equation*}
      \sigma(X) = -Y, \quad \sigma(Y) = -X, \quad \sigma(H) = -H.
    \end{equation*}
    It is anti-linear in the sense that
    \begin{equation}\label{eq:anti-linearity-of-sigma}
      \sigma(t Z) = \overline{t} \sigma(Z) \text{ for all }
      t \in \mathbb{C}, Z \in \mathfrak{g}.
    \end{equation}
    Also, let's note in this case for the modified Killing form
    \begin{equation}\label{eq:modified-killing-form}
      (x,y) := -\trace(\ad(x) \ad(\sigma(y)))
    \end{equation}
    on $\mathfrak{g}$, the basis $X,Y,H$ is orthogormal (i.e., $(X,Y) = (X,H) = (Y,H) = 0$), and also $(X,X) = (Y,Y) = (H,H) = 2$.  Thus $(,)$ is positive-definite; it is also clearly $\mathfrak{k}$-invariant.  (Note: I defined $(,)$ incorrectly in lecture.)

    This suggests the general strategy.  Let $\mathfrak{g}$ be a semisimple Lie algebra.  We use the Serre relations.  It thus has generators $X_\alpha, Y_\alpha, H_\alpha$ ($\alpha \in S$) satisfying some explicit relations.  We try to define an anti-linear involution $\sigma$ on $\mathfrak{g}$ by requiring that \eqref{eq:anti-linearity-of-sigma} hold and that on the generators, one has
    \begin{equation*}
      \sigma(X_\alpha) = -Y_\alpha, \quad \sigma(Y_\alpha) = -X_\alpha, \quad \sigma(H_\alpha) = -H_\alpha.
    \end{equation*}
    To check that this definition makes sense (i.e., extends from the generators to a real Lie algebra automorphism), we just need to check that it preserves the Serre relations, which is clear.  By Remark \ref{rmk:real-forms-vs-involutions}, we know that $\mathfrak{k} := \{X \in \mathfrak{g} : \sigma(X) = X\}$ is a real form.  We now define $(,)$ on $\mathfrak{g}$ by \eqref{eq:modified-killing-form} and check that it is positive definite on $\mathfrak{k}$ to see that $\mathfrak{k}$ is compact.
  \end{enumerate}
\end{proof}

\section{The center and fundamental group of a compact Lie group\label{sec:center-pi1-compact}}
\label{sec:orge2e02c5}
We spent most of the lecture stating and motivating the truth of one theorem.

We record the key definitions here for now; we will have more to say about them later.

Let $K$ be a compact Lie group.  Let $\mathfrak{k} := \Lie(K)$; it is a compact real Lie algebra.  Let $\mathfrak{g}$ denote its complexification; it is a reductive complex Lie algebra.  Let $\mathfrak{h} \leq \mathfrak{g}$ be a Cartan subalgebra, and suppose that $\mathfrak{t} = \mathfrak{k} \cap \mathfrak{t}$ is a real form of $\mathfrak{h}$, thus $\dim_\mathbb{R}(\mathfrak{t}) = \dim_\mathbb{C}(\mathfrak{h})$; we can arrange this using the presentation given by the Serre generators, for instance.  Define $\mathfrak{h}_\mathbb{R} := i \mathfrak{t}$ and $\mathfrak{h}_\mathbb{Z} := \ker(e)$, where $e : \mathfrak{h}_\mathbb{R} \rightarrow K$ is the map $e(x) := \exp(2 \pi i x)$.  Let $R$ be the root system of $\mathfrak{h}$.  Then $R \subseteq \mathfrak{h}_\mathbb{R}^*$.  Set $R^\wedge := \{\alpha^\wedge : \alpha \in R\}$, where $\alpha^\wedge = H_\alpha$.  Set
\begin{equation*}
  \mathfrak{h}_\mathbb{R}^* := \Hom_\mathbb{R}(\mathfrak{h}_\mathbb{R},\mathbb{R}) \cong \{\lambda \in \mathfrak{h}^* : \lambda(\mathfrak{h}_\mathbb{R}) \subseteq \mathbb{R} \}
\end{equation*}
and
\begin{equation*}
  \mathfrak{h}_\mathbb{Z}^* := \Hom_\mathbb{Z}(\mathfrak{h}_\mathbb{Z},\mathbb{Z}) \cong \{\lambda \in \mathfrak{h}_{\mathbb{R}}^* : \lambda(\mathfrak{h}_\mathbb{Z}) \subseteq \mathbb{Z} \} \cong \{\lambda \in \mathfrak{h}^* : \lambda(\mathfrak{h}_\mathbb{Z}) \subseteq \mathbb{Z} \}.
\end{equation*}
We then have
\begin{equation}
  \mathbb{Z} R
  \subseteq \mathfrak{h}_\mathbb{Z}^*
  \subseteq (\mathbb{Z} R^\wedge)^*
\end{equation}
(called respectively the \emph{root lattice}, the \emph{integers}, and the \emph{weight lattice}) and
\begin{equation}
  \mathbb{Z} R^\wedge
  \subseteq \mathfrak{h}_\mathbb{Z}
  \subseteq (\mathbb{Z} R)^*,
\end{equation}
(called respectively the \emph{coroot lattice}, the \emph{integers}, and the \emph{coweight lattice}), where $\mathbb{Z} R$ denotes the $\mathbb{Z}$-span of $R$, $\mathbb{Z} R^\wedge$ denotes the $\mathbb{Z}$-span of $R^\wedge$,
\begin{equation*}
  (\mathbb{Z} R^\wedge)^* := \{\lambda \in \mathfrak{h}_\mathbb{R}^* : \lambda(R^\wedge) \subseteq \mathbb{Z} \}
\end{equation*}
and
\begin{equation*}
  (\mathbb{Z} R)^* := \{H \in \mathfrak{h}_\mathbb{R} : R(H) \subseteq \mathbb{Z} \}.
\end{equation*}
Pontryagin duality for finite abelian groups give us non-canonical isomorphisms
\begin{equation}
  \mathfrak{h}_\mathbb{Z}/
  \mathbb{Z} R^\wedge
  \cong (\mathbb{Z} R)^* /  \mathfrak{h}_\mathbb{Z}^*,
  \quad
  (\mathbb{Z} R)^*
  / \mathfrak{h}_\mathbb{Z}
  \cong
  \mathfrak{h}_\mathbb{Z}^*
  /
  \mathbb{Z} R.
\end{equation}
\begin{theorem}
  The induced map $e : (\mathbb{Z} R)^* / \mathfrak{h}_\mathbb{Z} \rightarrow \Center(K)$ given by $e(x) := \exp(2 \pi i x)$ is a well-defined isomorphism.

  The map $f : \mathfrak{h}_\mathbb{Z} / \mathbb{Z} R^\wedge \rightarrow \pi_1(K)$, sending $H$ to the homotopy class $[\gamma]$ of the path $\gamma$ given by $\gamma(t) := e(t H)$, is a well-defined isomorphism.
\end{theorem}
We then explained in detail how this ``recovers'' the fact that $\pi_1(\SU(n)) =\{1\}$, $\Center(\SU(n)) \cong \mathbb{Z}/n$.

The theorem will take a bit of preparation to prove; we start in the next section.

\section{Tori in compact Lie groups\label{sec:tori-compact-lie-gps}}
\label{sec:orga4a1522}

\subsection{Basic definitions}
\label{sec:org2b9a950}
\begin{definition}
  A \emph{torus} is a Lie group isomorphic to $T^k := (\mathbb{R}/\mathbb{Z})^k$ for some $k \in \mathbb{Z}_{\geq 0}$.
\end{definition}
\begin{lemma}\label{lem:Characterize-tori}
  Let $G$ be a Lie group.  The following are equivalent.
  \begin{enumerate}
  \item $G$ is a torus.
  \item $G$ is connected, compact and abelian.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The forward direction is clear.  Conversely, suppose $G$ is connected, compact, and abelian.

  Since $G$ is connected and abelian, we know (by part of Homework \ref{hw:3-lie-first}) that $\exp : \mathfrak{g} \rightarrow G$ is a surjective homomorphism with discrete kernel $\Gamma$, thus $G \cong \mathfrak{g} / \Gamma$.  Since $G$ is compact, the subgroup $\Gamma$ is discrete and cocompact.  Fix an isomorphism $\mathfrak{g} \cong \mathbb{R}^k$.  One can show easily that every discrete cocompact subgroup of $\mathbb{R}^k$ is given by $\mathbb{Z}^k$ after a change of coordinates.  Thus $G \cong T^k$.
\end{proof}

\begin{definition}
  Let $G$ be a Lie group.  A \emph{torus in $G$} is a closed subgroup $T \leq G$ (hence a Lie subgroup) that is a torus.
\end{definition}
\begin{remark}
  Let $T$ be a torus, let $G$ be a Lie group, and let $j : T \rightarrow G$ be a morphism of Lie groups.  Since $T$ is compact, connected, and abelian, so is its image under $T$.  Thus $j(T)$ is a torus.  In particular, immersed Lie subgroups that are isomorphic to tori are in fact closed subgroups.  This explains why we restrict to closed subgroups in the previous definition.
\end{remark}

Hence let $K$ be a compact connected Lie group with Lie algebra $\mathfrak{k}$.

\begin{definition}
  A \emph{maximal torus} in $K$ is a torus $T \leq K$ that is not properly contained in any torus in $K$.
\end{definition}

\begin{exercise}
  The following are equivalent for a closed connected subgroup $T$ of $K$:
  \begin{enumerate}
  \item $T$ is a torus.
  \item $\mathfrak{t} := \Lie(T)$ is an abelian subalgebra of $\mathfrak{k}$.
  \end{enumerate}
\end{exercise}

Recall from a long time ago that if $H_1, H_2$ are two connected Lie subgroups of the same Lie group, then
\begin{itemize}
\item $\mathfrak{h}_1 = \mathfrak{h}_2$ if and only if $H_1 = H_2$,
\item $\mathfrak{h}_1 \subseteq \mathfrak{h}_2$ if and only if $H_1 \subseteq H_2$,
\end{itemize}
etc.
\begin{lemma}\label{lem:maximal-tori-vs-maximal-abelian-subalgebras}
  The following are equivalent for a closed connected subgroup $T$ of $K$:
  \begin{enumerate}
  \item $T$ is a maximal torus.
  \item $\mathfrak{t} := \Lie(T)$ is a maximal abelian subalgebra of $\mathfrak{k}$ (i.e., an abelian subalgebra that is not properly contained in any abelian subalgebra.)
  \end{enumerate}
\end{lemma}
\begin{proof}
  Suppose $T$ is a maximal torus.  Let $\mathfrak{t} ' \supseteq \mathfrak{t}$ be an abelian subalgebra that contains $\mathfrak{t}$.  Suppose there exists $X \in \mathfrak{t}' - \mathfrak{t}$.  Since $\mathfrak{t} '$ is abelian, we have $[X,\mathfrak{t}] = 0$.  Since $T$ is connected, it follows (by the usual differentiation/exponentiation technique) that the group
  \begin{equation*}
    H := \{e^{y X} t : y \in \mathbb{R}, t \in T\}
  \end{equation*}
  is abelian.  It is also connected.  Hence its closure $\overline{H}$ is abelian, connected, and closed inside the compact Lie group $K$, hence compact, hence a torus (Lemma \ref{lem:Characterize-tori}).  By Theorem \ref{thm:closed-implies-lie}, ${H}$ is a Lie subgroup, so we may consider its Lie algebra $\mathfrak{h}$; clearly $\mathfrak{h}$ contains $X$ and $\mathfrak{t}$.  Therefore the torus $H$ has Lie algebra $\mathfrak{h}$ properly containing $\mathfrak{t}$.  By the old result recalled above characterizing containments between closed Lie subgroups in terms of containments of their Lie algebras, we deduce that $\overline{H}$ is a torus properly containing $T$, which contradicts the assumed maximality of $T$.

  Conversely, if $\mathfrak{t}$ is a maximal abelian subalgebra and $T'$ is a torus properly containing $T$, then (by the same old fact recalled above) its Lie algebra $\mathfrak{t} '$ properly contains $\mathfrak{t}$ and is abelian, contradicting the assumed maximality of $\mathfrak{t}$.
\end{proof}

\begin{corollary}\label{cor:self-centralizing-algebra-of-maximal-torus}
  Let $T \leq K$ be a maximal torus in a compact Lie group.  Let $\mathfrak{t} \leq \mathfrak{k}$ be the induced inclusion of LIe algebras.  Then $\mathfrak{t}$ is self-centralizing: $\{X \in \mathfrak{g} : [X,\mathfrak{t}] = 0\} = \mathfrak{t}$.
\end{corollary}
\begin{proof}
  Otherwise there is $X \in \mathfrak{g}$ so that $[X,\mathfrak{t}] = 0$, hence $\mathfrak{t} ' := \mathbb{R} X + \mathfrak{t}$ is an abelian subalgebra of $\mathfrak{k}$ that properly contains $\mathfrak{t}$.  By Lemma \ref{lem:maximal-tori-vs-maximal-abelian-subalgebras}, this does not happen.
\end{proof}

\subsection{Characters of tori}
\label{sec:org56989a6}
\begin{definition}
  Let $T$ be a torus.  A \emph{character} of $T$ is a continuous homomorphism $\chi : T \rightarrow \mathbb{C}^{(1)} := \{z \in \mathbb{C}^\times : |z| = 1\}$.  The character group of $T$ is the group $\mathfrak{X}(T)$ consisting of all characters; the group law is given by multiplication.
\end{definition}

% \begin{lemma}
%   The character group of $T^k$ is isomorphic to $\mathbb{Z}^k$.
% \end{lemma}
% \begin{proof}
%   A character of $T^k$ pulls back to a character of $\mathbb{R}^k$.  Such characters a known
% \end{proof}<++>

\begin{lemma}
  Let $T$ be a torus and let $R : T \rightarrow \GL(V)$ be a representation on a finite-dimensional complex vector space.  Then $V$ decomposes as a direct sum of invariant one-dimensional subspaces on which $T$ acts by characters of $T$.

  More precisely, one has $V = \oplus_\chi V^\chi$, where $\chi$ traverses the set of characters of $T$ and $V^\chi := \{v \in V : R(t) = \chi(t) v\}$.  Any subspace of $V^\chi$ is invariant; by choosing a basis for each $V^\chi$, we obtain a decomposition of $V$ as a sum of one-dimensional invariant (irreducible) subspaces.
\end{lemma}
\begin{proof}
  By \S\ref{sec:linear-reductivity-compact-groups}, the representation is completely reducible.  To complete the proof, we just need to show that any irreducible representation $V$ of $T$ is one-dimensional.  To that end, it suffices to show that each $t \in T$ acts on $V$ by some scalar $\lambda$.

  Indeed, let $t_0 \in T$ be given.  Since $R(t_0) \in \GL(V)$ is a cmoplex matrix, it has some (nonzero) eigenvector $v_0 \in V$ and some eigenvalue $\lambda \in \mathbb{C}$.  Consider the eigenspace $W := \{v \in V : R(t_0) v = \lambda v \}$.  Our goal is to show that $W = V$.  Since $V$ is irreducible and $W$ is nonzero (after all, it contains $v_0$), it sufficse to show that $W$ is invariant.  Here we use the commutativity of $T$: if $t \in T$, then $R(t_0) R(t) = R(t_0 t) = R(t t_0) = R(t) R(t_0)$, hence for $v \in V$, we have $R(t_0) R(t) v = R(t) \lambda v = \lambda R(t) v$, hence $R(t) v \in W$, and so $W$ is $R(t)$-invariant, as required.
\end{proof}

\begin{lemma}\label{lem:characters-of-the-reals}
  Any continuous homomorphism $\chi : \mathbb{R} \rightarrow \mathbb{C}^{(1)}$ is of the form $\chi(x) = e(\xi x) := e^{2 \pi i \xi x}$ for some unique $\xi \in \mathbb{R}$.
\end{lemma}
\begin{proof}
  One can certainly do this directly, but we might as well deduce it from stuff we've seen in class:

  It is clear that $x \mapsto e(\xi x)$ is a character for each $\xi \in \mathbb{R}$, and that for $\xi_1 \neq \xi_2$, the characters obtained in this way are distinct.

  Conversely, recall that shortly after we proved the ``closed subgroups are Lie subgroups'' theorem, we indicated in class and assigned on the homework that continuous homomorphisms between Lie groups are automatically smooth, hence determined by their differentials.  In particular, $\chi : \mathbb{R} \rightarrow \mathbb{C}^{(1)}$ is determined by
  \begin{equation*}
    d \chi : \mathbb{R} = \Lie(\mathbb{R}) \rightarrow i \mathbb{R} = \Lie(\mathbb{C}^{(1)}),
  \end{equation*}
  which is then of the form $2 \pi i \xi$ for some $\xi \in \mathbb{R}$, etc.
\end{proof}
One obtains an analogous classifciation of the characters of $\mathbb{R}^k$ by taking products.  From this we deduce:

\begin{lemma}\label{lem:char-gp-of-Tk}
  The character group of $T^k$ is isomorphic to $\mathbb{Z}^k$: to each $\xi = (\xi_1,\dotsc,\xi_k) \in \mathbb{Z}^k$ one associates the character $T^k = (\mathbb{R}/\mathbb{Z})^k \ni x = (x_1,\dotsc,x_k) \mapsto e(\sum \xi_i x_i) \in \mathbb{C}^{(1)}$.
\end{lemma}
\begin{proof}
  A character of $T^k$ pulls back under the surjective homomorphism $\mathbb{R}^k \rightarrow T^k$ to a character of $\mathbb{R}^k$, which is in turn classified by real numbers $(\xi_1,\dotsc,\xi_k)$; conversely, such real numbers induce a character of $T^k$ precisely when they are all integers.
\end{proof}

Let $T$ be a torus.  Let $\mathfrak{t}$ denote its Lie algebra and $\mathfrak{h} := \mathfrak{t} \otimes_{\mathbb{R}} \mathbb{C}$ the complexification thereof.  Set $\mathfrak{h}_{\mathbb{R}} := i \mathfrak{t} \leq \mathfrak{h}$.  The map $e : \mathfrak{h}_\mathbb{R} \rightarrow T$ given by $e(X) := \exp(2 \pi i X)$ is a surjective homomorphism with discrete cocompact kernel.

Let $\chi : T \rightarrow \mathbb{C}^{(1)}$ be a character of $T$; as discussed above, it is smooth, so we can consider its differential $d \chi : \mathfrak{t} \rightarrow i \mathbb{R}$, which identifies with a linear map $d \chi : \mathfrak{h}_\mathbb{R} \rightarrow \mathbb{R}$.  $\chi(e(H)) = e(\lambda(H))$ for some $\lambda \in \mathfrak{h}_\mathbb{R}^*$.  Conversely, such a $\lambda$ defines a character $\chi$ if and only if it vanishes on $\mathfrak{h}_\mathbb{Z} := \ker(e : \mathfrak{h}_\mathbb{R} \rightarrow T)$, i.e., if and only if it belnogs to $\mathfrak{h}_\mathbb{Z}^*$ as defined earlier.  In summary:
\begin{lemma}
  Let $T$ be a torus.  Let $\mathfrak{h}_\mathbb{R} := i \mathfrak{t}$, as above, so that $e : \mathfrak{h}_\mathbb{R} / \mathfrak{h}_\mathbb{Z} \rightarrow T$ is an isomorphism.

  Then $\mathfrak{X}(T) \cong \mathfrak{h}_\mathbb{Z}^*$ via the bijection $\chi \hookrightarrow \lambda$ characterized by $2 \pi \lambda = d \chi$ and $\chi(e(H)) = e(\lambda(H))$ for all $H \in \mathfrak{h}_\mathbb{R}^*$.
\end{lemma}
\begin{definition}\label{defn:char-associated-to-functional}
  For $\lambda \in \mathfrak{h}_\mathbb{Z}^*$, denote by $e^\lambda$ the character of $T$ associated to it by the above bijection, so that for all $H \in \mathfrak{h}_\mathbb{R}$,
  \begin{equation*}
    e^\lambda(e(H)) = e(\lambda(H)).
  \end{equation*}
  This definition applies in particular to each $\alpha \in R \subseteq \mathfrak{h}_\mathbb{Z}^*$.
\end{definition}

\subsection{Topologies on character groups\label{sec:tops-on-char-gps}}
\label{sec:org00a6b97}
Let's talk briefly about topology.  Let $G$ be a topological group.  Let $\mathfrak{X}(G)$ denote the set of continuous homomorphisms $\chi : G \rightarrow \mathbb{C}^{(1)}$.  (The notation is consistent with that used above when $G$ is a torus.)  We equip $\mathfrak{X}(G)$ with the ``compact-open'' topology.  This means that a subbasis for the open sets in $\mathfrak{X}(G)$ is given by cosets of sets of the form $V(C,U) := \{\chi \in \mathfrak{X}(G) : \chi(C) \subseteq U\}$, where $C \subseteq T$ is compact and $U \subseteq \mathbb{C}^{(1)}$ is open.  Equivalently, a net $\chi^{(\alpha)} \in \mathfrak{X}(G)$ converges to some $\chi \in \mathfrak{X}(G)$ precisely when it converges uniformly on compact sets in the ordinary sense.  We may define on $\mathfrak{X}(G)$ the binary operation $\cdot$ given by $(\chi_1 \cdot \chi_2)(g) := \chi_1(g) \chi_2(g)$.
\begin{exercise}
  Show that $\mathfrak{X}(G)$ is a topological group with respect to this operation.
\end{exercise}

\begin{lemma}\label{lem:cpct-group-dual-is-discrete}
  Suppose that $G$ is compact.  Then $\mathfrak{X}(G)$ is discrete.
\end{lemma}
\begin{proof}
  Take $C := G$ and let $U \subseteq \mathbb{C}^{(1)}$ be an interval of length $1/10$ with center $1 \in \mathbb{C}^{(1)}$.  Let $\chi_0 \in \mathfrak{X}(G)$ denote the trivial character $\chi_0(g) := 1$.  We claim that $V(C,U) = \{\chi_0\}$.  Clearly $\chi_0 \in V(C,U)$.  Conversely, let $\chi \in \mathfrak{X}(G) - \{\chi_0\}$ be a nontrivial character, so that there exists $g \in G$ for which $\chi(g) \neq 1$.  Since $\chi(g) \in \mathbb{C}^{(1)} - \{1\}$, we can find some power of it, say $\chi(g)^n = \chi(g^n)$, which has negative real part.  But then $\chi(g^n) \notin U$, hence $\chi \notin V(C,U)$.  We now use that a topological group is discrete if and only if the set consisting of its identity element is open (if this wasn't an exercise before, it could be now).
\end{proof}

The lemma applies notably to the case that $G$ is a compact torus $T = (\mathbb{R}/\mathbb{Z})^k$.  We saw above that $\mathfrak{X}(T) \cong \mathbb{Z}^k$ as groups.  Lemma \ref{lem:cpct-group-dual-is-discrete} tells us moreover that $\mathfrak{X}(T)$ and $\mathbb{Z}^k$ are isomorphic as topological groups, each equipped with the discrete topology.

\subsection{Maximal tori give rise to Cartan subalgebras}
\label{sec:org1044d7f}
\begin{theorem}
  Let $T$ be a maximal torus in the compact connected Lie group $K$.  Let $\mathfrak{t} \leq \mathfrak{k}$ denote their Lie algebras and
  \begin{equation*}
    \mathfrak{h} := \mathfrak{t} \otimes \mathbb{C} \leq \mathfrak{g} := \mathfrak{k} \otimes \mathbb{C}
  \end{equation*}
  the complexifications.  Then $\mathfrak{h}$ is a Cartan subalgebra of $\mathfrak{g}$.  The roots are purely imaginary on $\mathfrak{t}$.
\end{theorem}
\begin{proof}
  According to our definition, we must check that $\mathfrak{h}$ is abelian, $\ad$-diagonalizable, and self-centralizing.

  \begin{enumerate}
  \item Since $T$ is abelian, so is $\mathfrak{t}$, hence $\mathfrak{h}$ is abelian.
  \item Consider the adjoint action $\Ad : K \rightarrow \End(\mathfrak{k})$.  Restrict it to obtain $\Ad : T \rightarrow \End(\mathfrak{k})$.  Extend it complex-linearly to obtain $\Ad : T \rightarrow \End(\mathfrak{g})$.  Since $T$ is compact, this complex linear representation of it is completely reducible.  By the previous lemma, it decomposes as a direct sum of one-dimensional invariant subspaces.  Differentiating this fact, we see that $\ad(\mathfrak{t})$ and hence (by linearity) $\ad(\mathfrak{h})$ is diagonalizable.

    The functionals $\lambda \in \mathfrak{h}^*$ for $\mathfrak{h}$ acting on $\mathfrak{g}$ by the adjoint map correspond to the characters $\chi$ of $T$ occurring in the decomposition described above.  It follows from our earlier discussion that each such $\lambda$ is real-valued on $\mathfrak{h}_\mathbb{R}$.
  \item Let $V_0 := \{X \in \mathfrak{g} : \Ad(t) X = X \text{ for all } t \in T\}$ be the subspace on which $\Ad(T)$ acts trivially.  By the usual differentiation/exponentiation trick, we have
    \begin{equation*}
V_0 = \{X \in \mathfrak{g} : [X,\mathfrak{t}] = 0\}
\end{equation*}
and
    \begin{equation*}
V_0 = \{X \in \mathfrak{g} : [X,\mathfrak{h}] = 0\}
\end{equation*}
.  From the first of these last two equations and linear algebra, we have
    \begin{equation*}
      V_0 = \{X \in \mathfrak{k} : [X,\mathfrak{t}] = 0\} \otimes \mathbb{C}.
    \end{equation*}
    By Corollary \ref{cor:self-centralizing-algebra-of-maximal-torus}, we see that $V_0 = \mathfrak{h}$.  This gives the required self-centralizing property of $\mathfrak{h}$.
  \end{enumerate}
\end{proof}

In general Lie groups, nontrivial tori (let alone maximal ones) need not exist.  But in compact Lie groups, things are better:
\begin{lemma}
  Let $K$ be a compact connected Lie group.  For any torus $S$ in $K$, there is a maximal torus $T$ in $K$ that contains $S$.  (Note that the trivial torus $S = \{1\}$ always exists.)
\end{lemma}
\begin{proof}
  If $S$ is not maximal, then it is contained in some strictly larger torus $S'$, which then (by consideration of LIe algebras) has strictly larger dimension.  Iterating the procedure $S \mapsto S'$ finitely many times, we wind up with a maximal torus.  (We can't iterate forever, because $\mathfrak{k}$ is finite-dimensional.
\end{proof}

\subsection{Some notation involving roots\label{sec:notationinvolvingroots-for-amxl-tori-section}}
\label{sec:org822bc0f}
So now we have the full theory of roots at our disposal.  Let's set up some notation.  Let $K$ be compact connected, and let $T \leq K$ be a maximal torus.  Let $\mathfrak{h},\mathfrak{g}$ be as above.  We can then decompose
\begin{equation}\label{eq:root-space-decmop-for-compact-LIe-group-amxiaml-trous}
  \mathfrak{g} = \mathfrak{h} \oplus (\oplus_{\alpha \in R} \mathfrak{g}^\alpha)
\end{equation}
where $R$ is a finite subset of $\mathfrak{h}_\mathbb{R}^* - \{0\}$.  In fact, the discussion above implies that $R \subseteq \mathfrak{h}_\mathbb{Z}^* - \{0\}$.  For each $H \in \mathfrak{h}_\mathbb{R}$ and $X \in \mathfrak{g}^\alpha$, we have
\begin{equation*}
 [H,X] = \alpha(H) X,
\end{equation*}
For $\lambda \in \mathfrak{h}_\mathbb{Z}^*$, let $e^\lambda \in \mathfrak{X}(T)$ be as in Definitnio \ref{defn:char-associated-to-functional}.  This applies in particular to $\alpha \in R \subseteq \mathfrak{h}_\mathbb{Z}^*$, and the above identity translates to: for $t \in T$ and $X \in \mathfrak{g}^\alpha$,
\begin{equation}\label{eq:}
  \Ad(t) X = e^\alpha (t) X.
\end{equation}
Note also that if $t = e(H)$ with $H \in \mathfrak{h}_\mathbb{R}$, then $e^\alpha (t) = e(\alpha(H))$, hence
\begin{equation}\label{eq:how-ad-e-H-acts-on-X}
  \Ad(e(H)) X = e(\alpha(H)) X.
\end{equation}

\subsection{The automorphism group of a compact torus\label{sec:aut-gp-compact-torus}}
\label{sec:orgc0705cb}
Let $T$ be a compact torus.  Fix an identification $T = (\mathbb{R}/\mathbb{Z})^k$ for some $k \in \mathbb{Z}_{\geq 0}$.  Then $T$ is a compact Lie group.  We may speak of its automorphism group $\Aut(T)$.  By definition, this consists of continuous homomorphisms $\sigma : T \rightarrow T$ that admit continuous inverse homomorphisms.  We may identify $\mathfrak{t} := \Lie(T)$ with $\mathbb{R}^k$.  Since $T$ is connected, any such $\sigma$ is determined by its differential $d \sigma : \mathbb{R}^k \rightarrow \mathbb{R}^k$, which is a linear map, call it $A$.  Since the exponential map $\mathfrak{t} \rightarrow T$ is given with respect to our identifications by the natural projection $\mathbb{R}^k \rightarrow \mathbb{R}^k / \mathbb{Z}^k$, we see that $\sigma$ is the map induced by a linear map $A : \mathbb{R}^k \rightarrow \mathbb{R}^k$.  For $\sigma$ to be well-defined, we must have $A(\mathbb{Z}^k) \subseteq \mathbb{Z}^k$.  For $\sigma$ to be an isomorphism, its inverse $\sigma^{-1}$ should exist and be well-defined, and so we should have $A(\mathbb{Z}^k) = \mathbb{Z}^k$.  But $\{A \in \GL_k(\mathbb{R}) : A \mathbb{Z}^k = \mathbb{Z}^k\} = \GL_k(\mathbb{Z})$.  We may thereby identify
\begin{equation*}
  \Aut(T) \cong \GL_k(\mathbb{Z}).
\end{equation*}
We define the topology on $\Aut(T)$ in this case to be the discrete topology.

There's another ``transposed'' way to make the above identification.  Given an automorphism $\sigma$ of $T$, we can attach the induced automorphism $\sigma^t$ of $\mathfrak{X}(T)$, which sends a character $\chi$ of $T$ to the new character $\sigma^t \chi \in \mathfrak{X}(T)$ given by $\sigma^t \chi(t) := \chi(\sigma t)$.  The map
\begin{equation*}
  \Aut(T) \ni \sigma \mapsto \sigma^{-t} := (\sigma^t)^{-1} = (\sigma^{-1})^t \in \Aut(\mathfrak{X}(T))
\end{equation*}
is an isomorphism.  Since $\mathfrak{X}(T) \cong \mathbb{Z}^k$, we have $\Aut(\mathfrak{X}(T)) = \GL_k(\mathbb{Z})$.

\subsection{Generators\label{sec:gens-tori}}
\label{sec:orga53673f}
Recall that an abstract group $G$ is said to be \emph{cyclic} if it admits a \emph{generator}, i.e., an element $g \in G$ for which $G = \{g^n : n \in \mathbb{Z} \}$.  There aren't so many cyclic groups; they are all isomorphic either to $\mathbb{Z}$ or $\mathbb{Z}/n$ for some $n \in \mathbb{Z}_{\geq 1}$.

Given a topological group $G$, one says that $G$ is \emph{topologically cyclic} if it admits a \emph{topological generator}, i.e., an element $g \in G$ for which $G = \overline{\{g^n : n \in \mathbb{Z} \}}$ where $\overline{.}$ denotes closure.  For example, any abstract cyclic group (equipped with the discrete topology or any other topology, for that matter) is topologically cyclic, and any generator in the group-theoretic sense is a topological generator, but there are more interesting examples of topologically cyclic groups than just those that are cyclic in the ordinary sense.  (For example: the profinite integers $\hat{\mathbb{Z} }$, the $p$-adic integers $\mathbb{Z}_p$, etc.)

For our purposes, it will be useful to know that compact tori are topologically cyclic:
\begin{lemma}
  Let $T \cong \mathbb{R}^k/\mathbb{Z}^k$ be a compact torus.  Then the set of topological generators of $T$ is dense; in particular, it is nonempty.
\end{lemma}
\begin{proof}
  We presented the simple pigeonholing argument in lecture.  Fix a countable basis $B_1,B_2,\dotsc$ for $T$.  Take any open subset $U$ of $T$.  We want to show that we can find $g \in U$ so that for each $i \in \mathbb{Z}_{\geq 1}$ there exists an $n \in \mathbb{Z}$ so that $g^n \in B_i$.  (Then we're done.)

  We now aim to construct such a $g$.  For convenience of notation, let us realize $T$ as the additive group $\mathbb{R}^k/\mathbb{Z}^k$.  We aim to find for each $i \in \mathbb{Z}_{\geq 1}$
  \begin{itemize}
  \item a nonempty open set $U_i$, and
  \item an integer $N_i$
  \end{itemize}
  so that $U \supseteq U_1 \supseteq U_2 \supseteq U_3 \supseteq \dotsb$ and so that the set $N_i U_i := \{n_i c_i : n_i \in N_i, c_i \in U_i\}$ is contained in $B_i^0$, where $B_i^0$ denotes an open subset of $B_i$ for which $\overline{B_i^0} \subseteq B_i$.  To do this, set $U_0 := U$.  For each $i=1,2,3, \dotsc$, choose $N_i$ large enough that $N_i U_{i-1} = T$.  This is possible because $U_{i-1}$ is open.  Then set $U_i := \{u \in U_{i-1} : N_i u \in B_i^0\}$.

  The set $\cap_i \overline{U_i}$ is nonempty by the finite intersection property.  Any element of it is easily seen to be a generator.
\end{proof}

Since we are primarily interested in topological groups here (or indeed, in Lie groups), we henceforth abuse terminology slightly by saying \emph{generator} when we really mean ``topological generator.''

Generators are nice.  For example, suppose $g \in K$ satisfies $g t g^{-1} = t$ for some generator $t$ of $T$.  Then also $g t^n g^{-1} = t^n$ for all $n \in \mathbb{Z}$.  Since the set of all $x \in K$ for which $g x g^{-1} = x$ is closed, we deduce that $g x g^{-1} = x$ holds for all $x \in T$, i.e., that $g$ centralizes $T$.  We shall use arguments along these lines repeatedly.

\subsection{A maximal torus is the connected component of its normalizer}
\label{sec:org8429097}
Let $T$ be a torus in a compact connected Lie group $K$.  Let $N(T) := \{g \in K : g T g^{-1} = T\}$ denote its normalizer.  The condition defining $N(T)$ is closed, so $N(T)$ is a closed subgroup of $K$, hence is a Lie subgroup of $K$.  (One can also show this more directly along the lines of \S\ref{sec:detect-lie-stabilizers}.)  We can thus speak of the connected component $N(T)_0$.  In general, $N(T)_0$ can be quite large.  For example, if $T = \{1\}$ is the trivial torus, then $N(T) = N(T)_0 = 0$.  However:
\begin{lemma}
  Suppose $T$ is maximal.  Then $N(T)_0 = T$.
\end{lemma}
\begin{proof}
  Consider the map $f : N(T)_0 \rightarrow \Aut(T)$ given by $f(n) := [t \mapsto n t n^{-1}]$.  The domain $N(T)_0$ is connected, and the target $\Aut(T)$ is discrete.  Assuming for now that $f$ is continuous, it follows that its image must consist of a point, and so $N(T)_0$ actually centralizes $T$.  Suppose that $N(T)_0$ strictly contains $T$.  Since they are both connected Lie groups, this implies that we can find $X \in \mathfrak{k}, X \notin \mathfrak{t}$ that commutes with all of $\mathfrak{t}$.  But this contradicts Corollary \ref{cor:self-centralizing-algebra-of-maximal-torus}.

  It remains to verify that $f$ is continuous.  Let $\chi_1, \dotsc, \chi_k$ be a $\mathbb{Z}$-basis for the character group $\mathfrak{X}(T) \cong \mathbb{Z}^k$.  As in the discussion at the end of \S\ref{sec:aut-gp-compact-torus}, we can think of $f(n)^{-t} \in \Aut(\mathfrak{X}(T))$.  Suppose that $(n_i)_{i \in \mathbb{Z}_{\geq 1}}$ is a sequence of elements in $N(T)$ tending to some limit $n \in N(T)$.  Then we have to check that $f(n_i)^{-t} \rightarrow f(n)^{-t} \in \mathfrak{X}(T)$ with respect to the discrete topology.  This means that we have to show that for $i$ large enough, one has $f(n_i)^{-t} = f(n)^{-t}$.  Equivalently, we have to show for each $j \in \{1..k\}$, one has $f(n_i)^{-t} \chi_j = f(n)^{-t} \chi_j$ for $i$ large enough.  Since the character group $\mathfrak{X}(T)$ is discrete (see \S\ref{sec:tops-on-char-gps}), it suffices to show that $f(n_i)^{-t} \chi_j$ converges to $f(n)^{-t} \chi_j$ as functions, uniformly on compact sets.  This follows immediately from the continuity of the conjugation action of $N(T)$ on $T$ and the compactness of $T$.

  (There are probably simpler or slicker ways to write this proof; I hope in any event that it's clear.)
\end{proof}

\begin{corollary}
  Let $T$ be a maximal torus.  Then the quotient $N(T) / T$ is finite.
\end{corollary}
\begin{proof}
  Indeed, that quotient identifies with the set $N(T) / N(T)_0$ of connected components of $N(T)$.  Since $K$ is compact and $N(T)$ is closed, we see also that $N(T)$ is compact, hence has only finitely many connected components.
\end{proof}

\subsection{Conjugacy of maximal tori}
\label{sec:orgb02c776}
Let $K$ be a compact connected Lie group $K$.  Note that if $T$ is a maximal torus in $K$, then so is its conjugate $g T g^{-1}$ for any $g \in K$.  Here's the big theorem on maximal tori in compact Lie groups:
\begin{theorem}\label{thm:maximal-tori-conjugates-exhaust-K}
  Let $T$ be a maximal torus in a compact connected Lie group $K$.  Then $K = \cup_{g \in K} g T g^{-1}$.
\end{theorem}
The (standard) proof we'll record uses the Lefschetz fixed point theorem.  That theorem (or one variant of it) says that for a compact manifold $M$, one can attach to each continuous map $f : M \rightarrow M$ an integer $\Lambda(f)$ with the following properties:
\begin{enumerate}
\item $\Lambda(f)$ only depends upon the homotopy class of $f$.
\item If $f$ has \emph{isolated simple fixed points}, that is to say, if the set $\Fix(f) := \{x \in M : f(x) = x\}$ is finite and if for each $x \in \Fix(f)$, the linear map $T_x f : T_x M \rightarrow T_{f(x)} M = T_x M$ satisfies $\det(1 - T_x f) \neq 0$, then
  \begin{equation*}
    \Lambda(f) = \sum_{x \in \Fix(f)} \eps_x(f),
  \end{equation*}
  where $\eps_x(f) \in \{\pm 1\}$ denotes the sign of the nonzero real number $\det(1 - T_x f) \in \mathbb{R}^\times$.
\end{enumerate}
In particular, if $\Fix(f) = \emptyset$, then $\Lambda(f) = 0$.  This is a theorem from algebraic topology that we won't prove.  We record the definition anyway:
\begin{equation*}
  \Lambda(f) = \sum_{i \in \mathbb{Z}_{\geq 0}} (-1)^i \trace(f^* | H^i(M,\mathbb{Q}))
\end{equation*}
where the RHS involves singular cohomology groups with rational coefficients.  For the identity map $1 : M \rightarrow M$, one writes $\chi(M) := \Lambda(1)$.  The quantity
\begin{equation*}
  \chi(M) = \sum_{i \in \mathbb{Z}_{\geq 0}} (-1)^i \trace(f^* | H^i(M,\mathbb{Q}))
\end{equation*}
is called the \emph{Euler characteristic} of $M$.

Anyway, back to our goal.  We want to show that for each $x \in K$, there exists $g \in K$ so that $x \in g T g^{-1}$, or equivalently, so that $x g \in g T$, or equivalently, so that $x g T = g T$.  In other words, we want to show that the map
\begin{equation*}
  f_x : K/T \rightarrow K/T
\end{equation*}
\begin{equation*}
  f_x(g T) := x g T
\end{equation*}
has a fixed point.  The manifold $K/T$ is compact (since $K$ is), so we can apply the Lefschetz theorem.  Assuming for the sake of contradiction that $f_x$ had no fixed point, we'd deduce from the Lefschetz that $\Lambda(f_x) = 0$.  Let's note that since $K$ is a connected manifold, it is path-connected.  For any $x,y \in K$, we can find a path connected them; that path induces a homotopy between the maps $f_x$ and $f_y$, and in particular between them and the identity map $f_1$, for which $\Lambda(f_1) = \chi(M)$.  So we're done if we can show that $\chi(M) \neq 0$.  We'll actually show more precisely that
\begin{equation}
  \chi(M) = \# N(T)/T.
\end{equation}
As noted above, we can compute $\chi(M)$ as $\Lambda(f_x)$ for \emph{any} $x \in K$.  It is convenient to take for $x$ a generator (see \S\ref{sec:gens-tori}) of the torus $T$.  What, then, are the fixed points of $f_x$?  Well, $g T \in \Fix(f_x)$ if and only if $x g T = g T$, i.e., $g^{-1} x g \in T$; but since $x$ generates $T$, it follows then that $g^{-1} T g \subseteq T$.  One can then see in many ways that $g^{-1} T g = T$.  (For example, they are both maximal tori.)  Hence $g \in N(T)$.  Thus $\Fix(f_x) = N(T) / T$.

Henceforth abbreviate $f := f_x$.  For each $g \in N(T) / T$, we have a commutative diagram as drawn in class which shows that $\det(1 - T_{g T} f | T_{g T}(G/T))$ is independent of $g$, so we henceforth focus on the case that $g T = e T$ is the identity coset.

We can then identify
\begin{equation*}
  T_{e T}(G/T) = \mathfrak{k} / \mathfrak{t}
\end{equation*}
and hence
\begin{equation*}
  T_{e T}(G/T)_{\mathbb{C}} = \mathfrak{g} / \mathfrak{h} \cong \oplus_{\alpha \in R} \mathfrak{g}^\alpha.
\end{equation*}
with the usual notation.  Let's write $x = e(H)$ for some $H \in \mathfrak{h}_\mathbb{R}$.  Our assumption that $x$ is a generator entails in particular that $e(\alpha(H)) \neq 1$ for all $\alpha \in R$, as otherwise $x$ would belong to the codimension $1$ submanifold of $T$ consisting of elements $e(H)$ for which $e(\alpha(H)) = 1$.  By linear algebra, we can compute determinants after complexifying.  We can also write
\begin{equation*}
  T_x f : T_{e T}(G/T)_{\mathbb{C}} \rightarrow T_{e T}(G/T) _{\mathbb{C}}
\end{equation*}
as
\begin{equation*}
  \Ad(x) : \mathfrak{g}/\mathfrak{h} \rightarrow \mathfrak{g}/\mathfrak{h}
\end{equation*}
because, since $x \in T$, one has
\begin{equation*}
  x g T = x g x^{-1} T.
\end{equation*}
Thus
\begin{equation*}
  \det(1 - T_{e T} f | T_{e T}(G/T)) = \prod_{\alpha \in R} \det(1 - \Ad(x) | \mathfrak{g}^\alpha).
\end{equation*}
On the other hand
\begin{equation*}
  \det(1 - \Ad(x) | \mathfrak{g}^\alpha) = 1 - e(\alpha(H)).
\end{equation*}
We can split the product into a product over pairs $\pm \alpha \in R / \{\pm 1\}$ taken up to sign, giving
\begin{equation*}
  \det(1 - T_{e T} f | T_{e T}(G/T)) = \prod_{\pm \alpha \in R} (1 - e(\alpha(H))) (1 - e(-\alpha(H))).
\end{equation*}
For each $\alpha \in R$, write $\theta := 2 \pi i \alpha(H)$.  Then
\begin{equation*}
  0 \neq (1 - e(\alpha(H))) (1 - e(-\alpha(H))) = (1 - e^{i \theta}) (1 - e^{-i \theta}) = 2 - 2 \cos(\theta) \geq 0.
\end{equation*}
We conclude that
\begin{equation*}
  \det(1 - T_{e T} f | T_{e T}(G/T)) > 0
\end{equation*}
hence that (as explained more carefully in class via a commutative diagram)
\begin{equation*}
  \det(1 - T_{g T} f | T_{e T}(G/T)) > 0 \text{ for each } g \in N(T)/T
\end{equation*}
hence that $f$ has isolated fixed points $g T$ with signs $\eps_{g T}(f) = 1$.  Therefore
\begin{equation*}
  \Lambda(f) = \# N(T)/T,
\end{equation*}
as required.

\subsection{Basic consequences of the conjugacy theorem.}
\label{sec:org3dd568d}
Let $K$ be a connected compact Lie group, and let all other notation be as usual.  For now, we indicate some consequecnes relevant for answering the questions raised last time.
\begin{corollary}\label{cor:center-is-intersection-of-maxiaml-tori}
  The center $Z$ of $K$ is the intersection $\cap T$ of all (maximal) tori.
\end{corollary}
\begin{proof}
  Let $z \in Z$, and let $T$ be a maximal torus.  By the theorem, we may write $z = g t g^{-1}$ for some $t \in T$.  But then $t = g^{-1} z g = z$, because $z$ is in the center.  Hence $z$ belongs to $T$.

  Conversely, suppose $z \in K$ belongs to $\cap T$.  Let $x \in K$; we must show that $x$ and $z$ commute.  To that end, we apply Theorem \ref{thm:maximal-tori-conjugates-exhaust-K} to find a maximal torus $T$ that contains $x$.  Then $T$ contains $z$ and $x$; since $T$ is commutative, the elements $x$ and $z$ commute, as required.
\end{proof}

\begin{theorem}
  Let $T$ be a maximal torus in the compact connecteed Lie group $K$.  Let $\mathfrak{h}_\mathbb{R} := i \mathfrak{t}$ and $e : \mathfrak{h}_\mathbb{R}/h_\mathbb{Z} \xrightarrow{\cong }T$ be as usual.  Let $R$ denote the set of roots for $\mathfrak{h} := \mathfrak{t} \otimes \mathbb{C}$ acting on $\mathfrak{g} := \mathfrak{k} \otimes \mathbb{C}$ by the adjiont representation.  Let $Z$ denote the center of $K$.  Then $e$ induces an isomorphism
  \begin{equation*}
    (\mathbb{Z} R)^* /\mathfrak{h}_\mathbb{Z} \cong Z.
  \end{equation*}
\end{theorem}
\begin{proof}
  By Corollary \ref{cor:center-is-intersection-of-maxiaml-tori}, $Z$ is contained in $T$, hence the image of $e$ contains $Z$.  To complete the proof, all that remains to be showed is the following: for $H \in \mathfrak{h}_\mathbb{R}$, we have $e(H) \in Z$ if and only if $H \in (\mathbb{Z} R)^\wedge$.  This equivalence is demonstrated by noting that each of the following assertions is evidently equivalent to the next:
  \begin{enumerate}
  \item $e(H) \in Z$.
  \item $e(H) \in \ker(\Ad)$ (using here that $K$ is connected)
  \item $e(\alpha(H)) = 1$ for all $\alpha \in R$ (use \eqref{eq:root-space-decmop-for-compact-LIe-group-amxiaml-trous} and \eqref{eq:how-ad-e-H-acts-on-X}, and note that $\Ad(e(H))$ acts trivially on $\mathfrak{h}$ because $\mathfrak{t}$ is abelian)
  \item $\alpha(H) \in \mathbb{Z}$ for all $\alpha \in R$ (because $\mathbb{Z} = \{x \in \mathbb{R} : e^{2 \pi i x} = 1\}$)
  \item $H \in (\mathbb{Z} R)^*$ (by definition of the latter).
  \end{enumerate}
\end{proof}

\begin{lemma}
  Let $T_1, T_2$ be maximal tori in $K$.  Then $T_1,T_2$ are conjugate.
\end{lemma}
\begin{proof}
  Choose a generator $t_1$ for $T_1$.  By the cnojugacy theorem, we may find $g \in G$ so that $g t_1 g^{-1} \in T_2$.  Since $t_1$ is a generator, it follows that $g T_1 g^{-1} \subseteq T_2$.  Since $T_1,T_2$ are both maximal, we conclude that $g T_1 g^{-1} = T_2$, as required.
\end{proof}

\begin{definition}
  Given an element $u \in K$, the \emph{centralizer} $Z(u) := Z_K(u)$ is defined to be $Z(u) := \{g \in K : g u g^{-1} = u\}$.

  Similarly, for any subgroup $U \leq K$, the \emph{centralizer} $Z(U) := Z_K(U)$ is defined to be $Z(U) := \cap_{u \in U} Z(u) = \{g \in K : g u g^{-1} = u \text{ for all } u \in U\}$.
\end{definition}

\begin{lemma}
  Let $T$ be a maximal torus.  Then $Z(T) = T$.
\end{lemma}
\begin{proof}
  This is a tricky argument, so I've spelled the proof out a bit more verbosely.  (This one is worth studying and rewriting on your own, I think.)

  Since $T$ is abelian, we have $T \subseteq Z(T)$.  Conversely, let $g \in Z(T)$.  Let $H$ be the closure of the subgroup of $K$ generated by $T$ and $g$.  Since $g$ commutes with $T$ and $T$ is abelian, we know that $H$ is abelian.

  If we were lucky enough that $H$ happened to be connected, then we'd be done: $H$ would then be connected, compact, and abelian, hence a torus, but since $T$ is a maximal torus, the only possiblity is $H = T$, and thus $g \in T$.

  Unfortunately, there is no obvious reason for $H$ to be connected.  We are led to consider its connected component $H_0$.  Since $T$ is connected, we know that $H$ contains $T$.  Since $H_0$ is connected, abelian and compact (being closed inside $K$), it is a torus; since $T$ is maximal, we must have $H_0 = T$.

  If we were lucky enough that $g$ happened to belong to $H_0$, then again, we'd be done.  But there is no obvious reason for that be the case.  (Think about it.)  Fortunately, as we now explain, $H/H_0$ is not too complicated, so we can make the argument work anyway.

  Let's see.  Since $H_0 \subseteq H$ is open, the quotient $H/H_0$ is discrete; it is also compact, hence finite.  Moreover, by construction of $H$, that quotient is generated by the image $\overline{g} \in H/H_0$ of $g$.  Let $m \in \mathbb{Z}_{\geq 1}$ be the smallest natural number for which $\overline{g}^m = 1$, or equivalently, for which $g^m \in H_0 = T$.  Let $t \in T$ be a generator of the torus.  The torus is a divisible group, so we can find $s \in T$ for which $s^m = t g^{-m}$.

  Set $u := s g$.  We claim that $u$ is a generator of $H$.  To see that, we must check that the powers of $u$ are dense.  We have $u^m = t$, and $t$ is a generator of $T$, so the closure of the set of powers of $u$ contains $T$.  The full group $H$ is the union over $j \in \mathbb{Z}/m$ of the cosets $g^j H_0 = g^j T$.  The set of powers $u^{m n + j} = u^j t^n \in g^j T$ ($n \in \mathbb{Z}$) are dense in that coset.  Hence $u$ generates $H$.

  Now we use the big theorem on conjugacy of maximal tori to deduce that $u$ is contained in \emph{some} maximal torus $S$ of $G$ (e.g., a conjugate of $T$).  Since $u$ generates $H$, we must also have $H \subseteq S$.  So now we have the following containments:
  \begin{equation*}
    T \subseteq H \subseteq S.
  \end{equation*}
  Since $T$ is a \emph{maximal} torus, the only possiblity is that $T = S$, hence that $H = T$, hence that $g \in T$.

  Since $g \in Z(T)$ was arbitrary, we conclude finally that $Z(T) = T$, as required.
\end{proof}

\section{Regular and singular elements}
\label{sec:orgf4996f8}
\subsection{Definitions and basic properties}
\label{sec:orgedf2662}
Recall that every element of $K$ is contained in \emph{some} maximal torus.
\begin{definition}
  We say that an element $g \in K$ is \emph{regular} if it belongs to exactly one maximal torus; if otherwise it belongs to at least two maximal tori, then we call it \emph{singular}.
\end{definition}
Introduce the superscript $\reg$, as in $K^{\reg}$ or $T^{\reg}$ (for a maximal torus $T$), to denote ``subset of regular elements.''

Being regular is a property of conjugacy classes: if $x \in K$ is regular, then so is $g x g^{-1}$ for any $g \in K$, and vice-versa.  Since every element is conjugate to some element of any given maximal torus $T$, we can understand the regular elements pretty well if we understand which elements of $T$ are regular.

To that end, let $R$ denote the set of roots of $T$.  For each $\alpha \in R$, set
\begin{equation*}
  T_\alpha := \ker(e^\alpha) \leq T
\end{equation*}
where $e^\alpha : T \rightarrow \mathbb{C}^{(1)}$ is the character $T \ni e(H) \mapsto e(\alpha(H))$ (here $H \in \mathfrak{h}_\mathbb{R}$).  We refer to \S\ref{sec:notationinvolvingroots-for-amxl-tori-section} for any unexplained notation.

More verbosely, since $\alpha(H) \in \mathbb{Z}$ iff $e(\alpha(H)) = 1$, one has
\begin{equation*}
  T_\alpha = \left\{ e(H) : H \in \mathfrak{h}_\mathbb{R}, \alpha(H) \in \mathbb{Z} \right\}.
\end{equation*}
$T_\alpha$ need not be connected, but its connected component $(T_\alpha)_0$ is easily seen to be codimension one subtorus of $T$ with Lie algebra $\mathfrak{t}_\alpha = \ker(\alpha : \mathfrak{t} \rightarrow i \mathbb{R})$.
\begin{proposition}
  An element of $T$ is regular if and only if it doesn't belong to any the $T_\alpha$, i.e.,
  \begin{equation*}
    T^{\reg} = T - \cup_{\alpha \in R} T_\alpha.
  \end{equation*}
\end{proposition}
\begin{proof}
  For $t \in T$, let $Z(t) := \{g \in K : g t g^{-1} = t\}$ denote its centralizer.  It is a Lie subgroup of $K$ with Lie algebra $\mathfrak{z}(t) = \{X \in \mathfrak{k} : \Ad(t)X = X\}$ whose complexification is in turn
  \begin{equation*}
    V := \mathfrak{z}(t)_\mathbb{C} = \{Z \in \mathfrak{g} : \Ad(t)Z = Z\},
  \end{equation*}
  where $\mathfrak{g} := \mathfrak{k}_\mathbb{C}$ as usual.  It is clear that $Z(t) \supseteq T$, hence that $\mathfrak{z}(t) \supseteq \mathfrak{t}$, hence that $V \supseteq \mathfrak{h} := \mathfrak{t}_\mathbb{C}$.  Consider the root space decomposition $\mathfrak{g} = \mathfrak{h} \oplus (\oplus_{\alpha \in R} \mathfrak{g}^\alpha)$.  If $Z \in \mathfrak{g}$ has the form $Z = Z_0 + \sum Z_\alpha$ with $Z_0 \in \mathfrak{h}$ and $Z_\alpha \in \mathfrak{g}^\alpha$, then
  \begin{equation*}
    \Ad(t) Z_0 = Z_0, \quad \Ad(t) Z_\alpha = e^\alpha(t) Z_\alpha,
  \end{equation*}
  hence
  \begin{equation*}
    \Ad(t) Z = Z \iff e^\alpha(t) = 1 \text{ whenever } Z_\alpha \neq 0.
  \end{equation*}
  We deduce that the following are equivalent:
  \begin{enumerate}
  \item $V$ properly contains $\mathfrak{h}$.
  \item There exists $\alpha \in R$ so that $e^\alpha(t) = 1$.
  \item $t \in \cup_{\alpha \in R} T_\alpha$.
  \end{enumerate}

  We now prove the required equivalence.  Suppose first that $t$ is not regular.  Then it is contained in some maximal torus $T'$ other than $T$, hence $Z(t) \supseteq T'$, and thus $\mathfrak{z}(t) \supseteq \mathfrak{t} ' := \Lie(T')$; consequently $\mathfrak{z}(t)$ properly contains $\mathfrak{t}$ and so $V$ properly contains $\mathfrak{h}$; by the above, this is equivalent to $t$ belonging to $\cup_{\alpha \in R} T_\alpha$.

  Conversely, suppose $t$ is regular.  We claim then that $Z(t)_0 = T$.  Clearly $T \subseteq Z(t)_0$.  Conversely, let $g \in Z(t)_0$.  By the main theorem on maximal tori applied to $Z(t)_0$, we can find a maximal torus $S$ of $Z(t)_0$ containing $g$.  Since $t$ belongs to the center of $Z(t)_0$, we know also that $S$ contains $t$.  Let $S'$ be a maximal torus of $G$ that contains $S$.  Then $S'$ contains $t$; since $t$ is regular, this implies that $S' = T$.  Consequently $g \in S \subseteq S' = T$.  Since $g$ was arbitrary, the claim that $Z(t)_0 = T$ is proven.  Consequently $V = \mathfrak{h}$ and thus $t \notin \cup_{\alpha \in R} T_\alpha$, as required.
\end{proof}

\subsection{Singular elements have codimension at least three}
\label{sec:org776dd5b}
We know that $K = \cup_{g \in K} g T g^{-1}$.  In other words, the well-defined map
\begin{equation*}
  f : K / T \times T \rightarrow K
\end{equation*}
\begin{equation*}
  f(g,t) := g t g^{-1}
\end{equation*}
is surjective.  By the discussion above, the subset $K^{\sing}$ of singular elements is the union over $\alpha \in R$ of the images of the well-defined maps
\begin{equation*}
  f_\alpha : K / Z(T_\alpha) \times T_\alpha \rightarrow K.
\end{equation*}
Set $n := \dim(K)$ and $k := \dim(T)$.  Clearly $\dim(T_\alpha) = k - 1$.  On the other hand, as discussed more leisurely in class, $Z(T_\alpha)_\mathbb{C}$ contains $\mathfrak{h} \oplus \mathfrak{g}^\alpha \oplus \mathfrak{g}^{-\alpha}$.  Thus $\dim(Z(T_\alpha)) \geq k+ 2$, and so
\begin{equation*}
  \image(f_\alpha) \leq (n - (k+2)) + (k-1) \leq n - 3.
\end{equation*}
Therefore the subset of singular elements in $K$ has codimension $\leq 3$.

It is a general fact that given a manifold $M$ with submanifold $M_0$ for which $M - M_0$ has codimension $\geq 3$, the natural map $\pi_1(M_0) \rightarrow \pi_1(M)$ is an isomorphism.

A simpler example: if $M - M_0$ has codimension $\geq 2$, then the connected components of $M$ and $M_0$ are in natural bijection (i.e., $\pi_0(M_0) \rightarrow \pi_1(M)$ is a bijection).

\subsection{The key covering morphism\label{sec:key-cov-morph}}
\label{sec:org8a0ef0a}
We have a well-defined surjective map
\begin{equation*}
  f : K/T \times T^{\reg} \rightarrow K^{\reg}.
\end{equation*}
The following was explained in lecture, and is not so difficult:
\begin{lemma}
  This map is a covering map, i.e., a locally trivial fiber bundle with discrete fibers.  The fibers have cardinality $|N/T|$, where $N := N(T) := \{g \in K : g T g^{-1} = T\}$.
\end{lemma}

\subsection{The affine Weyl group and the components of the set of regular elements\label{sec:affine-weyl-gp}}
\label{sec:org8dfce1f}
Let
\begin{equation*}
  \mathfrak{h}_\mathbb{R}^{\sreg} := \{H \in \mathfrak{h}_\mathbb{R} : \alpha(H) \notin \mathbb{Z} \text{ for all } \alpha \in R\}.
\end{equation*}
Equivalently, $\mathfrak{h}_\mathbb{R}^{\sreg}$ is the preimage under $e : \mathfrak{h}_\mathbb{R} \rightarrow T$ of $T^{\reg}$.

The open subset $\mathfrak{h}_\mathbb{R}^{\sreg}$ of $\mathfrak{h}_\mathbb{R}$ is a union of complements of hyperplane.  Each connected component $P$ of $\mathfrak{h}_\mathbb{R}^{\sreg}$ is convex, and admits a definition of the shape
\begin{equation*}
  P = \{H \in \mathfrak{h}_\mathbb{R} : n_\alpha < \alpha(H) < n_\alpha + 1 \text{ for all } \alpha \in R \}
\end{equation*}
for some system of integral parameters $n_\alpha \in \mathbb{Z}$ attached to the roots $\alpha \in R$.  It is worth trying to draw some pictures of $\mathfrak{h}_\mathbb{R}^{\reg}$ in all the rank $2$ classical Lie algebras (as attempted in lecture for $B_2$).

The group $(\mathbb{Z} R)^*$ acts on $\{H \in \mathfrak{h}_\mathbb{R} : \alpha(H) \in \mathbb{Z}\}$ by translation: for $Z \in (\mathbb{Z} R)^*$, one has $\alpha(Z) \in \mathbb{Z}$, hence $\alpha(H+Z) \in \mathbb{Z}$ precisely when $\alpha(H) \in \mathbb{Z}$.  Consequently $(\mathbb{Z} R)^*$ acts also on $\mathfrak{h}_\mathbb{R}^{\sreg}$, by translation.

Recall that the Weyl group $W$ is generated by the root reflections $s_\alpha : \mathfrak{h}_\mathbb{R}^* \rightarrow \mathfrak{h}_\mathbb{R}^*$ defined for roots $\alpha \in R$ by $s_\alpha \lambda := \lambda - \lambda(H_\alpha) \alpha$.  Recall also that $s_\alpha(R) = R$.  For each $s \in W$, we may define the transpose element ${}^t s$, which acts now on the space $\mathfrak{h}_\mathbb{R}$ dual to the domain $\mathfrak{h}_\mathbb{R}^*$ of $s$; the action of ${}^t s$ is characterized by requiring that for $H \in \mathfrak{h}_\mathbb{R}$, one has for each $\lambda \in \mathfrak{h}_\mathbb{R}^*$ that $\lambda({}^t s H) = (s \lambda)(H)$.  This relation might be more pleasantly written
\begin{equation*}
  \langle \lambda, {}^t s H \rangle = \langle s \lambda, H \rangle.
\end{equation*}
\begin{exercise}
  Show that for $\alpha \in R$ and $H \in \mathfrak{h}_\mathbb{R}$, one has
  \begin{equation*}
 {}^t s_\alpha H = H - \alpha(H) H_\alpha.
  \end{equation*}
\end{exercise}
From now on we might abuse notation slightly by writing simply $s_\alpha := {}^t s_\alpha$ and identifying $W$ with the subgroup $\{s := {}^t s : s \in W\}$ of $\GL(\mathfrak{h}_\mathbb{R})$.  In this way, we regard $W$ as acting on $\mathfrak{h}_\mathbb{R}$.

This action of $W$ preserves $\mathbb{Z} R^\wedge$.  Indeed, for each $s \in W$, one has $s(R^\wedge) = R^\wedge$, so the generators get permuted.

For this reason, we can form the semidirect product $\mathbb{Z} R^\wedge \rtimes W$.  It is the group consisting of all pairs $(H,s)$, where $s \in W, Z \in \mathbb{Z} R^\wedge$.  The multiplication law is
\begin{equation*}
  (Z_1,s_1) (Z_2,s_2) = (Z_1 + s_1 Z_2, s_1 s_2).
\end{equation*}
This group acts naturally on $\mathfrak{h}_\mathbb{R}$ by the formula
\begin{equation*}
  (Z,s) \cdot H := Z + s H.
\end{equation*}

For each $\alpha \in R$ and $n \in \mathbb{Z}$, consider the linear map $s_{\alpha n} : \mathfrak{h}_\mathbb{R} \rightarrow \mathfrak{h}_\mathbb{R}$ given by reflection in the hyperplane $\{H : \alpha(H) = n\}$; explicitly,
\begin{equation*}
  s_{\alpha n}(H) = H - (\alpha(H) - n) H_\alpha.
\end{equation*}
Clearly $s_{\alpha 0} = s_\alpha \in W$.  On the other hand, it is easy to check (either by straightforward algebra or by drawing a picture) that
\begin{equation*}
  s_{\alpha 1} \circ s_{\alpha 0}(H) = H + H_\alpha
\end{equation*}
and more generally for $n \in \mathbb{Z}$ that
\begin{equation*}
  s_{\alpha,n+1 } \circ s_{\alpha, n}(H) = H + H_\alpha.
\end{equation*}
From these and the identities $s_{\alpha,n}^2 = 1$, we see that the following subgroups of $\GL(\mathfrak{h}_\mathbb{R})$ coincide:
\begin{enumerate}
\item The image of $\mathbb{Z} R^\wedge \rtimes W$.
\item The group generated by the $s_{\alpha,n}$, for $\alpha \in R$ and $n \in \mathbb{Z}$.
\end{enumerate}
Either group is called the \emph{affine Weyl group}.  I'll denote that group $W_a$.

By the above discussion, $W_a$ acts on the hyperplanes $\{H : \alpha(H) = n\}$ and hence on their complement $\mathfrak{h}_\mathbb{R}^{\sreg}$.
\begin{lemma}
  $W_a$ acts transitively on the set of connected components of $\mathfrak{h}_\mathbb{R}^{\sreg}$.
\end{lemma}
\begin{lemma}
  Given two such connected components $P_0, P_1$, take some basepoints $H_i \in P_i$ and draw a path $H_t$ from $H_0$ to $H_1$ that crosses at most one of the hyperplanes $\{H : \alpha(H) = n\}$ at a time.  One obtains in this way a sequence of pairs $(\alpha_1,n_1),\dotsc,(\alpha_k,n_k)$ so that as $t$ goes from $0$ to $1$, the point $H_t$ crosses the planes $\{H : \alpha_i(H) = n_i\}$ in order from $i=1$ to $i=k$.  Then the composition $s_{\alpha_k,n_k} \circ \dotsb \circ s_{\alpha_1,n_1}$ maps $P_0$ to $P_1$.
\end{lemma}


\section{The distinguished \(\SU(2)\)'s\label{sec:distinguished-su2}}
\label{sec:org25cd375}
Let notation be as in previous sections: $K$ is a compact connected Lie group, $T$ is a maximal torus in $K$, plus all the other usual notation.

Let $\mathfrak{k} := \Lie(K)$ and $\mathfrak{g} := \mathfrak{k}_\mathbb{C} := \mathfrak{k} \otimes _{\mathbb{R}} \mathbb{C}$, as usual.  Let $\theta : \mathfrak{g} \rightarrow \mathfrak{g}$ denote the involution given by complex conjugation on the second factor of $\mathfrak{k} \otimes_{\mathbb{R}} \mathbb{C}$, so that when we identify $\mathfrak{k}$ with a real Lie subalgebra of $\mathfrak{g}$, we have $\mathfrak{k} = \{X \in \mathfrak{g} : \theta(X) = X\}$.  (In typical examples such as $\mathfrak{k} = \u(n), \mathfrak{g} = \slLie_n(\mathbb{C})$, the operator $\theta$ is given by $X \mapsto -{}^t \overline{X}$.)  Then $\theta(H) = - H$ for all $H \in \mathfrak{h}_\mathbb{R} = i \mathfrak{t}$, $\mathfrak{t} := \Lie(T)$.

Let $\alpha$ belong to the set $R$ of roots for $T$.  Recall that $\mathfrak{g}$ contains the subalgebra $\mathfrak{s}_\alpha = \mathbb{C} H_\alpha \oplus \mathbb{C} X_\alpha \oplus \mathbb{C} Y_\alpha$, which is isomorphism in the evident way to $\slLie_2(\mathbb{C})$.  The subalgebra $\mathfrak{k}^{(\alpha)} := \{X \in \mathfrak{s}_\alpha : \theta(X) = X\}$ is a real Lie subalgebra of $\mathfrak{s}_\alpha$.  We can easily work out a basis for it.  Since $H_\alpha \in \mathfrak{h}_\mathbb{R}$, one has $\theta(H_\alpha) = - H_\alpha$.  One has $\theta (\mathfrak{g}^\alpha) \subseteq \mathfrak{g}^{-\alpha}$ and $\theta (\mathfrak{g}^{-\alpha}) \subseteq \mathfrak{g}^{\alpha}$, and $\theta^2 = 1$.  It follows that the trace of $\theta$ acting on $\mathfrak{s}_\alpha = 0$, so it has the same number of $+1$ and $-1$ eigenvalues, hence $\dim_{\mathbb{R}}(\mathfrak{k}^{(\alpha)}) = 3$.  If we let $x,y$ be any $\mathbb{R}$-basis of the $1$-dimensional $\mathbb{C}$-vector space $\mathfrak{g}^\alpha$, then it follows easily that $\{i H_\alpha, x + \theta(x), y + \theta(y)\}$ given an $\mathbb{R}$-basis of $\mathfrak{k}^{(\alpha)}$.  Using the existence of a positive-definite $K$-invariant inner product on $\mathfrak{k}$, we can show that $\mathfrak{k}^{(\alpha)} \cong \su(2)$.  (TODO: explain more.)  Since $\SU(2)$ is simply-connected, we get from this a morphism
\begin{equation*}
  F_\alpha : \SU(2) \rightarrow K
\end{equation*}
so that $(d F_\alpha)_\mathbb{C}$ is the natural map $\slLie_2(\mathbb{C}) \rightarrow \mathfrak{s}_\alpha$.  (Compare with homework \ref{hw:Falphaz}.)  The image of $F_\alpha$ is contained in $Z(T_\alpha)^0$.  We get an element
\begin{equation*}
  w_\alpha := F_\alpha (
\begin{pmatrix}
    0 & 1 \\
    -1 & 0
  \end{pmatrix}
  )
\end{equation*}
for which $\Ad(w_\alpha)$ gives the root reflection $s_\alpha$.

We (mostly) explained in lecture how we can use this to identify $N/T$ with the Weyl group $W$ (as defined using root reflections).  One of the key steps was to show that if an element $w$ of $N/T$ stabilizes a Weyl chamber $C$, then it is the identity (i.e., $w \in T$).  For this we reduced by an averaging trick to the case that $w$ actually fixes some element $H \in C$.  (TODO: explain more.)

\section{Proofs regarding the basic homomorphism describnig fundamental groups of compact Lie groups}
\label{sec:org1822737}
\subsection{Definition}
\label{sec:org1514b4c}
Let notation be as usual.  Recall that we initially define
\begin{equation*}
  f : \mathfrak{h}_\mathbb{Z} \rightarrow \pi_1(K)
\end{equation*}
by taking for $f(H)$ the homotopy class $[\gamma]$ of the path $\gamma : [0,1]\rightarrow K$ given by
\begin{equation*}
  \gamma(t) := e(t H) := \exp(2 \pi i t H).
\end{equation*}

\subsection{The basic homomorphism: checking that it's a homomorphism}
\label{sec:orgddabaf2}
Given $H_1, H_2 \in \mathfrak{h}_\mathbb{Z}$, we obtain paths $\gamma_1, \gamma_2 : [0,1] \rightarrow K$ as above.  Their composition in the fundamental group is the path (with domain $[0,1]$)
\begin{equation*}
  t \mapsto
  \begin{cases}
    \gamma_1(2 t) & t \leq 1/2 \\
    \gamma_2(2 t - 1) &  t \geq 1/2
  \end{cases}
\end{equation*}
which we can rewrite as
\begin{equation*}
  t \mapsto \left(
    \begin{cases}
      \gamma_1(2 t) & t \leq 1/2 \\
      1 &  t \geq 1/2
    \end{cases}
  \right) \cdot \left(
    \begin{cases}
      1 & t \leq 1/2 \\
      \gamma_2(2 t - 1) &  t \geq 1/2
    \end{cases}
  \right).
\end{equation*}
Introduce a deformation parameter $s \in [0,1]$.  Choose a continuous monotonically decreasing function $c : [0,1] \rightarrow [1,2]$ for which $c_0 = 2$ and $c_1 = 1$.  The above path is then homotopic to
\begin{equation*}
  t \mapsto \left(
    \begin{cases}
      \gamma_1(c_s t) & c_s t \leq 1 \\
      1 &  c_s t \geq 1
    \end{cases}
  \right) \cdot \left(
    \begin{cases}
      1 & 1 + c_s (t  - 1) \leq 0 \\
      \gamma_2(1 + c_s (t - 1)) &  1 + c_s(t-1) \geq 0.
    \end{cases}
  \right).
\end{equation*}
At deformation parameter $s=1$, the above path is given by
\begin{equation*}
  t \mapsto \gamma_1(t) \gamma_2(t) = e(t (H_1 + H_2)).
\end{equation*}
Therefore $f : \mathfrak{h}_\mathbb{Z} \rightarrow \pi_1(K)$ is a homomorphism.

\begin{exercise}
  Use an argument similar to that above to show that the fundamental group of any topological group is abelian.
\end{exercise}

\subsection{Checking that some stuff is in its kernel}
\label{sec:org9527204}
We now show that for $H \in \mathbb{Z} R^\wedge$ one has $f(H) = 0$.  Since $f$ is a homomorphism and $R^\wedge$ gives a basis for $\mathbb{Z} R^\wedge$, our task reduces to verifying for each $\alpha \in R$ that the path
\begin{equation}\label{eq:path-to-be-shown-is-nullhomotopic}
 [0,1] \ni t
  \mapsto e(t H_\alpha) \in K
\end{equation}
is null-homotopic.

To that end, observe first that $e(\tfrac{1}{2} H_\alpha) \in T_\alpha$; indeed, since $\alpha(H_\alpha) = 2$,
\begin{equation*}
  e^\alpha(\tfrac{1}{2} H_\alpha) = e(\alpha(\tfrac{1}{2} H_\alpha)) = e(1) = 1.
\end{equation*}
Recall from \S\ref{sec:distinguished-su2} that there exists $w_\alpha \in Z(T_\alpha)^0$ for which $\Ad(w_\alpha) H_\alpha = - H_\alpha$.  For $s \in [0,1]$, let $c_s \in Z(T_\alpha)^0$ be such that $c_0 = 1$ (the identity element) and $c_1 = w_\alpha$.  We then have
\begin{equation*}
  c_s e(\tfrac{1}{2} H_\alpha) c_s^{-1} = e(\tfrac{1}{2} H_\alpha) \text{ for all } s.
\end{equation*}
For this reason, the path \eqref{eq:path-to-be-shown-is-nullhomotopic} may be continuously deformed to
\begin{equation*}
  t \mapsto
  \begin{cases}
    e(t H_\alpha) &  t \leq 1/2 \\
    c_s e(t H_\alpha) c_s^{-1} & t \geq 1/2.
  \end{cases}
\end{equation*}
After we deform to $s = 1$, we get
\begin{equation*}
  w_\alpha e(t H_\alpha) w_\alpha^{-1} = e( t \Ad(w_\alpha)H_\alpha) = e(-t H_\alpha) = e((1-t) H_\alpha),
\end{equation*}
so we deduce that the path \eqref{eq:path-to-be-shown-is-nullhomotopic} is homotopic to
\begin{equation}\label{eq:boomerang-path}
  t \mapsto
  \begin{cases}
    e(t H_\alpha) &  t \leq 1/2 \\
    e((1-t) H_\alpha)  & t \geq 1/2.
  \end{cases}
\end{equation}
Now introduce another deformation parameter $r$, starting at $r=1/2$ and deforming to $r=0$.  The path \eqref{eq:boomerang-path} is then homotopic to
\begin{equation}\label{eq:boomerang-path-2}
  t \mapsto
  \begin{cases}
    e(t H_\alpha) &  t \leq r \\
    e((2r-t) H_\alpha)  & r \leq t \leq 2 r \\
    1 & 2 r \leq t.
  \end{cases}
\end{equation}
When $r = 0$, we get the trivial path $t \mapsto 1$.  Thus the path \eqref{eq:path-to-be-shown-is-nullhomotopic} is nullhomotopic.

\subsection{The basic homomorphism: checking that it's surjective}
\label{sec:orgea26b3f}
We now argue that $f$ is surjective.  Recall that $\pi^1(K) = \pi^1(K^{\reg})$.  Let $H_0$ be a small element of $\mathfrak{h}_{\mathbb{R}}^{\sreg}$, so that $e(H_0)$ is a small element of $T^{\reg}$.  Any element of $\pi_1(K)$ can be deformed a bit to start and end at $e(H_0)$, and then deformed a bit more so that it lies entirely in $K^{\reg}$.  Using the covering morphism of \S\ref{sec:key-cov-morph}, we can then uniquely lift our path to $K/T \times T^{\reg}$; this means concretely that we may express our path uniquely in the form
\begin{equation*}
 [0,1]
  \ni t \mapsto c_t \exp(H_t) c_t^{-1},
\end{equation*}
where $c_t \in K/T$ satisfies $c_0 = e T$, where $H_t$ belongs to the same connected component $P$ of $\mathfrak{h}_{\mathbb{R}}^{\sreg}$ as $H_0$ and satisfies $H_t|_{t=0} = H_0$, and finally
\begin{equation}\label{eq:final-condition-c1-H11-H00}
  c_1 \exp(H_1) c_1^{-1} = \exp(H_0).
\end{equation}
This last condition says in particular that $s := c_1$ satisfies $s t s^{-1} \in T^{\reg}$ for some $t := e(H_1) \in T^{\reg}$; since $t$ is regular, it belongs to exactly one maximal torus, and so since $t \in T$ and $t \in s^{-1} T s$ we deduce that $T = s^{-1} T s$, i.e., that $s$ belongs to the normalizer $N := N_K(T) := \{g \in K : g T g^{-1} = T\}$ of $T$.  We may also rewrite the condition \eqref{eq:final-condition-c1-H11-H00} in the form
\begin{equation}\label{eq:integrality-condition-on-s-H11-H00}
  s \cdot H_1 + H_0 \in \mathfrak{h}_\mathbb{Z} := \ker(e : \mathfrak{h}_\mathbb{R} \rightarrow T),
\end{equation}
where we abbreviate $s \cdot H_1 := \Ad(s) H_1$.

We now want to deform our path so that it is obviously in the image of $f$.  To that end, let us first translate it by $e(-H_0)$ so that its basepoint is at the origin again.  We are then left to stare at the path
\begin{equation*}
  t \mapsto e(-H_0) c_t e(H_t) c_t^{-1}.
\end{equation*}
Since $P$ is convex and $H_0,H_1 \in P$, there is no harm in assuming that $H_t$ is the straight line path from $H_0$ to $H_1$, given by $H_t = H_0 + t(H_1 - H_0)$.  Using \eqref{eq:integrality-condition-on-s-H11-H00} and the fact that $s$ and $s^{-1}$ preserve $\mathfrak{h}_\mathbb{Z}$, we may write
\begin{equation*}
  H_1 = s^{-1} \cdot H_0 + Z
\end{equation*}
for some $Z \in \mathfrak{h}_\mathbb{Z}$.  We are thus looking at the path
\begin{equation*}
  t \mapsto e(-H_0) c_t e(H_0 + t (s^{-1} \cdot H_0 + Z - H_0) ) c_t^{-1}.
\end{equation*}
In the above path, we may continuously deform $H_0$ to $0$.  This gives a family of loops based at the origin.  When we reach $H_0 = 0$, we end up with the path
\begin{equation}\label{eq:penult-path-before-showing-in-image-of-f}
  t \mapsto
  c_t e(t  Z ) c_t^{-1}.
\end{equation}
Since $Z \in \mathfrak{h}_\mathbb{Z}$, we have $e(0 Z) = e(1 Z) = 1$.  So we can now deform every element of $c_t$ to the identity element and get a homotopic path.  In other words, we can replace the above path by $t \mapsto c_{\eps t} e(t Z) c_{\eps t}^{-1}$ for $0 \leq \eps \leq 1$; we start with $\eps = 1$, giving \eqref{eq:penult-path-before-showing-in-image-of-f}, and then deform to $\eps = 0$, giving the path
\begin{equation*}
  t \mapsto e(t Z),
\end{equation*}
which is obviously in the image of $f$.

\subsection{The basic homomorphism: pinning down the kernel}
\label{sec:org8c8e428}
We've seen that we have a well-defined surjective map
\begin{equation*}
  f : \mathfrak{h}_\mathbb{Z} / \mathbb{Z} R^\wedge \rightarrow \pi_1(K).
\end{equation*}
We want to show that it's actually injective.  Let's observe first also that for any $Z \in \mathfrak{h}_\mathbb{Z}$ and $s \in W = N/T$, we have
\begin{equation*}
  f(s Z) = f(Z).
\end{equation*}
Indeed, $s Z - Z$ belongs to $\mathbb{Z} R^\wedge$ (check this on the generators $s = s_\alpha$ of $W$, using that $H_\alpha \in R^\wedge$), and $f$ is a homomorphism.  So this tells us that $f(Z)$ doesn't change if we replace $Z$ with anything in the same orbit under the affine Weyl group $W_a := \mathbb{Z} R^\wedge \rtimes W$ (see \S\ref{sec:affine-weyl-gp}).  Fix some $H_0 \in \mathfrak{h}_{\mathbb{R}}^{\reg}$ and let $P$ denote its connected component.  Since $W_a$ acts transitively on the connected components and since the union of their closures is all of $\mathfrak{h}_\mathbb{R}$, any $Z \in \mathfrak{h}_\mathbb{Z}$ is in the $W_a$-orbit of some element of the closure of $P - H_0$; since $Z \in \mathfrak{h}_\mathbb{Z}$, it can't lie on the boundary of $P - H_0$ (check this; it's easy), and so must lie in $P - H_0$ itself.

What we want to show now is that the above map is an isomorphism.  This means we should show that if $Z \in \mathfrak{h}_\mathbb{Z}$ has the property that the path $t \mapsto e(t Z)$ is nullhomotopic, then $Z$ belongs to $\mathbb{Z} R^\wedge$.  By the above discussion, we may assume that $Z \in P - H_0$.  Since $P$ is convex, there is then no loss in shifting basepoints a bit to suppose that we are considering the path in $K^{\reg}$ given by
\begin{equation*}
  \gamma(t) := e(H_0 + t Z).
\end{equation*}
Under the covering map $K/T \times T^{\reg} \rightarrow K^{\reg}$, the above path lifts uniquely to
\begin{equation*}
  \tilde{\gamma}(t) \mapsto (e T, H_0 + t Z).
\end{equation*}
The endpoint $\tilde{\gamma}(1) = t Z$ of this lifting is moreover invariant under base-and-end-point-preserving homotopies of $\gamma$.  So if $\gamma$ is nullhomotopic, then we must have $Z = 0$.  The proof is now complete.


\bibliography{refs}{} \bibliographystyle{plain}
\end{document}
