\documentclass[reqno]{amsart} \input{common.tex}

\author{Paul D. Nelson} \title{Representations of Lie groups
  \\
  ETH Z{\"u}rich, Spring 2019 }

\begin{document}

\begin{abstract}
  Notes for a semester course on the representation theory of Lie groups, focusing on compact groups and complex reductive groups, taught at ETH Z\"{u}rich in Spring 2019.
\end{abstract}

% \setcounter{tocdepth}{1}
  \maketitle
  \tableofcontents

  In mathematics and related fields, one often encounters vector spaces $V$, such as
  \begin{itemize}
  \item the vector space $V = L^2(X,\mu)$ consisting of functions $f : X \rightarrow \mathbb{C}$ on some nice topological space $X$ that are square-integrable with respect to some measure $\mu$, or
  \item the space of solutions $\phi$ to some linear differential equation $D \phi = 0$,
  \end{itemize}
  and so on.  The inputs to the construction of $V$ are sometimes invariant by some symmetry group $G$.  In the first example, $G$ might have a measure-preserving action on $(X,\mu)$.  In the second, $G$ might commute with the operator $D$ (for instance, many of the basic equations of mathematical physics are invariant by something like the rotation group $\SO(3)$).  In either case we obtain a linear action of $G$ on $V$, i.e., a homomorphism $G \rightarrow \GL(V)$.  The case that $G$ is a Lie group is simultaneously one of the most interesting cases (because of the many examples) and accessible (because, e.g., connected Lie groups are in many respects simpler than finite groups, thanks to powerful tools from calculus and linear algebra).  In this course, we still develop the basic theory of such actions.  We emphasize the general theory of compact groups, their relation with complex reductive groups, the structure theory and representations of compact Lie groups, and conclude with a bit about representations of complex reductive groups.

  \subsection*{Acknowledgments}
  We thank Subhajit Jana, Constantin Kogler, Kaj B{\"a}uerle and many other participants of the course for corrections and comments.


  \section{Prerequisites}
  We recall here some of the basics from linear algebra, topology, differential geometry, Lie theory, and functional analysis that will be used throughout the course.

  We won't assume any prior knowledge of representation theory.

  \subsection{Conventions}
  In this course,
  \begin{equation*}
  \text{``topological space''} := \text{``separable Hausdorff topological space.''}
\end{equation*}
The examples we consider will moreover be locally compact, but we will mention this hypothesis explicitly when it is used.  Similarly
\begin{equation*}
  \text{``vector space''} := \text{``complex vector space''}
\end{equation*}
unless we specify otherwise.

\subsection{Topological groups}
\begin{definition}
  A \emph{topological group} is a group $G$ equipped with a topology such that the maps
  \begin{equation*}
    G \times G \rightarrow G
  \end{equation*}
  \begin{equation*}
    (x,y) \mapsto x y
  \end{equation*}
  and
  \begin{equation*}
    G \rightarrow G
  \end{equation*}
  \begin{equation*}
    x \mapsto x^{-1}
  \end{equation*}
  are continuous.  Similarly, a \emph{topological vector space} is a vector space equipped with a topology such that the maps
  \begin{equation*}
    V \times V \rightarrow V
  \end{equation*}
  \begin{equation*}
    (x,y) \mapsto x + y
  \end{equation*}
  and
  \begin{equation*}
    \mathbb{C} \times V \rightarrow V
  \end{equation*}
  \begin{equation*}
    (\lambda,x) \mapsto \lambda x
  \end{equation*}
  are continuous.  We write in that case $\GL(V)$ for the group of continuous linear maps $\phi : V \rightarrow V$ that admit a continuous two-sided inverse.
\end{definition}
For instance, any finite group (always equipped with the discrete topology) is a topological group, as is any Lie group; any finite-dimensional vector space is naturally a topological vector space, as is any Hilbert space or Banach space.

\subsection{Lie theory}
We'll assume a large number of facts from basic Lie theory.  For instance, a Lie group $G$ has a Lie algebra $\mathfrak{g}$.  A continuous homomorphism $G_1 \rightarrow G_2$ between two Lie groups is automatically smooth, or even analytic, and differentiates to a Lie algebra homomorphism $\mathfrak{g}_1 \rightarrow \mathfrak{g}_2$.  For a finite-dimensional vector space $V$, the group $\GL(V)$ is a Lie group with Lie algebra $\End(V)$.


\subsection{Haar measure}
We abbreviate ``locally compact topological group'' to ``locally compact group,'' and similarly for ``compact group.''
\begin{definition}
  Let $G$ be a locally compact group.  Recall that a \emph{Radon measure} $d g$ on $G$ is a functional
  \begin{equation*}
    C_c(G) \rightarrow \mathbb{C}
  \end{equation*}
  \begin{equation*}
    f \mapsto \int_G f \, d g = \int_{g \in G} f(g) \, d g
  \end{equation*}
  such that $f \geq 0 \implies \int_G f \, d g \geq 0$.\footnote{The continuity and related conditions follow from the positivity, see, e.g., the Wikipedia entries on ``Radon measure'' and ``Riesz-Kakutani representation theorem''}  For each such $f$ and each $h \in G$, we denote by $\lambda_h f$ and $\rho_h f$ the left and right translates of $G$, normalized so that $\lambda_{h_1 h_2} = \lambda_{h_1} \lambda_{h_2}$ and $\rho_{h_1 h_2} = \rho_{h_1} \rho_{h_2}$:
  \begin{equation*}
    \lambda_h f(g) := f(h^{-1} g), \quad \rho_h f(g) := f(g h^{-1}).
  \end{equation*}
  Recall that $d g$ is a \emph{left Haar measure} if $\int_G f \, d g = \int_G \lambda_{h} f \, d g$ for all $h \in G$.  The notion of a \emph{right Haar measure} is defined analogously using $\rho_h$.  We say that $d g$ is a \emph{Haar measure} if it is both a left Haar measure and a right Haar measure.
\end{definition}

\begin{theorem}
  Let $G$ be a locally compact group.  Then left Haar measures exist, and any two are positive multiples of one another.  Similarly for right Haar measures.
  
  Suppose moreover that $G$ is compact.  Then Haar measures exist, and assign finite volume to $G$.
\end{theorem}
In particular, there is a unique Haar measure $d g$ on $G$ such that $\vol(G,d g) = 1$; we call it the \emph{probability Haar measure on $G$}.  We will often denote integration with respect to the probability Haar simply by $\int_G f := \int_G f \, d g$, omitting the $d g$ when it is clear by context.


For example, if $G$ is finite, then the probability Haar $d g$ is given by the normalized counting measure:
\begin{equation*}
  \int_G f \, d g = \frac{1}{|G|} \sum_{g \in G} f(g).
\end{equation*}

\subsection{Spectral theory for compact self-adjoint operators}\label{sec:spectr-theory-comp}
Let $V = (V, \langle , \rangle)$ be a Hilbert space and $T : V \rightarrow V$ a bounded operator.  Recall that $T$ is
\begin{itemize}
\item \emph{self-adjoint} if $\langle T v_1, v_2 \rangle = \langle v_1, T v_2 \rangle$ for all $v_1, v_2 \in V$, and
\item \emph{compact} if it maps the unit ball to a precompact set, or equivalently, if each bounded sequence $v_n$ in $V$ has a subsequence $v_{n_k}$ such that the sequence $T v_{n_k}$ converges.
\end{itemize}
A basic example is when $T$ is diagonalized by an orthonormal basis $e_1,e_2,\dotsc$ of $V$, thus $T e_j = \lambda_j e_j$ for some $\lambda_j \in \mathbb{C}$; then $T$ is compact precisely when $\lambda_j \rightarrow 0$ as $j \rightarrow \infty$.
\begin{theorem}\label{thm:spectral-theorem-compact}
  Let $T$ be compact and self-adjoint.  Then every eigenvalue $\lambda$ of $T$ is real.  For each such $\lambda$, let $V_\lambda \subseteq V$ denote the $\lambda$-eigenspace.  Then $V$ is the Hilbert direct sum $\hat{\oplus}_{\lambda} V_\lambda$, i.e., the closure of the algebraic direct sum $\oplus_{\lambda} V_\lambda$.  Moreover, for each $\eps > 0$, the space $\oplus_{\lambda : |\lambda| \geq \eps } V_\lambda$ is finite-dimensional.  In particular, if $V$ is nonzero, then $T$ has an eigenvector.
\end{theorem}
\begin{proof}
[Proof sketch]
  Let's note first that the finite-dimensionality of the spaces $\oplus_{\lambda : |\lambda| \geq \eps } V_\lambda$ is automatic: if any such space were infinite-dimensional, then we could find an infinite sequence of unit vectors $v_n$ for which $\|T v_n\| \geq \eps$, contrary to the assumed compactness of $T$.

  Turning to the main point of the proof, let $W$ denote the orthogonal complement of $\oplus_{\lambda} V_\lambda$.  We know that $T$ acts on $W$ and has no eigenvectors in $W$, while our task is to show that $W = \{0\}$.  The main point is thus to show that any compact self-adjoint operator on a nonzero Hilbert space admits an eigenvector.  See for instance \S9.2 of my ``Lie Groups'' notes on the course homepage.
\end{proof}
Recall also from linear algebra the following consequence of (e.g.)  the Jordan normal form:
\begin{lemma}
  Any linear operator on a nonzero finite-dimensional (complex) vector space has an eigenvector.
\end{lemma}


\subsection{Linear algebra}\label{sec:linear-algebra}
Let $V, V_1, V_2$ be finite-dimensional vector spaces.  We can then construct some additional vector spaces:
\begin{enumerate}
\item The dual space $V^* = \Hom(V,\mathbb{C})$ consisting of linear functionals $\ell : V \rightarrow \mathbb{C}$.  For $\ell \in V^*$ and $v \in V$, we sometimes write $\langle \ell, v \rangle = \ell(v)$ for the natural pairing.
\item The conjugate space $\overline{V}$.  By definition, this is a set equipped with a bijection $V \rightarrow \overline{V}$, denoted $v \mapsto \overline{v}$.  We define the vector space structure on $\overline{V}$ by requiring that $v \mapsto \overline{v}$ commute with addition (i.e., $\overline{v_1} + \overline{v_2} = \overline{v_1 + v_2}$) and intertwine scalar multiplication with conjugation, thus for $\lambda \in \mathbb{C}$,
  \begin{equation*}
    \lambda \overline{v} := \overline{(\overline{\lambda} v )}.
  \end{equation*}
\item The space $\Hom(V_1,V_2)$ of linear maps $T : V_1 \rightarrow V_2$.
\item The space $\End(V) := \Hom(V,V)$ of linear operators $T : V \rightarrow V$.
\item The direct sum $V_1 \oplus V_2$, for which linear maps $f : V_1 \oplus V_2 \rightarrow V$ are in natural bijection with pairs $(f_1,f_2)$ of linear maps $f_i : V_i \rightarrow V$.
\item The tensor product $V_1 \otimes V_2$, for which linear maps $f : V_1 \otimes V_2 \rightarrow V$ are in natural bijection with bilinear maps $f : V_1 \times V_2 \rightarrow V$.
\end{enumerate}
We have an isomorphism
\begin{equation*}
  V_1^* \otimes V_2 \rightarrow \Hom(V_1,V_2)
\end{equation*}
given by
\begin{equation*}
  \ell_1 \otimes v_2 \mapsto [w_1 \mapsto \ell_1(w_1) v_2]
\end{equation*}
The inverse map is described by taking coefficients of $T$ with respect to bases.  In more detail, fix bases $e_1,\dotsc,e_m$ of $V_1$ and $f_1,\dotsc,f_n$ of $V_2$ together with dual bases $e_1^*, \dotsc, e_m^*$ of $V_1^*$ and $f_1^*, \dotsc, f_n^*$ of $V_2^*$, so that $\langle e_i^*, e_j \rangle = \langle f_i^*, f_j \rangle = \delta_{i j}$.  Then for any $T \in \Hom(V_1,V_2)$ and $v \in V_1$, we have
\begin{equation*}
  v = \sum_i \langle e_i^*, v \rangle e_i
\end{equation*}
and
\begin{equation*}
  T e_j = \sum_i \underbrace{ \langle f_i^*, T e_j \rangle }_{=: a_{ij} } f_i,
\end{equation*}
so that
\begin{equation*}
  T v = \sum_{i,j} a_{i j} \langle e_j^*, v \rangle f_i,
\end{equation*}
and thus
\begin{equation*}
  \Hom(V_1,V_2) \ni T \leftrightarrow \sum_{i,j} a_{i j} e_j^* \otimes f_i \in V_1^* \otimes V_2.
\end{equation*}

In particular, we may identify
\begin{equation*}
  \End(V) \cong V^* \otimes V.
\end{equation*}
Under this identification,
\begin{equation*}
  \trace : \End(V) \rightarrow \mathbb{C}
\end{equation*}
corresponds to the linear map
\begin{equation*}
  V^* \otimes V \rightarrow \mathbb{C}
\end{equation*}
\begin{equation*}
  \ell \otimes v \mapsto \langle \ell, v \rangle.
\end{equation*}

\newpage


\section{Introduction}
\subsection{Representations}
\begin{definition}
  Let $G$ be a topological group and $V$ a topological vector space.  A \emph{representation} $\pi$ of $G$ on $V$ is a group homomorphism
  \begin{equation*}
    \pi : G \rightarrow \GL(V)
  \end{equation*}
  for which the map
  \begin{equation*}
    G \times V \rightarrow V
  \end{equation*}
  \begin{equation*}
    (g,v) \mapsto \pi(g) v =: g v
  \end{equation*}
  is continuous.
\end{definition}
Rather than saying ``$(\pi,V)$ is a representation of $G$,'' we might simply say ``let $V$ be a representation of $G$'' or ``let $G$ act linearly on $V$,'' with the action map $\pi = \pi_V$ defined implicitly and its continuity conditions imposed by default; we will then often denote the action by juxtaposition, $g v := \pi_V(g) v$, as above.  Other times we'll say ``let $\pi$ be a representation of $G$,'' with the underlying vector space $V = V_\pi$ defined implicitly.  Eventually we'll often write simply $\pi$ both for the action and the underlying vector space, but we'll avoid doing that for now.


\begin{remark}
  Suppose that $G$ is a Lie group and $V$ is finite-dimensional, so that $\GL(V)$ is a Lie group with Lie algebra $\End(V)$.  Then a theorem from ``Lie groups'' implies that $\pi$ is automatically smooth, hence differentiates to a Lie algebra homomorphism $\mathfrak{g} := \Lie(G) \rightarrow \End(V)$.
\end{remark}


\begin{example}
~
  \begin{enumerate}
  \item Let $X$ be a locally compact space equipped with a Radon measure $\mu$.  Suppose that $G$ acts on $X$, preserving $\mu$ (i.e., there is a continuous map $G \times X \rightarrow X$, satisfying the axioms for a group action, such that for each $g \in G$, the induced map $g : X \rightarrow X$ satisfies $g_* \mu = \mu$).  Then $G$ acts linearly on $L^2(X,\mu)$, i.e., we have a representation $\pi : G \rightarrow \GL(L^2(X,\mu))$.  These representations provide some of the most important examples.

    In more detail, if we are given a left action denoted ``$g x$,'' then for $g \in G$ and $f \in L^2(X,\mu)$, we set
    \begin{equation}\label{eq:action-on-functions-induced-by-left-translation}
      (\pi(g) f)(x) := f(g^{-1} x),
    \end{equation}
    while if we are instead given a right action denoted ``$x g$,'' then we should take
    \begin{equation}\label{eq:action-on-functions-induced-by-right-translation}
      (\pi(g) f)(x) := f(x g).
    \end{equation}
    As an exercise, check in either case that $\pi(g_1) \pi(g_2) = \pi(g_1 g_2)$ for all $g_1, g_2 \in G$.

    Check for instance for $G = \mathbb{R}$ acting on $X = \mathbb{R}$ by translation, and with $\mu$ Lebesgue measure, that the action map
    \begin{equation}\label{eq:}
      \mathbb{R} \times L^2(\mathbb{R}) \rightarrow L^2(\mathbb{R})
    \end{equation}
    is continuous; on the other hand, for any nonzero $x \in \mathbb{R}$ there exists $f \in L^2(\mathbb{R})$ with $\|f\| = 1$ so that $\|\pi(x) f - f \| \geq 1$ (e.g., take for $f$ a bump function supported in $[-|x|/10, |x|/10]$), so that the map from $\mathbb{R}$ to the space $\GL(L^2(\mathbb{R}))$ equipped with the operator norm is not continuous.  These observations motivate the definition of ``representation'' given above.
    
    Suppose for instance that $G$ is a compact group, equipped with its probability Haar measure $d g$.  Then the action of $G$ on itself by right multiplication preserves $d g$.  The above discussion specialized to $(X,\mu) = (G, d g)$ gives a representation
    \begin{equation*}
      \rho : G \rightarrow \GL(L^2(G,d g))
    \end{equation*}
    given by right translation as in \eqref{eq:action-on-functions-induced-by-right-translation}.  This representation is called the \emph{right regular representation}.  We may similarly define the \emph{left regular representation}
    \begin{equation*}
      \lambda : G \rightarrow \GL(L^2(G, d g))
    \end{equation*}
    using left translation as in \eqref{eq:action-on-functions-induced-by-left-translation}.  Note that if $G$ is finite, then $L^2(G, d g)$ is just the space $\mathbb{C}^G$ of functions $f : G \rightarrow \mathbb{C}$.  Note also that we didn't really require compactness: on any locally compact group, we can define the right (resp. left) regular representation using any right (resp. left) Haar measure.
  \item The \emph{trivial representation} of a group $G$ on the one-dimensional vector space $\mathbb{C}$ is the map $G \rightarrow \GL(\mathbb{C}) = \GL_1(\mathbb{C})$ given by $g \mapsto 1$.
  \item The \emph{zero representation} of $G$ on the zero-dimensional vector space $\{0\}$.  This representation is unimportant, and will practically never be considered in this course; we mention it for now just to disambiguate it from the (very important) trivial representation.
  \item Most of the classical groups $G$ that one encounters in Lie theory (e.g., $\GL_n(\mathbb{R}), \GL_n(\mathbb{C}), \O(n), \U(n)$ and $\Sp(n)$ for $n$ even) come with a ``standard representation'' $G \rightarrow \GL_n(\mathbb{C}) = \GL(\mathbb{C}^n)$.
  \item For a finite-dimensional vector space $V$, the group $\GL(V)$ is a Lie group.  Its finite-dimensional representations
    \begin{equation*}
      \rho : \GL(V) \rightarrow \GL(W)
    \end{equation*}
    play a special role in the theory: given any representation
    \begin{equation*}
\pi : G \rightarrow \GL(V),
\end{equation*}
    we may compose it with $\rho$ to get a new representation
    \begin{equation*}
\rho \circ \pi : G \rightarrow \GL(W).
\end{equation*}

    For example, we may take for $\rho$ the determinant representation $\det : \GL(V) \rightarrow \GL_1(\mathbb{C}) = \mathbb{C}^\times$, and form the determinant $\det \circ \pi : G \rightarrow \mathbb{C}^\times$ of a representation $\pi : G \rightarrow \GL(V)$.
  \item A Lie group $G$ defined over the reals has a Lie algebra $\mathfrak{g}$, which is a real vector space; its complexification $\mathfrak{g}_{\mathbb{C}} := \mathfrak{g} \otimes_{\mathbb{R}} \mathbb{C}$ is a complex vector space, and the adjoint representation $\Ad : G \rightarrow \GL_{\mathbb{R}}(\mathfrak{g})$ defines in particular a (complex) representation
    \begin{equation*}
      \Ad : G \rightarrow \GL(\mathfrak{g}_{\mathbb{C}}).
    \end{equation*}
    (If we start with a Lie group $G$ over the complex numbers, like $\GL_n(\mathbb{C})$, then we get a representation $\Ad : G \rightarrow \GL(\mathfrak{g})$ in the sense of this course without having to complexify.)
  \item The symmetric group $S(n)$ comes with a \emph{standard representation}
    \begin{equation*}
      \pi : S(n) \rightarrow \GL_n(\mathbb{C})
    \end{equation*}
    on $\mathbb{C}^n$, given by permuting the standard basis elements $e_1,\dotsc,e_n$: for $g \in S(n)$, we define $\pi(g) e_j := e_{g(j)}$.  This is just the representation of elements of the symmetric group by permutation matrices; for $n=2$ we get $
\begin{pmatrix}
      1 &  \\
        & 1
    \end{pmatrix}
, 
\begin{pmatrix}
      & 1 \\
      1 &
    \end{pmatrix}
$, for $n = 3$ we get $
\begin{pmatrix}
      1 &  &  \\
        & 1 &  \\
        & & 1
    \end{pmatrix}
$, $
\begin{pmatrix}
      & 1 &  \\
      1 &  &  \\
      & & 1
    \end{pmatrix}
$, and so on.  We can compose $\pi$ with the determinant map $\det : \GL_n(\mathbb{C}) \rightarrow \mathbb{C}^\times$ to get the \emph{sign representation}
    \begin{equation*}
      \sgn := \det \circ \pi : S(n) \rightarrow \{\pm 1\} \subseteq \mathbb{C}^\times.
    \end{equation*}
    Equivalently, $\sgn(g) = (-1)^k$ if $g = \tau_1 \dotsb \tau_k$, with each $\tau_j$ a transposition.  We will sometimes use the notation
    \begin{equation*}
      (-1)^g := \sgn(g)
    \end{equation*}
    for $g \in S(n)$.
  \item Most of the standard operations from linear algebra (e.g., those recalled in \S\ref{sec:linear-algebra}) induce corresponding operations on representations.  For instance, given finite-dimensional representations $(\pi,V),(\pi_1,V_1),(\pi_2,V_2)$ of $G$, we may form:
    \begin{itemize}
    \item the \emph{dual} (or \emph{contragredient}) \emph{representation} $(\pi^*, V^*)$ on the dual space $V^*$ by setting
      \begin{equation*}
        (\pi^*(g) \ell)(v) := \ell(\pi(g)^{-1} v),
      \end{equation*}
      so that
      \begin{equation*}
        \langle \pi^*(g) \ell, \pi(g) v \rangle = \langle \ell, v \rangle,
      \end{equation*}
    \item the \emph{conjugate representation} $(\overline{\pi}, \overline{V})$ by
      \begin{equation*}
        \overline{\pi }(g) \overline{v} := \overline{\pi(g) v},
      \end{equation*}
    \item the direct sum $(\pi_1 \oplus \pi_2, V_1 \oplus V_2)$ by
      \begin{equation*}
        g (v_1, v_2) := (g v_1, g v_2),
      \end{equation*}
    \item the tensor product $(\pi_1 \otimes \pi_2, V_1 \otimes V_2)$ by
      \begin{equation*}
        g (v_1 \otimes v_2) := g v_1 \otimes g v_2,
      \end{equation*}
      and
    \item the representation $(\Hom(\pi_1,\pi_2), \Hom(V_1,V_2))$ by, for $g \in G$ and $f \in \Hom(V_1,V_2)$,
      \begin{equation*}
        g f := [V_1 \ni v_1 \mapsto \pi_2(g) f(\pi_1(g)^{-1} v_1) \in V_2 ].
      \end{equation*}
    \end{itemize}
  \end{enumerate}
\end{example}

\subsection{Unitarity}

\begin{definition}
  Let $\pi : G \rightarrow \GL(V)$ be a representation as above.  Suppose that $V$ is equipped with an inner product $\langle , \rangle$ (e.g., if $V$ is a Hilbert space).  We say then that $\pi$ is \emph{unitary} (with respect to $\langle , \rangle$) if
  \begin{equation*}
    \langle g v_1, g v_2 \rangle = \langle v_1, v_2 \rangle
  \end{equation*}
  for all $g \in G$ and $v_1, v_2 \in V$; we might equivalently say that the inner product $\langle , \rangle$ is \emph{invariant} for the action of $G$.  We say in general that $\pi$ is \emph{unitarizable} if there exists an invariant inner product, i.e., an inner product $\langle , \rangle$ on $V$ with respect to which $\pi$ is unitary.
\end{definition}
We can already prove a basic theorem:
\begin{theorem}\label{thm:compact-unitarizability}
  Let $G$ be a compact group.  Let $V$ be a finite-dimensional representation, or a Hilbert space representation.  Then $V$ is unitarizable.
\end{theorem}
\begin{proof}
  In the finite-dimensional case, let $\langle , \rangle_0$ be any inner product on $V$; in the Hilbert case, let it denote the ``given'' inner product; in either case, note that it defines a continuous map $V \times V \rightarrow V$.  The idea is to average this inner product using the Haar measure to get an invariant inner product.  Turning to details, recall that the action map $G \times V \rightarrow V$ is assumed continuous.  For $v_1, v_2$, the function $G \ni g \mapsto \langle g v_1, g v_2 \rangle_0$ is then a continuous function on a compact set, hence is bounded, and in particular integrable with respect to the probabilty Haar $d g$.  We set
  \begin{equation*}
    \langle v_1, v_2 \rangle := \int_{g \in G} \langle g v_1, g v_2 \rangle_0 \, d g.
  \end{equation*}
  It's easy to see that $\langle , \rangle$ defines an inner product; for instance, if $v \neq 0$, then $\langle v,v \rangle_0 > 0$, hence (by continuity) $\langle g v, g v \rangle_0 > 0$ for all $g$ in some neighborhood of the identity element, hence (by regularity of Radon measures) $\langle v, v \rangle > 0$.  Using the right-invariance of $d g$, we verify for $h \in G$ that
  \begin{equation*}
    \langle h v_1, h v_2 \rangle = \int_{g \in G} \langle g h v_1, g h v_2 \rangle_0 \, d g = \int_{g \in G} \langle g v_1, g v_2 \rangle_0 \, d g = \langle v_1, v_2 \rangle.
  \end{equation*}
  Thus the representation $V$ is unitary with respect to the inner product $\langle , \rangle$.
\end{proof}

\subsection{Morphisms, equivariant maps, intertwiners, isomorphisms, equivalences, and so on}

\begin{definition}
  Let $(\pi_1, V_1)$ and $(\pi_2, V_2)$ be representations of $G$.  By a \emph{morphism} of representations $\phi : V_1 \rightarrow V_2$ we will mean a continuous linear map that is compatible with the given representations in the sense that $\pi_2(g) \phi(v_1) = \phi(\pi_1(g) v_1)$ for all $v_1 \in V_1$ and $g \in G$.  The following phrases will be used interchangeably with ``morphism'':
  \begin{itemize}
  \item $G$-equivariant map, or simply equivariant map.
  \item intertwining operator.
  \end{itemize}
  We denote by
  \begin{equation*}
    \Hom_G(V_1,V_2)
  \end{equation*}
  the set of such $\phi$.  This set is a vector space.
  
  This definition endows the set of representations of a given group $G$ with the structure of a category.
  
  An \emph{isomorphism} (or sometimes ``equivalence'', etc.)  of representations is a morphism that admits a two-sided inverse morphism; in the case of finite-dimensional representations, an isomorphism is the same thing as a bijective morphism.
\end{definition}

\begin{example}
~
  \begin{enumerate}
  \item For finite-dimensional representations $V_1, V_2$ of $G$, the isomorphism of vector spaces $V_1^* \otimes V_1 \cong \Hom(V_1,V_2)$ (as recalled in \S\ref{sec:linear-algebra}) defines an isomorphism of representations.
  \item Let $G = \mathbb{Z}/n \mathbb{Z}$; thus $G$ is the finite cyclic group of order $n$.  Take for $(\pi_1,V_1)$ the right regular representation on $V_1 = \mathbb{C}^G$, thus
    \begin{equation*}
      \pi_1(g) f(x) = f(x + g).
    \end{equation*}
    Take $V_2 = \mathbb{C}^n$, with $\pi_2$ the representation assigning to each $g \in G$ the diagonal matrix $\pi_2(g)$ with entries $1, e(g/n), e(2 g/n), e(3 g/n), \dotsc, e((n-1) g/n)$, where $e(x) := e^{2 \pi i x}$.  Then $(\pi_1,V_1)$ and $(\pi_2,V_2)$ are equivalent representations; an isomorphism is given by the finite Fourier transform
    \begin{equation*}
      \phi : V_1 \rightarrow V_2
    \end{equation*}
    \begin{equation*}
      \phi(f) := (\hat{f}(0), \hat{f}(1), \dotsc, \hat{f}(n-1)),
    \end{equation*}
    \begin{equation*}
      \hat{f}(x) := \sum_{y \in \mathbb{Z}/n} f(y) e(-x y/n).
    \end{equation*}
  \end{enumerate}
\end{example}
\footnote{End of lecture \#1, Tuesday, 19 Feb 2019}

We pause to introduce, for any representation $(\pi,V)$, the notation
\begin{equation}\label{eq:}
  V^G :=  \{v \in V : \pi(g) v = v \text{ for all } g \in G\}
\end{equation}
for the \emph{$G$-fixed subspace} of $V$.  Recall that we have defined for any representations $V_1, V_2$ a representation $\Hom(V_1,V_2)$; it thus makes sense to speak of the fixed subspace of the latter representation, and we verify readily from the definitions that
\begin{equation}\label{eq:}
  \Hom(V_1,V_2)^G = \Hom_G(V_1, V_2).
\end{equation}

We defined earlier the dual of any finite-dimensional representation.  More generally, for any representation for which the underlying vector space is given the structure of a Hilbert space, we define the dual representation using the \emph{continuous} dual, which is then itself a Hilbert space.
\begin{lemma}\label{lem:unitary-implies-conjugate-self-dual}
  Let $(\pi,V)$ be a representation of $G$ such that
  \begin{itemize}
  \item $V$ is a Hilbert space $(V, \langle , \rangle)$, and
  \item $\pi$ is unitary.
  \end{itemize}
  Then $\pi$ is isomorphic to its conjugate dual, and the conjugate and dual of $\pi$ are isomorphic to each other:
  \begin{equation*}
    \pi \cong \overline{\pi }^*, \quad \overline{\pi } \cong \pi^*.
  \end{equation*}
\end{lemma}
\begin{proof}
  Since $V$ is a Hilbert space, we have (by what is sometimes called the ``Riesz representation theorem'') an isomorphism of vector spaces
  \begin{equation*}
    V \rightarrow \overline{V}^*
  \end{equation*}
  \begin{equation*}
    v \mapsto \langle v, \cdot \rangle.
  \end{equation*}
  Since $\pi$ is unitary, this isomorphism is equivariant.  This establishes the first isomorphism; the second is obtained similarly.
\end{proof}


\subsection{Reduction and decomposition}
\begin{definition}
  Let $(\pi,V)$ be a representation of $G$.  A \emph{closed invariant subspace} $W \subseteq V$ is a closed subspace such that $\pi(g) W \subseteq W$ for all $g \in G$.  We note that:
  \begin{itemize}
  \item A subspace $W \subseteq V$ is closed if and only if the quotient space $V/W$, equipped with the quotient topology, is Hausdorff.  If $V$ is finite-dimensional, then every subspace $W$ is closed.  We will practically never discuss non-closed subspaces in this course.
  \item The condition $\pi(g) W \subseteq W$, applied both to $g$ and $g^{-1}$, implies that in fact $\pi(g) W = W$.
  \end{itemize}
  To each closed invariant subspace $W \subseteq V$ we may associate a \emph{subrepresentation} $\pi : G \rightarrow \GL(W)$ and a \emph{quotient representation} $\pi : G \rightarrow \GL(V/W)$.  We will often use ``subrepresentation'' as a synonym for ``closed invariant subspace.''
\end{definition}
For instance, if $\dim(W) = 2$ and $\dim(V) = 5$, then we can extend a basis $e_1,e_2$ for $W$ to a basis $e_1,\dotsc,e_5$ for $V$, and the matrix entries of our representation expressed in terms of this basis look like
\begin{equation*}
  \pi(g) =
  \begin{pmatrix}
    \ast & \ast & \ast & \ast & \ast \\
    \ast & \ast & \ast & \ast & \ast \\
    0 & 0 & \ast & \ast & \ast \\
    0 & 0 & \ast & \ast & \ast \\
    0 & 0 & \ast & \ast & \ast
  \end{pmatrix}
.
\end{equation*}
The upper-left $2 \times 2$ block corresponds to the subrepresentation on $W$, the lower-right $3 \times 3$ block to the quotient representation on $V/W$.

\begin{example}
  \begin{enumerate}
  \item For any representation $(\pi,V)$ of $G$, the fixed subspace $V^G \subseteq V$ is a closed invariant subspace.
  \item If $G$ acts in a measure-preserving fashion on some measured space $(X,\mu)$ of finite volume, then the representation $V = L^2(X,\mu)$ contains the closed invariant subspace $\mathbb{C}$ consisting of constant functions.
  \end{enumerate}
\end{example}

\begin{definition}
  Let $(\pi,V)$ be a nonzero representation (thus $V$ is not the zero-dimensional space $\{0\}$; it might be the one-dimensional trivial representation).

  We say that $V$ is \emph{reducible} if there exists a closed invariant subspace $W \subseteq V$ with $W \neq \{0\}, V$.  We say otherwise that $V$ is \emph{irreducible}; this means that $\{0\}$ and $V$ are the only closed invariant subspaces.
\end{definition}

\begin{example}
  ``Most'' $V$ considered previously are reducible.  If $\dim(V) = 1$, then $V$ is irreducible.
\end{example}
The irreducible representations are a bit like the prime numbers, with closed invariant subspaces playing the role of divisors and the zero space $\{0\}$ a bit like the unit element $1$.

\begin{theorem}
[Schur's lemma]
  Let $(\pi_1, V_1)$ and $(\pi_2, V_2)$ be irreducible finite-dimensional representations of some group $G$.  Then
  \begin{equation}\label{eq:}
    \dim \Hom_G (V_1, V_2) = 
\begin{cases}
      1 & \text{ if } V_1 \cong V_2, \\
      0 & \text{ otherwise},
    \end{cases}
  \end{equation}
  where ``$V_1 \cong V_2$'' means ``isomorphic as representations of $G$.''  If moreover $V = V_1 = V_2$, then $\End_G(V) := \Hom_G(V,V)$ is the space $\mathbb{C} \id$ of scalar multiples of the identity $\id : V \rightarrow V$.
\end{theorem}
\begin{proof}
  Let $\phi$ be a nonzero element of $\Hom_G(V_1,V_2)$.  Then $\image(\phi) \subseteq V_2$ and $\ker(\phi) \subseteq V_1$ are invariant subspaces.  Since $\phi \neq 0$, we must have $\image(\phi) \neq 0$ and $\ker(\phi) \neq V_1$.  Thus by the irreducibility of $V_1, V_2$, we have $\image(\phi) = V_1$ and $\ker(\phi) = 0$.  Thus $\phi$ is an isomorphism.  In particular, if $V_1, V_2$ are non-isomorphic, then $\Hom_G(V_1,V_2) = \{0\}$.  If $V_1$ and $V_2$ are isomorphic, then the spaces $\Hom_G (V_1, V_2) \cong \Hom_G(V_1,V_1)$ are isomorphic via composition with an isomorphism $V_1 \rightarrow V_2$, so in particular $\dim \Hom_G (V_1, V_2) = \dim \Hom_G(V_1,V_1)$; we thereby reduce to establishing the final assertion concerning $\phi \in \End_G(V)$.  ({\bf Note: this last step was treated incorrectly in lecture!})  Since $V$ is a nonzero finite-dimensional vector spaces over the complex numbers, the operator $\phi$ has an eigenvalue $\lambda$.  Let $V_\lambda$ denote the corresponding eigenspace.  For any $g \in G$ and $v \in V_\lambda$, we have
  \begin{equation}
    \phi(\pi(g) v)
    =
    \pi(g) \phi(v)
    = \pi(g) \lambda v
    = \lambda \pi(g) v,
  \end{equation}
  thus $\pi(g) v \in V^\lambda$, i.e., $V^\lambda$ is a nonzero invariant subspace; since $V$ is irreducible, it follows that $V^\lambda = V$, and so $\phi = \lambda \cdot \id$, as required.
\end{proof}

\begin{remark}
  Inserted to keep numbering consistent with the numbering that I messed up in lecture; maybe I'll think of something clever to put here later.
\end{remark}
\begin{remark}
  Same.
\end{remark}

\begin{corollary}\label{cor:schur-ab}
  Suppose that $G$ is abelian and $(\pi,V)$ is finite-dimensional and irreducible.  Then $\dim(V) = 1$.
\end{corollary}
\begin{proof}
  Let $h \in G$.  Then for all $g \in G$, $\pi(h) \pi(g) = \pi(h g) = \pi(g h) = \pi(g) \pi(h)$.  Thus $\pi(h) \in \End_G(V) = \mathbb{C} \id$, and so $\pi(h)$ stabilizes every line in $V$.  Thus any line in $V$ is a nonzero invariant subspace; by irreducibility, any such line is equal to $V$, and so $\dim(V) = 1$.
\end{proof}

As a matter of notation, we now define for a compact group $G$ the set
\begin{equation*}
  \Irr(G) := \{\text{isomorphism classes of finite-dimensional irreducible representations $\pi = (\pi,V_\pi)$ of $G$}\}.
\end{equation*}

\begin{theorem}\label{thm:complete-reducibility-compact-group}
  Let $G$ be compact and $(\pi,V)$ finite-dimensional.  Then there exists for each $\sigma = (\sigma,W_\sigma) \in \Irr(G)$ a nonnegative integer $n(\sigma)$ so that we have an isomorphism of representations
  \begin{equation}\label{eq:}
    V \cong \oplus_{\sigma \in \Irr(G)} W_\sigma^{\oplus n(\sigma)}.
  \end{equation}
  (Here $W^{\oplus n} := W \oplus \dotsb \oplus W$, with $n$ copies.)  The $n(\sigma)$ are determined uniquely by $V$.
\end{theorem}
\begin{proof}
  Since $G$ is compact, we may equip $V$ with an invariant inner product $\langle , \rangle$.  We may assume also that $V$ is nonzero, since the conclusion of the theorem holds in that case.  Then there exist nonzero invariant subspaces $U_1$ of $V$, such as $V$ itself.  Choose a \emph{minimal} such subspace $U_1$.  Then the orthogonal complement $U_1^\perp$ is an invariant subspace, since for $v \in U_1^\perp$ and $g \in G$, we have for each $u \in U_1$ that likewise $g^{-1} \in U_1$, and so
  \begin{equation*}
    \langle g v, u \rangle = \langle v, g^{-1} u \rangle = 0.
  \end{equation*}
  By choosing (if possible) another minimal nonzero invariant subspace $U_2 \leq U_1^\perp$ and inducting on dimension (note that $\dim(U_1^\perp) < \dim(V) < \infty$), we see that we may write $V$ as the (orthogonal) direct sum
  \begin{equation}\label{eq:}
    V = U_1 \oplus U_2 \oplus \dotsb \oplus U_n,
  \end{equation}
  where each $U_j$ is an irreducible invariant subspace.  We now group the $U_j$ according to their isomorphism class, giving a partition of $\{1, \dotsc, n\}$ by $\Irr(G)$; this gives the required decomposition.

  The uniqueness of the $n(\sigma)$ may be deduced as in the proof of the Jordan--H{\"o}lder theorem; we will give an alternative proof later.
\end{proof}

This last result suggests two problems for a given $G$:
\begin{itemize}
\item Determine the set $\Irr(G)$ explicitly.
\item Explicitly decompose ``interesting'' $V$ in the above sense.
\end{itemize}

\subsection{Characters}
Throughout this section we take $G$ compact and $(\pi,V)$ finite-dimensional.
\begin{definition}
  The \emph{character} of $\pi$ is the function
  \begin{equation*}
    \chi_\pi : G \rightarrow \mathbb{C}
  \end{equation*}
  \begin{equation*}
    g \mapsto \trace(\pi(g)).
  \end{equation*}
  We sometimes write $\chi_V$ for $\chi_\pi$.
  
  A \emph{class function} $f : G \rightarrow \mathbb{C}$ is a function that is constant on conjugacy classes, thus $f(g^{-1} x g) = f(x)$ for all $x,g \in G$.  Note that characters are class functions (because trace is conjugation-invariant).
\end{definition}
The idea is that, given a linear operator on a finite-dimensional vector space, the only way to linearly assign to that operator a scalar without choosing a basis is to take the trace (or a multiple thereof).  We're looking to study isomorphism classes of representations, which suggests looking at their characters.  Here are some basic properties:
\begin{lemma}\label{lem:characters-basic}
  Let $\pi, \pi_1, \pi_2$ be finite-dimensional representations of the compact group $G$, as above.
  \begin{enumerate}
  \item $\chi_{\pi}$ depends only upon the isomorphism class of $\pi$.
  \item $\chi_{\overline{\pi }} = \overline{\chi_\pi}$
  \item $\chi_{\pi_1 \oplus \pi_2}= \chi_{\pi_1} + \chi_{\pi_2}$
  \item $\chi_{\pi_1 \otimes \pi_2}= \chi_{\pi_1} \chi_{\pi_2}$
  \item $\chi_{\pi^*} = \chi_{\overline{\pi }} = \overline{\chi_\pi }$
  \item $\chi_{\Hom(\pi_1,\pi_2)} = \overline{\chi_{\pi_1}} \chi_{\pi_2}$
  \end{enumerate}
\end{lemma}
\begin{proof}
  The only ``tricky'' part is that since $G$ is compact, $\pi$ is unitarizable, and so (by Lemma \ref{lem:unitary-implies-conjugate-self-dual}) $\overline{\pi } \cong \pi^*$, thus $\chi_{\pi^*} = \chi_{\overline{\pi }}$.  We also use that $\Hom(\pi_1,\pi_2) \cong \pi_1^* \otimes \pi_2$.
\end{proof}

The key to unlocking the power of characters in the case of compact groups is the following identity between the dimension of the fixed subspace and the average value of the character:
\begin{lemma}
  Define a linear map
  \begin{equation*}
    p : V \rightarrow V
  \end{equation*}
  \begin{equation*}
    v \mapsto \int_{g \in G} \pi(g) v \, d g.
  \end{equation*}
  (Here $d g$ denotes as usual the probability Haar, and we are integrating a function $g \mapsto \pi(g) v$ valued in the finite-dimensional vector space $V$; this can be defined by choosing a basis and integrating each coordinate, for instance.)  Then
  \begin{equation}\label{eq:}
    \dim(V^G) = \trace(p)
    = \int_{g \in G} \chi_\pi(g) \, d g.
  \end{equation}
\end{lemma}
\begin{proof}
  We check readily that $p(V) \subseteq V^G$ and that $p$ restricts to the identity on $V^G$, so that $p$ is a projection onto $V^G$; its trace is thus the dimension of that subspace, giving the first identity.

  The second identity follows from the linearity of the trace map.  (In more detail, let's fix a basis $e_1,\dotsc,e_n$ for $V$ and dual basis $e_1^*,\dotsc,e_n^*$ and write $\pi_{ij}(g) := \langle e_j^*, \pi(g) e_i \rangle$ for the coefficients of $\pi(g)$ with respect to that basis.  Let $p_{ij} := \langle e_j^*, p e_i \rangle$ denote the coefficients of $p$.  By definition, $p_{i j} = \langle e_j^*, \int_{g \in G} \pi(g) e_i \, d g \rangle$.  The coefficients of a vector-valued integral are obtained by integrating the coefficients, i.e., $\langle e_j^*, \int_{g \in G} \pi(g) e_i \, d g \rangle = \int_{g \in G} \langle e_j^*, \pi(g) e_i \rangle \, d g$, thus $p_{i j} = \int_{g \in G} \pi_{i j}(g) \, d g$.  Summing over $i = j$ gives $\trace(p) = \sum_i p_{i i} = \sum_{i} \int_{g \in G} \pi_{i i}(g) \, d g =\int_{g \in G} \sum_{i} \pi_{i i}(g) \, d g = \int_{G} \chi_\pi$, as required.
\end{proof}


\begin{theorem}\label{thm:basic-orthogonality-characters}
  \begin{enumerate}
  \item Let $\pi, \pi ' \in \Irr(G)$.  Then
    \begin{equation}\label{eq:characters-orthonormal}
      \langle \chi_\pi, \chi_{\pi '} \rangle_{L^2(G)}
      = 
\begin{cases}
        1 & \text{ if $\pi \cong \pi '$}, \\
        0 & \text{ otherwise.}
      \end{cases}
    \end{equation}
    Thus $\{\chi_\pi : \pi \in \Irr(G)\}$ is an orthonormal subset of
    \begin{equation*}
L^2(G)^{\class} := \{\text{class functions in $L^2(G)$}\}.
    \end{equation*}
  \item Let $(\pi,V)$ be any finite-dimensional representation, with decomposition
    \begin{equation*}
V = \oplus_{\sigma \in \Irr(G)} W_\sigma ^{\oplus n(\sigma)}
\end{equation*}
 as before.  Then
    \begin{equation}\label{eq:}
      n(\sigma) = \langle \chi_\pi, \chi_\sigma  \rangle.
    \end{equation}
    In particular, $\chi_\pi$ determines $\pi$ up to isomorphism.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
  \item By definition, $\langle \chi_{\pi}, \chi_{\pi '} \rangle = \int_G \chi_\pi \overline{\chi_{\pi '}}$.  We have seen that $\chi_\pi \overline{\chi_{\pi '}} = \chi_{\Hom(\pi ', \pi)}$, and that the average value of the latter is the dimension of $\Hom(\pi ', \pi)^G = \Hom_G(\pi ', \pi)$; the required conclusion now follows from Schur's lemma.
  \item We have $\chi_\pi = \sum_\sigma n(\sigma) \chi_\sigma$, so the conclusion follows from \eqref{eq:characters-orthonormal}.
  \end{enumerate}
\end{proof}

\begin{theorem}[Part of the Peter--Weyl theorem]\label{thm:peter-weyl-for-chars}
  Recall that $G$ is compact.  The orthonormal subset $\{\chi_\pi : \pi \in \Irr(G)\}$ of $L^2(G)^{\class}$ is in fact an orthonormal basis, i.e., has dense span.
\end{theorem}
\begin{proof}
  We postpone this proof until a bit later in the course; it is not so difficult, but we prefer to compute some actual characters first.
\end{proof}

\begin{corollary}
  If $G$ is finite, then $\Irr(G)$ is a finite set whose cardinality is the number of conjugacy classes $C$ of $G$.
\end{corollary}
\begin{proof}
  In that case $L^2(G)^{\class}$ has a basis given by the characteristic functions $1_C$ of each such $C$.
\end{proof}

\subsection{The connected compact abelian case}\label{sec:conn-comp-abel}
Let's illustrate the content of Peter--Weyl by explicating it completely in the case of a connected compact abelian Lie group.  In such a group, conjugation is trivial, so ``class function'' just means ``function;'' also, we have seen that any irreducible finite-dimensional representation is one-dimensional, hence may be identified with a group homomorphism $\pi : G \rightarrow \GL_1(\mathbb{C}) = \mathbb{C}^\times$, which is thus the ``same'' as its character, i.e., $\pi(g)$ and $\chi_\pi(g)$ are the same complex scalar for all $g \in G$.  Recall that
\begin{equation*}
  \U(1) := \{z \in \mathbb{C}^\times : |z| = 1\} \cong \mathbb{R}/\mathbb{Z},
\end{equation*}
\begin{equation*}
  e(\theta) := e^{2 \pi i \theta} \mapsfrom \theta.
\end{equation*}
\begin{lemma}\label{lem:reps-of-tori}
  Let $G$ be a compact connected abelian Lie group of dimension $n \geq 0$.  Then
  \begin{equation*}
    G \cong \U(1)^n \cong (\mathbb{R}/\mathbb{Z})^n \cong \mathbb{R}^n/\mathbb{Z}^n.
  \end{equation*}
  One has a bijection
  \begin{equation*}
    \Irr(G) \leftrightarrow \mathbb{Z}^n
  \end{equation*}
  given by associating to each $\lambda = (\lambda_1,\dotsc,\lambda_n) \in \mathbb{Z}^n$ the one-dimensional representation
  \begin{equation}\label{eq:defn-char-of-U1-to-the-n}
    \U(1)^n
    \ni z = (z_1,\dotsc,z_n)
    \mapsto z^{\lambda} :=
    z_1^{\lambda_1} \dotsb z_n^{\lambda_n}
    \in \U(1) \subseteq \GL_1(\mathbb{C}).
  \end{equation}
\end{lemma}
\begin{proof}
  We verify first that $G \cong \U(1)^n$:
  \begin{enumerate}
  \item Set $\mathfrak{g} = \Lie(G)$.  Since $G$ is abelian, $\exp : \mathfrak{g} \rightarrow G$ is a homomorphism.
  \item Since $G$ is connected and $\exp$ is a homomorphism whose image contains a neighborhood of the identity element of $G$, it follows that $\exp$ is surjective.
  \item The kernel $\Lambda \subseteq \mathfrak{g}$ of $\exp$ is a subgroup.  Using that the exponential map is a local diffeomorphism near the origin, we see that $\Lambda$ is discrete.  (Indeed, if there were a sequence $v_n \in \Lambda$ that converged to some element of $\Lambda$, then the sequence of differences $v_{n+1} - v_n$ would converge to $0$.)
  \item By general theory on quotients of Lie groups (see e.g. Theorem 157 in my Fall 2016 notes), we have $G \cong \mathfrak{g}/\Lambda$.  In particular, $\Lambda$ is cocompact.
  \item It's not hard to see that for every discrete cocompact subgroup $\Lambda$ of an $n$-dimensional Euclidean space $\mathfrak{g}$ there exists an isomorphism $\mathfrak{g} \cong \mathbb{R}^n$ under which $\Lambda$ identifies with $\mathbb{Z}^n$.  (Indeed, induct on $n$.  We may assume that $n \geq 1$.  Fix an arbitrary Euclidean norm on $\mathfrak{g}$.  Since $\Lambda$ is discrete, we can find a nonzero element $e_1 \in V$ of minimal norm.  We might suppose having normalized our norm so that the norm of $e_1$ is exactly $1$.  In any event, the minimality of this norm implies that $\mathbb{R} e_1 \cap \Lambda = \mathbb{Z} e_1$.  Let $W$ be a subspace of $\mathfrak{g}$ complementary to $\mathbb{R} e_1$ (e.g., the orthogonal complement), and let $p : V \rightarrow W$ be the projection with kernel $\mathbb{R} e_1$.  We then have a short exact sequence of $\mathbb{Z}$-modules
    \begin{equation*}
0 \rightarrow \mathbb{Z} e_1 \rightarrow \Lambda \rightarrow p(\Lambda) \rightarrow 0.
\end{equation*}
 We claim that $p(\Lambda) \subseteq W$ is a discrete subgroup; it follows then inductively that $p(\Lambda)$ is a finite free $\mathbb{Z}$-module of rank at most $\dim(W) = n-1$, hence that $\Lambda$ is a free $\mathbb{Z}$-module of rank at most $n$; the assumed compactness of $\mathfrak{g}/\Lambda$ then forces the rank to equal $n$, and so $\Lambda \cong \mathbb{Z}^n$ with respect to some coordinates.

    To verify the claim, it suffices to bound from below the norms of nonzero elements of $p(\Lambda)$ (this implies that such elements can't accumulate at the origin, hence neither can their differences, giving the required discreteness).  So let $v \in \Lambda$ with $p(v) \neq 0$, i.e.,
    \begin{equation*}
      v \notin \mathbb{R} e_1.
    \end{equation*}
    The difference $v - p(v)$ then lies in $\mathbb{R} e_1$.  We can find an integral multiple $m e_1$ of $e_1$ that ``best approximates'' this difference in the sense that
    \begin{equation*}
v - p(v) - m e_1 \in [-1/2,1/2] e_1.
\end{equation*}
 We have $v \neq m e_1$, so $v - m e_1$, being a nonzero element of $\Lambda$, has norm bounded from below by $1$ (the minimal norm of any such element).  But elements of $[-1/2,1/2] e_1$ have norm at most $1/2$.  The triangle inequality thus forces $p(v)$ to have norm at least $1/2$.)
  \end{enumerate}
  Having proved that $G \cong \U(1)^n$, we turn to classifying its irreducible finite-dimensional representations $\pi$.  By Corollary \ref{cor:schur-ab}, any such $\pi$ is one-dimensional, and so may be regarded as a homomorphism
  \begin{equation}\label{eq:}
    \pi : G \rightarrow \GL_1(\mathbb{C}) = \mathbb{C}^\times.
  \end{equation}
  Compose this with the isomorphism
  \begin{equation}\label{eq:}
    \mathbb{R}^n/\mathbb{Z}^n \ni \theta = (\theta_1,\dotsc,\theta_n) \mapsto
    e(\theta)  := (e(\theta_1),\dotsc,e(\theta_n)) \in G
  \end{equation}
  to get a morphism of Lie groups
  \begin{equation}\label{eq:}
    \tau : \mathbb{R}^n/\mathbb{Z}^n
    \ni \theta \mapsto \pi(e(\theta)) \in \mathbb{C}^\times.
  \end{equation}
  Differentiate this to obtain a morphism of Lie algebras
  \begin{equation}\label{eq:}
    \mathbb{R}^n \ni \theta \mapsto d \tau(\theta) \in \Lie(\mathbb{C}^\times) = \mathbb{C}.
  \end{equation}
  Any such morphism is of the form
  \begin{equation*}
    d \tau(\theta) = \lambda_1 \theta_1 + \dotsb + \lambda_n \theta_n \text{ for some $\lambda = (\lambda_1,\dotsc,\lambda_n) \in \mathbb{C}^n$.}
  \end{equation*}
  By the compatibility between Lie group morphisms, Lie algebra morphisms and the exponential map, we have
  \begin{equation*}
    \tau(\theta) = e(\lambda_1 \theta_1 + \dotsb + \lambda_n \theta_n).
  \end{equation*}
  Since $\tau(\mathbb{Z}^n) = \{1\}$, we have in particular
  \begin{equation}\label{eq:}
    e(\lambda_1) = \dotsb = e(\lambda_n) = 1,
  \end{equation}
  and so $\lambda_1,\dotsc,\lambda_n \in \mathbb{Z}$, as required.
  
  We note finally that all of the one-dimensional representations of $G = \U(1)^n$ that we have defined are inequivalent; for instance, we verify readily that they (and hence, what amounts to the same, their characters) are orthogonal to one another in $L^2(G)$, thanks to repeated application of the basic identity: for $\ell \in \mathbb{Z}$,
  \begin{equation}\label{eq:}
    \int_{\theta \in [0,1]}
    e(\ell \theta)
    \, d \theta
    =
    \begin{cases}
      1 & \text{ if } \ell = 0, \\
      0 & \text{ otherwise.}
    \end{cases}
  \end{equation}
\end{proof}
The content of Peter--Weyl in the setting of the compact connected abelian Lie groups $\U(1)^n$ is thus that, as $\lambda$ varies over $\mathbb{Z}^n$, the ``trigonometric polynomials'' $\U(1)^n \ni z \mapsto z^{\lambda}$ have dense span in $L^2(\U(1)^n)$.  This may be a familiar fact from abelian Fourier analysis.  It can be established using (e.g.) the Stone--Weierstrass theorem, or a bit of functional analysis.  The proof of the general case of the Peter--Weyl theorem, to be given a bit later, will involve similar arguments.\footnote{End of lecture \#2, Thursday, 21 Feb 2019}

\newpage

\section{Character theory of compact unitary groups}\label{sec:char-theory-comp}
\subsection{Recap}
Before beginning, let's pause to attach some vocabulary to results from last time.
\begin{definition}
  A \emph{torus} $T$ is a compact connected abelian Lie group.
\end{definition}
We saw last time that any $n$-dimensional torus $T$ is isomorphic to $\U(1)^n$, and that its irreducible representations are indexed by $\lambda \in \mathbb{Z}^n$ and given by
\begin{equation*}
  e^\lambda : \U(n) \rightarrow \U(1)
\end{equation*}
\begin{equation*}
  t = (t_1,\dotsc,t_n) \mapsto t^\lambda := (t_1^{\lambda_1},\dotsc,t_n^{\lambda_n}).
\end{equation*}

\subsection{Conjugacy classes in \texorpdfstring{$\U(n)$}{U(n)}}
\begin{definition}
  The compact unitary group $\U(n)$ is defined by
  \begin{equation}
    \U(n) := \{g \in \GL_n(\mathbb{C}) : \langle g v, g w
    \rangle
    = \langle v, w \rangle \text{ for all } v,w
    \in \mathbb{C}^n\},
  \end{equation}
  where $\langle v, w \rangle := \sum_{j=1}^n v_j \overline{w_j}$ denotes the standard inner product.  We can also describe it as the group of matrices $g = (g_{i j})$ whose rows (or columns) form an orthonormal basis, i.e.,
  \begin{equation*}
    \U(n) = \{g : \sum_k g_{i k} \overline{g_{j k}} = \delta_{i j} \} = \{g : \sum_k g_{k i} \overline{g_{k j}} = \delta_{i j} \}.
  \end{equation*}
\end{definition}
From the second description we see in particular that each $|g_{i j}| \leq 1$, hence that $\U(n)$ is compact.

Henceforth set
\begin{equation*}
  G := \U(n).
\end{equation*}
We're interested in studying (finite-dimensional) representations $\pi$ of $G$.  As we saw last time, we can do this by studying their characters $\chi_\pi$.  Characters are class functions, so we might get started by recalling what the conjugacy classes in $G$ look like.

Let $T \leq G$ denote the subgroup of diagonal elements.  Then
\begin{equation*}
  T \cong \U(1)^n
\end{equation*}
\begin{equation*}
  \begin{pmatrix}
    t_1 &  &  \\
        & \ddots  &  \\
        & & t_n
  \end{pmatrix}
  \leftrightarrow (t_1,\dotsc,t_n).
\end{equation*}
Let $W \leq G$ denote the subgroup of permutation matrices, i.e., the image of the permutation representation of the symmetric group discussed previously.  Then $W$ acts on $T$ by conjugation, permuting coordinates:
\begin{equation*}
  w \cdot t := w t w^{-1} = (t_{w^{-1}(1)}, t_{w^{-1}(2)},\dotsc , t_{w^{-1}(n)}).
\end{equation*}
Here in writing $w^{-1}(j)$ we regard $w$ as a permutation of $\{1,\dotsc,n\}$.  We have $|W| = n!$.

\begin{lemma}\label{lem:describe-conjugacy-classes-Un}
  Every conjugacy class in $G$ intersects $T$, and two elements of $T$ are conjugate in $G$ precisely when they have the same $W$-orbit.
\end{lemma}
\begin{proof}
  This is presumably known from a linear algebra course as the ``spectral theorem for unitary operators,'' but perhaps it won't hurt to recall the proof:
  
  Let $g \in G$.  We can then find an orthonormal basis $v_1,\dotsc,v_n$ for $\mathbb{C}^n$ consisting of eigenvectors for $g$.  (Take for $v_1$ any eigenvector, normalize it to have norm $1$, observe that the unitarity of $g$ implies that $g$ stabilizes the orthogonal complement $v_1^\perp$, take for $v_2$ any norm one eigenvector of $g$ in $v_1^\perp$, and so on.  Alternatively, let $\Gamma$ denote the closure of the group generated by $g$; then $\Gamma$ is a compact abelian group to which Theorem \ref{thm:complete-reducibility-compact-group} applies.  The ``two proofs'' are basically the same, of course.)  Let $h \in G$ have rows $v_1,\dotsc,v_n$.  Let $e_1,\dotsc,e_n$ denote the standard orthonormal basis of $\mathbb{C}^n$.  Then $h e_j = v_j$, so $t := h^{-1} g h$ is a diagonal element of $G$, i.e., $t \in T$.  This shows that every conjugacy class in $G$ intersects $T$.  If two elements of $T$ lie in the same $W$-orbit, then they are obviously conjugate in $G$, because $W \leq G$.  Conversely, if two elements of $T$ are conjugate in $G$, then they have the same multiset of eigenvalues, so we can find a permutation $w \in W$ sending one to the other.
\end{proof}

\begin{corollary}\label{cor:class-fns-via-T}
  Restriction defines a bijection
  \begin{equation*}
    \{\text{class functions on } G\} \leftrightarrow \{\text{$W$-invariant functions on } T\}.
  \end{equation*}
\end{corollary}

\subsection{Weight space decompositions
  of representations of \texorpdfstring{$\U(n)$}{U(n)}}\label{sec:weight-decmop-U-n}
In particular, let $(\pi,V)$ be any finite-dimensional representation of $G$.  Then its character $\chi_\pi : G \rightarrow \mathbb{C}$ is a class function, hence determined by its restriction $\chi_\pi|_{T}$ to $T$, which is $W$-invariant.  The restriction $\chi_{\pi}|_{T}$ may be regarded as the character of the restriction $\pi|_T : T \rightarrow \GL(V)$ of the representation $\pi$.  By our general discussion of representations of tori (Lemma \ref{lem:reps-of-tori}) and of compact groups (Theorem \ref{thm:complete-reducibility-compact-group}), we may decompose $\pi|_T$ as a direct sum of irreducible representations $e^\lambda : t \mapsto t^\lambda$ of $T$, each occurring with some multiplicities $m_\pi(\lambda) \in \mathbb{Z}_{\geq 0}$ (denoted ``$n(\sigma)$'' in the cited theorem).  Of course $\dim(\pi) = \sum _{\lambda} m_\pi (\lambda)$, so only finitely many of the $m_\pi(\lambda)$ are nonzero.  Thus for $t \in T$,
\begin{equation}\label{eq:}
  \chi_\pi(t)
  =
  \sum_{\lambda}
  m_\pi(\lambda) t^\lambda.
\end{equation}
Setting
\begin{equation*}
V^{\lambda} := \{v \in V : \pi(t) v = t^\lambda v \text{ for all } t \in T\},
\end{equation*}
 we have 
\begin{equation*}
\dim V^{\lambda} = m_\pi(\lambda)
\end{equation*}
 and
\begin{equation*}
V = \oplus_{\lambda} V^{\lambda}.
\end{equation*}
 If $m_\pi(\lambda) > 0$ (equivalently, $V^{\lambda} \neq 0$), we say that $\lambda$ is a \emph{weight} of $\pi$ and refer to $m_\pi(\lambda)$ as the \emph{multiplicity} of $\lambda$ in $\pi$, to $V^{\lambda}$ as the corresponding \emph{weight space}, and to nonzero elements of $V^{\lambda}$ as \emph{weight vectors}.

Some examples will presumably clarify matters.  In what follows we identify characters with functions on $T$ as above.  We also write, e.g., $t_j$ as shorthand for the map $T \ni t \mapsto t_j$.
\begin{example}
  \begin{enumerate}
  \item The trivial representation $\pi$ has character $1$, so the only weight is $\lambda = 0$, with multiplicity $1$.
  \item The standard representation $\mathbb{C}^n$ has a basis of weight vectors $e_1,\dotsc,e_n$ (the standard basis) with corresponding weights $t_1,\dotsc,t_n$ and each of multiplicity $1$, so the character is $t_1 + \dotsb + t_n$.
  \item The dual $(\mathbb{C}^n)^*$ of the standard representation, with basis of weight vectors $e_1^*,\dotsc,e_n^*$ (the dual of the standard basis, i.e., $\langle e_i^*, e_j \rangle = \delta_{i j}$) with corresponding weights $t_1^{-1},\dotsc,t_n^{-1}$ each of multiplicity $1$, hence character given by $t_1^{-1} + \dotsb + t_n^{-1}$.
  \item The complexified adjoint representation $\Ad : G \rightarrow \GL_{\mathbb{R}}(\mathfrak{g}) \rightarrow \GL(\mathfrak{g}_{\mathbb{C}})$.  Here $\mathfrak{g}_{\mathbb{C}} \cong \glLie_n(\mathbb{C})$ because $\glLie_n(\mathbb{C}) \cong \mathfrak{g} \oplus i \mathfrak{g}$ via the map $x \mapsto (\tfrac{1}{2} (x - \overline{x}^t), \tfrac{1}{2} (x + \overline{x}^t))$.  We can thus identify $\mathfrak{g}_{\mathbb{C}}$ with the space of $n \times n$ complex matrices, which has a standard basis $E_{i j}$.  The action is described in this optic by $\Ad(g) x = g x g^{-1}$.  We have $\Ad(t) E_{i j} = (t_i/t_j) E_{i j}$, so the character is given by $\sum_{i,j} t_i/t_j$, which we may rewrite as $n + \sum_{i \neq j} t_i/t_j$.  Thus the trivial character of $T$ is a weight with multiplicity $n$.  The nontrivial weights are indexed by $i \neq j$, occur with multiplicity one, and are given by $\eps_i - \eps_j : t \mapsto t_i/t_j$.  (These, the nontrivial weights for the adjoint action, are in general called \emph{roots}.)
    
    We note incidentally that the representation $\Ad : G \rightarrow \GL(\mathfrak{g}_{\mathbb{C}})$ is isomorphic to $\End(\mathbb{C}^n)$, i.e., to the tensor product $\mathbb{C}^n \otimes (\mathbb{C}^n)^*$ of the standard representation and its dual; this isomorphism is reflected in the character identity $\sum_{i,j} t_i/t_j = (\sum_i t_i) (\sum_j t_j^{-1})$.
  \item The $k$th symmetric power $\Sym^k(\mathbb{C}^n)$ of the standard representation has a basis of weight vectors $e_{i_1} \dotsb e_{i_k}$ indexed by $i_1 \leq \dotsb \leq i_k$, with corresponding weights $t_{i_1} \dotsb t_{i_k}$, each occurring with multiplicity one; the character is
    \begin{equation}\label{eq:wts-sym-k}
      \sum_{i_1 \leq \dotsb \leq i_k} t_{i_1} \dotsb t_{i_k}.
    \end{equation}
  \item The $k$th exterior power $\Lambda^k(\mathbb{C}^n)$ of the standard representation has a basis of weight vectors $e_{i_1} \wedge \dotsb \wedge e_{i_k}$ indexed by $i_1 < \dotsb < i_k$, with corresponding weights $t_{i_1} \dotsb t_{i_k}$, each occurring with multiplicity one; the character is
    \begin{equation*}
      \sum_{i_1 < \dotsb < i_k} t_{i_1} \dotsb t_{i_k},
    \end{equation*}
    which is often called the \emph{$k$th elementary symmetric function}.
  \item The $k$th tensor power $(\mathbb{C}^n)^{\otimes k}$ of the standard representation has basis of weight vectors $e_{i_1} \otimes \dotsb \otimes e_{i k}$ indexed by any $i_1,\dotsc,i_k$, with corresponding weights $t_{i_1} \dotsb t_{i_k}$.  The character is
    \begin{equation*}
      \sum_{i_1,\dotsc,i_k} t_{i_1} \dotsb t_{i_k} = (t_1 + \dotsb + t_n)^k,
    \end{equation*}
    i.e., the $k$th power of the character of the standard representation, as we could have predicted from Lemma \ref{lem:characters-basic}.  Note that nontrivial multiplicities $m_\pi(\lambda)$ occur in general, and are given by multinomial coefficients.
  \end{enumerate}
\end{example}
Note that in all of these examples, we indeed obtained a $W$-invariant Laurent polynomial $\sum_{\lambda} m_\pi(\lambda) t^\lambda$.

\subsection{The Weyl integral formula
  for \texorpdfstring{$\U(n)$}{U(n)}}\label{sec:weyl-integr-form}
Enough examples for now.  Our goal is to use the orthogonality relations for characters of irreducible representations to derive formulas for them.  Since we've seen that the characters assume a particularly simple form when we restrict them to $T$, we might try first to understand how the integral over $G$ of a class function may be expressed in terms of the restriction of that function to $T$.
\begin{theorem}[Weyl integral formula]\label{thm:weyl-int}
  For a continuous class function $f : G \rightarrow \mathbb{C}$, we have
  \begin{equation*}
    \int_G f = \frac{1}{|W|} \int_T |\Delta|^2 f,
  \end{equation*}
  where
  \begin{equation*}
    \Delta(t) := \prod_{i < j} (t_i - t_j).
  \end{equation*}
  Here both integrals are taken with respect to probability Haar measures.  More generally, for any continuous $f : G \rightarrow \mathbb{C}$, we have
  \begin{equation*}
    \int_G f = \frac{1}{|W|} \int_{g \in G} \int_{t \in T} |\Delta(t)|^2 f(g t g^{-1}) \, d t \, d g.
  \end{equation*}
\end{theorem}
This should be thought of as a bit like the formula for integrating in polar coordinates in $\mathbb{R}^3$.  We give the proof below.  For further reading, see
\begin{itemize}
\item Weyl's original treatment (see section V.17 of ``Group theory and quantum mechanics'' in the course references),
\item the treatment given in sections I.5 and IV.1 of BTD (the course reference with those author initials)
\item section VIII.5 of Knapp's ``Lie groups beyond and introduction,''
\item section 6.4 of Rossmann's book,
\item and others from the reference list.
\end{itemize}

\begin{proof}
  We first review some basics concerning integration on manifolds.  Let $M$ be a connected $N$-manifold, and let $\omega$ be a nowhere vanishing differential $N$-form on $M$.  Then $\omega$ defines an orientation and a volume measure $C_c(M) \ni f \mapsto \int_M f \, d \omega$: for any function $f$ supported in an oriented coordinate chart $(x_1,\dotsc,x_N)$, we set
  \begin{equation}\label{eq:}
    \int_M f \, d \omega :=
    \int_{x_1,\dotsc,x_N}
    f(x_1,\dotsc,x_N)
    \omega(x_1 \wedge \dotsb \wedge x_N)
    \, d x_1 \dotsb d x_N,
  \end{equation}
  and then extend this definition to general $f$ via a suitable partition of unity.  If $(M,\omega)$ and $(M',\omega ')$ are two such pairs and $\phi : M \rightarrow M'$ is a differentiable map, that we may express the pullback $\phi^* \omega '$ as $\det(\phi) \omega$ for some function $\det(\phi) : M \rightarrow \mathbb{R}$ (the \emph{determinant} of $\phi$ with respect to $\omega$ and $\omega '$).  If $\phi$ is a diffeomorphism, then we have the change of variables formulas: for $f \in C_c(M')$,
  \begin{equation}\label{eq:change-of-vars}
    \int_{M'}
    f \, d \omega ' = \int_M (f \circ \phi) \cdot |\det(\phi)| \, d \omega.
  \end{equation}
  Also, determinant is multiplicative with respect to composition in the obvious sense.

  We next recall how to integrate on a compact Lie group using differential forms.  Let $\omega$ be a nonzero element of $\Lambda^N(\mathfrak{g})^*$.  Then, via the correspondence between $\mathfrak{g}$ and left-invariant vector fields on $G$, we may identify $\omega$ with a left-invariant $N$-form $\tilde{\omega}$ on $G$.  For each $g \in G$, the right translation map $R_g : G \rightarrow G$ has some determinant $\det(R_g)$ (defined with respect to $\tilde{\omega}$ both on source and target).  We check readily that $g \mapsto \det(R_g)$ defines a continuous homomorphism $G \rightarrow \mathbb{R}^\times_+$, but since $G$ is compact, any such map is trivial.  Thus $\tilde{\omega}$ is both left and right invariant, and so the corresponding volume measure is a Haar measure.  We assume $\omega$ normalized so that $\int_G \, d \tilde{\omega} = 1$; then $f \mapsto \int_G f \, d \tilde{\omega}$ is the probability Haar on $G$.  The two-sided invariance means that for any $g_1, g_2 \in G$, the local chart $g_1 e^x g_2 \mapsto x \in \mathfrak{g}$ at $g_1 g_2$ has determinant one at $g_1 g_2$.

  We may similarly realize the probability Haar on $T$ as $f \mapsto \int_T f d \, \tilde{\alpha}$, where $\tilde{\alpha}$ corresponds to some nonzero element $\alpha \in \Lambda^n(\mathfrak{t})^*$.

  Equip $\mathfrak{g}$ with any $\Ad(T)$-invariant scalar product (e.g., by averaging), and let $\mathfrak{g}/\mathfrak{t}$ denote the orthogonal complement to $\mathfrak{t}$.  Then $\mathfrak{g} = \mathfrak{t} \oplus \mathfrak{g}/\mathfrak{t}$.  The complexification $(\mathfrak{g}/\mathfrak{t})_{\mathbb{C}}$ of $\mathfrak{g}/\mathfrak{t}$ is then an $\Ad(T)$-invariant complement to $\mathfrak{t}_{\mathbb{C}}$ in $\mathfrak{g}_{\mathbb{C}}$; by our discussion of the adjoint representation above, we see that
  \begin{equation}\label{eq:}
    (\mathfrak{g}/\mathfrak{t})_{\mathbb{C}}
    = \oplus_{i \neq j} \mathbb{C} E_{i j},
  \end{equation}
  i.e., it is the space of matrices whose diagonal entries vanish.
  
  There is a unique $\beta \in \Lambda^{N-n}(\mathfrak{g}/\mathfrak{t})^*$ so that $\omega = \alpha \wedge \beta$.  For $t \in T$, the operator $\Ad(t)^*$ sends $\beta$ to $\det(\Ad(t)|_{\mathfrak{g}/\mathfrak{t}}) \beta$, but (either by explicit calculation, or using the compactness of $T$) the determinant in question is trivial, so $\beta$ is $\Ad(T)^*$-invariant.  It thus corresponds to a $G$-invariant $(N-n)$-form $\tilde{\beta}$ on the $(N-n)$-dimensional manifold $G/T$, whose corresponding volume form is invariant under left translation by $G$.  Fubini's theorem applied in local coordinates implies that for any $f \in C_c(G)$,
  \begin{equation}\label{eq:}
    \int_G f \, d \tilde{\omega}
    =
    \int_{g \in G/T}
    (\int_{t \in T}
    f(g t) \, d \tilde{\alpha}(t))
    \, d \tilde{\beta}(g).
  \end{equation}
  (Alternatively, note that the RHS defines a Haar probability measure on $G$, hence equals the LHS by uniqueness of Haar measures.)  Applying this relation with $f = 1$ gives in particular that $\int_{G/T} \, d \tilde{\beta} = 1$, hence that the integral of a function $G/T \rightarrow \mathbb{R}$ with respect to $d \tilde{\beta}$ is the same as the integral of its pullback to $G$ with respect to the probability Haar.  We can now restate the desired integral formula as
  \begin{equation}\label{eq:weyl-integral-via-diff-forms}
    \frac{1}{|W|}
    \int_{g \in G/T}
    (\int_{t \in T}
    |\Delta(t)|^2
    f(g t g^{-1}) \, d \tilde{\alpha}(t))
    \, d \tilde{\beta}(g)
    =
    \int_{G}  f \, d \tilde{\omega}.
  \end{equation}
  The basic idea of the proof is that the map
  \begin{equation*}
    \phi : G/T \times T \rightarrow G
  \end{equation*}
  \begin{equation*}
    (g T,t) \mapsto g t g^{-1}
  \end{equation*}
  is generically $|W|$-to-$1$ and has determinant $|\Delta|^2(t)$.

  Turning to rigorous details, let's compute $\det(\phi)$ with respect to the differential forms $\tilde{\beta} \times \tilde{\alpha}$ on $G/T \times T$ and $\tilde{\omega}$ on $G$.  The scalar $\det(\phi)(g T, t)$ is the same as the determinant at the origin of the composition
  \begin{equation*}
    \mathfrak{g}/\mathfrak{t} \oplus \mathfrak{t} \xdashrightarrow{(x,y) \mapsto (g \exp(x) T, \exp(y) t)} G/T \times T \xrightarrow{\phi } G \xdashrightarrow{g \exp(x'+y) t g^{-1} \mapsto (x',y)} \mathfrak{g}/\mathfrak{t} \oplus \mathfrak{t},
  \end{equation*}
  since in this composition the outermost two arrows have determinant one at the relevant argument by construction.  (A dashed arrow denotes a partial map, defined on a suitable open.)  But since
  \begin{equation*}
    (g e^x) (e^y t) (g e^x)^{-1} \simeq g \exp(x - \Ad(t) x + y) t g^{-1},
  \end{equation*}
  where $\simeq$ means ignoring terms that depend quadratically or higher upon $x$ and $y$, we see that the above composition has the same determinant at the origin as the linear map
  \begin{equation*}
    (x,y) \mapsto (x - \Ad(t) x, y).
  \end{equation*}
  We may compute the determinant of the operator $x \mapsto x - \Ad(t) x$ on $\mathfrak{g}/\mathfrak{t}$ after complexifying and using the explicit basis of weight vectors $E_{i j}$.  We obtain in this way that
  \begin{equation}\label{eq:compute-det-phi}
    \det(\phi)(g T, t)
    =
    \prod_{i \neq j}
    (1 - t_i/t_j)
    = |\Delta|^2(t).
  \end{equation}

  We observe next that the function $|\Delta|^2 : T \rightarrow \mathbb{R}_{>0}$ extends to a conjugacy-invariant function on $G$ sending $g \in G$ to the product of the squared magnitudes of the differences between its distinct eigenvalues, with $|\Delta|^2(g) = 0$ precisely when $g$ has a repeated eigenvalue.  Let $G_{\sing}$ denote the \emph{singular subset} consisting of $g \in G$ having a repeated eigenvalue; it is the zero locus of $|\Delta|^2$, hence a compact subset of $G$.  The function $|\Delta|^2$ is real-analytic (e.g., because it is a polynomial in the coefficients of the characteristic polynomial) and not identically zero on $G$, so its zero locus $G_{\sing}$ has measure zero; moreover, the volume of $\{g \in G : |\Delta|^2 < \eps \}$ tends to zero as $\eps \rightarrow 0$.  By an approximation argument, we can thus reduce to establishing \eqref{eq:weyl-integral-via-diff-forms} in the special case that $f$ is supported on $\{g \in G : |\Delta|^2 \geq \eps\}$ for some $\eps > 0$.  In particular, we may assume that $f$ is supported on a compact subset of the \emph{regular subset} $G_{\reg} := G - G_{\sing}$ consisting of $g \in G$ having distinct eigenvalues.

  Set $T_{\reg} := G_{\reg} \cap T$.  We observe that it consists of those $t \in T$ having trivial $W$-stabilizer.  (This turns out to be a special feature of $G = \U(n)$; what's essential is that the subset of $t \in T_{\reg}$ with trivial $W$-stabilizer has full measure.)  We observe that for $t \in T_{\reg}$, its $G$-centralizer is $T$ (indeed, any centralizing element must stabilize its eigenspaces, which are the one-dimensional spaces $\mathbb{C} e_j$).  From this and the fact (Lemma \ref{lem:describe-conjugacy-classes-Un}) that the conjugates of $x$ in $T$ lie in a single $W$-orbit, we deduce that $w \cdot (g T, t) := (g w^{-1} T, w t w^{-1})$ defines a simply-transitive action of $W$ on the fiber $\{(g T, t) : g t g^{-1} = x\}$ above $x \in G_{\reg}$.  In particular, the fibers all have cardinality $|W|$.  On the other hand, we see from \eqref{eq:compute-det-phi} that the induced map
  \begin{equation}\label{eq:}
    \phi : G/T \times T_{\reg} \rightarrow G_{\reg}
  \end{equation}
  has everywhere nonvanishing differential.  By the inverse function theorem, it follows that $\phi$ defines $|W|$-fold covering map.  By a partition of unity, we may assume that $f$ is supported in a small connected neighborhood $U \subseteq G_{\reg}$ of some $x \in G_{\reg}$ over which $\phi^{-1}(U) \rightarrow U$ is the trivial $|W|$-fold cover.  We conclude by applying \eqref{eq:change-of-vars} and \eqref{eq:compute-det-phi} on each connected component of $\phi^{-1}(U)$.
\end{proof}
\footnote{End of lecture \#3, Tuesday, 26 Feb 2019}

\subsection{Primer on symmetric polynomials}
We have encountered already several elements of the ring of Laurent polynomials in the variable $t_1,\dotsc,t_n$.  For their further study, it will be convenient to denote that ring by some letter:
\begin{equation*}
  L := \mathbb{Z}[t_1^{\pm 1},\dotsc,t_n^{\pm}]
\end{equation*}
As a $\mathbb{Z}$-module, $L$ has the basis consisting of the monomials $e^\lambda : t \mapsto t^{\lambda} = t_1^{\lambda_1} \dotsb t_n^{\lambda_n}$ considered previously, thus
\begin{equation*}
  L = \oplus_{\lambda \in \mathbb{Z}^n} \mathbb{Z} e^\lambda.
\end{equation*}
As above, we refer to the $\lambda$ as \emph{weights}.

The symmetric group $W \cong S(n)$ acts on $L$ by $w \cdot f(t_1,\dotsc,t_n) := f(t_{w(1)}, \dotsc, t_{w(n)})$.
\begin{definition}
  We say that $f \in L$ is \emph{symmetric} if $w \cdot f = f$ for all $w \in W$, and \emph{alternating} if $w \cdot f = (-1)^w f$ for all $w \in W$.  We denote by $L^{\sym}$ and $L^{\alt}$ the respective spaces of symmetric and alternating Laurent polynomials.
\end{definition}
We have seen that for any finite-dimensional representation $\pi$ of $G = \U(n)$, the restriction $\chi_\pi|_{T}$ to $T$ of its character $\chi_\pi$ is a symmetric polynomial.  To be pedantic, $\chi_\pi|_{T}$ is the function $\U(1)^n \rightarrow \mathbb{C}$ associated to a unique symmetric polynomial; we will identify polynomials with their associated functions freely in what follows.  We will also abbreviate the restriction $\chi_\pi|_{T}$ simply by $\chi_\pi$, keeping in mind that the character is determined by its restriction in view of Corollary \ref{cor:class-fns-via-T}.  Subject to these identifications, $\chi_\pi \in L^{\sym}$.

On the other hand, the discriminant polynomial $\Delta(t) = \prod_{i < j} (t_i - t_j)$ that appeared in the Weyl character formula is alternating.  One can make this more visible by using the Vandermonde determinant formula $\Delta(t) = \det(t_i^{n-j})_{i,j=1..n}$.  We can see that $\Delta \in L^{\alt}$ by recalling that the determinant changes sign when we swap a pair of its rows or columns.

The \emph{monomial symmetric polynomials} are defined for $\lambda \in \mathbb{Z}^n$ by
\begin{equation*}
M_{\lambda} := \sum_{\mu \in W \cdot \lambda} e^\mu.
\end{equation*}
 They obviously belong to $L^{\sym}$.  Note that we sum over the $W$-orbit of $\lambda$ without multiplicity.  Note also that $M_\lambda(t)$ is symmetric not only with respect to the argument $t$, but also with respect to the index $\lambda$, i.e., $M_{w \cdot \lambda} = M_\lambda$.  For example, if $n = 2$, then $M_{(7,6)}(t) = t_1^7 t_2^6 + t_1^6 t_2^7 = M_{(6,7)}(t)$, while $M_{(7,7)}(t) = t_1^7 t_2^7$.  We get a basis for $L^{\sym}$ from the $M_\lambda$'s by considering one $\lambda$ from each $W$-orbit on $\mathbb{Z}^n$.  It will be convenient to choose an explicit set of representatives:
\begin{definition}
  We say that $\lambda \in \mathbb{Z}^n$ is \emph{dominant} if $\lambda_1 \geq \dotsb \geq \lambda_n$, and \emph{strictly dominant} if $\lambda_1 > \dotsb > \lambda_n$.
\end{definition}
Thus the dominant $\lambda$ give a set of representatives for the $W$-orbits on $\mathbb{Z}^n$, and so the $M_\lambda$ for dominant $\lambda$ give a basis for $L^{\sym}$.

The \emph{elementary symmetric polynomials} are defined for $k=1,\dotsc,n$ by
\begin{equation*}
  \sigma_k(t) := \sum_{1 \leq i_1 < \dotsb < i_k \leq n} t_{i_1} \dotsb t_{i_k}.
\end{equation*}
For example, $\sigma_1 = t_1 + \dotsb + t_n$, $\sigma_2 = t_1 t_2 + t_1 t_3 + + \dotsb + t_{n-1} t_n$, while $\sigma_n = t_1 \dotsb t_n$.  We might recall from \S\ref{sec:weight-decmop-U-n} that $\sigma_k$ is the character of the $k$th exterior power $\Lambda^n(\mathbb{C}^n)$ of the standard representation $\mathbb{C}^n$.  The $\sigma_k$ together with the inverse of $\sigma_n$ are well-known to generate $L^{\sym}$; for completeness we state this below and sketch the proof.
\begin{lemma}\label{lem:symmetric-functino-theory}
  We have
  \begin{equation*}
    L^{\sym} = \oplus _{\lambda:\text{dominant}} \mathbb{Z} M_\lambda = \mathbb{Z}[\sigma_1,\sigma_2,\dotsc,\sigma_{n-1},\sigma_n,1/\sigma_n].
  \end{equation*}
\end{lemma}
\begin{proof}
  The first identity was noted earlier.  For the second identity, the containment ``$\supseteq$'' is clear; we must verify the reverse containment ``$\subseteq$'', i.e., that $M_\lambda$ belongs to the RHS for all dominant $\lambda$.  Replacing $M_\lambda$ with $M_\lambda/\sigma_n^{\lambda_n}$ has the effect of replacing $\lambda$ with $(\lambda_1 - \lambda_n, \lambda_2 - \lambda_n, \dotsc, \lambda_{n-1} - \lambda_n, 0)$.  We may reduce in this way to the case that $\lambda_n = 0$.  Having performed this reduction, we now induct on $\lambda$ with respect to lexicographical ordering (i.e., ordering first by $\lambda_1$, then using $\lambda_2$ to break the tie if necessary, and so on).  For the base case $\lambda = (0,\dotsc,0)$ we have $M_\lambda = 1 \in \mathbb{Z}$.  In general, we observe that
  \begin{align*}
    M_\lambda(t)
    &= t_1^{\lambda_1} \dotsb t_n^{\lambda_n}
      = t_1^{\lambda_1 - \lambda_2}
      (t_1 t_2)^{\lambda_2 - \lambda_3}
      \dotsb (t_1 \dotsb t_{n-1})^{\lambda_{n-1} - \lambda_n} \\
    &= \sigma_1^{\lambda_1 - \lambda_2}
      \sigma_2^{\lambda_2 - \lambda_3} \dotsb
      \sigma_{n-1}^{\lambda_{n-1} - \lambda_n}
      (t)
      + \dotsb,
  \end{align*}
  where $\dotsb$ denotes an integral linear combination of $M_{\mu}$ taken over dominant $\mu$ lexicographically lower than $\lambda$.  By our inductive hypothesis, $M_{\mu}/\sigma_n^{\mu_n}$ belongs to $\mathbb{Z}[\sigma_1,\dotsc,\sigma_{n-1}]$ for each such $\mu$, hence also $M_{\lambda} \in \mathbb{Z}[\sigma_1,\dotsc,\sigma_{n-1}]$, as required.
\end{proof}


The \emph{monomial alternating polynomials} are defined by
\begin{equation*}
  A_{\lambda} := \sum_{w \in W} (-1)^w e^{w \cdot \lambda}.
\end{equation*}
They obviously define elements of $L^{\alt}$.  The discriminant $\Delta$ arises in this way.  Indeed, we may expand the determinental formula for $\Delta(t)$ noted earlier as $\sum_{w \in W} (-1)^w \prod_j t_{w(j)}^{n-j}$ or as $\sum_{w \in W} (-1)^w \prod_i t_{i}^{n-w(i)}$.  Introducing the notation
\begin{equation}\label{eq:}
  \rho := (n-1,n-2,\dotsc,2,1,0) \in \mathbb{Z}^n,
\end{equation}
so that $t^\rho = t_1^{n-1} t_2^{n-2} \dotsb t_{n-2}^2 t_{n-1}$, we obtain $\Delta(t) = \sum_{w \in W} (-1)^w t^{w \cdot \rho}$, that is to say,
\begin{equation*}
  \Delta = A_{\rho}.
\end{equation*}

As in the case of monomial symmetric polynomials, the $A_\lambda$ are alternating not just with respect to their argument but also their index: $A_{w \cdot \lambda} = (-1)^w A_\lambda$.  In particular, if $\lambda_i = \lambda_j$ for some $i \neq j$, then $A_{\lambda} = 0$.  For instance, if $n = 2$, then $M_{(7,6)}(t) = t_1^7 t_2^6 - t_1^6 t_2^7 = - M_{(6,7)}(t)$, while $M_{(7,7)}(t) = t_1^7 t_2^7 - t_1^7 t_2^7 = 0$.  (To established the required vanishing in general, take for $w$ the transposition swapping $i$ and $j$, so that $w \cdot \lambda = \lambda$; then $A_\lambda = (-1)^w A_{w \cdot \lambda} = - A_\lambda$, i.e., $2 A_\lambda = 0$, and so $A_\lambda = 0$.  Alternatively, consider cosets in $W$ of the two-element group generated by $w$, and note that the sum in the definition of $A_{\lambda}$ vanishes over each such coset.)  If $\lambda$ has distinct components, then $A_\lambda \neq 0$; indeed, its lexicographically highest term is $e^{\lambda}$.

As $\lambda$ runs over the $W$-orbits of $\lambda \in \mathbb{Z}^n$ having distinct components, the $A_\lambda$ furnish a basis for $L^{\alt}$.  The strictly dominant $\lambda$ give a system of representatives for such orbits.  An equivalent system is given by the $\lambda + \rho$ for dominant $\lambda$, noting that the map $\lambda \mapsto \lambda + \rho$ induces a bijection between dominant and strictly dominant weights.  Thus
\begin{equation}\label{eq:}
  L^{\alt}
  = \oplus_{\lambda:\text{strictly dominant}}
  \mathbb{Z} A_\lambda
  = \oplus_{\lambda:\text{dominant}}
  \mathbb{Z} A_{\lambda + \rho}.
\end{equation}

\begin{lemma}
  Multiplication by $\Delta$ induces an isomorphism of $\mathbb{Z}$-modules
  \begin{equation}\label{eq:}
    L^{\sym} \xrightarrow{f \mapsto \Delta f} L^{\alt}.
  \end{equation}
\end{lemma}
\begin{proof}
  The indicated map is clearly defined (because $w \cdot (\Delta f) = (w \cdot \Delta) (w \cdot f) = (-1)^w \Delta f$) and injective, so the content here is that any alternating Laurent polynomial $h$ may be divided by $\Delta$ inside the ring of Laurent polynomials.  After clearing denominators by multiplying $h$ by a sufficiently large power of $t_1 \dotsb t_n$, it suffices to show that any alternating polynomial $h$ is divisible by $\Delta$ in the ring $\mathbb{Z}[t_1,\dotsc,t_n]$ of ordinary polynomials.  For this we induct on $n$.  The case $n \geq 1$ is tautological, because then $\Delta = 1$, so suppose $n \geq 2$.  We note (since $2$ is not a zerodivisor in $\mathbb{Z}$) that the alternating condition on $h$ implies that it vanishes under any substitution $t_i := t_j$ ($i \neq j$).  (Indeed, from some perspectives it would be better to take this property as the \emph{definition} of alternating.)  On the other hand, by division with remainder in the variable $t_n$, we may write $h = (t_n - t_1) q + r$ where $q \in \mathbb{Z}[t_1,\dotsc,t_n]$ and $r \in \mathbb{Z}[t_1,\dotsc,t_{n-1}]$, but then substituting $t_n := t_1$ shows that $r = 0$, and so $h = (t_n - t_1) q$.  Since $h$ vanishes under the substitution $t_n := t_2$ and our polynomial rings are integral domains, we see that $q$ vanishes under the substitution $t_n := t_2$.  We may thus iterate the above division with remainder argument to see that $h = (t_n - t_1) (t_n - t_2) \dotsb (t_n - t_{n-1}) q'$ for some $q' \in \mathbb{Z}[t_1,\dotsc,t_n]$.  We may expand $q '$ in powers of $t_n$ as $\sum a_j t_n^j$, with coefficients $a_j \in \mathbb{Z}[t_1,\dotsc,t_{n-1}]$.  We may likewise expand $h$ in powers of $t_n$; each coefficient in this expansion is then an alternating polynomial in the variables $t_1,\dotsc,t_{n-1}$, so the same holds for the $a_j$.  By our inductive hypothesis, $a_j$ is divisible by $\prod_{i < j \leq n-1} (t_i - t_j)$.  It follows as required that $f$ is divisible by $\Delta$.
\end{proof}

In particular, the division in the following definition makes sense:
\begin{definition}
  The \emph{Schur polynomial} attached to a dominant $\lambda \in \mathbb{Z}^n$ is
  \begin{equation*}
    s_\lambda := \frac{A_{\lambda + \rho}}{A_\rho } \in L^{\sym}.
  \end{equation*}
\end{definition}
If $t$ is \emph{regular} in the sense that $t_i \neq t_j$ for $i \neq j$, then
\begin{equation}\label{eq:evaluation-Schur-rational}
  s_\lambda(t)
  =  
  \frac{\det(t_i^{\lambda_j + n - j})}{\prod_{i < j} (t_i -
    t_j)}.
\end{equation}
Otherwise, $s_\lambda(t)$ must be understood by continuity, or by first simplifying the above rational function to a polynomial.  For instance, when $n = 2$, we have for $\lambda _1 \geq \lambda_2$ that
\begin{align*}
  s_{(\lambda_1,\lambda_2)}(t)
  &=
    \frac
    {    \det    
\begin{pmatrix}
      t_1^{\lambda_1+1} &
                                                          t_1^{\lambda_2} \\      t_2^{\lambda_1+1} &
                                                                                                      t_2^{\lambda_2}    
\end{pmatrix}
    }
    {
    t_1 - t_2
    }
    =
    \frac{t_1^{\lambda_1 +1} t_2^{\lambda_2}
    - t_1^{\lambda_2} t_2^{\lambda_1 + 1}
    }{
    t_1 - t_2
    }
  \\
  &=
    t_1^{\lambda_1} t_2^{\lambda_2}
    +
    t_1^{\lambda_1 - 1} t_2^{\lambda_2 + 1}
    + \dotsb
    +
    t_1^{\lambda_2} t_2^{\lambda_1}.
\end{align*}


Since the $A_{\lambda+\rho}$ give a basis for $L^{\alt}$, we deduce:
\begin{lemma}
  The $s_\lambda$ give a basis for $L^{\sym}$:
  \begin{equation*}
    L^{\sym} = \oplus _{\lambda:\text{dominant}} \mathbb{Z} s_\lambda.
  \end{equation*}
\end{lemma}
We'll need later the computation of the values of the Schur polynomials at the identity element $t = 1 = (1,\dotsc,1) \in \U(1)^n$:
\begin{lemma}\label{lem:schur-evalu-at-1-U-n}
  Let $\lambda \in \mathbb{Z}^n$ be dominant.  Temporarily abbreviate $\lambda ' := \lambda + \rho$, i.e., $\lambda_j' := \lambda_j + n - j$.  Then
  \begin{equation*}
    s_\lambda(1) = \prod_{i < j} \frac{\lambda_i' - \lambda_j'}{j - i}.
  \end{equation*}
  In particular, $s_\lambda(1) > 0$.
\end{lemma}
\begin{proof}
  A direct attempt via \eqref{eq:evaluation-Schur-rational} gives ``$0/0$,'' so we need to take a limit along some sequence of regular elements $t$ tending to $1$.  A clever choice is given by $t_j := z^{n-j}$, where $z \in \U(1)$ traverses a sequence tending to $1$.  The numerator then simplifies to another Vandermonde determinant:
  \begin{equation*}
    \det(t_i^{\lambda_j + n-j}) = \det(z^{(n-i) \lambda_j'}) = \prod_{i < j} (z^{\lambda_i'} - z^{\lambda_j'}).
  \end{equation*}
  Using that $z^a - z^b \sim \log(z) (a - b)$, we arrive at the required formula.
\end{proof}

We record some inner product formulas:
\begin{lemma}\label{lem:inner-products-symmetric-functions}
  Let $\langle , \rangle$ denote the inner product in $L^2(T)$ taken with respect to the probability Haar, so that $\langle e^\lambda, e^\mu \rangle = \delta_{\lambda \mu}$.  For strictly dominant $\lambda, \mu$, we have
  \begin{equation*}
    \langle \Delta s_\lambda, \Delta s_\mu \rangle = \langle A_{\lambda+\rho}, A_{\mu+\rho} \rangle = |W| \delta_{\lambda \mu}.
  \end{equation*}
\end{lemma}
\begin{proof}
  Immediate.
\end{proof}

\subsection{Weyl character and dimension formulas for \texorpdfstring{$\U(n)$}{U(n)}}

\begin{theorem}\label{thm:weyl-char-Un}
  Let $(\pi,V)$ belong to the set $\Irr(\U(n))$ of (isomorphism classes of) irreducible finite-dimensional representations of $\U(n)$.
  \begin{enumerate}
  \item There exists a unique dominant weight $\lambda \in \mathbb{Z}^n$ so that (the restriction to the torus of) the character $\chi_\pi$ is equal to the Schur polynomial $s_\lambda$.  We write in this case $(\pi,V) = (\pi_\lambda ,V_\lambda)$.  The weight space $V^{\lambda}$ is one-dimensional, while $V^{\mu} = \{0\}$ for any lexicographically larger weight $\mu$.
  \item Every dominant weight $\lambda$ arises in this way, giving a bijection
    \begin{equation*}
      \Irr(\U(n)) \leftrightarrow \{\text{dominant } \lambda \in \mathbb{Z}^n\}.
    \end{equation*}
  \end{enumerate}
\end{theorem}
\begin{proof}
  We first prove part (i).  Recall the notation $G := \U(n) \geq T \cong \U(1)^n$.  Write $\chi_\pi = \sum_{\mu} m_\pi(\mu) e^\mu$ as before.  Let $\lambda$ be the lexicographically highest weight for which $m_\pi(\lambda) \neq 0$.  Since $\chi_\pi$ is $W$-invariant, we know that $\lambda$ is dominant, as otherwise it would be lexicographically smaller than something in its $W$-orbit.  In any event,
  \begin{equation}
    \chi_\pi = m_\pi(\lambda) e^\lambda + \dotsb,
  \end{equation}
  where $\dotsb$ denotes the contribution from lexicographically smaller weights.  Since also $\Delta = e^\rho + \dotsb$, it follows that
  \begin{equation}
    \Delta \chi_\pi = m_\pi(\lambda) e^{\lambda + \rho} + \dotsb.
  \end{equation}
  Since the Laurent polynomial $\Delta \chi_\pi$ is alternating, we may group its terms into monomial alternating functions, giving
  \begin{equation}\label{eq:expand-delta-chi-pi}
    \Delta \chi_\pi = m_\pi(\lambda) A_{\lambda + \rho} + \dotsb,
  \end{equation}
  where $\dotsb$ denotes a linear combination of $A_{\mu + \rho}$ taken over dominant $\mu$ lexicographically lower than $\lambda$.  We combine this expansion with the orthogonality relations for characters (Theorem \ref{thm:basic-orthogonality-characters}), the Weyl integral formula (Theorem \ref{thm:weyl-int}), and the inner product formulas for monomial alternating functions (Lemma \ref{lem:inner-products-symmetric-functions}) to see that
  \begin{equation}\label{eq:}
    1
    = \langle \chi_\pi, \chi_\pi \rangle_G
    =
    \frac{1}{|W|}
    \langle     \Delta \chi_\pi
    ,    \Delta \chi_\pi
    \rangle_T
    \geq
    \frac{1}{|W|}
    \langle     m_\pi(\lambda) A_{\lambda + \rho},     m_\pi(\lambda) A_{\lambda + \rho} \rangle_T
    =
    |m_\pi(\lambda)|^2.
  \end{equation}
  % \begin{equation}\label{eq:}
  %   1
  %   = \|\chi_\pi \|_G^2
  %   =
  %   \frac{1}{|W|}
  %   \|\Delta \chi_\pi \|_T^2
  %   \geq
  %   \frac{1}{|W|}
  %   \|m_\pi(\lambda) A_{\lambda + \rho}\|^2
  %   \geq
  %   |\mu_\pi(\lambda)|^2.
  % \end{equation}
  But since $m_\pi(\lambda)$ is a positive integer, it follows that $m_\pi(\lambda) = 1$ and also that equality holds in each step.  In particular, the remainder terms ``$\dotsb$'' in \eqref{eq:expand-delta-chi-pi} must vanish identically, i.e., $\Delta \chi_\pi = A_{\lambda+\rho}$, giving the required formula $\chi_\pi = s_\lambda$.  The uniqueness of $\lambda$ is clear from the orthogonality relations for the Schur polynomials.

  We give two proofs that every dominant weight $\lambda$ arises in this way:
  \begin{enumerate}
  \item Assume for the sake of contradiction that $\lambda$ does not arise, i.e., that $\chi_\pi \neq s_\lambda$ for all $\pi \in \Irr(G)$.  We may uniquely extend the $W$-invariant function $s_\lambda$ on $T$ to a class function on $G$.  For each $\pi \in \Irr(G)$, we may write $\chi_\pi = s_{\lambda '}$ for some dominant $\lambda ' \neq \lambda$.  By the orthogonality relations for the Schur polynomials, it follows then that $s_\lambda$ is orthogonal to $\chi_\pi$ in $L^2(G)$.  But this contradicts the Peter--Weyl theorem (Theorem \ref{thm:peter-weyl-for-chars}).
  \item Let $W$ denote the following representation of $G$, whose definition may be motivated by the proof of Lemma \ref{lem:symmetric-functino-theory}:
    \begin{equation*}
      W := \Lambda(\mathbb{C}^n)^{\otimes \lambda_1 - \lambda_2} \otimes \Lambda^2(\mathbb{C}^n) ^{\otimes \lambda_2 - \lambda_3} \otimes \dotsb \otimes \Lambda^{n-1}(\mathbb{C}^n)^{\otimes \lambda_{n-1}-\lambda_n} \otimes \Lambda^n(\mathbb{C}^n)^{\lambda_n}.
    \end{equation*}
    (Note that $\Lambda^n(\mathbb{C}^n)$ is the one-dimensional determinant representation, so it makes sense to raise it to any integral power $\lambda_n$, giving the one-dimensional representation $\U(n) \ni g \mapsto \det(g)^{\lambda_n} \in \U(1)$ denoted above by $\Lambda^n(\mathbb{C}^n)^{\lambda_n}$.)  Thus, as before,
    \begin{equation*}
      \chi_W = \sigma_1^{\lambda_1 - \lambda_2} \sigma_2^{\lambda_2 - \lambda_3} \dotsb \sigma_{n-1}^{\lambda_{n-1} - \lambda_n} \sigma_n^{\lambda_n} = e^\lambda + \dotsb,
    \end{equation*}
    where $\dotsb$ denotes the contribution of lexicographically smaller weights.  On the other hand, we may decompose $W$ into irreducible subspaces (Theorem \ref{thm:basic-orthogonality-characters}), taking into account the recently-established description of $\Irr(\U(n))$:
    \begin{equation*}
      W = \oplus_{\text{dominant }\mu} V_\mu^{\oplus n(\mu)}.
    \end{equation*}
    (By convention, here we sum only over those $\mu$ which we as yet know to arise from $\Irr(\U(n))$.)  At the level of characters, this decomposition reads $\chi_W = \sum_{\mu} n(\mu) \chi_\mu$.  Multiplying through by $\Delta = e^\rho + \dotsb$ and taking into account the formula $\Delta \chi_\mu = A_{\mu + \rho} = e^{\mu+\rho} + \dotsb$, we obtain
    \begin{equation*}
      e^{\lambda + \rho} + \dotsb = n(\lambda) e^{\lambda+\rho} + \dotsb.
    \end{equation*}
    Thus $n(\lambda) = 1$, that is to say, $V_\lambda$ occurs (exactly once) as a subrepresentation of $W$.  In particular, the representation $V_\lambda$ exists in the first place, as required.
    
    (A third proof, closely related to the second: if $s_\lambda$ is orthogonal to the character of every irreducible representation, then it is orthogonal in particular to any of the ring $\mathbb{Z}[\sigma_1, \dotsc, \sigma_{n-1}, \sigma_n, 1/\sigma_n]$ generated by the characters of the exterior powers of the standard representation and the inverse of the determinant, but we have seen that this ring is equal to $L^{\sym}$ .)
  \end{enumerate}
\end{proof}
We note incidentally that the second proof given above of the surjectivity of the map $\Irr(\U(n)) \rightarrow \{\text{dominant } \lambda \in \mathbb{Z}^n\}$ gives an independent proof of the Peter--Weyl theorem in this case.  We note also that, with notation as in that proof, we may realize $V_\lambda$ inside $W$ explicitly, as follows.  The proof shows that the weight space $W[\lambda]$ is one-dimensional, hence coincides with the one-dimensional weight space $V_\lambda[\lambda]$.  Consequently, $V_\lambda$ is the representation generated by (i.e., spanned by the $G$-orbit of) any nonzero element of $W[\lambda]$, such as $e_1^{\otimes \lambda_1 - \lambda_2} \otimes (e_1 \wedge e_2)^{\otimes \lambda_2 - \lambda_3} \otimes \dotsb$.


This result gives a satisfying description of the character theory of $\U(n)$.  It has some immediate applications:
\begin{enumerate}
\item A formula for the multiplicity in $m_{\pi_\lambda}(\mu)$ of the weight $\mu$ in the representation $\pi_\lambda$, as the coefficient of $e^\mu$ in $s_\lambda = A_{\lambda + \rho}/\Delta$.  One way to ``evaluate'' this further is to write $\Delta(t) = t^\rho \prod_{i < j} (1 - t_j/t_i)$ and expand $(1 - t_j/t_i)^{-1}$ as a geometric series (look up the \emph{Kostant partition formula}).
\item A formula for the coefficients of the decomposition of a tensor product of irreducible representations of $\U(n)$: writing
  \begin{equation}\label{eq:}
    V_{\lambda '}
    \otimes V_{\lambda '}
    \cong \oplus_{\lambda}
    n(\lambda) V_{\lambda},
  \end{equation}
  we have $\chi_{\lambda '} \chi_{\lambda ''} = \sum_{\lambda} n(\lambda) \chi_\lambda$, hence
  \begin{equation*}
    \sum_{\lambda} n(\lambda) A_{\lambda+\rho} = \Delta \chi_{\lambda '} \chi_{\lambda ''} = \frac{A_{\lambda ' + \rho} A_{\lambda '' + \rho}}{\Delta }.
  \end{equation*}
  We may then determine $n(\lambda)$ as the coefficient of $e^{\lambda+\rho}$ on both sides.  (Look up the \emph{Steinberg multiplicity formula}.)
\item A proof of the irreducibility of several of the representations considered in \S\ref{sec:weight-decmop-U-n}, such as $\Sym^k(\mathbb{C}^n)$ or $\Lambda^k(\mathbb{C}^n)$.  (A direct ``algebraic'' proof of their irreducibility is not very difficult, so the present argument should be regarded as an alternative ``analytic'' proof.)  For example, consider $V = \Sym^k(\mathbb{C}^n)$.  By ``stars and bars,'' we have (TODO: fix binomial coefficients)
  % $\dim(V) = \binom{k+n-1}{n-1} = \binom{n+k-1}{k}$.
  On the other hand, $\lambda = (k,0,\dotsc,0)$ is the lexicographically highest weight of $V$.  Thus $\lambda$ is lexicographically highest among the dominant weights $\mu$ contributing to the decomposition $\chi_V = \sum_{\mu} n(\mu) \chi_\mu$, and so $V_\lambda$ must occur as an irreducible subrepresentation of $V$.  In particular, $\dim(V_\lambda) \leq \dim(V)$.  On the other hand, a short calculation with the dimension formula gives readily the ``numerical coincidence'' $\dim(V_\lambda) = \dim(V)$ (see the homework), from which we deduce that $V \cong V_\lambda$; in particular, $V$ is irreducible.  A similar argument applies to the exterior powers.
\end{enumerate}
\footnote{End of lecture \#4, Thursday, 28 Feb 2019}


\subsection{Some groups closely related to \texorpdfstring{$\U(n)$}{U(n)}}\label{sec:some-groups-closely}
Set
\begin{equation}\label{eq:}
  \SU(n) := \{g \in \U(n) : \det(g) = 1 \}.
\end{equation}
Let
\begin{equation}
  Z := \{
\begin{pmatrix}
    z &  &  \\
      & \dotsb  &  \\
      &  & z 
  \end{pmatrix}
 \in \U(n) : z \in \U(1)\} \cong \U(1)
\end{equation}
denote the center of $\U(n)$, and let
\begin{equation}\label{eq:}
  \PU(n) := \U(n)/Z
\end{equation}
denote the corresponding quotient.  Observe that
\begin{itemize}
\item given a representation of $\PU(n)$, we obtain a representation of $\U(n)$ by pullback, i.e., by composing with the projection $\U(n) \twoheadrightarrow \PU(n)$, and
\item given a representation of $\U(n)$, we obtain a representation of $\SU(n)$ by restriction, i.e., by composing with the inclusion $\SU(n) \rightarrow \U(n)$.
\end{itemize}
\begin{theorem}\label{thm:SU-PU}
  The operations of pullback and restriction just described preserve irreducibility, inducing an injective map
  \begin{equation*}
    \Irr(\PU(n)) \hookrightarrow \Irr(\U(n))
  \end{equation*}
  and a surjective map
  \begin{equation*}
    \Irr(\U(n)) \twoheadrightarrow \Irr(\SU(n)).
  \end{equation*}
  The latter maps and the bijection $\Irr(\U(n)) \leftrightarrow \{\text{dominant elements of }\mathbb{Z}^n\}$ as in the Weyl character formula are compatible with bijections
  \begin{equation*}
\Irr(\SU(n))
    \leftrightarrow \{\text{dominant elements of }\mathbb{Z}^n / \mathbb{Z} e_0 \}, \quad e_0 := (1,\dotsc,1)
\end{equation*}
 and 
\begin{equation*}
\Irr(\PU(n)) \leftrightarrow \{\text{dominant elements of }\mathbb{Z}^n_0 := \{\lambda \in \mathbb{Z}^n : \sum_j \lambda_j = 0\} \}.
\end{equation*}
\end{theorem}
\begin{proof}
  We leave most of this to the homework; here we just record a few remarks that hopefully make the description of what happens under restriction to $\SU(n)$ seem plausible.  Observe that if $\lambda,\mu$ are dominant elements of $\mathbb{Z}^n$ differing by a multiple of $e_0$, say $\lambda = \mu + \ell e_0$, then the corresponding irreducible representations $\pi_\lambda, \pi_\mu$ of $\U(n)$ satisfy
  \begin{equation}\label{eq:}
    \pi_{\lambda} \cong \pi_{\mu} \otimes {\det}^{\ell}.
  \end{equation}
  Indeed, we may check this by comparing the characters of these representations, and we have
  \begin{equation}\label{eq:}
    \chi_\lambda(t)
    = \frac{\det(t_i^{\lambda_j + n - j})}{\Delta(t)}
    =
    \underbrace{(t_1 \dotsb t_n)^{\ell}}_{\det(t)^{\ell}}
    \frac{\det(t_i^{\mu_j + n - j})}{\Delta(t)}.
  \end{equation}
  It follows that $\pi_\lambda$ and $\pi_\mu$ have isomorphic restrictions to $\SU(n-1)$.
\end{proof}

\subsection{The case of \texorpdfstring{$\SU(2)$}{SU(2)}}\label{sec:case-su2}

\begin{theorem}
  We have bijections
  \begin{equation}\label{eq:}
    \Irr(\SU(2))
    \leftrightarrow
    \mathbb{Z}_{\geq 0}
    \leftrightarrow 
    \{\text{dominant }
    \lambda = (\lambda_1,\lambda_2) \in
    \mathbb{Z}^2/\mathbb{Z}(1,1)\}
  \end{equation}
  \begin{equation*}
    \pi_\lambda \leftrightarrow \lambda \leftrightarrow (\text{the class of }(\lambda,0))
  \end{equation*}
  such that the character $\chi_\lambda$ of $\pi_\lambda$ is given by
  \begin{equation}\label{eq:char-formula-SU2}
    \chi_\lambda 
\begin{pmatrix}
      t &  \\
        & t^{-1}
    \end{pmatrix}
    =
    \frac{\det 
\begin{pmatrix}
      t^{\lambda+1} & 1 \\
      t^{-\lambda-1} & 1
    \end{pmatrix}
}{ t - t^{-1}} = t^{\lambda} + t^{\lambda-2} + t^{\lambda-4} + \dotsb + t^{-\lambda}.
\end{equation}
We have $\dim(\pi_\lambda) = \lambda + 1$.  We have
\begin{equation}
  \pi_\lambda
  \cong \Sym^\lambda(\mathbb{C}^2)
  \cong
  \Sym^\lambda ((\mathbb{C}^2)^*),
\end{equation}
where $\mathbb{C}^2$ denotes as usual the standard representation of $\SU(2)$.  If we let $x,y \in (\mathbb{C}^2)^*$ denote the standard basis elements of the dual given in coordinates $z = (z_1,z_2)$ by $x : z \mapsto z_1, y : z \mapsto z_2$, then $\pi_\lambda \cong \Sym^\lambda((\mathbb{C}^2)^*)$ identifies with the space $\mathbb{C}[x,y]^{(\lambda)}$ of homogeneous polynomials $f$ of degree $\lambda$ in the variables $x$ and $y$, with the action of $g = 
\begin{pmatrix}
  a & b \\
  c & d
\end{pmatrix}
 \in \SU(2)$ given by $g \cdot f(x,y) = f((x,y) g) = f(x a + y c, x b + y d)$; a basis of weight vectors for $T = \left\{ 
\begin{pmatrix}
  t &  \\
    & t^{-1}
\end{pmatrix}
 \right\} \leq \SU(2)$ is given by
\begin{equation}
  x^{\lambda},
  x^{\lambda-1} y,
  x^{\lambda-2} y^2,
  \dotsc,
  y^{\lambda},
\end{equation}
with corresopnding weights
\begin{equation}\label{eq:}
  \begin{pmatrix}
    t &  \\
      & t^{-1}
  \end{pmatrix}
  \mapsto
  t^{\lambda}, t^{\lambda-2},
  t^{\lambda-4},
  \dotsc,
  t^{-\lambda}.
\end{equation}
Finally, we have the Clebsch--Gordan rule: for $\lambda, \mu \in \mathbb{Z}_{\geq 0}$,
\begin{equation}\label{eq:clebsch-gordan}
  \pi_{\lambda}
  \otimes \pi_{\mu}
  \cong \pi_{\lambda+\mu}
  \oplus
  \pi_{\lambda+\mu-2}
  \oplus
  \pi_{\lambda+\mu-4}
  \oplus
  \dotsb 
  \oplus
  \pi_{|\lambda-\mu|}.
\end{equation}
\end{theorem}
\begin{proof}
  Almost every assertion follows immediately from Theorem \ref{thm:SU-PU} and the description \eqref{eq:wts-sym-k} of the weights of symmetric powers of the standard representation, together with the analogous description of those of its dual.  To verify the existence of the isomorphism \eqref{eq:clebsch-gordan}, we just need to check that $\chi_\lambda \chi_\mu = \chi_{\lambda+\mu} + \dotsb + \chi_{|\lambda-\mu|}$, which is easy to derive from \eqref{eq:char-formula-SU2}.
\end{proof}

\subsection{Branching problems}
Given compact groups $H \leq G$ and a representation $(\pi,V)$ of $G$, we may form the restricted representation $\pi|_{H} : H \rightarrow \GL(V|_{H})$ of $H$; here $V|_{H}$ is just $V$, but regarded as a representation of $H$.  The branching problem is to describe this restriction in terms of $\pi$.

We assume henceforth that $\pi$ is irreducible.  Its restriction is then ``typically'' not irreducible, but may in any event be decomposed as
\begin{equation}\label{eq:}
  V|_{H}
  \cong \oplus_{\sigma \in \Irr(H)}
  W_\sigma ^{\oplus n_\pi(\sigma)}
\end{equation}
for some nonnegative integers $n_\pi(\sigma)$, called ``branching coefficients.''  The problem is to describe these.

We start with some examples that are either basic or have already been discussed implicitly.
\begin{enumerate}
\item If $H = G$, then $n_\pi(\sigma)$ is $1$ if $\sigma \cong \pi$ and $0$ otherwise.
\item If $H = \{1\}$ is the trivial subgroup, then $\Irr(H)$ is a singleton consisting of the trivial representation $\sigma_{\text{trivial}}$, and we have $n_\pi(\sigma_{\text{trivial}}) = \dim(\pi)$.
\item If $G = \U(n)$ and $H \cong \U(1)^n$ is the diagonal subgroup, then $\sigma$ is the one-dimensional representation $\sigma = \sigma_\mu := e^{\mu} : t \mapsto t^{\mu} = t_1^{\mu_1} \dotsb t_n^{\mu_n}$ attached to some weight $\mu \in \mathbb{Z}^n$.  The branching coefficient $n_\pi(\sigma)$ is just the weight multiplicity $m_\pi(\mu)$ considered in \S\ref{sec:weight-decmop-U-n} and onwards.  These integers are typically $> 1$ when $n \geq 3$ (cf. homework).
\item If $G = \U(n)$ and $H = \SU(n)$, then Theorem \ref{thm:SU-PU} implies that for each $\pi$ there is exactly one $\sigma$ (namely, the restriction of $\pi$) for which $n_{\pi}(\sigma)$ is nonzero, and for that $\sigma$, we have $n_{\pi}(\sigma) = 1$.
\end{enumerate}

The overall theme here is that the larger the subgroup $H$, the more likely the representations of $G$ are to have irreducible restriction to $H$, or at least to have ``mild'' branching coefficients.  This theme is supported further by the example
\begin{equation}\label{eq:}
  H := \U(n-1)
  \hookrightarrow G := \U(n)
\end{equation}
\begin{equation*}
  h \mapsto 
\begin{pmatrix}
    h & 0 \\
    0 & 1
  \end{pmatrix}
,
\end{equation*}
which we now address.  As a matter of notation, for dominant elements $\lambda \in \mathbb{Z}^n$ and $\mu \in \mathbb{Z}^{n-1}$, let us write $\pi_\lambda \in \Irr(\U(n))$ and $\sigma_{\mu} \in \Irr(\U(n-1))$ for the corresponding irreducibles, and $n_\lambda(\mu) := n_{\pi_\lambda}(\sigma_\mu)$ for the branching coefficient.  We say that $\mu$ \emph{interlaces} $\lambda$, denoted $\mu \prec \lambda$, if
\begin{equation}\label{eq:}
  \lambda_1 \geq \mu_1 \geq \lambda_2 \geq \mu_2
  \geq \dotsb \geq \mu_{n-1} \geq \lambda_n.
\end{equation}
\begin{theorem}
  We have $n_\lambda(\mu) = 0$ unless $\mu \prec \lambda$, in which case $n_\lambda(\mu) = 1$.
\end{theorem}
\begin{proof}
  We must verify that
  \begin{equation}\label{eq:restrict-Un-Unmi1-reps}
    \pi_{\lambda}|_{\U(n-1)}
    \cong
    \oplus_{\mu \prec \lambda}
    \sigma_{\mu}.
  \end{equation}
  It suffices to check that the representations of $\U(n-1)$ appearing on both sides have the same character.  It suffices to compare their characters on the representatives $t = \diag(t_1,\dotsc,t_{n-1},1)$ for the conjugacy classes in $\U(n-1) \leq \U(n)$.  We'll undertake this comparison in detail when $n = 3$; the general case is similar, but with many occurrences of $(\dotsb)$.  By the character formula, our task is to show that
  \begin{equation}\label{eq:}
    \frac{1}{(t_1 - t_2) (t_1 - 1) (t_2 - 1)}
    \det 
\begin{pmatrix}
      t_1^{\lambda_1 + 2} & t_1^{\lambda_2 + 1} &
                                                  t_1^{\lambda_3} \\
      t_2^{\lambda_1 + 2} & t_2^{\lambda_2 + 1} &
                                                  t_2^{\lambda_3} \\
      1 & 1 & 1      
    \end{pmatrix}
    =
    \frac{1}{t_1 - t_2}
    \sum_{\mu \prec \lambda}
    \det
    \begin{pmatrix}
      t_1^{\mu_1 + 1} & t_1^{\mu_2} \\
      t_2^{\mu_1 + 1} & t_2^{\mu_2}
    \end{pmatrix}
.
  \end{equation}
  We simplify the $3 \times 3$ determinant using elementary column operations, replacing its columns $a,b,c$ with $a-b, b-c, c$.  That determinant then simplifies to
  \begin{equation}\label{eq:}
    % \det \begin{pmatrix} t_1^{\lambda_1 + 2} & t_1^{\lambda_2 + 1} &
                                                                       %                                                                        t_1^{\lambda_3} \\
    %   t_2^{\lambda_1 + 2} & t_2^{\lambda_2 + 1} &
    %   t_2^{\lambda_3} \\
    %   1 & 1 & 1
    % \end{pmatrix}
    % =
    \det 
\begin{pmatrix}
      t_1^{\lambda_1 + 2} - t_1^{\lambda_2 + 1}
      & t_1^{\lambda_2 + 1} - t_1^{\lambda_3} &
      t_1^{\lambda_3} \\
      t_2^{\lambda_1 + 2} -
      t_2^{\lambda_2 + 1} &
      t_2^{\lambda_2 + 1} - t_2^{\lambda_3}  &
      t_2^{\lambda_3} \\
      0 & 0 & 1      
    \end{pmatrix}
    =
    \det 
\begin{pmatrix}
      t_1^{\lambda_1 + 2} - t_1^{\lambda_2 + 1}
      & t_1^{\lambda_2 + 1} - t_1^{\lambda_3} \\
      t_2^{\lambda_1 + 2} -
      t_2^{\lambda_2 + 1} & t_2^{\lambda_2
        + 1} - t_2^{\lambda_3}      \\
    \end{pmatrix}
.
  \end{equation}
  Using that the determinant is linear in the rows, and expanding out the definition of ``$\mu \prec \lambda$,'' our task reduces to checking that
  \begin{equation}\label{eq:}
    \det 
\begin{pmatrix}
      \frac{t_1^{\lambda_1 + 2} - t_1^{\lambda_2 + 1}}{t_1 - 1}
      & \frac{t_1^{\lambda_2 + 1} - t_1^{\lambda_3}}{t_1-1} \\
      \frac{t_2^{\lambda_1 + 2} -
        t_2^{\lambda_2 + 1}}{t_2-1} &
      \frac{t_2^{\lambda_2
          + 1} - t_2^{\lambda_3}}{t_2-1}      \\
    \end{pmatrix}
    =
    \sum_{\lambda_1 \geq \mu_1 \geq \lambda_2 \geq \mu_2 \geq \lambda_3}
    \det
    \begin{pmatrix}
      t_1^{\mu_1 + 1} & t_1^{\mu_2} \\
      t_2^{\mu_1 + 1} & t_2^{\mu_2}
    \end{pmatrix}
.
  \end{equation}
  This identity follows from the linearity of the determinant with respect to columns and geometric series identities such as $\frac{t_1^{\lambda_1 + 2} - t_1^{\lambda_2 + 1}}{t_1 - 1} = \sum_{\lambda_1 \geq \mu_1 \geq \lambda_2} t^{\mu_1 + 1}$.
\end{proof}

Groups $(G,H)$ with the property that every branching coefficient $n_\pi(\sigma)$ belongs to $\{0,1\}$ are called \emph{strong Gelfand pairs}; we may discuss them in more detail later.

\subsection{Gelfand--Tsetlin bases}
We now describe briefly how to deduce from the formulas for the branching coefficients for $\U(n-1) \leq \U(n)$ a canonical decomposition of any irreducible representation of $\U(n)$.  Let $\mu \in \mathbb{Z}^n$ be dominant, and $(\pi_\mu,V_\mu) \in \Irr(\U(n))$ the corresponding representation.  The decomposition \eqref{eq:restrict-Un-Unmi1-reps} implies that for each dominant $\nu \in \mathbb{Z}^{n-1}$ that interlaces $\mu$, there is a unique $\U(n-1)$-subrepresentation $V _{
\begin{pmatrix}
    \mu   \\
    \nu
  \end{pmatrix}
}$ of $V_{\mu}$ of parameter $\nu$, and moreover
\begin{equation}\label{eq:first-dcemop}
  V_\mu = \oplus_{\nu \prec \mu} V _{
\begin{pmatrix}
      \mu   \\
      \nu 
    \end{pmatrix}
}.
\end{equation}
The idea is now to iterate this decomposition using the chain of subgroups $\U(n-2), \U(n-3), \dotsc$, all the way down to $\U(1)$, whose irreducible representations are all one-dimensional.  In preparation for this iteration, we set $\lambda^{(n)} := \mu$ and denote by $\lambda^{(j)}$ a dominant element of $\mathbb{Z}^{j}$.  With this notation, \eqref{eq:first-dcemop} now reads
\begin{equation}\label{eq:}
  V_\mu |_{\U(n-1)}
  =
  \oplus_{\lambda^{(n-1)} \prec \lambda^{(n)} = \mu }
  V _{
\begin{pmatrix}
      \lambda^{(n)}  \\
      \lambda^{(n-1)}
    \end{pmatrix}
  }.
\end{equation}
We now apply the same reasoning to the restriction to $\U(n-2)$ of each representation $V _{
\begin{pmatrix}
    \lambda^{(n)}  \\
    \lambda^{(n-1)}
  \end{pmatrix}
}$ of $\U(n-1)$ arising above, giving
\begin{equation}\label{eq:}
  V_\mu |_{\U(n-2)}
  =
  \oplus_{\lambda^{(n-2)} \prec \lambda^{(n-1)} \prec \lambda^{(n)} = \mu }
  V _{
\begin{pmatrix}
      \lambda^{(n)}  \\
      \lambda^{(n-1)} \\
      \lambda^{(n-2)}
    \end{pmatrix}
  }.
\end{equation}
Proceeding, we eventually obtain a canonical decomposition of $V_\mu$ into one-dimensional subspaces:
\begin{equation}\label{eq:}
  V_\mu
  =
  \oplus_{\lambda^{(1)} \prec
    \lambda^{(2)} \prec \dotsb \prec \lambda^{(n)} = \mu }
  V _{
\begin{pmatrix}
      \lambda^{(n)}  \\
      \dotsb \\
      \lambda^{(1)}
    \end{pmatrix}
  }.
\end{equation}
Let's formalize things a bit:
\begin{definition}
  A \emph{Gelfand--Tsetlin pattern of order $n \geq 1$} is a column vector $\lambda = 
\begin{pmatrix}
    \lambda^{(n)}  \\
    \dotsb   \\
    \lambda^{(1)}
  \end{pmatrix}
  $, with $\lambda^{(j)} \in \mathbb{Z}^j$, so that $\lambda^{(1)} \prec \dotsb \prec \lambda^{(n)}$.  For instance, if $n = 3$, then we can visualize $\mathrm{GT}(n)$ as the space of all triangular arrays
  \begin{equation}\label{eq:}
    \lambda =
    \begin{pmatrix}
      \lambda^{(3)}_1 &  & \lambda^{(3)}_2 & & \lambda^{(3)}_3 \\
      & \lambda^{(2)}_1 &  & \lambda^{(2)}_2 &  \\
      &  & \lambda^{(1)}_1 &  &  \\
    \end{pmatrix}
  \end{equation}
  satisfying the interlacing conditions
  \begin{equation}
    \lambda^{(3)}_1 \leq \lambda^{(2)}_1 \leq \lambda^{(3)}_2
    \leq \lambda^{(2)}_2
    \leq \lambda^{(3)}_3
  \end{equation}
  and
  \begin{equation}
    \lambda^{(2)}_1 \leq \lambda^{(1)}_1 \leq \lambda^{(2)}_2.
  \end{equation}
  We denote by $\mathrm{GT}(n)$ the set of all Gelfand--Tsetlin patterns $\lambda$ of order $n$, and by
  \begin{equation}\label{eq:}
    \mathrm{GT}(n)_{\mu} := \{\lambda \in \mathrm{GT}(n) : \lambda^{(n)}
    = \mu \}
  \end{equation}
  the subset of Gelfand--Tsetlin patterns having ``top row'' equal to $\mu$.
\end{definition}

\begin{theorem}
  For each dominant $\mu \in \mathbb{Z}^n$ there is a unique decomposition
  \begin{equation*}
    V_\mu = \oplus_{\lambda \in \mathrm{GT}(n)_\mu} V_\lambda
  \end{equation*}
  of $V_\mu \in \Irr(\U(n))$ into one-dimensional subspaces $V_\lambda$, indexed by $\lambda \in \mathrm{GT}(n)_\mu$, with the property that for each $j \in \{1..n\}$ and $\nu \in \mathbb{Z}^{(j)}$, the subspace
  \begin{equation*}
    \oplus_{\lambda \in \mathrm{GT}(n)_\mu : \lambda^{(j)} = \nu} V_\lambda
  \end{equation*}
  of $V_\mu$ is $\U(j)$-invariant and isomorphic as a representation of $\U(j)$ to a direct sum of isomorphic copies of $V_\nu \in \Irr(\U(j))$; it is then the maximal subspace with this property.
\end{theorem}
\begin{proof}
  More-or-less immediate from the above discussion; ask me if anything seems unclear.
\end{proof}
This decomposition can be refined further by choosing basis elements for the one-dimensional spaces $V_\lambda$ and describing the representation explicitly in terms of that basis, but we will not pursue such refinements here.


\newpage


\section{Matrix coefficients for compact groups and the Peter--Weyl theorem}

\subsection{Spaces of matrix coefficients}
Let $G$ be a compact group and $(\pi,V)$ a finite-dimensional representation.  So far, our main tool for saying anything interesting about $\pi$ has been via its character $\chi_\pi : G \rightarrow \mathbb{C}$, defined as the composition
\begin{equation}\label{eq:}
  G \xrightarrow{\pi } \GL(V) \hookrightarrow \End(V)
  \xrightarrow{\trace} \mathbb{C}.
\end{equation}
To say more, we replace the trace map by a general linear functional $\alpha \in \End(V)^*$.  We denote by $\pi_\alpha : G \rightarrow \mathbb{C}$ the resulting composition
\begin{equation}\label{eq:}
  G \xrightarrow{\pi } \GL(V)
  \hookrightarrow \End(V)
  \xrightarrow{\alpha } \mathbb{C}.
\end{equation}
\begin{definition}
  A \emph{matrix coefficient} of $\pi = (\pi,V)$ is a function $G \rightarrow \mathbb{C}$ of the form $\pi_\alpha$ for some $\alpha \in \End(V)$.  We denote by
  \begin{equation}\label{eq:}
    \mathcal{A}(\pi) := \{\pi_\alpha : \alpha \in \End(V)^*\}
  \end{equation}
  the space of matrix coefficients of $\pi$.
\end{definition}
For example, if $\alpha$ is the trace map, then $\pi_\alpha = \chi_\pi$, so the character $\chi_\pi \in \mathcal{A}(\pi)$ is a matrix coefficient.\footnote{End of lecture \#5, Tuesday, 5 March}

\subsection{Uniqueness of invariant inner products}
It will be convenient in discussing matrix coefficients to suppose that our representations are unitary, so that we can work simply with orthonormal bases for a given representation rather than bases and dual bases for a representation and its dual.  In making this assumption it's convenient to know the following:
\begin{lemma}
  For $(\pi,V) \in \Irr(G)$, any two invariant inner products $\langle , \rangle_1$ and $\langle , \rangle_2$ on $V$ differ by a positive scalar: there exists $C > 0$ so that $\langle u,v \rangle_1 = C \langle u,v \rangle_2$ for all $u,v \in \pi$.
\end{lemma}
\begin{proof}
  We may identify inner products $\langle , \rangle$ with certain linear maps
  \begin{equation*}
    V \rightarrow \overline{V}^*
  \end{equation*}
  \begin{equation*}
    v \mapsto \langle v, \cdot \rangle.
  \end{equation*}
  The inner product is invariant if and only if the linear map is equivariant, i.e., defines an element of the space $\Hom_G(\pi, \overline{\pi }^*)$.  But Schur's lemma implies that the latter space is one-dimensional.  Hence any two invariant inner products differ by a scalar.  Positive-definiteness forces the scalar to be positive.
\end{proof}

\subsection{Matrix entries as a basis
  for the space of matrix coefficients}
We suppose henceforth that $\pi$ is unitary, and fix an orthonormal basis $\{e_i\}_{i=1}^{\dim(\pi)}$ for $V$.  Each $T \in \End(V)$ then defines a matrix with entries $T_{i j} = \langle T e_j, e_i \rangle$.  Write $\eps_{i j} \in \End(V)^*$ for the map $\End(V) \rightarrow \mathbb{C}$ assigning to an operator $T$ the matrix entry $T_{i j}$.  Then the $\eps_{i j}$ give a basis for $\End(V)^*$, and so the functions $\pi_{i j} := \pi_{\eps_{i j}}$ span the space $\mathcal{A}(\pi)$ of matrix coefficients of $\pi$.  (They need not in general be linearly independent: consider for instance a direct sum of copies of the trivial representation, for which $\pi_{i i}(g) = 1$ for all $i$ and $g$.)  The numbers $\pi_{i j}(g)$ are the matrix entries of $\pi(g)$.  We may write the map $\trace : \End(V) \rightarrow \mathbb{C}$ as the sum of diagonal matrix entries $\sum_i \eps_{i i}$, so that $\chi_\pi = \sum_i \pi_{i i}$.

\subsection{Special functions as matrix coefficients}
``Most'' interesting special functions in mathematics and mathematical physics (Bessel, Whittaker, Legendre, Laguerre, $\dotsc$) arise as matrix coefficients of representations, and may be profitably studied from this perspective.  For instance, $G = \mathbb{R}/2 \pi \mathbb{Z}$ has a representation $\pi$ on $V = \mathbb{C}^2$ given by the rotations
\begin{equation*}
  \pi(\theta) = 
\begin{pmatrix}
    \cos \theta  &  \sin \theta  \\
    - \sin \theta & \cos \theta
  \end{pmatrix}
  = 
\begin{pmatrix}
    \pi_{11}(\theta) & \pi_{1 2}(\theta) \\
    \pi_{21}(\theta) & \pi_{22}(\theta)
  \end{pmatrix}
,
\end{equation*}
whose matrix coefficients are thus the trigonometric functions.  Their addition law is obtained by writing the homomorphism property $\pi(\theta_1 + \theta_2) = \pi(\theta_1) \pi(\theta_2)$ as $\pi_{i j}(\theta_1 + \theta_2) = \sum_k \pi_{i k}(\theta_1) \pi_{k j}(\theta_2)$, i.e.,
\begin{equation}\label{eq:}
  \begin{pmatrix}
    \cos (\theta_1 + \theta_2)  &  \sin (\theta_1 + \theta_2)  \\
    - \sin (\theta_1 + \theta_2) & \cos (\theta_1 + \theta_2)
  \end{pmatrix}
  = 
  \begin{pmatrix}
    \cos \theta_1  &  \sin \theta_1  \\
    - \sin \theta_1 & \cos \theta_1
  \end{pmatrix}
  \begin{pmatrix}
    \cos \theta_2  &  \sin \theta_2  \\
    - \sin \theta_2 & \cos \theta_2
  \end{pmatrix}
,
\end{equation}
giving a convenient way to remember the formulas $\cos (\theta_1 + \theta_2) = \cos(\theta_1) \cos(\theta_2) - \sin(\theta_1) \sin(\theta_2)$ and $\sin (\theta_1 + \theta_2) = \cos(\theta_1) \sin(\theta_2) + \sin(\theta_1) \cos(\theta_2)$.  See the book by Vilenkin for many more examples like this.

\subsection{Some actions of \texorpdfstring{$G \times G$}{G x G}}
Anyway, the association $\alpha \mapsto \pi_\alpha$ defines a map
\begin{equation}\label{eq:map-End-V-star-to-A-pi}
  \End(V)^* \rightarrow \mathcal{A}(\pi).
\end{equation}
The spaces involved in this map are naturally representations of the product group $G \times G$: For $(g_1,g_2) \in G \times G$ and $\alpha \in \End(V)^*$, we denote by $(g_1,g_2) \cdot \alpha$ the functional
\begin{equation*}
  \End(V) \rightarrow \mathbb{C}
\end{equation*}
\begin{equation*}
  T \mapsto \alpha(\pi(g_1)^{-1} T \pi(g_2))
\end{equation*}
Another way to arrive at the same definition is via the external tensor product $V_1 \boxtimes V_2$ of representations $V_1$ of $G_1$ and $V_2$ of $G_2$ as in the homework.  This is the representation on the tensor product space given by $(g_1,g_2) \cdot (v_1 \otimes v_2) = g_1 v_1 \otimes g_2 v_2$.  We have equivariant identifications
\begin{equation}\label{eq:identify-End-V-star}
  \End(V)^*
  \cong
  (V^* \boxtimes V)^*
  \cong
  V^* \boxtimes V.
\end{equation}
For any function $f : G \rightarrow \mathbb{C}$, we write $(g_1,g_2) \cdot f$ for the function
\begin{equation*}
  G \rightarrow \mathbb{C}
\end{equation*}
\begin{equation*}
  x \mapsto f(g_1^{-1} x g_2).
\end{equation*}
This definition applies to $f \in \mathcal{A}(\pi)$, and the space $\mathcal{A}(\pi)$ is $G \times G$-invariant.  The map \eqref{eq:map-End-V-star-to-A-pi} is equivariant for these actions of $G \times G$.

We denote by $\Delta G$ the diagonal subgroup $\{(g,g) : g \in G\}$ of $G \times G$.  Then the fixed subspace $L^2(G)^{\Delta G}$ is the space of class functions $L^2(G)^{\class}$.

\subsection{Burnside's lemma}
The spaces of matrix coefficients are particularly well-behaved in the case of an irreducible representation:
\begin{lemma}\label{lem:schur-for-inner-products}
  Let $(\pi,V) \in \Irr(G)$.  Then
  \begin{enumerate}
[(i)]
  \item $\End(V)^*$ and $\mathcal{A}(\pi)$ define irreducible representations of $G \times G$, and the map $\End(V)^* \rightarrow \mathcal{A}(\pi)$ is an isomorphism.
  \item The $\Delta G$-fixed subspace $(\End(V)^*)^{\Delta G}$ is the one-dimensional space $\mathbb{C} \trace$ of multiples of $\trace : \End(V) \rightarrow \mathbb{C}$, whereas $\mathcal{A}(\pi)^{\Delta G}$ is the one-dimensional space $\mathbb{C} \chi_\pi$ of multiples of the character of $\pi$.
  \item (Burnside's lemma) The matrix entries $\pi_{i j}$ define linearly independent functions on $G$.  The set $\{\pi(g) : g \in G\}$ spans $\End(V)$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item From the homework, we know that if $V_1$ and $V_2$ are irreducible representations of the (compact) groups $G_1$ and $G_2$, then $V_1 \boxtimes V_2$ is an irreducible representation of $G_1 \times G_2$.  From this and \eqref{eq:identify-End-V-star} we deduce that $\End(V)^*$ is $G \times G$-irreducible.  The map $\End(V)^* \rightarrow \mathcal{A}(\pi)$ is equivariant, surjective (by definition) and nonzero (since its image contains $\chi_\pi$), hence its kernel $W$ is a proper invariant subspace of $\End(V)^*$, but since $\End(V)^*$ is irreducible, we must have $W = \{0\}$.  Therefore the map in question is an isomorphism; in particular, $\mathcal{A}(\pi)$ is irreducible.
  \item It's clear that $\trace$ and $\chi_\pi$ define nonzero elements of $\End(V^*)^{\Delta G}$ and $\mathcal{A}(\pi)^{\Delta G}$, so we just need to check that the latter two spaces are at most one-dimensional.  Part (i) implies that they are isomorphic, so we may conclude via the identification $\End(V)^* \cong \End(V^*)$ and Schur's lemma in the form $\dim \End_G(V^*) \leq 1$.
  \item The $\pi_{i j}$ are the images of the basis elements $\eps_{i j}$ under the isomorphism $\End(V)^* \rightarrow \mathcal{A}(\pi)$, so they are linearly independent.  For the second assertion, let $W \leq \End(V)$ denote the span of $\{\pi(g) : g \in G\}$.  If $W \neq \End(V)$, then we can find a nonzero $\alpha \in \End(V)^*$ vanishing on $W$.  Then $\pi_\alpha(g) = \alpha(\pi(g)) \in \alpha(W) = \{0\}$ for all $g \in G$, so $\pi_\alpha = 0$, contrary to (i).
  \end{enumerate}
\end{proof}

\subsection{Schur orthogonality}\label{sec:schur-orthogonality}

\begin{definition}
  The space $\End(V)^*$ comes with a natural $G \times G$-invariant inner product (``dual of the Hilbert--Schmidt inner product'') given explicitly in coordinates by requiring that the $\eps_{i j}$ be an orthonormal basis, i.e.,
  \begin{equation}\label{eq:}
    \langle
    \sum a_{i j} \eps_{i j},
    \sum b_{i j} \eps_{i j}
    \rangle
    = \sum a_{i j} \overline{b_{i j}}
  \end{equation}
  and more invariantly as the composition
  \begin{equation}
    \End(V)^* \otimes \overline{\End(V)^*}
    \cong V \otimes V^* \otimes \overline{V} \otimes
    \overline{V}^*
    \rightarrow \mathbb{C} 
  \end{equation}
  where the second arrow sends $v_1 \otimes v_2 \otimes \overline{v_3} \otimes \overline{v_4}$ to $\langle v_1, v_3 \rangle_V \langle v_2, v_4 \rangle_{V^*}$, where $V^*$ is equipped with the inner product obtained from that on $V$ via the duality isomorphism $V \ni \mapsto \langle v, \cdot \rangle \in \overline{V}^*$ induced by the given inner product on $V$.
\end{definition}

Another inner product on $\End(V)^*$ is obtained from that on $L^2(G)$ by taking matrix coefficients.  It's natural to ask how the two inner products compare:
\begin{theorem}
[Schur orthogonality relations]
  Let $(\pi,V), (\pi ',V') \in \Irr(G)$, with $G$ compact as usual.  Then for $\alpha \in \End(V)^*$ and $\beta \in \End(V')^*$,
  \begin{equation}\label{eq:}
    \langle
    \pi_{\alpha}, \pi_{\beta }'
    \rangle
    = 
\begin{cases}
      0 &  \text{ if } \pi \not\cong \pi ', \\
      \frac{1}{\dim(\pi)} \langle \alpha, \beta  \rangle
      & \text{ if } (\pi,V) = (\pi ', V'),
    \end{cases}
  \end{equation}
  where the inner product on the left is in $L^2(G)$ with respect to the probability Haar, while $\langle \alpha, \beta \rangle$ denotes the dual Hilbert--Schmidt inner product defined above.  Explicitly,
  \begin{equation}
    \langle \pi_{i j}, \pi_{k \ell}' \rangle
    = \frac{1}{\dim(\pi)}
    \delta_{\pi, \pi'}
    \delta_{i, k} \delta_{j, \ell}.
  \end{equation}
\end{theorem}
\begin{proof}
  The proof is basically as in Lemma \ref{lem:schur-for-inner-products}.  If $\pi$ is not $G$-isomorphic to $\pi '$, then $\End(\pi)^*$ is likewise not $G \times G$-isomorphic to $\End(\pi ')^*$ (as may be checked for instance by verifying that their characters are orthogonal, as on the homework), but the map
  \begin{equation*}
    \End(V)^* \rightarrow \overline{\End(V')^*}^* \cong \End(V')^*
  \end{equation*}
  \begin{equation*}
    \alpha \mapsto [\beta \mapsto \langle \pi_\alpha, \pi_\beta \rangle]
  \end{equation*}
  is $G \times G$-equivariant, so Schur's lemma implies that it vanishes identically.  In the case $(\pi, V) = (\pi ', V')$, we use Lemma \ref{lem:schur-for-inner-products} (a basic consequence of Schur's lemma) to write
  \begin{equation*}
    \langle \pi_\alpha, \pi _\beta \rangle = C \langle \alpha, \beta \rangle
  \end{equation*}
  for some $C > 0$, which may depend upon $\pi$, but not upon $\alpha$ and $\beta$.  To compute $C$, we take
  \begin{equation*}
    \alpha = \beta = \trace,
  \end{equation*}
  so that
  \begin{equation*}
    \pi_\alpha = \pi_\beta = \chi_\pi,
  \end{equation*}
  and note that
  \begin{equation*}
    \langle \trace, \trace \rangle = \langle \sum \eps_{i i}, \sum \eps_{i i} \rangle = \dim(\pi)
  \end{equation*}
  and recall that $\langle \chi_\pi, \chi_\pi \rangle = 1$.
\end{proof}

\subsection{The coefficient ring of a compact group}
Having now studied in detail the matrix coefficients of irreducibles, we piece them together as follows.
\begin{definition}
  Let $G$ be, as usual, a compact group.  The \emph{coefficient ring} of $G$ is defined to be the set
  \begin{equation*}
    \mathcal{A}(G) := \cup _{(\pi,V) \text{ finite-dimensional}} \mathcal{A}(\pi) \subseteq L^2(G).
  \end{equation*}
\end{definition}
The terminology is justified by:
\begin{theorem}\label{thm:coeff-ring-characterizations}
  \begin{enumerate}
[(i)]
  \item $\mathcal{A}(G)$ is a $\mathbb{C}$-algebra, closed under complex conjugation, and $G \times G$-invariant.
  \item $\mathcal{A}(G)$ is the orthogonal direct sum in $L^2(G)$ of the subspaces $\mathcal{A}(\pi)$ for $\pi \in \Irr(G)$.  The functions
    \begin{equation}\label{eqn:ONB-via-matrix-coefs}
      \sqrt{\dim(\pi)} \pi_{i j} : G \rightarrow \mathbb{C},
    \end{equation}
    for $\pi$ running over $\Irr(G)$ and $i,j = 1..\dim(\pi)$, form an orthonormal basis of $\mathcal{A}(G)$.  Similarly, $\mathcal{A}(G)^{\Delta G}$ is the orthogonal direct sum in $L^2(G)^{\class}$ of the subspaces $\mathbb{C} \chi_\pi$.  In particular, $\Irr(G)$ is at most countably infinite.
  \item For any $f \in L^2(G)$, the following are equivalent:
    \begin{enumerate}
[(a)]
    \item $f \in \mathcal{A}(G)$
    \item $f$ is \emph{right-finite}: $\dim \spann \{f(\cdot g) : g \in G\} < \infty$.
    \item $f$ is \emph{left-finite}: $\dim \spann \{f(g^{-1} \cdot ) : g \in G\} < \infty$.
    \item $f$ is \emph{bi-finite}, or simply \emph{finite}: $\dim \spann \{f(g_1^{-1} \cdot g_2 ) : g_1, g_2 \in G\} < \infty$.
    \end{enumerate}
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item The proof is similar to what we did for characters (Lemma \ref{lem:characters-basic}): The conjugate of a matrix coefficient is a matrix coefficient for the conjugate representation, namely, for $\alpha \in \End(\pi)^*$, we have $\overline{\pi _\alpha } = \overline{\pi }_{\overline{\alpha}}$ with $\overline{\alpha } \in \overline{\End(\pi)^*} \cong \End(\overline{\pi })^*$, so $\mathcal{A}(G)$ is closed under complex conjugation.  Similarly, the sum (resp. product) of two matrix coefficients is a matrix coefficient for the direct sum (resp. product) of the corresponding representations, i.e., $\pi_{\alpha} + \pi_{\beta '} = (\pi \oplus \pi ')_{\alpha \oplus \beta}$ and $\pi_{\alpha} \pi_{\beta '} = (\pi \otimes \pi ')_{\alpha \otimes \beta}$ where $\alpha \oplus \beta \in \End(\pi)^* \oplus \End(\pi ')^* \hookrightarrow \End(\pi \oplus \pi ')^*$ and $\alpha \otimes \beta \in \End(\pi)^* \otimes \End(\pi ')^* \hookrightarrow \End(\pi \otimes \pi ')^*$.
  \item We have seen that finite-dimensional $(\pi,V)$ may be assumed unitary and then decomposed as orthogonal direct sums $W_1 \oplus \dotsb \oplus W_m$ of irreducibles $\sigma_1,\dotsc,\sigma_m$.  For each $k = 1..m$, fix an orthonormal basis $\{e_i^{(k)}\}_i$ for $W_k$.  Then $\mathcal{A}(\pi)$ is spanned by the functions
    \begin{equation*}
      G \ni g \mapsto \langle e_i^{(k)}, g e_j^{(\ell)} \rangle.
    \end{equation*}
    We have $g e_j^{(\ell)} \in W_{\ell}$, which is orthogonal to $e_i^{(k)}$ unless $k = \ell$, in which case the above function belongs to $\mathcal{A}(\sigma_k)$.  Thus $\mathcal{A}(\pi) = \mathcal{A}(\sigma_1) + \dotsb + \mathcal{A}(\sigma_k)$.  Taking unions, we obtain $\mathcal{A}(G) = \sum_{\pi \in \Irr(G)} \mathcal{A}(\pi)$.  The required orthogonality follows from the Schur orthogonality relations.
  \item If $f \in \mathcal{A}(G)$, then $f \in \mathcal{A}(\pi)$ for some finite-dimensional $\pi$, so the $G \times G$-span of $f$ is contained in the space $\mathcal{A}(\pi)$ whose dimension is finite (e.g., bounded by $\dim(\pi)^2$).  To complete the proof it suffices to verify that if a function $f$ is right- or left-finite, then it belongs to $\mathcal{A}(G)$.  We treat the case that $f$ is right-finite, the other case being similar.  Let $V$ denote the span of the right translates $f(\cdot g)$ of $f$, so that $\dim(V) < \infty$ by assumption.  Take for $\pi$ the representation of $G$ on $V$ given by right translation.  (Note {\bf as I did not in lecture} that this is actually a representation: $V$ is a finite-dimensional subspace of $L^2(G)$, hence is closed, and so the continuity of the action of $G$ on $V$ follows from that of $G$ on $L^2(G)$.)
    
    We attempt now to define the ``act on $f$, then evaluate at identity'' functional $\alpha \in \End(V)^*$ by the formula
    \begin{equation}\label{eq:evaluate-at-identity-functional}
      \alpha(T) := (T f) (e),
    \end{equation}
    with $e \in G$ denoting the identity element.  (Note {\bf as I did not in lecture} that this definition does not obviously make sense, because $f$ is merely an ``$L^2$-function,'' and so is not obviously defined pointwise except off some unspecified set of measure zero.  However, as we will verify below (non-circularly) in \S\ref{sec:autom-cont-finite}, our assumptions imply that every element of $V$ is actually represented by a \emph{continuous} function, to which the definition applies.)  Then $\pi_\alpha(g) = (\pi(g) f)(e) = f(e g) = f(g)$, so $f = \pi_\alpha \in \mathcal{A}(\pi) \subseteq \mathcal{A}(G)$, as required.
  \end{enumerate}
\end{proof}




\subsection{Peter--Weyl theorem: statement and proof sketch}\label{sec:peter-weyl-theorem}
We can now state the more complete form of Theorem \ref{thm:peter-weyl-for-chars}:
\begin{theorem}[Peter--Weyl theorem]\label{thm:P-W-general}
  Let $G$ be a compact group.
  \begin{enumerate}
[(i)]
  \item $\mathcal{A}(G)$ is dense in the space $C(G)$ of continuous functions on $G$ equipped with the topology defined by the supremum norm.
  \item $\mathcal{A}(G)$ is dense in $L^2(G)$, hence $L^2(G) = \hat{\oplus}_{\pi \in \Irr(G)} \mathcal{A}(\pi)$; here $\hat{\oplus}$ denotes the ``Hilbert direct sum,'' i.e., the closure of the ``ordinary'' or ``algebraic'' direct sum $\oplus$.  Thus the functions \eqref{eqn:ONB-via-matrix-coefs} give an orthonormal basis (in the sense of Hilbert space) for $L^2(G)$.
  \item $\mathcal{A}(G)^{\Delta G} = \oplus_{\pi \in \Irr(G)} \mathbb{C} \chi_\pi$ is dense in $L^2(G)^{\class}$ (thus $L^2(G)^{\class} = \hat{\oplus}_{\pi \in \Irr(G)} \mathbb{C} \chi_\pi$) and also dense in $C(G)^{\class}$.
  \item If $(\pi,V)$ is any irreducible representation of $G$ on any space $V$ such that ``one has a reasonable theory of $V$-valued integrals'' (e.g., Hilbert, Banach, Frechet, or ``locally convex quasi-complete''), then $\dim(V) < \infty$.
  \end{enumerate}
\end{theorem}
The main assertion here is that $\mathcal{A}(G)$ is dense in $L^2(G)$; we will deduce the remaining assertions either from this one or via easy modifications of its proof.  As ``warm-up,'' we observe some easy special cases:
\begin{itemize}
\item If $G$ is a finite group, then both $\mathcal{A}(G)$ and $L^2(G)$ consist of all functions $G \rightarrow \mathbb{C}$, so there is nothing to show.
\item A \emph{profinite group} $G$ is a compact group which admits an open basis $\mathcal{U}$ at the identity element consisting of compact open normal subgroups $U$.  (This might not be the standard definition, but is equivalent to it, and suits the purposes of our discussion.)  For instance, the group $G = \GL_n(\mathbb{Z}_p)$, which may be identified with the inverse limit of the groups $\Gamma_k = \GL_n(\mathbb{Z}/p^k \mathbb{Z})$ as $k$ runs over the positive integers, is profinite, with the compact open normal subgroups $U_k := \ker(G \rightarrow \Gamma_k)$ giving a neighborhood basis of the identity.

  For any profinite $G$ and any $U \in \mathcal{U}$, the quotient group $G/U$ is finite; indeed, $G$ is compact and $G = \cup_{g \in G/U} g U$ is an open cover.  Because continuous functions on a compact space are uniformly continuous, we may find for each $f \in C(G)$ and $\eps > 0$ a subgroup $U \in \mathcal{U}$ so that $f$ varies by at most $\eps$ on each coset of $U$, i.e.,
  \begin{equation*}
|f(g u) - f(g)| \leq \eps\text{ for all $g \in G$ and $u \in U$}.
\end{equation*}
 Let $C(G)^U \subseteq C(G)$ denote the space of functions $G \rightarrow \mathbb{C}$ that are constant on the cosets of $U$, and let $f_U \in C(G)^U$ be defined by averaging $f$ over $U$-cosets, i.e.,
  \begin{equation*}
    f_U(g) := \mathbb{E}_{u \in U} f(g u),
  \end{equation*}
  where $\mathbb{E}$ denotes the integral with respect to the probability Haar on the compact group $U$.  Then $\sup |f - f_U| \leq \eps$.  Thus $\cup_{U \in \mathcal{U} } C(G)^U$ is dense in $C(G)$.  On the other hand, the spaces $C(G)^U$ are finite-dimensional and $G \times G$-invariant, hence contained in $\mathcal{A}(G)$.  It follows that $\mathcal{A}(G)$ is dense in $C(G)$.
\end{itemize}

The proof for general compact groups $G$ will be similar to what we just did in the profinite case, but a bit more technically involved because we don't in general have such a spectacularly convenient neighborhood basis of subgroups.  (Think of $\U(1)$, which visibly has no nontrivial subgroups contained in $\{z \in \U(1) : |z - 1| < 1/10\}$.)  We'll construct instead some ``approximate analogues'' of the maps $f \mapsto f_U$ by convolving $f$ with functions $\phi \in C(G)$ that are supported in small neighborhoods of the identity element and satisfy the normalization $\int_G \phi = 1$.  Set $T_\phi f(g) := \int_{g \in G} \phi(g) f(g) \, d g$, where $d g$ denotes the probability Haar on $G$.  (For instance, if $G$ is profinite and $\phi = \vol(U, d g)^{-1} 1_U$ for some $U \in \mathcal{U}$, then $T_\phi f = f_U$ as above.)  If $\phi$ is sufficiently concentrated, then $T_\phi f$ will approximate $f$.  Since $G$ is compact, we may assume that $\phi$ is a class function (corresponding to $U$ being normal).  We might as well assume also that $\phi$ is real-valued and invariant by inversion (corresponding to $U$ being closed under inversion).  We'll see then that $T_\phi$ defines a compact self-adjoint equivariant operator on $L^2(G)$.  The spectral theory of such operators will then allow us to approximate $T_\phi f$ by its projection to the eigenspaces of $T_\phi$ of eigenvalue $\lambda$ with $|\lambda| > \eps$; since the eigenspaces of such operators with nonzero eigenvalue are finite-dimensional and $G$-invariant, this gives the required approximation of $T_\phi f$ and hence also of $f$ via finite functions.  Details next time.\footnote{End of lecture \#6, Thursday, 7 March}

Let's note one final easy special case.  Suppose that $G$ is a compact \emph{matrix} group, i.e., a compact subgroup of some $\GL_n(\mathbb{C})$.  (By Theorem \ref{thm:compact-unitarizability} applied to the identity representation $G \rightarrow \GL_n(\mathbb{C})$, we might assume further that $G$ is contained in the unitary group $\U(n)$.)  Let $\mathcal{O}$ denote the space of functions $f : G \rightarrow \mathbb{C}$ given by the restriction $f = \tilde{f}|_G$ of some function $\tilde{f} : \GL_n(\mathbb{C}) \rightarrow \mathbb{C}$ for which $\tilde{f}(g)$ is a polynomial function of the matrix entries of $g$.  We may write $\mathcal{O} = \cup_{d \geq 0} \mathcal{O}_d$, where $\mathcal{O}_d$ denotes the subspace of polynomials of degree at most $d$.  Each space $\mathcal{O}_d$ is finite-dimensional and $G$-invariant, hence $\mathcal{O}_d \subseteq \mathcal{A}(G)$, hence $\mathcal{O} \subseteq \mathcal{A}(G)$.  Since $\mathcal{O}$ is a $\mathbb{C}$-algebra that is closed under complex conjugation and separates points on $G$, we deduce by Stone--Weierstrass that $\mathcal{O}$ is dense in $C(G)$.  It follows also that $\mathcal{A}(G)$ is dense in $C(G)$ (and that $\mathcal{A}(G) = \mathcal{O}$).

On a related note, we observe the following consequence of Peter--Weyl:
\begin{corollary}\label{sec:cor-peter-weyl-matrix}
  Let $G$ be a compact Lie group.  Then $G$ is isomorphic to a compact matrix Lie group.
\end{corollary}
Thus Peter--Weyl for compact \emph{Lie} groups $G$ is essentially equivalent to the fact that any such $G$ is isomorphic to a matrix Lie group.
\begin{proof}
  Enumerate $\Irr(G)$ as $V_1,V_2,\dotsc$, and let $G_n \leq G$ denote the kernel of the map $G \rightarrow \GL(V_1 \oplus \dotsb \oplus V_n)$ given by the direct sum of the representations $V_1,\dotsc,V_n$.  Then $G_n$ is a closed subgroup of $G$, hence a Lie subgroup, so we may speak of its Lie algebra $\mathfrak{g}_n$, which is a subalgebra of the Lie algebra $\mathfrak{g}$ of $G$.  The subgroups $G_n$ decrease with $n$, hence so do the subspaces $\mathfrak{g}_n$.  By Peter--Weyl, that the matrix coefficients of the irreducible representations separate points in $G$.  It follows that $\cap_{n} G_n = \{1\}$ and thus $\cap_{n} \mathfrak{g}_n = \{0\}$.  Since the $\mathfrak{g}_n$ decrease and are finite-dimensional, we may find $n_0$ so that $\mathfrak{g}_{n} = \{0\}$ for $n \geq n_0$; for such $n$, the Lie group $G_n$ is thus compact and zero-dimensional, hence finite.  Using again that $\cap_{n} G_n = \{1\}$, we may find $n$ so that $G_{n} = \{1\}$.  Then $G \rightarrow \GL(V_1 \oplus \dotsb \oplus V_n)$ is an injective homomorphism with compact domain (and Hausdorff codomain), hence defines an isomorphism (of topological groups) onto its image.
\end{proof}

The proof shows also that any compact group $G$ is isomorphic to an inverse limit of compact matrix Lie groups.

We'll give the proof of Peter--Weyl below after developing some preliminaries concerning integral operators, which are of independent interest.


\newpage
\section{Integral operators}\label{sec:integral-operators}

\subsection{Integrating functions taking values in Hilbert spaces}
\begin{lemma}
  Let $(\Omega,\mu)$ be a locally compact space equipped with a Radon measure, let $V$ be a Hilbert space, and let $f$ be an element of the space $C_c(\Omega \rightarrow V)$ of compactly-supported continuous functions from $\Omega$ to $V$.  Then there is a unique element $v \in V$ so that for each element $\ell$ of the continuous dual $V^*$, we have
  \begin{equation}\label{eq:gelfand-pettis-condition}
    \ell(v) = \int_{x \in \Omega} \ell(f(x)) \, d \mu(x).
  \end{equation}
  We then use the notation $\int_\Omega f \, d \mu := v \in V$.
\end{lemma}
\begin{proof}
  We may define $v$ first as an algebraic functional $V^* \rightarrow \mathbb{C}$ via \eqref{eq:gelfand-pettis-condition}.  By Hilbert space duality, our task reduces to showing that $v$ belongs to the double dual of $V$, i.e., that $|\ell(v)| \leq C \|\ell\|$ for some constant $C \geq 0$ and all $\ell \in V^*$.  We may take $C = C_1 C_2$, where $C_1 := \mu(\supp(f)) < \infty$ and $C_2 := \max_{x \in \supp(f)} \|f(x)\| < \infty$.
\end{proof}

This lemma and hence our subsequent development can be generalized further (to Banach spaces, Frechet spaces, etc.), but the present generality should suffice for our aims.

\subsection{Definition of integral operators
attached to representations}
From now on we take for $G$ a locally compact group and $(\pi,V)$ a \emph{Hilbert representation}; by this we mean that $V$ is a Hilbert space, without requiring that $\pi$ be unitary.

\begin{lemma}
  For every compactly-supported signed Radon measure $\mu$ on $G$ there is a unique bounded linear operator $\pi(\mu) : V \rightarrow V$ such that
  \begin{equation}\label{eq:}
    \ell(\pi(\mu) v)
    = \int_{g \in G} \ell(\pi(g) v) \, d \mu(g)
  \end{equation}
  for all $v \in V$ and $\ell \in V$.  We then use the notation $\pi(\mu) =: \int_{g \in G} \pi(g) \, d \mu(g)$.
\end{lemma}
\begin{proof}
  Set $\Omega := \supp(\mu)$.  For each $v \in V$, we may apply the previous lemma to the function $f_v : \Omega \rightarrow V$ given by $g \mapsto \pi(g) v$.  We obtain elements $\pi(\mu) v \in V$ and a linear map $\pi(\mu) : V \rightarrow V$.  It remains to show that $\pi(\mu)$ is bounded.  For this it suffices to show that there exists $C \geq 0$ (depending upon $\Omega$) so that the operator norm $\|\pi(g)\|$ is bounded by $C$ for all $g \in \Omega$.  The conclusion follows by Banach--Steinhaus (presumably covered in the functional analysis course), which we record here for convenience:
  \begin{center}
    \emph{A pointwise bounded family $(T_\alpha)_{\alpha \in I}$ of bounded linear maps $T_\alpha : X \rightarrow Y$ from a Banach space $X$ to a normed space $Y$ is uniformly bounded, i.e., if for each $x \in X$ there exists $C(x) \geq 0$ so that $\|T_\alpha x\| \leq C(x) \|x\|$ for all $\alpha$, then there exists $C \geq 0$ so that $\|T_\alpha x\| \leq C \|x\|$ for all $\alpha,x$.}
  \end{center}
  We apply this to the family $(\pi(g))_{g \in \Omega}$ of bounded linear maps $\pi(g) : V \rightarrow V$; the pointwise boundedness property follows from the continuity of $G \times V \rightarrow V$, while the uniform boundedness gives the conclusion that we seek.

  For convenience and as a reminder of what's going on ``under the hood,'' we record a proof of Banach--Steinhaus as quoted above.  (This was not presented in lecture, and is not otherwise relevant to the course.)  If the conclusion fails, then we can find a sequence of nonzero vectors $x_n \in X$ and elements $T_n$ of the family $(T_\alpha)$ so that $\|T_n x_n\|/ \|x_n\| \rightarrow \infty$.  After normalizing the $x_n$ by a suitable scalar, we may arrange that
  \begin{equation*}
    \| x_n\| \rightarrow 0, \quad \|T_n x_n\| \rightarrow \infty.
  \end{equation*}
  We now inductively choose a ``sufficiently sparse'' subsequence $(x_{n_k})_{k \geq 1}$, as follows.  For $k \geq 1$, having chosen $n_1 < \dotsb < n_{k-1}$, choose $n_{k}$ sufficiently large that
  \begin{itemize}
  \item $\|x_{n_{k}}\| \leq 2^{-k}$,
  \item $\sum_{\ell=1..k-1} \|T_{n_{\ell}} x_{n_{k}} \| \leq 2^{-k}$ (as we may, because each $T_{n_{\ell}}$ is bounded), and
  \item $\|T_{n_{k}} x_{n_{k}} \| \geq \sum_{\ell =1..k-1} \| T_{n_{k}} x_{n_{\ell}} \| + 2^{k}$ (as we may, because the family $(T_\alpha)$ is assumed pointwise bounded).
  \end{itemize}
  Since $\sum_{k} \|x_{n_k}\| < \infty$ and $X$ is complete, we have $x := \sum_k x_{n_k} \in X$.  But
  \begin{equation*}
    \|T_{n_k} x\| \geq \|T_{n_k} x_{n_k}\| - \sum_{\ell=1..k-1} \| T_{n_{k}} x_{n_{\ell}} \| - \sum_{\ell \geq k+1} \| T_{n_{k}} x_{n_{\ell}} \| \geq 2^k - \sum_{\ell \geq k+1} 2^{-\ell} \rightarrow \infty,
  \end{equation*}
  contrary to the assumed pointwise boundedness of the family $(T_\alpha)$.
\end{proof}

\subsection{Basic properties}
From now on we assume moreover (mainly for convenience) that $G$ is \emph{unimodular}, so that we may fix a (left and right) Haar measure $d g$.  For $f \in C_c(G)$, the above discussion then applies to $\mu = f \, d g$, giving us operators on $V$ that we denote by $\pi(f) := \pi(f \, d g)$, thus $\pi(f) = \int_{g \in G} f(g) \pi(g) \, d g$.

\begin{lemma}\label{lem:integral-ops-basic-properties}
  Let $f, f_1, f_2 \in C_c(G)$ and $g \in G$.
  \begin{enumerate}
[(i)]
  \item $\pi(f_1) \pi(f_2) = \pi(f_1 \ast f_2)$, where $f_1 \ast f_2 \in C_c(G)$ denotes the convolution
    \begin{equation*}
      f_1 \ast f_2(x) := \int_{g \in G} f_1(g) f_2(g^{-1} x) \, d g.
    \end{equation*}
  \item $\pi(g) \pi(f) = \pi(f(g^{-1} \cdot))$, $\pi(f) \pi(g) = \pi(f(\cdot g^{-1}))$.
  \item If $f \in C_c(G)^{\class}$, then $\pi(f)$ is equivariant.
  \item If $(\pi,V)$ is unitary, then the adjoint $\pi(f)^*$ is given by $\pi(f^*)$, where $f^*(x) := \overline{f(x^{-1})}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item By substituting $g_2 \mapsto g_1^{-1} g_2$,
    \begin{align*}
      \pi(f_1) \pi(f_2)
      &= \int \int f_1(g_2) f_2(g_2)
        \underbrace{\pi(g_1) \pi(g_2)}{\pi(g_1 g_2)} \, d
        g_1 \, d g_2
      \\
      &=
        \int (\underbrace{\int f_1(g_1) f_2(g_1^{-1} g_2) \,
        d g_1}_{(f_1 \ast f_2)(g_2)})
        \pi(g_2) \, d g_2
      \\
      &=
        \pi(f_1 \ast f_2).
    \end{align*}
  \item By substituting $x \mapsto g^{-1} x$, $\pi(g) \pi(f) = \int f(x) \pi(g) \pi(x) \, d x = \int f(g^{-1} x) \pi(x) \, d x = \pi(f(g^{-1} \cdot))$; similarly for the other assertion.
  \item We've seen that $\pi(g) \pi(f) \pi(g^{-1}) = \pi([x \mapsto f(g^{-1} x g)])$, which coincides with $\pi(f)$ if $f$ is a class function.
  \item Since $\pi$ is unitary, we have $\pi(g)^* = \pi(g^{-1})$, hence $\pi(f)^* = \int \overline{f(g)} \pi(g^{-1}) \, d g$; by the change of variables $g \mapsto g^{-1}$, this coincides with $\pi(f^*)$.
  \end{enumerate}
\end{proof}

\subsection{Approximating vectors by their images under integral operators}\label{sec:appr-vect-their}

\begin{lemma}\label{lem:approx-v-pi-f-v}
  For each $v \in V$ and $\eps > 0$ there exists $f \in C_c(G)$ so that $\|\pi(f) v - v\| \leq \eps$
\end{lemma}
\begin{proof}
  By the continuity of the action, we may find a neighborhood $U$ of the identity element in $G$ so that $\|\pi(g) v - v\| \leq \eps$ for all $g \in U$.  By Urysohn, we may find $f \in C_c(U)$ such that $\int_G f \, d g = 1$ and $f \geq 0$.  Then
  \begin{equation*}
    \|\pi(f) v - v\| = \|\int_{g \in G} f(g) (\pi(g) v - v) \, d g\| \leq (\int f \, d g) \eps = \eps.
  \end{equation*}
\end{proof}

\begin{lemma}\label{lem:approx-v-pi-f-v-2}
  In lemma \eqref{lem:approx-v-pi-f-v}, we may arrange moreover that $f \geq 0$ and $f(g) = f(g^{-1})$, and for $G$ compact also that $f \in C_c(G)^{\class}$.
\end{lemma}
\begin{proof}
  Exercise.
\end{proof}

These results may be generalized and refined further.  For instance, if $G$ is a Lie group, then we may assume that $f$ is smooth.


\subsubsection{Automatic continuity for finite-dimensional
  subrepresentations of $L^2(G)$}\label{sec:autom-cont-finite}
We now fulfill the promise made after the statement of Theorem \ref{thm:coeff-ring-characterizations} by explaining why for a compact group $G$, the finiteness (right, left or bi) of $f \in L^2(G)$ implies that $f$ is represented by a continuous function.  Suppose for instance that $f$ is right-finite, so that it is contained in some finite-dimensional subspace $V \subseteq L^2(G)$ invariant by the right regular representation $\rho$ of $G$.  For each $v \in V$ and $\eps > 0$, we see by Lemma \ref{lem:approx-v-pi-f-v} that there exists $\phi \in C(G)$ so that $\|\rho(\phi) v - v\|_{L^2} \leq \eps$.  In particular, the subspace of $V$ consisting of elements of the form $\rho(\phi) v$ is dense.  Since $V$ is finite-dimensional, it follows that every element of $V$ is of this form.  Note that each such element is pointwise defined.

We verify now that every element of $V$, say $v' = \rho(\phi) v$, is in fact continuous.  For $g \in G$, we have (by Lemma \ref{lem:integral-ops-basic-properties}) $\rho(g) v' = \rho(\phi(g^{-1} \cdot)) v$, hence
\begin{equation*}
  \sup_{x \in G} |v'(x g) - v'(x)| = \|\rho(g) v' - v'\|_{L^\infty} = \|\rho(\phi(g^{-1} \cdot) -\phi ) v\|_{L^\infty} \leq \|\rho(\phi(g^{-1} \cdot) -\phi )\|_{L^2} \| v\|_{L^2},
\end{equation*}
in the last step by Cauchy--Schwarz.  Since the left regular representation of $G$ on $L^2(G)$ is continuous, the required continuity of $v'$ follows.


\subsection{Some functional-analytic considerations}

Recall (from, e.g., \S\ref{sec:spectr-theory-comp}) the definition of a compact operator on a Hilbert space.

I don't think the following definition is standard, but I like it.
\begin{definition}\label{defn:compact-type}
  We say that $(\pi,V)$ is of \emph{compact type} if $\pi(f)$ is compact for each $f \in C_c(G)$.
\end{definition}

We recall a bit more functional analysis background.
\begin{lemma}
  Let $(X,\mu)$ be a locally compact space equipped with a positive Borel measure $\mu$ such that $V := L^2(X)$ is a separable Hilbert space.  Let $k \in L^2(X \times X)$.  Then we may define a bounded operator $T : V \rightarrow V$ by $T v(x) = \int_{y} k(x,y) \, d \mu(y)$.  Any such operator is compact.
\end{lemma}
\begin{proof}
[Proof sketch; omitted in lecture]
  Choose an orthonormal basis $e_1,e_2,\dotsc$ for $L^2(X)$ and set $a_{i j} := \langle T e_i, e_j \rangle$.  Then $\sum_{i,j} |a_{i j}|^2 = \|k\|_{L^2(X \times X)}^2 < \infty$.  Let $v = \sum_j v_j e_j \in V$.  Then $T v = \sum_i b_i e_i$ with $b_i := \sum_j a_{i j} v_j$.  By Cauchy--Schwartz, $|b_i|^2 \leq \|v\|^2 C_i$, where $C_i := \sum_j |a_{i j}|^2$.  Thus the image under $T$ of the unit ball in $V$ is contained in the set $\{\sum b_i e_i : |b_i|^2 \leq C_i \}$.  Using that $\sum_i C_i < \infty$ and a diagonalization argument, we verify readily that this set is precompact.
\end{proof}

\begin{lemma}\label{thm:G-compact-implies-compact-type}
  If $G$ is a compact group, then its right regular representation $(\rho,L^2(G))$ is of compact type.
\end{lemma}
\begin{proof}
  For $f \in C_c(G) = C(G)$ and $v \in L^2(G)$, we may write $\rho(f) v(x) = \int f(y) v(x y) \, d y = \int v(y) k(x,y) \, d y$, where $k(x,y) := f(x^{-1} y)$.  Thus the previous lemma gives the required conclusion.  (Sketch of a slightly more direct proof, omitted in lecture: since $G$ is compact and $k$ is continuous, it is uniformly continuous, and so the function $(x,y) \mapsto k(x^{-1} y)$ may be approximated uniformly by step functions; thus $\rho(f)$ is approximated in norm by finite rank operators, hence is compact.)
\end{proof}

\subsection{Proof of the \texorpdfstring{$L^2$}{L2}-density
  part of the Peter--Weyl theorem}
We now prove part (ii) of Theorem \ref{thm:P-W-general}.  Let $G$ be a compact group, $v \in L^2(G)$ and $\eps > 0$.  Let $\rho$ denote the right regular representation.  By lemma \ref{lem:approx-v-pi-f-v-2}, we may find $f \in C_c(G)^{\class}$ so that $f \geq 0, f(g^{-1}) = f(g)$ and
\begin{equation}\label{eq:T-v-approx-v}
  \|\rho(f) v - v\| \leq \eps.
\end{equation}
Then $f^* = f$, so $T := \rho(f)$ is self-adjoint.  By lemma \ref{thm:G-compact-implies-compact-type}, $T$ is compact.  Set $V := L^2(G)$.  By Theorem \ref{thm:spectral-theorem-compact}, we may write $V$ as the orthogonal Hilbert direct sum of the eigenspaces $V_\lambda$ of $T$, taken over $\lambda \in \mathbb{R}$; moreover, setting $V' := \oplus_{|\lambda| \geq \eps} V_\lambda$ and $V'' := \oplus_{|\lambda| < \eps} V_\lambda$, the space $V'$ is finite-dimensional.  Since $f$ is a class function, we know (by lemma \ref{lem:integral-ops-basic-properties}) that the operator $T$ is equivariant (i.e., $\rho(g) T = T \rho(g)$ for all $g \in G$), thus its eigenspaces $V_\lambda$ and thus their sums $V'$ and $V''$ are $\rho(G)$-invariant.  Thus
\begin{equation}\label{eq:V'-finite}
  V' \subseteq \mathcal{A}(G).
\end{equation}
Let $v' \in V'$ and $v'' \in V''$ denote the components of $v$, so that $v = v' + v''$.  We have
\begin{equation}\label{eq:estimate-T-v-double-prime}
  \|T v''\|
  = \|\sum_{|\lambda| < \eps } \lambda v_\lambda \|
  \leq \eps \|v\|.
\end{equation}
By \eqref{eq:T-v-approx-v} and \eqref{eq:estimate-T-v-double-prime}, we deduce that
\begin{equation}\label{eq:}
  \|v - T v'\| \leq \eps (1 + \|v\|).
\end{equation}
On the other hand, $T v' = \sum_{|\lambda| \geq \eps} \lambda v_\lambda \in V' \subseteq \mathcal{A}(G)$.  Thus $\mathcal{A}(G)$ is dense in $L^2(G)$.\footnote{End of lecture \#7, Tuesday, 12 March}

\subsection{Proof of the uniform density
part of the Peter--Weyl theorem}
We now deduce part (i) of Theorem \ref{thm:P-W-general}.  We retain the above notation.  Let $v \in C(G)$ and $\eps > 0$.  Since $G$ is compact, $v$ is uniformly continuous; by an easy variant of Lemma \ref{lem:approx-v-pi-f-v}, we may thus find $f \in C(G)$ so that $\|\rho(f) v - v\|_{L^\infty} \leq \eps$.  By the $L^2$-density part of the Peter--Weyl theorem that we have already proved, we may find $f' \in \mathcal{A}(G)$ so that $\|f - f'\|_{L^2} \leq \eps$.  Since $(G, d g)$ is a probability space, it follows in particular that $\|f - f'\|_{L^1} \leq \eps$.  Since
\begin{equation*}
  \rho(f) v - \rho(f') v = \int_{f \in G} (f - f')(g) \pi(g) v \, d g
\end{equation*}
and $\|\pi(g) v\|_{L^\infty} = \|v\|_{L^\infty}$, it follows that
\begin{equation*}
  \|\rho(f) v - \rho(f') v\|_{L^\infty} \leq \|f - f'\|_{L^1} \|v\|_{L^\infty} \leq \eps \|v\|_{L^\infty}.
\end{equation*}
Thus
\begin{equation*}
  \|\rho(f') v - v \| \leq \eps (1 + \|v\|_{L^\infty}).
\end{equation*}
Observe finally that for any $g \in G$, we have
\begin{equation*}
  \rho(g) \rho (f') v = \rho(f'(g^{-1} \cdot )) v.
\end{equation*}
Thus if $W$ is any left-$G$-invariant subspace of $\mathcal{A}(G)$ that contains $f'$, then $\rho (f') v$ belongs to the $\rho(G)$-invariant finite-dimensional space $\rho(W) v$, and so $\rho (f') v \in \mathcal{A}(G)$.  We conclude as required that $\mathcal{A}(G)$ is dense in $C(G)$.

Part (iii) of the Peter--Weyl theorem can be proved analogously, or deduced from parts (i) and (ii) by a simple averaging trick; we'll take the latter approach.  Given an element $v$ of $L^2(G)^{\class}$ or $C(G)^{\class}$ and $\eps > 0$, we first find $v' \in \mathcal{A}(G)$ so that $\|v' - v\| \leq \eps$, where $\|.\|$ denotes either the $L^2$-norm or $L^\infty$-norm.  We then introduce the averaging operator
\begin{equation*}
  \mathbb{E} : \{\text{functions }f: G \rightarrow \mathbb{C}\} \rightarrow \{\text{class functions } \mathbb{E} f: G \rightarrow \mathbb{C}\}
\end{equation*}
\begin{equation*}
  \mathbb{E} f(x) := \int_{g \in G} f(g x g^{-1}) \, d g.
\end{equation*}
Since $f$ and $x \mapsto f(g x g^{-1})$ have the same norms, the triangle inequality implies that $\|\mathbb{E} f\| \leq \|f\|$.  Set $v'' := \mathbb{E} v'$.  Then $v'' \in \mathcal{A}(G)^{\class}$.  Since $v$ is a class function, we have $v = \mathbb{E} v$, and so $(v'' - v) = \mathbb{E} (v' - v)$, hence $\|v'' - v\| \leq \|v' - v\| \leq \eps$, giving the required approximation of $v$ by an element of $\mathcal{A}(G)^{\class}$.

\subsection{Finite-dimensionality of irreducibles}\label{sec:finite-dimens-irred}
We prove part (iv) of Peter--Weyl in the special case of an irreducible Hilbert representation $(\pi,V)$; the more general conclusion can be deduced similarly after developing integration more generally.  The proof is very similar to that of the $C(G)$-density assertion (indeed, a unified statement and proof involving Banach space representations could be given, but I feel like it doesn't hurt to see the argument essentially repeated, since the method is so widely applicable).  Let $v$ be a nonzero element of $V$, and let $\eps > 0$ be sufficiently small.  We can find an open neighborhood $U$ of the identity element of $G$ so that $\|\pi(g) v - v\| \leq \eps$ for all $g \in U$.  Take $f \in C_c(U)$ with $f \geq 0, \int f = 1$.  Then $\|\pi(f) v - v\| \leq \eps$.  Use the $L^2$-density part of the Peter--Weyl theorem to produce $f' \in C(G)$ so that $\|f - f'\|_{L^2} \leq \eps$.  Then likewise $\|f - f'\|_{L^1} \leq \eps$, and so, as before, $\|\pi(f') v - v \| \leq \eps (1 + \|v\|)$.  For small enough $\eps$, this implies in particular that $\pi(f') v \neq 0$.  Since $f'$ belongs to $\mathcal{A}(G)$, it is contained in some finite-dimensional $G \times G$-invariant subspace $W$.  Then $\pi (f') v$ belongs to the space $\pi(W) v := \{\pi(w) v : w \in W\}$.  The latter space is finite-dimensional (with dimension bounded by $\dim(W)$) and nonzero (because it contains $\pi (f') v$).  Most significantly, it is invariant: for $w \in W$ and $g \in G$, we have (by Lemma \ref{lem:integral-ops-basic-properties})
\begin{equation*}
  \pi(g) \pi(w) v = \pi(w(g^{-1} \cdot )) v.
\end{equation*}
Since $V$ is irreducible, this forces $V = \rho(W) v$, hence $\dim(V) \leq \dim(W) < \infty$, as required.


\subsection{Fourier analysis on a compact group \texorpdfstring{$G$}{G}}\label{sec:four-analys-comp}
Given $f \in C(G)$ (say), its Fourier transform is defined to be the collection $(\pi(f))_{\pi \in \Irr(G)}$ of integral operators that it induces on the (equivalence classes of) finite-dimensional irreducible representations of $G$.  We can restate some of our results concerning orthogonality and completeness of matrix coefficients in terms of these collections of operators.  In preparation for doing so, we define on any finite-dimensional Hilbert space $V$ the \emph{Hilbert--Schmidt inner product} of $T_1, T_2 \in \End(V)$ by the formula
\begin{equation*}
  \langle T_1, T_2 \rangle := \trace(T_1 T_2^*),
\end{equation*}
where as before $T_2^*$ denotes the hermitian adjoint, given with respect to an orthonormal basis by the conjugate transpose.  If $T_1$ and $T_2$ are respected by matrices $(a_{i j})$ and $(b_{ij})$ defined with respect to an orthonormal basis, then we may verify readily that
\begin{equation*}
  \langle T_1, T_2 \rangle = \sum_{i,j} a_{i j} \overline{b_{i j}}.
\end{equation*}
This is the dual of the inner product considered in \S\ref{sec:schur-orthogonality}.

\begin{theorem}
  \begin{enumerate}
[(i)]
  \item For $f_1, f_2 \in C(G)$,
    \begin{equation*}
      \langle f_1, f_2 \rangle_{L^2(G)} = \sum_{\pi \in \Irr(G)} \dim(\pi) \langle \pi(f_1), \pi(f_2) \rangle.
    \end{equation*}
  \item Under ``some assumptions'' on $f$,
    \begin{equation}\label{eq:pointwise-fourier-expn}
      f(1)
      = \sum_{\pi \in \Irr(G)}
      \dim(\pi) \chi_\pi(f),
    \end{equation}
    where
    \begin{equation*}
      \chi_\pi(f) := \trace(\pi(f)) = \int_{g \in G} f(g) \chi_\pi(g) \, d g.
    \end{equation*}
    For instance, a sufficient assumption is that $f$ have the form $f_1 \ast f_2$ for some $f_1, f_2 \in L^2(G)$.
  \item For $\sigma \in \Irr(G)$, define $\alpha_\sigma := \dim(\sigma) \overline{\chi_\sigma } \in C(G)$.  Then for each $\pi \in \Irr(G)$, we have
    \begin{equation}\label{eq:alpha-sigma-acts-on-pi}
      \pi(\alpha_\sigma)
      = 
\begin{cases}
        0  & \text{ if } \pi \not\cong \sigma, \\
        1 & \text{ if } \pi \cong \sigma.
      \end{cases}
    \end{equation}
    Moreover, for $\sigma_1, \sigma_2 \in \Irr(G)$,
    \begin{equation}\label{eqn:convolving-idemps}
      \alpha_{\sigma_1} \ast \alpha_{\sigma_2}
      = 
\begin{cases}
        \alpha_{\sigma} & \text{ if }
        \sigma_1 = \sigma_2 =: \sigma, \\
        0 & \text{ otherwise}.
      \end{cases}
    \end{equation}
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item With respect to an orthonormal basis of $\pi$, we have
    \begin{equation}\label{eq:}
      \pi(f)_{i j}
      = \int_{g \in G}
      f(g) \pi_{i j}(g) \, d g,
    \end{equation}
    so $\langle \pi(f_1), \pi(f_2) \rangle = \langle f_1, \overline{\pi_{i j}} \rangle \langle \overline{\pi_{i j}}, f_2 \rangle$.  The conclusion follows from part (i) of the Peter--Weyl theorem, which we have seen implies that the function $\sqrt{\dim(\pi)} \pi_{i j}$ (and hence likewise the functions $\sqrt{\dim(\pi)} \overline{\pi_{i j}}$) form an orthonormal basis of $L^2(G)$.
  \item Suppose for instance that $f = f_1 \ast f_2$ with $f_1, f_2 \in L^2(G)$.  Then $f(1) = \int_{g \in G} f_1(g) f_2(g^{-1}) \, d g = \langle f_1, f_2^* \rangle$, while $\langle \pi(f_1), \pi(f_2^*) \rangle = \trace(\pi(f_1) \pi(f_2)) = \trace(\pi(f_1 \ast f_2)) = \chi_\pi(f)$; the required conclusion thus follows from part (i).
  \item Using the expansion $\chi_\sigma = \sum_{k } \sigma_{k k}$, we compute that
    \begin{align*}
      \pi(\alpha_\sigma)_{i j}
      &=
        \int_{g \in G}
        \alpha_\sigma(g) \pi_{i j}(g) \, d g
      \\
      &=
        \sum_k
        \dim(\sigma)
        \langle \pi_{i j}, \sigma_{k k} \rangle
      \\
      &= \delta_{\sigma,\pi}
        \sum_k
        \delta_{i k}
        \delta_{j k}
      \\
      &= \delta_{\sigma,\pi}
        \delta_{i j}.
    \end{align*}
    These matrix entries are of the required form.

    To prove \eqref{eqn:convolving-idemps}, it suffices by the injectivity of the association $f \mapsto (\pi(f))_{\pi \in \Irr(G)}$ to check that $\pi(\alpha_{\sigma_1} \ast \alpha_{\sigma_2})$ is given by $\pi(\alpha_\sigma)$ when $\sigma_1 \cong \sigma_2$ and vanishes otherwise, which follows from \eqref{eq:alpha-sigma-acts-on-pi}.
  \end{enumerate}
\end{proof}

The pointwise decomposition \eqref{eq:pointwise-fourier-expn} fails in general for $f \in C(G)$, as one can see already when $G = \U(1)$; there are well-known constructions of continuous functions on the circle whose Fourier series do not converge pointwise.  A sufficient condition for the validity of \eqref{eq:pointwise-fourier-expn} is that $f$ be smooth.  (A \emph{smooth function} on a compact Lie group is what you'd expect.  A general compact group $G$ can be written, as indicated in \S\ref{sec:peter-weyl-theorem}, as an inverse limit of compact Lie groups $G_n$; a smooth function on $G$ is then a function that factors through a smooth function on one of the quotients $G_n$ of $G$.)  The proof isn't so different from the case $G = \U(1)$, but we won't go into it.  Lipschitz continuity should also suffice.

\subsection{Isotypic decomposition}\label{sec:isotyp-decomp}

\begin{definition}
  Let $(\pi,V)$ be a representation of a compact group $G$, and let $\sigma \in \Irr(G)$.  We say that a nonzero vector $v \in V$ is \emph{$\sigma$-isotypic} if there are closed invariant subspaces $W_1,\dotsc,W_n$ of $V$, with each $W_j$ isomorphic to $\sigma$, so that $v \in \sum_{j} W_j$.  We write $V(\sigma) := \{0\} \cup \{\text{$\sigma$-isotypic } v \in V\}$.
\end{definition}

\begin{theorem}\label{thm:isotyp-decomp-1}
  Let $(\pi,V)$ be a Hilbert representation of the compact group $G$.
  \begin{enumerate}
[(i)]
  \item $\pi(\alpha_\sigma)$ defines a projection $V \rightarrow V(\sigma)$; if $\pi$ is unitary, then it is the orthogonal projection.
  \item $V = \hat{\oplus }_{\sigma \in \Irr(G)} V(\sigma)$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item If $v \in V$, then $\pi(\alpha_\sigma) v$ is contained in the space $\pi(\mathcal{A}(\overline{\sigma })) v$.  By Lemma \ref{lem:integral-ops-basic-properties}), the map
    \begin{equation*}
      \mathcal{A}(\overline{\sigma }) \ni f \mapsto \pi(f) v \in V
    \end{equation*}
    is equivariant for the left regular representation on the domain.  We have seen that $\mathcal{A}(\overline{\sigma }) \cong \mathcal{A}(\sigma ^*) \cong \End(\sigma^*) \cong \sigma \otimes \sigma^*$ as $G \times G$-representations.  Under the left regular representation, $\mathcal{A}(\overline{\sigma })$ is thus isomorphic to $\sigma^{\oplus \dim(\sigma)}$, so we may write $v = \sum_{j=1}^{\dim(\sigma)} \pi(f_j) v$ with each $f_j$ in a subspace $\tilde{W}_j$ of $\mathcal{A}(\overline{\sigma })$ isomorphic to $\sigma$.  Setting $v_j := \pi(f_j) v \in W_j := \pi(\tilde{W}_j)$, we have $v = \sum v_j$.  If $W_j$ is nonzero, then the irreducibility of $\sigma$ implies that $W_j \cong \sigma$.  It follows that $v$ is $\sigma$-isotypic.

    If $v \in V(\sigma)$, then $\pi(\alpha_\sigma) v = v$, because $\pi(\alpha_\sigma)$ acts by the identity on each of the subspaces $W_j$ that arise in the definition of ``$\sigma$-isotypic.''

    We have shown that $\pi(\alpha_\sigma)$ defines a projection $V \rightarrow V(\sigma)$.  We have $\alpha_\sigma^*(g) = \overline{\alpha_\sigma(g^{-1})} = \dim(\sigma) \chi_\sigma(g^{-1}) = \dim(\sigma) \overline{\chi_\sigma(g)} = \alpha_\sigma(g)$, so if $\pi$ is unitary, then (by Lemma \ref{lem:integral-ops-basic-properties}) $\pi(\alpha_\sigma)$ is self-adjoint, and so defines the orthogonal projection.
  \item We argue essentially as in \S\ref{sec:finite-dimens-irred}: for each $v \in V$ and $\eps > 0$, we can find $f \in C(G)^{\class}$ so that $\|\pi(f) v - v\| \leq \eps$.  We can then decompose $f$ in $L^2(G)^{\class}$ as $\sum_{\sigma \in \Irr(G)} c_\sigma \alpha_\sigma$, say; in particular, we can find a finite subset $\Sigma$ of $\Irr(G)$ so that $f' := \sum_{\sigma \in \Sigma} c_\sigma \alpha_\sigma$ satisfies $\|f' - f\|_{L^2} \leq \eps$.  Arguing as before, we get $\|\pi(f') v - v\| \leq \eps (1 + \|v\|)$.  But $\pi(f') v = \sum_{\sigma \in \Sigma} c_\sigma \pi(\alpha_\sigma) v \in \oplus_{\sigma \in \Sigma} V(\sigma)$.  Thus $\sum V(\sigma)$ is dense in $V$.  The sum is direct because if $v \in V(\sigma_1) \cap V(\sigma_2)$ with $\sigma_1 \not\cong \sigma_2$, then $v = \pi(\alpha_{\sigma_1}) \pi(\alpha_{\sigma_2}) v = \pi(\alpha_{\sigma_1} \ast \alpha_{\sigma_2}) v$, which vanishes thanks to \eqref{eqn:convolving-idemps}.
  \end{enumerate}
\end{proof}

\footnote{End of half-lecture \#8, Thursday, 14 March}

\subsection{Decompositions of compact-type representations}
The decompositions of representations of a compact group noted above (e.g., in theorems \ref{thm:complete-reducibility-compact-group}, \ref{thm:P-W-general} and \ref{thm:isotyp-decomp-1}) do not hold in general when $G$ is non-compact and $V$ is infinite-dimensional.  The main issue is that there may then exist nontrivial representations having no irreducible subrepresentations.  For instance, the (right) regular representation of $\mathbb{R}$ on $L^2(\mathbb{R})$ has the property.  A basic result of Gelfand--Graev--Piatetski-Shapiro shows that a sufficient condition on the representation for it to decompose as a sum of irreducibles is that it be unitary and of compact type (Definition \ref{defn:compact-type}).  This is very handy in applications involving non-compact groups.  Recall that ``subrepresentation'' means ``closed invariant subspace.''
\begin{theorem}
  Let $G$ be locally compact and unimodular, and let $(\pi,V)$ be a unitary Hilbert representation of compact type.  Then $V$ decomposes as a Hilbert direct sum of irreducible subrepresentations $V_j$, each occurring with finite multiplicity (i.e., for each $j$, the number of $k$ with $V_j \cong V_k$ is finite).
\end{theorem}
\begin{proof}
  By Zorn's lemma, we may find a maximal collection $(V_j)$ of mutually orthogonal irreducible subrepresentations $V_j$ of $V$.  We must show that $\oplus V_j$ is dense in $V$.  If not, then its orthogonal complement $V'$ is a nonzero representation, satisfying the same hypotheses as $V$, which contains no irreducible subrepresentations.  Replacing $V$ by $V'$ if necessary, our task is thus to show that if $V$ is nonzero, then it contains at least one irreducible subrepresentation.
  
  The results of \S\ref{sec:appr-vect-their} (applied to any nonzero vector $v \in V$, and with $\eps$ taken sufficiently small in terms of $v$) imply that there exists $f \in C_c(G)$, real-valued and with $f(g) = f(g^{-1})$, so that the integral operator $\pi(f)$ is not identically zero.  Since $\pi$ is assumed of compact type, the operator $\pi(f)$ is compact.  By construction, it is self-adjoint.  By the spectral theory of self-adjoint compact operators, we may find a nonzero real number $\lambda$ so that the $\lambda$-eigenspace $V_\lambda$ of $\pi(f)$ is nonzero and finite-dimensional.
  
  Take any nonzero $v \in V_\lambda$, let $G v$ denote its orbit under $G$, and let $\langle G v \rangle$ denote the closure of the span of $G v$.  Then $\langle G v \rangle$ is a subrepresentation of $V$; it is the smallest subrepresentation containing $v$.

  The key observation is that any decomposition of the representation $\langle G v \rangle$ descends to a decomposition of the vector $v$ inside the finite-dimensional eigenspace $V_\lambda$.  Indeed, if $\langle G v \rangle$ is not reducible, then it contains a nonzero proper subrepresentation $W_1$, whose orthogonal complement $W_1'$ in $\langle G v \rangle$ is likewise invariant, giving a decomposition $\langle G v \rangle = W_1 \oplus W_1'$.  The subspaces in this decomposition are invariant by $G$, hence also by $T := \pi(f) - \lambda$, so writing $v = v_1 + v_1'$ with $v_1 \in W_1$ and $v_1' \in W_1'$, we deduce from the identity $T v = 0$ that in fact $T v_1 = T v_1' = 0$, hence that $v_1, v_1' \in V_\lambda$.

  We know that $v_1$ and $v_1'$ are both nonzero, because if (say) $v_1' = 0$, then $v$ belongs to the proper subrepresentation $W_1$ of $\langle G v \rangle$, contrary to the construction of the latter.  Since $v_1'$ belongs to $V_\lambda$ and to $\langle G v \rangle$ but not to $W_1$, we have the strict containment
  \begin{equation}\label{eq:GGPS-monotonicity}
    V_\lambda \cap \langle G v_1 \rangle
    \subseteq 
    V_\lambda \cap W_1
    \subsetneq
    V_\lambda \cap \langle G v \rangle
  \end{equation}
  between finite-dimensional spaces.  We now repeat the same construction but with $v$ replaced by $v_1$.  If $\langle G v_1 \rangle$ is reducible, then we may decompose it as $W_2 \oplus W_2'$ and likewise $v_1$ as $v_2 + v_2'$, with $v_2, v_2' \in V_\lambda$.  If $\langle G v_2 \rangle$ is reducible, then we may decompose $v_2 = v_3 + v_3'$.  Thanks to \eqref{eq:GGPS-monotonicity}, this procedure gives us an irreducible subrepresentation after at most $\dim(V_\lambda \cap \langle G v \rangle)$ iterations.
  
  The slicker way to write the proof is of course to assume from the outset that $v$ was chosen to minimize the dimension of $V_{\lambda} \cap \langle G v \rangle$; reducibility of $\langle G v \rangle$ then gives a contradiction.
  
  For the finiteness of multiplicity, we can find for each $V_j$ a self-adjoint integral operator $\pi(f)$ having some nonzero eigenvalue $\lambda$ in $V_j$.  The same eigenvalue then shows up in every $V_k$ that is isomorphic to $V_j$.  Since $\pi(f)$ is assumed compact, its eigenspaces are finite-dimensional, and so the number of such $V_k$ is finite.
  % we win after finitely many iterations of this procedure because the dimension can only drop down finitely many times.  To conclude the proof efficiently, we assume that $v$ was chosen so that the finite-dimensional space $V_\lambda \cap \langle G v \rangle$ has minimal dimension.  We have seen then that the reducibility of $\langle G v \rangle$ leads to the inequality \eqref{eq:GGPS-monotonicity}, contradicting the assumed minimality.  Thus $\langle G v \rangle$ gives the required irreducible subrepresentation.
\end{proof}

\newpage



\section{Algebraicity of compact Lie groups}
Our goal is to explain the meaning and proof of the phrase ``every compact Lie group is algebraic.''  This serves both as an application of the theory developed so far and as a tool for further study also of non-compact groups.

\subsection{Preliminaries on algebraic groups}
First, we need to ``review'' some basics on algebraic groups.
\begin{definition}
  Let $k$ be an infinite field and $n \geq 0$.  For a collection $S \subseteq k[x_1,\dotsc,x_n]$ of polynomials, the \emph{vanishing locus} $V(S)$ is the set
  \begin{equation*}
    V(S) := \{p \in k^n : f(p) = 0 \text{ for all } f \in S\}
  \end{equation*}
  of common zeros.  An \emph{algebraic subset} of $k^n$ is a subset of the form $V(S)$ for some $S$; that is to say, it is a subset defined by polynomial equations.

  The \emph{vanishing ideal} of a subset $X$ of $k^n$ is given by
  \begin{equation*}
    I(X) := \{f \in k[x_1,\dotsc,x_n] : f(p) = 0 \text{ for all } p \in X\}.
  \end{equation*}
  The \emph{coordinate ring} of an algebraic subset $X \subseteq k^n$ is the ring
  \begin{equation*}
k[X] := k[x_1,\dotsc,x_n]/I(X),
\end{equation*}
 which may be regarded as the space of ``polynomial functions from $X$ to $k$.''  The \emph{Zariski topology} on $k^n$ (or any algebraic subset thereof) is that for which the closed subsets are the algebraic sets.
\end{definition}

We note that Hilbert's basis theorem implies that for each $S$ there exists a finite subset $S_0 \subset S$ so that $V(S) = V(S_0)$, so that any algebraic set may be defined by finitely-many polynomial equations.

It's not hard to check that the ``Zariski topology'' as described above defines an actual topology.  By comparison, a basic theorem in topology says that in a nice enough topological space $X$ (e.g., a compact metric space), for any closed subset $Z \subseteq X$ there exists a continuous function $f : X \rightarrow \mathbb{R}$ so that $Z = \{p \in X : f(p) = 0 \}$, hence that the closed subsets of $X$ are precisely those that can be defined by \emph{continuous} equations.  The analogy with the Zariski topology should be clear.

\emph{Morphisms} between algebraic sets $X \subseteq k^m$ and $Y \subseteq k^n$ are maps $X \rightarrow Y$ defined by polynomials in the coordinates.  They are obviously Zariski continuous.  We obtain in this way a category, with objects the algebraic sets and morphisms the morphisms.  An \emph{affine variety} is basically the same thing as an algebraic set, but (by some conventions) without emphasis on any particular embedding in some $k^n$.


Given two sets $X \subseteq k^m$ and $Y \subseteq k^n$, we may define their product $X \times Y \subseteq k^{m+n}$.  If $X$ and $Y$ are algebraic, then so is $X \times Y$: if $X = V(S_1)$ and $Y = V(S_2)$, then $X \times Y = V(\{f_1 \otimes f_2 : f_1 \in S_1, f_2 \in S_2\})$, where $(f_1 \otimes f_2)(p,q) := f_1(p) f_2(q)$ for $p \in k^m, q \in k^n$.  We may thus speak of the Zariski topology on $X \times Y$ with respect to $k^{m + n}$.  We caution that this is not in general the same as the product of the Zariski topologies on $X$ and $Y$.

Given an algebraic set $X$, we'll sometimes abuse notation slightly by defining $V(S)$ for a subset $S$ of $k[X]$ to be the set of common zeros inside $X$ of elements of $S$, and, for a subset $Y$ of $X$, by $I(Y)$ the ideal in $k[X]$ consisting of $f$ that vanish on $Y$.  Any such abuse should be clear by context.

As a basic example of reasoning with these definitions, we verify the following:
\begin{lemma}
  For any subset $X \subseteq k^n$, the Zariski closure $\Zcl(X)$ is equal to $V(I(X))$.
\end{lemma}
\begin{proof}
  Well, $V(I(X))$ contains $X$ (``$X$ vanishes under any polynomial that vanishes on $X$''), and $V(I(X))$ is algebraic, hence Zariski closed.  Conversely, let $Y$ be any Zariski closed set containing $X$, thus $Y = V(T)$ for some $T \subseteq k[x_1,\dotsc,x_n]$.  Since $X \subseteq V(T)$ (``$X$ vanishes under every element of $T$''), we have $T \subseteq I(X)$ (``every element of $T$ vanishes on $X$''), and so $V(T) \supseteq V(I(X))$ (``the more equations, the fewer solutions'').  Thus $V(I(X))$ is the smallest Zariski closed set containing $X$, as required.
\end{proof}

The group $\GL_n(k)$ of invertible matrices is not obviously algebraic, because it is defined inside the ambient space of matrices $M_n(k) \cong k^{n^2}$ by the polynomial \emph{inequation} $\det \neq 0$ rather than by a system of polynomial \emph{equations}.  We ``make it algebraic'' by using the embedding
\begin{equation*}
  \GL_n(k) \cong \{(x,y) \in M_n(k)^2 : x y = 1 \} \subseteq M_n(k)^2 \cong k^{2 n^2}
\end{equation*}
\begin{equation*}
  g \mapsto (g, g^{-1}),
\end{equation*}
to view $\GL_n(k)$ as an algebraic subset of $k^{2 n^2}$.  Then the coordinate ring $k[\GL_n(k)]$ consists of functions $\GL_n \rightarrow k$ given by polynomials in the entries of a matrix together with its inverse.

\begin{definition}\label{sec:alg-gp-ad-hoc}
  An \emph{algebraic group} is an abstract subgroup of $\GL_n(k)$ that is also an algebraic set.
\end{definition}
When $k = \mathbb{C}$, we speak of a complex algebraic group.  When $k = \mathbb{R}$, we speak of a real algebraic group.  Note that we may think of $\GL_n(\mathbb{C})$ as a real algebraic subgroup of $\GL_{2n}(\mathbb{R})$.  Note also that algebraic groups over $k = \mathbb{R}$ or $\mathbb{C}$ are Lie groups (e.g., by the general theorem that closed subgroups of matrix groups are Lie groups).

Definition \ref{sec:alg-gp-ad-hoc} is a bit \emph{ad hoc}.  In much the same way that we defined compact Lie groups first without reference to any matrix embedding and then showed as a consequence (Corollary \ref{sec:cor-peter-weyl-matrix}) of the Peter--Weyl theorem that such an embedding exists, one might more properly define an algebraic group to be an algebraic set equipped with a group law for which multiplication and inversion are described in coordinates via polynomials.  Arguments similar to those in the proof of Corollary \ref{sec:cor-peter-weyl-matrix} then confirm that this apparently more general definition is ultimately equivalent to what we've given here.

\begin{lemma}\label{lem:zcl-gp-alg}
  If $G$ is an algebraic group and $H < G$ is an abstract subgroup, then the Zariski closure $\Zcl(H)$ of $H$ inside $G$ is an algebraic subgroup of $G$.
\end{lemma}
\begin{proof}
  We just need to check that $\Zcl(H)$ is closed under multiplication and inversion.  We check this for multiplication, leaving the case of inversion to the reader (alternatively, one could get fancy and consider maps like $(p,q) \mapsto p q^{-1}$).  Let $p,q \in \Zcl(H)$, and let $p q \in U \subseteq G$ be a neighborhood.  Since matrix multiplication is described by polynomials, it is Zariski continuous, and so we can find a neighborhood $p \in V_1 \subseteq G$ so that $p' q \in U$ for all $p' \in V_1$.  Since $p$ belongs to the closure of $H$, we know that $V_1$ intersects $H$, so choose some $p' \in V_1 \cap H$.  Then $p' q \in U$, so by the same argument, we can find a neighborhood $q \in V_2 \subseteq G$ so that $p' q' \in U$ for all $q' \in V_2$.  Since $q$ is in the closure of $H$, we know that $V_2$ intersects $H$, so we may find some $q' \in V_2 \cap H$.  Since $H$ is a subgroup, we then have $p' q' \in U \cap H$, so $U$ intersects $H$.  Since $U$ was arbitrary, we conclude that $p q \in \Zcl(H)$.
\end{proof}


\subsection{Complexification of a compact Lie group}
Now let $K$ be a compact subgroup of $\GL_n(\mathbb{C})$, hence a Lie subgroup.  Recall also, by Corollary \ref{sec:cor-peter-weyl-matrix}, that every compact Lie group is of this form.  Since the standard representation $K \hookrightarrow \GL_n(\mathbb{C})$ is unitarizable, we know that $K$ is conjugate to a subgroup of $\U(n)$.  Let's assume for convenience that in fact $K \leq \U(n)$.  Let $G$ denote the complex Zariski closure of $K$ in $\GL_n(\mathbb{C})$, thus
\begin{equation*}
  G = \{g \in \GL_n(\mathbb{C}) : f(g) = 0 \text{ for all } f \in \mathbb{C}[\GL_n(\mathbb{C})] \text{ with } f|_K = 0 \},
\end{equation*}
where as before $\mathbb{C}[\GL_n(\mathbb{C})]$ denotes the space of functions $f : \GL_n (\mathbb{C} ) \rightarrow \mathbb{C}$ given by polynomials in the entries of a matrix together with its inverse.  (For example, if $K = \U(n)$, then $G = \GL_n(\mathbb{C})$.)  We denote by
\begin{equation*}
  \Theta : \GL_n(\mathbb{C}) \rightarrow \GL_n(\mathbb{C})
\end{equation*}
the ``inverse conjugate transpose'' map
\begin{equation*}
  \Theta(g) := {}^t \overline{g}^{-1} = (g^*)^{-1}, \quad g^* := {}^t \overline{g}
\end{equation*}
and by
\begin{equation*}
  \theta : \glLie_n(\mathbb{C}) \rightarrow \glLie_n(\mathbb{C})
\end{equation*}
\begin{equation*}
  \theta(x) := - {}^t \overline{x} = - x^*, \quad x^* := {}^t \overline{x}
\end{equation*}
its differential.  Observe that $\U(n) = \{ g \in \GL_n(\mathbb{C}) : \Theta(g) = g\}$ and $\mathfrak{u}(n) = \{ x \in \glLie_n(\mathbb{C}) : \theta(x) = x\}$.
\begin{theorem}\label{thm:complexification-compact-lie-basic-properties}
  With notation and assumptions as above:
  \begin{enumerate}
[(i)]
  \item $G$ is a complex algebraic group that is closed under $\Theta$.
  \item The map $\mathbb{C}[G] \rightarrow \{\text{functions } K \rightarrow \mathbb{C} \}$ given by restriction $f \mapsto f|_K$ induces an isomorphism $\mathbb{C}[G] \cong \mathcal{A}(K)$.
  \item $K = G \cap \U(n)$.  In particular, $K$ is a real algebraic group.\footnote{End of half-lecture \#9, Tuesday, 19 March}
  \item $K$ is a maximal compact subgroup of G.
  \item $\mathfrak{g}$ is closed under $\theta$.  We have $\mathfrak{k} = \mathfrak{g} \cap \mathfrak{u}(n) = \{x \in \mathfrak{g} : \theta(x) = x \}$.  Setting $\mathfrak{p} := \{x \in \mathfrak{g} : \theta(x) = - x\}$, we have $\mathfrak{p} = i \mathfrak{k}$ and $\mathfrak{g} = \mathfrak{k} \oplus \mathfrak{p}$.
  \end{enumerate}
\end{theorem}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item By lemma \ref{lem:zcl-gp-alg}, $G$ is a complex algebraic group.  Since $\Theta$ fixes $\U(n)$ pointwise and $\U(n)$ contains $K$, we have in particular $\Theta(K) = K$.  It follows that the space $I(K)$ of polynomials $f \in \mathbb{C}[\GL_n(\mathbb{C})]$ that vanish on $K$ is closed under the map
    \begin{equation*}
      f \mapsto \overline{f \circ \Theta} : g \mapsto \overline{f(\Theta(g))} = \overline{f({}^t \overline{g}^{-1})},
    \end{equation*}
    hence that $G = V(I(K))$ is closed under $\Theta$.
  \item We observe first (as in the discussion of \S\ref{sec:peter-weyl-theorem}) that $f|_K$ belongs to $\mathcal{A}(K)$: indeed, $\mathbb{C}[G]$ is the union over $d \geq 0$ of the subspaces defined by polynomials of degree bounded by $d$, each of which is finite-dimensional and $K$-invariant.  We thus get a well-defined map of $\mathbb{C}$-algebras $\mathbb{C}[G] \rightarrow \mathcal{A}(K)$.  The injectivity of this map is clear from the definitions of $G$ and of $\mathbb{C}[G] = \mathbb{C}[\GL_n(\mathbb{C})]/I(K)$: if $f|_K = 0$, then $f \in I(K)$, i.e., $f = 0$ as an element of $\mathbb{C}[G]$.  We henceforth identify $\mathbb{C}[G]$ with its image in $\mathcal{A}(K)$.  It remains to show that in fact $\mathbb{C}[G] = \mathcal{A}(K)$.  To that end, observe first that $\mathbb{C}[G]$ is invariant under the action of $K \times K$ on $\mathcal{A}(K)$ by left and right translation (using here that matrix multiplication is described by polynomials and that $K$ is a subgroup of $G$).  Recall next that $\mathcal{A}(K)$ is an orthogonal direct sum of finite-dimensional subspaces $\mathcal{A}(\pi)$, taken over $\pi \in \Irr(K)$.  Note finally, thanks to the identity $g = \Theta(g)$ satisfied by every $g \in K$, that $\mathbb{C}[G]$ contains all polynomials in the real and imaginary parts of the entries of a matrix $g \in K$ together with its inverse.  It follows by Stone-Weierstrass (as in the discussion of \S\ref{sec:peter-weyl-theorem}) that $\mathbb{C}[G]$ is dense in $\mathcal{A}(K)$ with respect to the topology of $C(G)$.

    To complete the proof, we just need to check that any $K \times K$-invariant dense subspace of $\mathcal{A}(K)$ is actually equal to $\mathcal{A}(K)$.  Maybe there's a simpler way to see this, but let's see.  We claim that for any finite subset $\Pi$ of $\Irr(K)$ and any $K \times K$-invariant subspace $V$ of $\oplus_{\pi \in \Pi} \mathcal{A}(\pi)$, one has $V = \oplus_{\pi \in \Pi_0} \mathcal{A}(\pi)$ for some subset $\Pi_0$ of $\Pi$.  (To see that the claim suffices, note that for each $\pi \in \Irr(K)$, we may find $f \in \mathbb{C}[G]$ so that the component $f_\pi \in \mathcal{A}(\pi)$ of $f$ is nonzero, because otherwise $\mathbb{C}[G]$ would be contained in the proper closed subspace of $\mathcal{A}(K)$ given by the orthogonal complement of $\mathcal{A}(\pi)$; then take for $\Pi$ the set of all $\pi '$ for which $f_{\pi '} \neq 0$ and for $V$ the span of the $K \times K$-orbit of $f$.  It follows from the claim that then $\mathbb{C}[G]$ contains $\mathcal{A}(\pi)$.  Since $\pi$ was arbitrary, we conclude that $\mathbb{C}[G] = \mathcal{A}(K)$.)  The claim follows immediately from character theory for $K \times K$, which gives that $\chi_V = \sum_{\pi \in \Pi} n(\pi) \overline{\chi_\pi } \otimes \chi_\pi$ for some $n(\pi) \in \{0,1\}$; here $(\overline{\chi_\pi } \otimes \chi_\pi)(g_1,g_2) := \overline{\chi _\pi }(g_1) \chi_\pi(g_2)$.
  \item Set $M := G \cap \U(n)$, so that $K \leq M \leq G$.  Then $M$ is compact, and $G$ is also the Zariski closure of $M$, so by what we've already shown, the map $\mathbb{C}[G] \rightarrow \mathcal{A}(M)$ is an isomorphism, hence in particular has dense image in $C(M)$.  It follows readily that $M = K$.  Indeed, suppose otherwise that $M$ is strictly larger than $K$.  We let $M$ act on $C(M)$ and $\mathcal{A}(M)$ by the right regular representation, and use superscripts as in $C(M)^K$ to denote the fixed subspace, as usual.  We argue as follows:
    \begin{itemize}
    \item \emph{$C(M)^K$ contains non-constant functions.}  Indeed, take any nonzero nonnegative $f \in C(M)$ that vanishes on $K$, and consider the function $M \ni x \mapsto \int_{k \in K} f(x k)$, where here and henceforth $\int$ denotes an integral with respect to the probability Haar.
    \item \emph{$\mathbb{C}[G]^K$ contains non-constant functions.}  We have seen that $\mathbb{C}[G]$ maps isomorphically to $\mathcal{A}(M)$, hence has dense image in $C(M)$.  Take $f \in C(M)^K$ non-constant, thus $f(x_1) \neq f(x_2)$ for some $x_1, x_2 \in M$.  Choose $\eps > 0$ so that $|f(x_1) - f(x_2)| \geq 3 \eps$.  Choose $f' \in \mathbb{C}[G]$ so that $\|f - f'\|_{\infty} \leq \eps$.  Set $f''(x) := \int_{k \in K} f'(x k)$.  Then $f'' \in \mathbb{C}[G]^K$.  By the right $K$-invariance of $f$, we have $(f - f'')(x) = \int_{k \in K} (f - f')(x k)$, hence by the triangle inequality we have $\|f - f''\|_{\infty} \leq \|f - f' \|_{\infty} \leq \eps$.  By another application of the triangle inequality, we deduce that $|f''(x_1) - f''(x_2)| \geq \eps$.  Thus $f''$ is non-constant.
    \item \emph{We obtain a contradiction.}  We have seen that $\mathbb{C}[G]$ injects into $\mathcal{A}(K)$, hence likewise $\mathbb{C}[G]^K$ into $\mathcal{A}(K)^K$, but every element of $\mathcal{A}(K)^K$ is manifestly constant.
    \end{itemize}
    % then we could find $f$ in $\mathbb{C}[G]$ vanishing on $K$ but not on $M$, contrary to the injectivity of the map $f \mapsto f|_K$ established earlier.
  \item If $M$ is a larger compact subgroup, then we may assume (after conjugating) that it is contained in $\U(n)$.  Then both $M$ and $K$ are contained in $\U(n)$ and have $G$ as their Zariski closure, so by (iii), we have $K = G \cap \U(n) = M$.
  \item The identity $\mathfrak{k} = \mathfrak{g} \cap \mathfrak{u}(n)$ follows from part (iii).  Since $\theta^2 = 1$, we can decompose $\mathfrak{g}$ into its $\pm 1$ eigenspaces for $\theta$, i.e., $\mathfrak{g} = \mathfrak{k} \oplus \mathfrak{p}$; explicitly, $x = x^+ + x^-$, where $x^+ \in \mathfrak{k}, x^- \in \mathfrak{p}$ are given by $x^{\pm} := (x \pm \theta (x))/2$.  Since $\theta$ is anti-linear, multiplication by $i$ induces an isomorphism $x^+ \cong x^-$, i.e., $\mathfrak{p} = i \mathfrak{k}$.
  \end{enumerate}
\end{proof}

\begin{remark}\label{rmk:theta-vs-sigma}
  It wasn't essential to refer to the group $\U(n)$ here.  The definition of $G$ doesn't involve it.  The involution $f \mapsto \overline{f \circ \Theta}$ on $\mathbb{C}[G]$ corresponds under the isomorphism $\mathbb{C}[G] \cong \mathcal{A}(K)$ to the involution $\sigma$ on $\mathcal{A}(K)$ given by $\sigma(f)(k) = \overline{f(k)}$ for all $k \in K$.
\end{remark}


\subsection{A glimpse of Tannakian duality}
Here we address the question: how can we describe a compact Lie group $K$ in terms of its coefficient ring $\mathcal{A}(K)$?  We warm-up with some motivating analogues.
\begin{itemize}
\item For an algebraic set $X \subseteq k^n$ (over an infinite field $k$) with coordinate ring $k[X] = k[x_1,\dotsc,x_n]/I(X)$, we have a bijection
  \begin{equation*}
    X \leftrightarrow \Hom_k(k[X],k)
  \end{equation*}
  \begin{equation*}
    p \mapsto [f \mapsto f(p)]
  \end{equation*}
  \begin{equation*}
    (\ell(x_1),\dotsc,\ell(x_n)) \mapsfrom \ell
  \end{equation*}
  between points of $X$ and functionals on the coordinate ring.  Moreover, we can reconstruct $X$ from the abstract $k$-algebra $k[X]$ by choosing a system of generators $x_1,\dotsc,x_n$.
\item For an algebraic group $G$ over $k$, we can recover $G$ as an algebraic set from $\Hom_k(k[G],k)$, as above, but we'd also like to keep track of the group law $G \times G \rightarrow G$.  It corresponds to the pullback map
  \begin{equation*}
\Delta : k[G] \rightarrow k[G \times G] \cong k[G] \otimes k[G],
\end{equation*}
  called \emph{comultiplication}, and characterized by the identity
  \begin{equation*}
    \Delta(f)(g_1,g_2) = f(g_1 g_2) \text{ for all } f \in k[G] \text{ and } g_1,g_2 \in G.
  \end{equation*}
\end{itemize}

Now let $K \leq \U(n)$ be a compact Lie group, with Zariski closure $G \leq \GL_n(\mathbb{C})$ as above.  Then
\begin{equation}\label{eq:G-via-tannaka}
  G \cong \Hom_{\mathbb{C}}(\mathbb{C}[G],\mathbb{C}) \cong \Hom_{\mathbb{C}}(\mathcal{A}(K),\mathbb{C}).
\end{equation}
The comultiplication $\Delta$ on $\mathbb{C}[G]$ corresponds under the isomorphism with $\mathcal{A}(K)$ to the map
\begin{equation*}
  \Delta : \mathcal{A}(K) \rightarrow \mathcal{A}(K) \otimes \mathcal{A}(K)
\end{equation*}
described explicitly in terms of matrix coefficients by the usual matrix multiplication rule
\begin{equation*}
  \Delta(\pi_{i j}) = \sum_{k} \pi_{i k} \otimes \pi_{k j}
\end{equation*}
and in basis-free manner as the map induced by the maps $\End(V)^* \rightarrow \End(V)^* \otimes \End(V)^*$ ($V$ a finite-dimensional representation of $G$) coming from the multiplication maps $\End(V) \otimes \End(V) \rightarrow \End(V)$.

These observations already show that $G$ is determined as an algebraic group by the pair $(\mathcal{A}(K),\Delta)$; in particular, $G$ is independent of the choice of linear embedding used in its construction.  We will henceforth refer to $G$ as the \emph{complexification} of $K$.

We can say something similar about $K$:

\begin{theorem}
  Let $K$ be a compact Lie group.  Then the map $k \mapsto [f \mapsto f(k)]$ defines a bijection
  \begin{equation}\label{eq:K-via-tannaka}
    K \leftrightarrow \{\ell \in
    \Hom_{\mathbb{C}}(\mathcal{A}(K),\mathbb{C})
    :
    \ell(\sigma(f)) = \overline{f}
    \text{ for all } f \in \mathcal{A}(K)
    \},
  \end{equation}
  where $\sigma : \mathcal{A}(K) \rightarrow \mathcal{A}(K)$ is given by complex conjugation.  The group law on $K$ is described by $\Delta$, i.e., for all $f \in \mathcal{A}(K)$ and $k_1,k_2 \in K$, we have $f(k_1 k_2) = \Delta(f) (k_1,k_2)$.
\end{theorem}
Informally, ``$(\mathcal{A}(K),\Delta)$ determines $K$.''
\begin{proof}
  Let $G$ be as constructed above, and let $\ell$ be an element of the RHS of \eqref{eq:K-via-tannaka}.  By \eqref{eq:G-via-tannaka}, we may write $\ell(f) = f(g)$ for some $g \in G$.  By Remark \ref{rmk:theta-vs-sigma}, we have $\overline{f(\Theta(g))} = f(g)$ for all $f \in \mathbb{C}[G]$, hence $\Theta(g) = g$.  By Theorem \ref{thm:complexification-compact-lie-basic-properties}, we conclude that $g \in K$.
\end{proof}

There's much more to say about this topic than we will here.  Some keywords: Tannaka--Krein duality, Hopf algebras.

\subsection{Cartan decomposition}\label{sec:cartan-decomposition}
Set
\begin{equation*}
  P(n) := \{g \in \GL_n(\mathbb{C}) : p^* = p, \text{ positive-definite} \},
\end{equation*}
\begin{equation*}
  \mathfrak{p}(n) := \{x \in \glLie_n(\mathbb{C}) : x^* = x \}.
\end{equation*}
Then the map $\exp : \mathfrak{p}(n) \rightarrow P(n)$ is a bijection; indeed, by the spectral theorem for hermitian matrices, we have for each $x \in \mathfrak{p}(n)$ and $p \in P(n)$ that
\begin{equation*}
  x \sim
  \begin{pmatrix}
    x_1 &  &  \\
    & \dotsb  &  \\
    & & x_n
  \end{pmatrix}
, \quad p \sim
  \begin{pmatrix}
    p_1 &  &  \\
    & \dotsb  &  \\
    & & p_n
  \end{pmatrix}
\end{equation*}
for some $x_i \in \mathbb{R}, p_j \in \mathbb{R}_{>0}$, where $\sim$ denotes conjugacy.  We deduce readily that $\exp : \mathfrak{p}(n) \rightarrow P(n)$ and $\log : P(n) \rightarrow \mathfrak{p}(n)$ (with the latter defined by the ``functional calculus,'' i.e., by taking the logarithm of each diagonal entry) define mutually inverse bijections.  Note that for $p \in P(n)$, we can define $p^t$ for any complex number $t$, either via the logarithm as $\exp(t \log p)$ or directly via the functional calculus.

The following might be known from linear algebra:
\begin{lemma}
[Polar decomposition]
  The map $\U(n) \times P(n) \rightarrow \GL_n(\mathbb{C})$ given by $(k,p) \mapsto k p$ is bijective.
\end{lemma}
\begin{proof}
  Injectivity: if $g = k p$, then $g^* g = p^* k^* k p = p^2$, so $p = \sqrt{p^2} := \exp(\tfrac{1}{2} \log p)$ and thus also $k = g p^{-1}$ are determined by $g$.  Surjectivity: observe that $g^* g \in P(n)$ and define $p := \sqrt{g^* g} \in P(n)$, $k := g p^{-1} \in \U(n)$.
\end{proof}
\begin{lemma}\label{lem:polar-decomp}
  The map
  \begin{equation*}
    \U(n) \times \mathfrak{p}(n) \rightarrow \GL_n(\mathbb{C})
  \end{equation*}
  \begin{equation*}
    (k,x) \mapsto k \exp(x)
  \end{equation*}
  is a diffeomorphism.
\end{lemma}
\begin{proof}
  We have already checked the bijectivity.  The forward map is smooth.  We need to know that the inverse is smooth.  This boils down to the fact that $\exp : \mathfrak{p}(n) \rightarrow P(n)$ is submersive, which is a consequence of the following lemma and the fact that for $x \in \mathfrak{p}(n)$, the eigenvalues of $\ad_x$ are real.
\end{proof}

The following is part of the BCHD law, and describes the derivative of the exponential map on any Lie group.
\begin{lemma}\label{lem:deriv-of-expo}
  Let $G$ be any Lie group, Fix $x \in \mathfrak{g}$.  Let $z \in \mathfrak{g}$ be small enough.  Let $y \in \mathfrak{g}$ be the small element for which
  \begin{equation}\label{eq:}
    \exp(x + z) = \exp(x) \exp(y).
  \end{equation}
  Then
  \begin{equation}\label{eq:}
    y = F(\ad_x) z + \O(|z|^2),
  \end{equation}
  where
  \begin{equation}\label{eq:}
    F(t) := \frac{1 - e^{-t }}{t}
    = 1 - \frac{t}{2!}
    + \frac{t^2}{3!} - \dotsb.
  \end{equation}
  In particular, $\exp$ is a local diffeomorphism at $x$ iff $F(\ad_x)$ is invertible iff $\ad_x$ has no eigenvalues of the form $2 \pi i k$, with $0 \neq k \in \mathbb{Z}$.
\end{lemma}
\begin{proof}
  (Omitted from lecture.)  For $t \in [0,1]$, set $f(t) := \log (\exp(-t x) \exp(t (x+z)))$, so that $f(0) = 0$ and $f(1) = y$.  We compute that for small $\eps > 0$,
  \begin{equation}\label{eq:}
    f(t + \eps)
    = \Ad_{-t x} (\eps z) + t z + \O(\eps |z|^2 + \eps^2),
  \end{equation}
  where $\Ad_x := e^{\ad_x} = \Ad(e^x)$.  (To see this, first write $\exp(-(t + \eps) x) \exp((t+\eps)(x+z)) = \exp(-t x) \exp(-\eps x) \exp(\eps(x+z)) \exp(t(x+z))$, then approximate $\exp(-\eps x) \exp(\eps(x+z)) = \exp(\eps z + \O(\eps^2))$ and deduce that $f(t+\eps) = \log(\exp(\Ad_{-t x}(\eps z) + \O(\eps^2)) \exp(\O(z))) = \Ad_{-t x}(\eps z) + \O(\eps |z|^2 + \eps^2)$, as required.)  Thus
  \begin{equation*}
    f'(t) = \Ad_{-t x}(z) + \O(|z|^2)
  \end{equation*}
  and so
  \begin{equation*}
    y = \int_{t = 0}^{1} e^{-t \ad_x} z \, d t + \O(|z|^2),
  \end{equation*}
  which leads to the required conclusion by the formula $\int_{t=0}^1 e^{-t u} \, d t = F(u)$.
\end{proof}

\begin{theorem}
  Let $K$ be a compact Lie group, with complexification $G$.  Write $\mathfrak{g} = \mathfrak{k} \oplus \mathfrak{p}$ as in Theorem \ref{thm:complexification-compact-lie-basic-properties}.  Then the map
  \begin{equation*}
    K \times \mathfrak{p} \rightarrow G
  \end{equation*}
  \begin{equation*}
    (k,x) \mapsto k \exp(x)
  \end{equation*}
  is a diffeomorphism.  In particular,
  \begin{itemize}
  \item $K$ is a deformation retract of $G$,
  \item $K$ meets every connected component of $G$,
  \item $K$ is connected if and only if $G$ is connected,
  \end{itemize}
  and so on.
\end{theorem}
\begin{proof}
  In view of Lemma \ref{lem:polar-decomp}, we just need to check that if $g \in G$ is written $g = k p$ with $k \in \U(n)$ and $p \in P(n)$, then in fact $p \in P := \exp(\mathfrak{p})$; it follows then $p \in G$, hence that $k = g p^{-1} \in G \cap \U(n) = K$.  It will suffice to show that $p^t \in G$ for all $t \in \mathbb{R}$, because then $\partial_{t=0} p^t$ belongs to $\mathfrak{g} \cap \mathfrak{p}(n) = \mathfrak{p}$.  Since $G = V(I(G))$, it will suffice to show for each $f \in I(G)$ that $f(p^t) = 0$ for all $t \in \mathbb{R}$.  Since $p^2 = g^* g$ and $G$ is a group, we know that $f(p^{t}) = 0$ for all $t \in 2 \mathbb{Z}$.  Let $e^{x_1},\dotsc,e^{x_n}$ denote the diagonal entries of $p$ with respect to some basis.  Then $f(p^t)$ is a polynomial in the quantities $e^{\pm t x_1}, \dotsc, e^{\pm t x_n}$.  Collecting common exponents, we may write
  \begin{equation*}
    f(p^t) = \sum_{\gamma \in \Gamma} c_\gamma e^{\gamma t}
  \end{equation*}
  for some finite subset $\Gamma$ of $\mathbb{R}$ and some nonzero complex coefficients $c_\gamma$.  Suppose for the sake of contradiction that $f(p^t)$ is nonzero.  Then $\Gamma$ is nonempty.  Let $\gamma \in \Gamma$ be the largest element.  Then $\lim_{t \rightarrow \infty} e^{- \gamma t} f(p^t) = c_\gamma \neq 0$.  But $f(p^t) = 0$ for all $t \in 2 \mathbb{Z}$, giving the required contradiction.
\end{proof}

\subsection{Algebraic representations}
\begin{definition}
  A \emph{morphism} of algebraic groups is a group homomorphism that is also a morphism of the underlying algebraic sets, i.e., a map described in coordinates by polynomials.
  
  Let $G$ be an algebraic group over an infinite field $k$.  An \emph{algebraic representation} of $G$ is a finite-dimensional $k$-vector space $V$ equipped with a morphism of algebraic groups $\pi : G \rightarrow \GL(V)$.
\end{definition}

\begin{theorem}\label{thm:alg-rep}
  Let $K$ be a compact Lie group.  Let $G$ denote its complexification.  Then algebraic representations $(\sigma,W)$ of $G$ are in natural bijection with finite-dimensional representations $(\pi,V)$ of $K$.  The bijection sends $V$ to $W$ and $W$ to $V$.  To get from $\sigma$ to $\pi$, one restricts.  To get from $\pi$ to $\sigma$, one writes $\pi$ in matrix form as $(\pi_{i j})$, with $\pi_{i j} \in \mathcal{A}(K)$; one then uses the isomorphism $\mathcal{A}(K) \cong \mathbb{C}[G]$ to identify each $\pi_{i j}$ with a regular function on $G$, and takes $\sigma = (\pi_{i j})$ the algebraic representation whose matrix entries are given by those regular functions.
\end{theorem}
\begin{proof}
  The reader is encouraged to check that this follows readily from Theorem \ref{thm:complexification-compact-lie-basic-properties}.
\end{proof}

\footnote{End of lecture \#10, Tuesday, 26 March}

\subsection{Reductive groups}
Which groups $G$ arise as in Theorem \ref{thm:complexification-compact-lie-basic-properties}, i.e., as the Zariski closure of a compact subgroup of $\GL_n(\mathbb{C})$?  Recall that an element $g \in \GL_n(\mathbb{C})$ is \emph{unipotent} if $(g - 1)^n = 0$.  We say that a subgroup of $\GL_n(\mathbb{C})$ is \emph{unipotent} if each of its elements is unipotent.

\begin{definition}
  A complex algebraic group $G \leq \GL_n(\mathbb{C})$ is called \emph{reductive} if it contains no nontrivial (i.e., other than $\{1\}$) normal unipotent subgroups.
\end{definition}

For instance, the group $
\begin{pmatrix}
  1 & \mathbb{C}  \\
  & 1
\end{pmatrix}
$ is not reductive, because the whole group is unipotent.

\begin{theorem}\label{thm:reductive-groups}
  Let $G \leq \GL_n(\mathbb{C})$ be a complex algebraic group.  The following are equivalent:
  \begin{enumerate}
[(i)]
  \item $G$ is reductive.
  \item $G$ has a Zariski dense compact subgroup $K$.
  \item $G$ is conjugate to a group that is closed under $\Theta : g \mapsto {}^t \overline{g} ^{-1}$.
  \item Every algebraic representation $V$ of $G$ is completely reducible, i.e., decomposes as a direct sum $\oplus W_i$ of invariant irreducible subspaces.
  \end{enumerate}
\end{theorem}
\begin{proof}
  The proof that (i) implies (iii) implies (ii) is lengthy, and will not be discussed in detail.  The basic idea is to complete most of the classifications of reductive algebraic groups and of compact Lie groups, and to observe that the same objects (root data) arise in both classifications.  We will soon discuss these ideas for compact Lie groups; a parallel discussion applies in the algebraic setting.

  That (ii) implies (iii) follows from Theorem \ref{thm:complexification-compact-lie-basic-properties}.

  Let's assume (ii) and deduce (iv).  We have seen (Theorem \ref{thm:complete-reducibility-compact-group}) that $V$ decomposes as a direct sum $\oplus W_i$ of $K$-invariant $K$-irreducible subspaces.  We claim that if a subspace $W \subseteq V$ of $V$ is $K$-invariant, then it is likewise $G$-invariant.  It's obvious that any $K$-irreducible $G$-invariant subspace is likewise $G$-irreducible, so the claim suffices.  To verify the claim, the point is that leaving a subspace invariant is an algebraic condition, which thus extends Zariski-continuously from $K$ to $G$.  In more detail, consider the set $X := \{g \in \GL(V) : g W \subseteq W\}$.  Then $X$ is algebraic; for instance, if we extend a basis $e_1,\dotsc,e_m$ for $W$ to a basis $e_1,\dotsc,e_n$ for $V$ and denote by $e_1^*,\dotsc,e_n^*$ the corresponding dual basis, then $X = \{g : e_j^*(g e_i) = 0 \text{ for } i=1..m, j=m+1..n\}$.  Let $\pi : G \rightarrow \GL(V)$ denote the action map.  Then $\pi^{-1}(X)$ is algebraic.  Since $W$ is $K$-invariant, we have $K \subseteq \pi^{-1}(X)$.  Since $K$ is Zariski dense in $G$, it follows that $G \subseteq \pi^{-1}(X)$, hence that $X$ is $G$-invariant, as required.

  Let's finally assume (iv) and deduce (i).  Let $N$ be a normal unipotent subgroup.  We must show that $N = \{1\}$.  Let $V = \mathbb{C}^n$ denote the standard representation of $G$, thus $G \subseteq \GL(V)$.  By assumption, $V = \oplus W_i$ with each $W_i$ an irreducible subrepresentation.  By Engel's theorem (or perhaps a variant), the fixed space $W_i^N = \{w \in W_i : n w_i = w_i \text{ for all } n \in N\}$ is nontrivial.  Since $N$ is normal, $W_i^N$ is $G$-invariant.  (Indeed, if $v \in W_i^N$, $g \in G$ and $n \in N$, then $n' := g^{-1} n g \in N$, so $n g v = g n' v = g v$, hence $g v \in W_i^N$.)  Since $W_i$ is irreducible, it follows that $W_i = W_i^N$, hence that $V = V^N$.  This says that every element $n$ of $N$ fixes every element of $V$, i.e., that $n = 1$, i.e., that $N = \{1\}$, as required.
\end{proof}



\subsection{Unitary trick}
We record a further variant of the above considerations.  We focus on an example; what's relevant here is that $\SL_n(\mathbb{C})$ and $\SU(n)$ are connected and simply-connected.  Note that by the Cartan decomposition (\S\ref{sec:cartan-decomposition}), knowing either of these properties for one of these groups implies the same property for the other group.
\begin{theorem}\label{thm:unitary-trick}
  Let $V$ be a finite-dimensional complex vector space.  The following are all in natural bijection:
  \begin{enumerate}
  \item Algebraic representations $\SL_n(\mathbb{C}) \rightarrow \GL(V)$
  \item Holomorphic representations $\SL_n(\mathbb{C}) \rightarrow \GL(V)$ (i.e., morphisms of complex Lie groups)
  \item Holomorphic representations $\slLie_n(\mathbb{C}) \rightarrow \glLie(V)$ (i.e., morphisms of complex Lie algebras)
  \item Representations $\SU(n) \rightarrow \GL(V)$
  \item Representations $\su(n) \rightarrow \GL(V)$
  \item Representations $\SL_n(\mathbb{R}) \rightarrow \GL(V)$
  \item Representations $\slLie_n(\mathbb{R}) \rightarrow \GL(V)$
  \end{enumerate}
  These bijections are compatible with irreducibility and direct sum decompositions.  Any such representation is completely reducible, i.e., decomposes as a direct sum of irreducible representations.
\end{theorem}
\begin{proof}
  We go from (1) to (2) using that polynomials are holomorphic.  We'll leave it as a homework problem to show that any holomorphic representation of a complex reductive group is automatically polynomial, giving the arrow from (2) to (1).  (2) and (3) are equivalent by Lie theory and the fact that $\SL_n(\mathbb{C})$ is connected and simply-connected.  (4) and (5) are likewise equivalent thanks to the analogous properties for $\SU(n)$.  (3), (5) and (7) are all equivalent thanks to the identities
  \begin{equation}\label{eq:}
    \su(n) \oplus i \su(n) = \slLie_n(\mathbb{C})
    = \slLie_n(\mathbb{R}) \oplus i \slLie_n(\mathbb{R})
  \end{equation}
  and the fact that an $\mathbb{R}$-linear map $W \rightarrow U$ from a real vector space $W$ to a complex vector space $U$ extends uniquely to a $\mathbb{C}$-linear map $W_{\mathbb{C}} \rightarrow U$ from the complexification $W_{\mathbb{C}} = W \otimes_{\mathbb{R}} \mathbb{C} = W \oplus i W$.  We go from (2) to (6) by restricting and from (6) to (7) by differentiating.  That these bijections are compatible with irreducibility and decompositions is either clear or follows as in the proof of Theorem \ref{thm:reductive-groups}.  We've seen that complete reducibility holds for representations of $\SU(n)$, so the same follows for each of the other classes of representations.
\end{proof}

We've already (\S\ref{sec:some-groups-closely}) classified the (irreducible) finite-dimensional representations of $\SU(n)$, so the stated equivalence tells us that we've implicitly classified each of the other six classes of representations.

We note that Theorems \ref{thm:alg-rep} and \ref{thm:unitary-trick} are mostly independent.

We record the homework problem promised above as a theorem:
\begin{theorem}
  Let $G$ be a reductive complex algebraic group and $V$ a finite-dimensional complex vector space.  Then any holomorphic representation $\pi : G \rightarrow \GL(V)$ is algebraic.
\end{theorem}
Note that by contrast, non-reductive groups can have holomorphic non-algebraic representations, e.g.,
\begin{equation*}
  \begin{pmatrix}
    1 & \mathbb{C}  \\
    & 1
  \end{pmatrix}
  \ni
  \begin{pmatrix}
    1 & z \\
    & 1
  \end{pmatrix}
  \mapsto
  \begin{pmatrix}
    e^z &  \\
    & 1
  \end{pmatrix}
  \in \GL_2(\mathbb{C}).
\end{equation*}

We've only scratched the tip of the iceberg here; for further reading I recommend the book by Onishchik--Vinberg.

\newpage


\section{Structure of compact Lie groups}
We aim next to show that any compact Lie group $K$ has structure similar to that observed in \S\ref{sec:char-theory-comp} for the unitary groups.  We'll use this to classify such $K$ together with their representations.

\subsection{Notation related to a torus}\label{sec:notat-relat-torus}
Let $T$ be an $n$-dimensional torus.  Recall from \S\ref{sec:char-theory-comp} that this means that $T$ is a connected compact abelian Lie group of dimension $n$ and that any such $T$ is isomorphic to $\U(1)^n$.  We may fix such an isomorphism and regard $T$ as the diagonal subgroup of $\U(n)$.  In \S\ref{sec:char-theory-comp}, we worked with the explicit coordinates defined by this embedding.  It'll be useful now to work in a more basis-free manner.  This requires setting up a bit of notation.


The \emph{character group} of $T$ is
\begin{equation*}
  X(T) := \Hom(T,\U(1)),
\end{equation*}
while the \emph{cocharacter group} is
\begin{equation*}
  X^\vee (T) := \Hom(\U(1),T).
\end{equation*}
We view these as additive groups.  They are free $\mathbb{Z}$-modules of rank $n$.  We can identify $X(T)$ and with $\mathbb{Z}^n$ by associating to $\lambda, \gamma \in \mathbb{Z}^n$ the character
\begin{equation*}
  T \ni t \mapsto t^{\lambda} := t_1^{\lambda_1} \dotsb t_n^{\lambda_n}
\end{equation*}
and the cocharacter
\begin{equation*}
  \U(1) \ni z \mapsto z^{\gamma} := \diag(z^{\gamma_1},\dotsc,z^{\gamma_n}).
\end{equation*}

There is a natural pairing
\begin{equation*}
  \langle , \rangle : X(T) \otimes_{\mathbb{Z}} X^\vee(T) \rightarrow \mathbb{Z}
\end{equation*}
given by observing that the composition $\U(1) \rightarrow T \rightarrow \U(1)$ is a morphism $\U(1) \rightarrow \U(1)$, and that any such morphism is of the form $z \mapsto z^k$ for some integer $k$ (e.g., by \S\ref{sec:conn-comp-abel}).  For $\lambda, \gamma \in \mathbb{Z}^n$, we have $(z^\gamma)^{\lambda} = z^{\gamma_1 \lambda_1 + \dotsb + \gamma_n \lambda_n}$, so $\langle \lambda, \gamma \rangle = \gamma_1 \lambda_1 + \dotsb + \gamma_n \lambda_n$.

We denote by $\mathfrak{t}$ the Lie algebra of $T$ and by $\mathfrak{t}_{\mathbb{C}} = \mathfrak{t}\otimes_{\mathbb{R}} \mathbb{C} = \mathfrak{t} \oplus i \mathfrak{t}$ its complexification.  We may identify
\begin{equation*}
  \mathfrak{t}_{\mathbb{C}} =
  \begin{pmatrix}
    \mathbb{C}  &  &  \\
    & \ddots  &  \\
    & & \mathbb{C}
  \end{pmatrix}
.
\end{equation*}
We (somewhat awkwardly) set $\mathfrak{t}_{\mathbb{R}} := i \mathfrak{t}$, so that
\begin{equation}\label{eq:describe-t-R}
  \mathfrak{t}_{\mathbb{R}} =
  \begin{pmatrix}
    \mathbb{R}  &  &  \\
    & \ddots  &  \\
    & & \mathbb{R}
  \end{pmatrix}
.
\end{equation}
We obtain a surjective covering map
\begin{equation*}
  \mathfrak{t}_{\mathbb{R}} \rightarrow T
\end{equation*}
\begin{equation*}
  x \mapsto \exp(i x).
\end{equation*}

We write $\mathfrak{t}_{\mathbb{C}}^* := \Hom_{\mathbb{C}}(\mathfrak{t}_{\mathbb{C}},\mathbb{C}) \cong \mathbb{C}^n$ and $\mathfrak{t}_{\mathbb{R}}^* := \Hom_{\mathbb{R}}(\mathfrak{t}_{\mathbb{R}},\mathbb{R}) \cong \mathbb{R}^n$.  We may identify $\mathfrak{t}_{\mathbb{R}}^*$ with the subspace of $\mathfrak{t}_{\mathbb{C}}^*$ mapping $\mathfrak{t}_{\mathbb{R}}$ to $\mathbb{R}$.  The identification of $\mathfrak{t}_{\mathbb{R}}^*$ with $\mathbb{R}^n$ is given by $\lambda = (\lambda_1,\dotsc,\lambda_n)$ if $\lambda(x) = \lambda_1 x_1 + \dotsb + \lambda_n x_n$ for $x = \diag(x_1,\dotsc,x_n) \in \mathfrak{t}_{\mathbb{R}}$.

Each $\lambda \in X(T)$ identifies with an element $\lambda \in \mathfrak{t}_{\mathbb{R}}^*$; this identification is determined by requiring that for every $x \in \mathfrak{t}_{\mathbb{R}}$ (so that $\exp(i x) \in T$), we have
\begin{equation*}
  (\exp(i x))^{\lambda} = e^{i \lambda(x)}.
\end{equation*}
We write $\mathfrak{t}_{\mathbb{Z}}^* \subseteq \mathfrak{t}_{\mathbb{R}}^*$ for the image of $X(T)$ under this identification.  The identification above of $\mathfrak{t}_{\mathbb{R}}^*$ with $\mathbb{R}^n$ then carries $\mathfrak{t}_{\mathbb{Z}}^*$ to $\mathbb{Z}^n$.

Each $\gamma \in X^\vee(T)$ identifies with an element $\gamma \in \mathfrak{t}_{\mathbb{R}}$, characterized as follows: for $\theta \in \mathbb{R}$ (so that $\exp(i \theta) \in \U(1)$),
\begin{equation*}
  (\exp(i \theta))^{\gamma} = \exp(i \theta \gamma).
\end{equation*}
We denote by $\mathfrak{t}_{\mathbb{Z}} \subseteq \mathfrak{t}_{\mathbb{R}}$ the image of $X^\vee(T)$ under this identification.  Then under \eqref{eq:describe-t-R}, we have
\begin{equation}\label{eq:describe-t-Z}
  \mathfrak{t}_{\mathbb{Z}} =
  \begin{pmatrix}
    \mathbb{Z}  &  &  \\
    & \ddots  &  \\
    & & \mathbb{Z}
  \end{pmatrix}
.
\end{equation}

We note that $X(T)$ and $X^\vee(T)$ identify respectively with $\mathfrak{t}_{\mathbb{Z}}$ and $\mathfrak{t}_{\mathbb{Z}}^*$, and the pairing $X(T) \otimes X^\vee(T) \rightarrow \mathbb{Z}$ (tensor product over $\mathbb{Z}$) discussed above corresponds to the natural pairing $\langle , \rangle : \mathfrak{t}_{\mathbb{Z}} \otimes \mathfrak{t}_{\mathbb{Z}}^* \rightarrow \mathbb{Z}$ induced by the canonical duality between $\mathfrak{t}_{\mathbb{R}}$ with $\mathfrak{t}_{\mathbb{R}}^*$.  In coordinates, $\langle \lambda, \gamma \rangle = \lambda_1 \gamma_1 + \dotsb + \lambda_n \gamma_n$, as before.

We should note that $\mathfrak{t}_{\mathbb{Z}}$ and $\mathfrak{t}_{\mathbb{Z}}^*$ depend upon $T$, not just upon $\mathfrak{t}$.

Recall (\S\ref{sec:conn-comp-abel}) that every finite-dimensional irreducible representation of $T$ is one-dimensional and of the form $t \mapsto t^{\lambda}$ for some $\lambda \in \mathfrak{t}_{\mathbb{Z}}^*$.  Given a finite-dimensional representation $V$ of $T$, recall (\S\ref{sec:weight-decmop-U-n}) that we may decompose
\begin{equation*}
  V = \oplus_{\lambda \in \mathfrak{t}_{\mathbb{Z}}^*} V^{\lambda},
\end{equation*}
where $V^{\lambda} = \{v \in V : t v = t^{\Lambda} v \text{ for all } t \in T\}$ is the weight space with weight $\lambda$.

\subsection{Maximal tori: definition and existence}\label{sec:maxim-tori:-defin}
Let $K$ be a compact Lie group.  By a \emph{torus} $T \leq K$ we mean a closed subgroup that is a torus.  Note that if $T$ is an abstract torus and $j : T \hookrightarrow K$ is a injective immersive Lie group morphism (i.e., a ``virtual Lie subgroup''), then $j(T)$ is the image under a continuous map of a compact set, hence $j(T)$ compact, hence (since $K$ is Hausdorff) $j(T)$ is closed, and so $j(T)$ is a torus in $G$.

\begin{definition}
  A \emph{maximal torus} $T$ of $K$ is a torus $T \leq K$ that is not contained in any strictly larger torus in $K$.
\end{definition}

\begin{lemma}\label{lem:maximal-tori-vs-maxl-ab-subalg}
  Let $K$ be a compact Lie group.
  \begin{enumerate}
[(i)]
  \item A torus $T \leq K$ is maximal if and only if $\mathfrak{t}$ is a maximal abelian subalgebra of $\mathfrak{k}$.
  \item Maximal tori exist in $K$.
  \item If $\dim(K) > 0$, then nontrivial tori exist in $K$. \label{item:existence-nontrivial-tori}
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item We will verify the equivalence in contrapositive form.
    
    If $S$ is a torus in $K$ that strictly contains $T$, then (by the Lie correspondence) $\mathfrak{s}$ is an abelian subalgebra of $\mathfrak{k}$ that strictly contains $\mathfrak{t}$.

    Conversely, suppose $\mathfrak{s}$ is an abelian subalgebra of $\mathfrak{k}$ that strictly contains $\mathfrak{t}$.  Let $x \in \mathfrak{s} - \mathfrak{t}$.  Then $\exp(\mathbb{R} x)$ commutes with $T$, so $\exp(\mathbb{R} x) T$ is an abelian subgroup of $G$.  Its closure $\overline{\exp(\mathbb{R} x) T}$ is a closed abelian subgroup of $G$, hence compact.  The connected component $T' := (\overline{\exp(\mathbb{R} x) T})^0$ of that closure is then connected, compact and abelian, hence is a torus.  Since $x \in \Lie(T') - \mathfrak{t}$, we see that $T'$ strictly contains $T$.
  \item Use that the dimensions of the (abelian) subalgebras of $\mathfrak{k}$ are bounded from above.
  \item Consider $(\overline{\exp(\mathbb{R} x)})^0$ for any $0 \neq x \in \mathfrak{k}$.
  \end{enumerate}
\end{proof}

\subsection{Roots and root space decomposition}
Let $K$ be a compact Lie group with maximal torus $T$.  Let $G$ denote the complexification of $K$, so that $\mathfrak{g}$ is the complexification of $\mathfrak{k}$.  We then have the (complexified) adjoint representation
\begin{equation*}
  \Ad : K \rightarrow \GL(\mathfrak{g}),
\end{equation*}
which we may restrict to $T$ and decompose:
\begin{equation}\label{eq:decmopose-Ad-g-into-weightspaces}
  \mathfrak{g} =
  \oplus_{\lambda \in \mathfrak{t}_{\mathbb{Z}}^*}
  \mathfrak{g}^{\lambda}.
\end{equation}

\begin{definition}
  A \emph{root} for $(K,T)$ is a nonzero element $\alpha$ of $\mathfrak{t}_{\mathbb{Z}}^*$ for which $\mathfrak{g}^{\alpha}$ is nonzero.  We denote by $\Phi := \Phi(K:T)$ the set of roots, and refer to $\mathfrak{g}^{\alpha}$ as the \emph{root space} attached to the root $\alpha$.
\end{definition}

For instance, suppose that $K = \U(n)$, with $T = \U(1)^n \hookrightarrow K$ the diagonal subgroup.  We've seen (\S\ref{sec:weight-decmop-U-n}) that $\Phi$ consists of the set of differences $\eps_i - \eps_j$, where $i,j$ are distinct elements of $\{1..n\}$ and $\eps_i$ denote the standard basis elements of $\mathfrak{t}_{\mathbb{Z}}^*$.  The root space $\mathfrak{g}^{\eps_i-\eps_j}$ is spanned by the elementary matrix $E_{i j}$, for which $\Ad(t) E_{i j} = (t_i / t_j) E_{i j} = t^{\eps_i - \eps_j} E_{i j}$.  For instance, if $n = 3$, then
\begin{equation*}
  \Ad 
\begin{pmatrix}
    t_1 &  &  \\
    & t_2 &  \\
    & & t_3
  \end{pmatrix}
  \begin{pmatrix}
    a_{11} & a _{12} &  a _{13} \\
    a _{21} & a _{22} & a _{23} \\
    a _{31} & a _{32} & a _{33}
  \end{pmatrix}
  =
  \begin{pmatrix}
    a_{11} & (t_1/t_2) a _{12} &  (t_1/t_3)a _{13} \\
    (t_2/t_1) a _{21} & a _{22} & (t_2/t_3) a _{23} \\
    (t_3/t_1) a _{31} & (t_3/t_2) a _{32} & a _{33}
  \end{pmatrix}
.
\end{equation*}
We might observe in this case that $\mathfrak{g} = \glLie_n(\mathbb{C})$ is the direct sum of the diagonal subspace $\mathfrak{t}_{\mathbb{C}}$ and the root spaces, each of which is one-dimensional.  We'll see eventually that these features are general, i.e., hold for any compact Lie group.  For starters:


\begin{lemma}
  Let $K$ be a compact Lie group with maximal torus $T$ and $\mathfrak{g}$ as usual.  Then
  \begin{equation}\label{eq:root-space-decomp}
    \mathfrak{g} = \mathfrak{t}_{\mathbb{C}} \oplus (\oplus_{\alpha \in \Phi} \mathfrak{g}^{\alpha}).
  \end{equation}
\end{lemma}
\begin{proof}
  By \eqref{eq:decmopose-Ad-g-into-weightspaces}, it suffices to show that $\mathfrak{g}^0 = \{x \in \mathfrak{g} : \Ad(t) x = x \text{ for all } t \in T\}$ is equal to $\mathfrak{t}_{\mathbb{C}}$.  We note that $\mathfrak{g}^0$ is closed under complex conjugation, hence $\mathfrak{g}^0 = \mathfrak{k}^0 \oplus i \mathfrak{k}^0$ with $\mathfrak{k}^0 := \{x \in \mathfrak{k} : \Ad(t) x = x \text{ for all } t \in T\}$; indeed if $x \in \mathfrak{g}$, then we may write $x = x_1 + i x_2$ with $x_1,x_2 \in \mathfrak{k}$, while for $t \in T$ we have $\Ad(t) x = \Ad(t) x_1 + i \Ad(t) x_2$, hence $x \in \mathfrak{g}^0$ iff $x_1,x_2 \in \mathfrak{k}^0$, which leads to the required decomposition.  It is thus enough to show that $\mathfrak{k}^0 = \mathfrak{t}$.  Suppose otherwise that there exists $x \in \mathfrak{k}^0 - \mathfrak{t}$.  Then $\mathbb{R} x \oplus \mathfrak{t}$ is an abelian subalgebra of $\mathfrak{k}$ (note that $[x,\mathfrak{t}] = \{0\}$ because $\Ad(t) x = x$ for all $t \in T$) that strictly contains $\mathfrak{t}$.  Since $T$ is a maximal torus, we obtain a contradiction via Lemma \ref{lem:maximal-tori-vs-maxl-ab-subalg}.
\end{proof}

The root spaces of $\mathfrak{g}$ move the weight spaces of any other representation $V$ predictably, as we now explain.  Let $K$ be a compact Lie group with maximal torus $T$ and complexification $G$.  Let $\pi : K \rightarrow \GL(V)$ be a finite-dimensional representation of $K$.  It induces a representation $d \pi : \mathfrak{k} \rightarrow \End(V)$ of the Lie algebra, which complexifies to a holomorphic representation $d \pi : \mathfrak{g} \rightarrow \End(V)$ of the complexified Lie algbera.  The basic relationship between these is that for $g \in K$, $x \in \mathfrak{g}$ and $v \in V$, we have
\begin{equation*}
  \pi(g) d \pi(x) v = d \pi(\Ad(g) x) \pi(g) v.
\end{equation*}
Abbreviating $g v := \pi(g) v$ and $x v := d \pi(x) v$ and $g x g^{-1} := \Ad(g) x$, this identity reads more simply as
\begin{equation*}
  g x v = (g x g^{-1}) g v.
\end{equation*}
\begin{lemma}\label{lem:root-spaces-permute-weight-spaces}
  For $\alpha,\lambda \in \mathfrak{t}_{\mathbb{Z}}^*$, we have
  \begin{equation*}
    d \pi(\mathfrak{g}^\alpha) V^{\lambda} \subseteq V^{\lambda + \alpha}.
  \end{equation*}
  In particular, taking $(\pi,V) = (\Ad,\mathfrak{g})$, we have for any $\alpha, \beta \in \mathfrak{t}_{\mathbb{Z}}^*$ that
  \begin{equation*}
    [\mathfrak{g}^\alpha, \mathfrak{g}^{\beta}] \subseteq \mathfrak{g}^{\alpha+\beta}.
  \end{equation*}
\end{lemma}
\begin{proof}
  Let $x \in \mathfrak{g}^*$, $v \in V^{\lambda}, t \in T$.  Then
  \begin{equation}
    \pi(t) (d \pi(x) v)
    = d \pi(\Ad(t) x) \pi(t) v
    = d \pi(t^\alpha x) \pi(t) v
    = d \pi(t^\alpha  x) t^{\lambda} v
    = t^{\lambda+\alpha} d \pi(x) v,
  \end{equation}
  so $d \pi(x) v \in V^{\lambda+\alpha}$, as required.  We record the same argument but expressed using the above abbreviations:
  \begin{equation*}
    t x v = (t x t^{-1}) t v = (t^{\alpha} x) (t^{\lambda} v)= t^{\lambda + \alpha} x v.
  \end{equation*}
  The conclusion regarding $\alpha, \beta$ is immediate, but it's worth repeating the proof in that special case for the sake of illustration: for $x \in \mathfrak{g}^\alpha$, $y \in \mathfrak{g}^{\beta}$ and $t \in T$, we have
  \begin{equation*}
    \Ad(t) [x,y] = [\Ad(t) x, \Ad(t) y] = [t^{\alpha} x, t^{\beta} y] = t^{\alpha + \beta}[x,y],
  \end{equation*}
  hence $[x,y] \in \mathfrak{g}^{\alpha+\beta}$.
\end{proof}
\footnote{End of lecture \#11, Thursday, 28 March}

By choosing a unitary structure on a faithful representation of $K$ and then taking an orthonormal basis consisting of weight vectors for $T$, we obtain an embedding $K \hookrightarrow \U(n)$ that carries $T$ to a subgroup of the diagonal subgroup of $\U(n)$.  We have noted (Theorem \ref{thm:complexification-compact-lie-basic-properties}) that then the complexified Lie algebra $\mathfrak{g}$ is closed under $\theta : x \mapsto - x^*$, where $x^* := {}^t \overline{x}$.  This fact has an important consequence:
\begin{lemma}\label{lem:negative-roots-exist}
  If $\alpha \in \Phi$, then $- \alpha \in \Phi$, and $\dim(\mathfrak{g}^\alpha) = \dim(\mathfrak{g}^{-\alpha})$.
\end{lemma}
For example, if $K = \U(n)$ and (with notation as before) $\alpha = \eps_i - \eps_j$, then $\mathfrak{g}^\alpha = \mathbb{C} E_{i j}$, $\theta(E_{ij}) = - E_{j i}$, $- \alpha = \eps_j - \eps_i$.
\begin{proof}
  Since $\mathfrak{g}$ is closed under $\theta$, it is enough to show that $\theta(\mathfrak{g}^\alpha) \subseteq \mathfrak{g}^{-\alpha}$.  (Equality then follows from, for instance, the involutivity of $\theta$ and the same argument applied to $-\alpha$.)  Let $x \in \mathfrak{g}^\alpha$, and let $t \in T$.  With matrix realizations as above, we have $t^* = t^{-1}$ (indeed, we may write $t = \diag(e^{i \theta_1}, \dotsc e^{i \theta_n})$, which makes this obvious), while $A^* B^* = (B A)^*$ for any matrices $A,B$, thus
  \begin{align*}
    \Ad(t) \theta(x)
    &=
      -
      t
      x^*
      t^{-1}
      =
      - (t^{-1})^* x^* t^*
    \\
    &=
      -
      (t x t^{-1})^*
      =
      - (\Ad(t) x)^*
      = -(t^\alpha x)^*
      =
      \overline{t^\alpha } \theta(x)
      = t^{-\alpha} \theta(x).
  \end{align*}
  Since $t$ was arbitrary, we deduce as required that $\theta(x) \in \mathfrak{g}^{-\alpha}$.
\end{proof}

\subsection{Generators}
Tori are topologically cyclic, and this fact is very useful.

\begin{definition}
  Let $T$ be any torus.  A \emph{generator} $t \in T$ is an element such that $\overline{\langle t \rangle} = T$; here $\langle t \rangle := \{t^n : n \in \mathbb{Z} \}$ denotes the subgroup generated by $t$, and the closure of that subgroup $\overline{\langle t \rangle}$.
\end{definition}
\begin{lemma}\label{lem:generators-exist}
  Let $T$ be any torus.  Then generators of $T$ exist and are dense.
\end{lemma}
\begin{proof}
  Write $n = \dim(T)$, so that $T \cong (\mathbb{R}/\mathbb{Z})^n$ and $t = (x_1,\dotsc,x_n)$.  In the case $n=1$, Weyl's equidistribution criterion implies that $t$ is a generator iff $x_1 \notin \mathbb{Q}$.  For general $n$, the same criterion says that $t$ is a generator iff $1,x_1,\dotsc,x_n$ are linearly independent over $\mathbb{Q}$.  These conditions are obviously satisfied by a dense collection of elements.

  Alternatively, here's an elementary pigeonhole argment (which I think I learned from Adams' book).  For each $t \in T$ and $\eps > 0$, write $B_t(\eps)$ for the open $\eps$-ball at $t$, defined with respect to the Euclidean metric on $(\mathbb{R}/\mathbb{Z})^n$.  Fix an enumeration $(t_i,\eps_i)$, indexed by $i \geq 1$, of all pairs consisting of
  \begin{itemize}
  \item an element $t_i \in T$ whose entries with respect to the isomorphism $T \cong (\mathbb{R}/\mathbb{Z})^n$ are rational, and
  \item a rational number $\eps \in (0,1)$.
  \end{itemize}
  (Thus the $B_{t_i}(\eps_i)$ give a countable open basis for $T$.)  Let $U$ be a closed subset of $T$ with nonempty interior $U^0$.  If the natural number $N_1$ is large enough, then $N_1 U := \{N_1 u : u \in U\}$ coincides with $T$.  Thus $U_1 := \{u \in U : |N_1 u - t_1| \leq \eps_1/2 \}$ is a closed subset of $U$, with nonempty interior, such that $N_1 U_1 \subseteq B_{t_1}(\eps_1)$.  We may similarly find a natural number $N_2$ and a closed subset $U_2$ of $U_1$, with nonempty interior, so that $N_2 U_2 \subseteq B_{t_2}(\eps_2)$.  Continuing in this way, we obtain a descending chain of closed subsets with nonempty interiors $U \supset U_1 \supseteq U_2 \supseteq \dotsb$ and natural numbers $N_1, N_2, \dotsc$ so that for each $i \geq 1$, we have $N_i U_i \subseteq B_{t_i}(\eps_i)$.  By compactness, the intersection $\cap_{i \geq 1} U_i$ is nonempty.  Let $t$ be any element of that intersection.  Then $N_i t \in B_{t_i}(\eps_i)$.  Hence the group $\langle t \rangle = \mathbb{Z} t$ (here we use additive notation for $T \cong (\mathbb{R}/\mathbb{Z})^n$) intersects every constituent $B_{t_i}(\eps_i)$ of an open basis of $T$.  Hence its closure $\overline{\langle t \rangle }$ is all of $T$.  Since $U$ was arbitrary, we deduce moreover that the set of generators is dense.
\end{proof}

Generators are useful because they reduce properties involving an entire torus to individual elements.  We give an example.  To state it, recall that the centralizer and normalizer of a subgroup $H$ of a group $G$ are defined by $Z_G(H) := \{g \in G : g h g^{-1} = h \text{ for all } h \in H\}$ and $N_G(H) := \{g \in G : g h g^{-1} \in H \text{ for all } h \in H\}$; the definition of $Z_G(H)$ applies more generally to any subset $H$ of $G$, and we abbreviate $Z_G(h) := Z_G(\{h\})$ when that subset consists of a single element $h \in G$.
\begin{lemma}\label{lem:generators-cent-norm}
  Let $K$ be a Lie group, let $T \leq K$ be any torus, and let $t \in T$ be a generator.  Then $Z_K(T) = Z_K(t) = \{g \in K : g t g^{-1} = t\}$ and $N_K(T) = \{g \in K : g t g^{-1} \in T\}$
\end{lemma}
\begin{proof}
  We just note that the subsets $\{s \in T : g s g^{-1} = s\}$ and $\{s \in T : g s g^{-1} \in T\}$ are closed subgroups of $T$, hence they coincide with $T$ iff they contain the generator $t$.
\end{proof}

\subsection{Definition and finitude of the Weyl group of a maximal torus}
\begin{definition}
  Let $K$ be a compact Lie group, with maximal torus $T$.  The \emph{Weyl group} $W := W(K:T)$ is defined to be $N(T)/T$, where $N(T) := N_K(T)$ denotes the normalizer of the torus.
\end{definition}
For instance, for $K = \U(n)$, we saw that the permutation representation $S(n) \rightarrow \U(n)$ defines an isomorphism $\S(n) \cong W$.

We note in general that $W$ acts on $T$ by conjugation (in a well-defined manner):
\begin{equation*}
  w \cdot t := w t w^{-1}.
\end{equation*}

The following basic lemma (which may be proved in a few ways) gives a decent illustration of how to argue using the root space decomposition \eqref{eq:root-space-decomp}.
\begin{lemma}
  Let $K$ be a compact Lie group with maximal torus $T$.  Then $N(T)^0 = T$ and $|W| < \infty$.
\end{lemma}
\begin{proof}
  We show first that $N(T)^0 = T$.  The containment $N(T)^0 \supseteq T$ is clear, so we just need to check that $\Lie(N(T)^0) \subseteq \mathfrak{t}$.  We have $\Lie(N(T)^0) = \{x \in \mathfrak{k} : [x,\mathfrak{t}] \subseteq \mathfrak{t} \}$.  Let $x \in \Lie(N(T)^0)$.  Using \eqref{eq:root-space-decomp}, we may write
  \begin{equation*}
    x = x_0 + \sum_{\alpha \in \Phi} x_\alpha
  \end{equation*}
  with $x_0 \in \mathfrak{t}_{\mathbb{C}}$ and $x_\alpha \in \mathfrak{g}^{\alpha}$.  For $y \in \mathfrak{t}$, we then have
  \begin{equation*}
    \mathfrak{t} \ni - [x,y] = [y,x] = [y,x_0] + \sum_{\alpha \in \Phi} [y,x_\alpha] = \sum_{\alpha \in \Phi} \alpha(y) x_\alpha,
  \end{equation*}
  hence $\alpha(y) x_\alpha = 0$ for all $\alpha$ and all $y$.  Since each $\alpha$ is nonzero, we deduce that $x_\alpha = 0$ for all $\alpha$, hence that $x = x_0$, and so $x \in \mathfrak{t}$, as required.

  It follows that $W = N(T)/N(T)^0$ is a compact $0$-dimensional Lie group, and so is finite.
\end{proof}

\begin{remark}
  The definition of $W$ given here is a bit \emph{ad hoc}.  We'll later define $W(K:S)$, for \emph{any} torus $S$, to be the quotient $N_K(S)/Z_K(S)$ of the normalizer divided by the centralizer.  This will be seen to coincide with the definition given above in the case of a \emph{maximal} torus $T$ only after we've seen that $Z_K(T) = T$ for such tori, but this is a deep fact which will require proof.  The group $W = W(K:T)$ as defined above will be relevant for the proof of that fact.
\end{remark}

\subsection{Weyl integral formula}\label{sec:weyl-integr-form-general}
We want a generalization of the formula that we proved earlier for $\U(n)$ (see \S\ref{sec:weyl-integr-form}).  That formula expressed the integral over $K$ in terms of integrals over conjugates of a maximal torus $T$.  Since tori are connected, we'd better assume that $K$ is connected to have any hope for such a formula.

So let $K$ be a connected compact Lie group, with maximal torus $T$.  Write $N := \dim(K)$ and $n := \dim(T)$.  The basic setup will be the same as in the case of $\U(n)$.  Write $N := \dim(K)$ and $n := \dim(T)$.  The subspace $\mathfrak{t} = \Lie(T)$ of $\mathfrak{k} = \Lie(K)$ admits a natural $\Ad(T)$-invariant complement $\mathfrak{k}/\mathfrak{t} := \mathfrak{k} \cap (\sum_{\alpha \in \Phi} \mathfrak{g}^{\alpha})$.  Fix a nonzero $\omega_1 = \alpha_1 \wedge \beta_1$ in $\Lambda^N(\mathfrak{k})$, with $\alpha_1 \in \Lambda^{n}(\mathfrak{t})$ and $\beta_1 \in \Lambda^{N-n}(\mathfrak{k}/\mathfrak{t})$.  The compactness of $T$ implies that $\beta_1$ is $\Ad(T)$-invariant.  We obtain left-invariant differential forms $\omega \in \Omega^N(K), \alpha \in \Omega^n(T), \beta \in \Omega^{N-n}(K/T)$.  (We'll denote here and henceforth by $\Omega^k(M)$ for the space of smooth $k$-forms on a manifold $M$, and $\Omega_c^k(M)$ for the subspace of compactly-supported $k$-forms.)  The compactness of $K$ implies that $\omega$ is also right-invariant.  We obtain Haar measures $|\omega|,|\alpha|,|\beta|$ on $K, T, K/T$.  We may assume $\omega$ and $\alpha$ normalized so that $|\omega|$ and $|\alpha|$ are probability Haar measures; the same is then true for $|\beta|$ by the analogue of Fubini's theorem.  We define
\begin{equation*}
  q : K/T \times T \rightarrow K
\end{equation*}
\begin{equation*}
  q(g, t) := g t g^{-1},
\end{equation*}
and write
\begin{equation*}
  q^* \omega = \det(q) (\beta \wedge \alpha).
\end{equation*}
\begin{lemma}\label{lem:jacobian-computation-WIF-general}
  \begin{enumerate}
[(i)]
  \item For all $(g, t) \in K/T \times T$, we have
    \begin{equation*}
      \det(q)(g,t) = D(t),
    \end{equation*}
    where
    \begin{equation*}
      D(t) := \det(1 - \Ad(t) | \mathfrak{k}/\mathfrak{t}) = \prod_{\alpha \in \Phi} (1 - t^\alpha)^{\dim(\mathfrak{g}^\alpha)}.
    \end{equation*}
  \item We have $D(t) \geq 0$ for all $t \in T$.  We have $D(t) > 0$ precisely when $t^\alpha \neq 1$ for all $\alpha \in \Phi$.
  \end{enumerate}
\end{lemma}
We'll see a bit later that each root space $\mathfrak{g}^{\alpha}$ is exactly one-dimensional, so that the dimensions appearing in the exponents may be omitted, but we include them for now.
\begin{proof}
  The computation of $\det(q)$ is exactly as in the case of $\U(n)$.
  
  By lemma \ref{lem:negative-roots-exist}, we have $\alpha \in \Phi$ iff $-\alpha \in \Phi$, and moreover $\dim \mathfrak{g}^\alpha = \dim \mathfrak{g}^{-\alpha}$.  Thus $D(t)$ is a product taken over all distinct pairs $\{\alpha, - \alpha \} \subseteq \Phi$ of the $\dim(\mathfrak{g}^\alpha)$th power of the quantities $(1 - t^\alpha)(1 - t^{-\alpha})$.  Each such quantity is nonnegative.  (Writing $t^{\alpha} = e^{i \theta}$, the quantity in question is $2 - 2 \cos \theta$.)
\end{proof}

\begin{lemma}
  The formula
  \begin{equation*}
    D(g) := \text{ coefficient of } \lambda^{\dim(T)} \text{ in } \det(\lambda + 1 - \Ad(g))
  \end{equation*}
  defines a class function $D : K \rightarrow \mathbb{R}$ that extends the function $D : T \rightarrow \mathbb{R}_{\geq 0}$ defined above.
\end{lemma}
We'll see eventually that in fact $D \geq 0$ on all of $K$.
\begin{proof}
  For $t \in T$, $\Ad(t)$ acts on both $\mathfrak{t}$ and $\mathfrak{k}/\mathfrak{t}$, so we may factor the determinant of $\lambda + 1 - \Ad(t)$ for the action on on $\mathfrak{k}$ as the product of the determinants for the actions on $\mathfrak{t}$ and $\mathfrak{k}/\mathfrak{t}$; we have $\Ad(t)|_{\mathfrak{t}} = 1$, so the first of the latter two determinants is just $\lambda^{\dim(T)}$, while the second is $D(t) + \O(\lambda)$.
\end{proof}

\begin{definition}
  An element $g \in K$ is called \emph{regular} if $D(g) \neq 0$.  We denote by $K^{\reg}$ the set of all regular elements $g \in K$, and set $T^{\reg} := T \cap K^{\reg}$.
\end{definition}
We observe that $K^{\reg}$ is a conjugacy-invariant subset of $K$ and that $T^{\reg} = \{t \in T : t^\alpha \neq 1 \text{ for all } \alpha \in \Phi \}$.  For example, if $K = \U(n)$ and $T \cong \U(1)^n$ is the diagonal torus, then $t \in T$ is regular iff $t_i \neq t_j$ for all $i \neq j$.  It's clear that any generator is regular, while the converse is not true in general.  Note also that ``being a generator'' is intrinsic to $T$, while ``being regular'' depends also upon the ambient group $K$.

We note that for any $w \in W$ and $(g T,t) \in K/T \times T$ and $w \in W$, the action
\begin{equation*}
  w \cdot (g T, t) := (g w^{-1} T, w \cdot t) = (g w^{-1} T, w t w^{-1})
\end{equation*}
is well-defined.  It induces defines an action of $W$ on each fiber of $q$; indeed,
\begin{equation*}
  q(w \cdot (g T, t)) = (g w^{-1}) (w t w^{-1}) (g w^{-1})^{-1} = g t g^{-1} = q(g T, t).
\end{equation*}
We denote this relationship by the diagram
\begin{equation*}
  W \circlearrowright K/T \times T \xrightarrow{q} K.
\end{equation*}

In the $\U(n)$ case, the fibers of $q$ correspond to choices of orthonormal basis for a given unitary matrix; the action of $W$ corresponds to permuting the basis elements and is thus free, and transitive on the fiber above a regular element because such elements admit a unique (up-to-reordering) basis of eigenvectors.

The action of $W$ (on each fiber of $q$) is free in general, since if $(g w T, w \cdot t) = (g T, t)$ then $g w T = g T$, hence $w T = T$, i.e., $w \equiv 1$ in $W$.  We'll see below that the action on regular fibers is transitive, as in the $\U(n)$ case.  The action can be far from transitive on irregular fibers; for instance, $q^{-1}(1) = K/T \times \{1\}$, and the action of $W$ on $K/T$ fails to be transitive whenever $\dim(K/T) > 0$.

We're now prepared to state the Weyl integral formula for $K$, together with some related consequences:
\begin{theorem}\label{thm:WIF-general}
  Let notation and assumptions be as above: $K$ is an $N$-dimensional connected compact Lie group with $n$-dimensional maximal torus $T$ and Weyl group $W$.
  \begin{enumerate}
[(i)]
  \item\label{item:q-surj} $q$ is surjective, i.e., $K = \cup_{g \in K} g T g^{-1}$.
  \item\label{item:q-cover} The restriction
    \begin{equation*}
      q : K/T \times T^{\reg} \rightarrow K^{\reg}
    \end{equation*}
    is a $|W|$-to-one covering map, with the group $W$ acting simply-transitively on the fibers.  This restriction is orientation-preserving at every point.
  \item\label{item:WIF-dif} For all $\nu \in \Omega^N(K)$, we have
    \begin{equation}\label{eq:WIF-dif}
      \int_{K /T \times T}
      q^* \nu = |W| \int_K \nu.
    \end{equation}
  \item\label{item:WIF-fun} For all $f \in C(K)$ (i.e., continuous $f : K \rightarrow \mathbb{C}$) we have
    \begin{equation*}
      \int_{(g,t) \in K/T \times T} D(t) f(g t g^{-1}) = |W| \int_K f,
    \end{equation*}
    with all integrals taken with respect to probability Haar measures.
  \end{enumerate}
\end{theorem}
\begin{proof}
[Proof that \ref{item:WIF-dif} implies \ref{item:WIF-fun}]
  Take $\nu = f \omega$, with $\omega \in \Omega^N(K)$ as used above to describe the probability Haar on $K$.  Then $q^* \nu = f q^* \omega = f D (\beta \wedge \alpha)$.
\end{proof}
\begin{proof}
[Proof that \ref{item:WIF-dif} implies \ref{item:q-surj}]
  Let $E$ denote the image of $q$.  Suppose otherwise that we may find some $x \in K - E$.  Since $K/T \times T$ is compact and $q$ is continuous, the set $E$ is compact, hence closed, so we may find an open neighborhood $U$ of $x$ disjoint from $E$.  Choose $\nu \in \Omega_c^N(U)$ with $\int \nu \neq 0$.  Then $q^* \nu = 0$.  Using (ii), we deduce that
  \begin{equation*}
    0 = \int q^* \nu = |W| \int \nu \neq 0,
  \end{equation*}
  giving the required contradiction.
\end{proof}
\begin{proof}
[Proof of \ref{item:q-cover} and \ref{item:WIF-dif}]
  First, let $x \in T^{\reg}$.  Lemma \ref{lem:jacobian-computation-WIF-general} implies then that $\det(q) > 0$ on $q^{-1}(x)$.  Thus for each $y \in q^{-1}(x)$, the map $q$ is an orientation-preserving local diffeomorphism at $y$.  In particular, $q^{-1}(x)$ is a closed $0$-dimensional subset of the compact set $K/T \times T$, and so $q^{-1}(x)$ is finite.  We may thus find a small neighborhood $U$ of $x$ and, for each $y \in q^{-1}(x)$, a small neighborhood $U_y$ of $y$ so that $q$ induces diffeomorphisms $U_y \cong U$.  By shrinking $U$ and hence each $U_y$ if necessary, we may assume that the $U_y$ are all disjoint.  Using the compactness of the domain of $q$, we see that the image under $q$ of the complement of $\cup_z U_z$ is compact, hence closed, so by shrinking $U$ if necessary, we may assume that the image in question is disjoint from $U$.  Then
  \begin{equation}\label{eq:}
    q^{-1}(U) = \sqcup_{y \in q^{-1}(x)} U_y,
  \end{equation}
  with the map $q^{-1}(U) \rightarrow U$ a trivial cover.

  We've noted already that $W$ acts freely on every fiber of $q$.  We claim now moreover that $W$ acts \emph{transitively} on every fiber of $q$ in $U$, i.e., that $W$ permutes the ``pancakes'' $U_y$ lying above $U$.  It suffices to verify that $W$ acts transitively on the fibers above any individual element $t \in U$.  By Lemma \ref{lem:generators-exist}, we may take for $t$ a generator of $T$.  As a ``basepoint'' for $q^{-1}(t)$, we may take the pair $(e T,t)$, with $e \in G$ the identity element.  If $(g T,s)$ is any other element of $q^{-1}(t)$, then $g s g^{-1} = t$, and so $g^{-1} t g = s \in T$, which (by Lemma \ref{lem:generators-cent-norm}) forces $g$ to lie in $N(T)$, i.e., $w := g T$ to lie in $W$.  Thus $(g T, s) = w \cdot (e T, t)$, and so $W$ acts transitively on $q^{-1}(t)$, as required.  It follows in particular that $\# q^{-1} (x) =\# q^{-1}(t) = |W|$, so that for any $\nu \in \Omega_c^N(U)$,
  \begin{equation}\label{eq:partial-WIF-good-U}
    \int q^* \nu = |W| \int \nu.
  \end{equation}

  We have nearly established \ref{item:q-cover}; what remains to show is just that $q : K/T \times T^{\reg} \rightarrow K^{\reg}$ is surjective.  The required surjectivity will follow from \ref{item:q-surj} and hence from \ref{item:WIF-dif}, so it remains only to establish \ref{item:WIF-dif}.
  
  To that end, let us call an open subset $U$ of $K$ \emph{good} if \eqref{eq:WIF-dif} holds for all $\nu \in \Omega_c^N(U)$.  By a partition of unity argument and the compactness of $K$, it will suffice to show that every element of $K$ admits a good neighborhood.  We've seen already in \eqref{eq:partial-WIF-good-U} that every element of $T^{\reg}$ admits a good neighborhood.  The set $T^{\reg}$ is nonempty.  The idea now will be to use Stokes' theorem to ``propagate'' goodness from any one open set to the rest.  The following elementary fact from differential geometry, known as the \emph{Poincar{\'e} lemma}, will be of use:
  \begin{center}
    \emph{Let $\nu_1, \nu_2 \in \Omega_c^N((0,1)^N)$ be compactly-supported top-degree differential forms on the open unit cube such that $\int \nu_1 = \int \nu_2$.  Then there exists $\lambda \in \Omega_c^{N-1}((0,1)^N)$ so that $\nu_2 = \nu_1 + d \lambda$.}
  \end{center}
  For example, if $N = 1$, then $\nu_1 - \nu_2 = f(x) d x$ for some $f \in C_c^\infty((0,1))$ with $\int_0^1 f(x) \, d x = 0$, so $\lambda(x) := \int_{0}^x f(y) \, d y$ defines an element $\lambda \in C_c^\infty((0,1))$ for which $d \lambda = \nu_1 - \nu_2$.  The proof in the general case is similar and recorded below for completeness.  Assuming the Poincar{\'e} lemma for the moment, we may complete the proof of \ref{item:WIF-dif} as follows:
  \begin{enumerate}
  \item Say that an open subset $U$ of $K$ is \emph{small} if it admits a chart $U \cong (0,1)^N$.  We claim that \emph{if $U_1, U_2$ are small, $U_1 \cap U_2$ is nonempty, and $U_1$ is good, then $U_2$ is good.}  To see this, let $\nu_2 \in \Omega_c^N(U_2)$ be given.  Choose any $\nu_1 \in \Omega_c^N(U_1 \cap U_2)$ with $\int \nu_1 = \int \nu_2$.  By the Poincar{\'e} lemma applied in the chart for $U_2$, we may find $\lambda \in \Omega_c^{N-1}(U_2)$ so that $\nu_2 = \nu_1 + d \lambda$.  We have $q^* d \lambda = d q^* \lambda$, whose integral vanishes thanks to Stokes' theorem.  By the assumed goodness of $U_1$ and the construction of $\nu_1$, we deduce that $\int q^* \nu_2 = \int q^* \nu_1 = |W| \int \nu_1 = |W| \int \nu_2$.  Since $\nu_2$ was arbitrary, we deduce as required that $U_2$ is good.
  \item We have seen that there exists \emph{some} small good subset $U$ of $K$ (namely, any small enough open neighborhood of a regular element of $T$).  It will suffice to show that any other small open subset $U'$ of $K$ is likewise good.  To that end, let us choose a curve $\gamma : [0,1] \rightarrow K$ with $\gamma(0) \in U$ and $\gamma(1) \in U'$.  We can find for each $s \in [0,1]$ a good neighborhood of $\gamma(s)$.  By a compactness argument applied to the preimages of those neighborhoods, we may find a sequence $U = U_0, U_1, \dotsc, U_{m-1}, U_m = U'$ of small open subsets of $K$ with $U_j \cap U_{j-1} \neq \emptyset$ for all $j=1..m$.  By what was shown above, the goodness of $U_0 = U$ inductively implies the goodness of every $U_j$, hence in particular of $U'$.
  \end{enumerate}
  The proof is now complete.
\end{proof}


\begin{proof}
[Proof of the Poincar{\'e} lemma]
  (Omitted from lecture, recorded here for completeness; probably a good exercise.)  Let's work with $n$ instead of $N$.  Write
  \begin{equation*}
    \nu_1 - \nu_2 = f(x_1,\dotsc,x_n) \, d x_1 \wedge \dotsb \wedge d x_n.
  \end{equation*}
  By hypothesis, $f$ is supported in $(0,1)^n$ and has mean zero.  Our aim is to show that $f(x_1,\dotsc,x_n) \, d x_1 \wedge \dotsb \wedge d x_n$ is of the form $d \lambda$ for some $(n-1)$-form $\lambda$ supported in $(0,1)^n$.  Equivalently, we must show that
  \begin{equation}\label{eq:f-as-sum-of-partial-gs}
    f = \sum_{j=1}^n \partial_j g_j
  \end{equation}
  for some $g_1,\dotsc,g_n$ supported in $(0,1)^n$, where $\partial_j$ denotes the partial derivative taken with respect to $x_j$.

  We induct on $n$.  The case $n = 0$ is tautological, while the case $n = 1$ was treated above.  For the case $n \geq 2$, we apply our inductive hypothesis to write
  \begin{equation*}
    \int_{y=0}^1 f(y,x_2,\dotsc,x_n) \, d y = \sum_{j=2..n} \partial_{j} h_j(x_2,\dotsc,x_n).
  \end{equation*}
  Fixing $\chi \in C_c^\infty((0,1))$ with $\int \chi = 1$, the function
  \begin{equation}\label{eq:}
    g_j(x_1,\dotsc,x_n) :=
    \chi(x_1) h_j(x_2,\dotsc,x_n)
  \end{equation}
  is supported in $(0,1)^n$.  Set $f' := f - \sum_{j \geq 2} \partial_j g_j$.  Then
  \begin{equation*}
    \int_{y=0}^1 f'(y,x_2,\dotsc,x_n) \, d y = 0 \text{ for all } x_2,\dotsc,x_n,
  \end{equation*}
  so that
  \begin{equation*}
    g_1(x_1,\dotsc,x_n) := \int_{y=0}^{x_1} f'(y,x_2,\dotsc,x_n) \, d y
  \end{equation*}
  is supported in $(0,1)^n$ and satisfies $\partial_1 g_1 = f'$.  The required identity \eqref{eq:f-as-sum-of-partial-gs} then holds by construction.
\end{proof}

\begin{remark}
  I wanted to present the proof as elementarily as possible, but I would be remiss not to mention that what the above arguments ``really'' show is that for any connected $N$-manifold $M$, the compactly-supported top-degree de Rham cohomology group
  \begin{equation}\label{eq:}
    H^N_c(M) := \Omega_c^N(M) / d \Omega_c^{N-1}(M),
  \end{equation}
  with coefficients taken in either $k = \mathbb{R}$ or $\mathbb{C}$, is isomorphic to $k$, with the isomorphism
  \begin{equation}\label{eq:HNc-isom-k}
    H^N_c(M) \xrightarrow{\cong} k
  \end{equation}
  defined by integrating an $N$-form over $M$.  (Convince yourself that our arguments actually establish this.)  Since $K/T \times T$ and $K$ are both connected compact $N$-manifolds, we can replace $H^N_c$ with $H^N$, and the formula \eqref{eq:WIF-dif} says that the dual map $q^*$ on de Rham cohomlogy fits into a commutative diagram
  \begin{equation*}
    \begin{CD}         
      H^N(K)   @>q^*>>H^N(K/T \times T)\\
      @V\cong VV  @VV\cong V \\
      k @>>|W|> k,\\
    \end{CD}
  \end{equation*}
  where we write $|W|$ for the ``multiplication by $|W|$'' map.  We know that every continuous $k$-linear map $k \rightarrow k$ is given by multiplication by \emph{something}, so the issue is just to compute the proportionality constant for at least one $N$-form $\nu$.  We managed to do this for all $\nu$ supported in a small enough neighborhood of a regular element of the torus.

  It's also worth noting that one can see \emph{a priori} that proportionality constant (in this case $|W|$) must an \emph{integer} by applying the comparison isomorphism relating de Rham cohomology to singular cohomology and working instead with $\mathbb{Z}$-coefficients for the latter, using Poincar{\'e} duality to obtain the analogue over $\mathbb{Z}$ of the integration isomorphism \eqref{eq:HNc-isom-k}.  For details see the BTD course reference.
\end{remark}
\begin{remark}
  The intuitive idea behind the proof of the surjectivity assertion \ref{item:q-surj} of Theorem \ref{thm:WIF-general} may be illustrated as follows.  Let $C$ be a smooth periodic oriented curve in the ``cylinder'' $X := \mathbb{R}/\mathbb{Z} \times \mathbb{R}$, thus $C$ is the image of some smooth map $\gamma : \mathbb{R}/\mathbb{Z} \rightarrow X$.  Consider the map $\phi : C \rightarrow \mathbb{R}/\mathbb{Z}$ given by the projection $(x,y) \mapsto x$ onto the first coordinate.  Suppose that $\phi$ fails to be surjective; say $x_0 \notin \phi(C)$.  Take a ``generic'' nonempty fiber $\phi^{-1}(x)$.  It's then intuitively plausible that the signed sum of the orientations of the elements $y \in \phi^{-1}(x)$ must be zero, because the trajectories along $C$ departing from such elements and moving in the direction of $x_0$ must pair off with one another in a complementary fashion to be compatible with $\phi^{-1}(x_0)$ being empty.  This intuitive plausibility is in fact the case, and may be formulated and proved rigorously by the above arguments.  This explains why $q$ failing to be surjective is incompatible with the fact that $\det(q)$ is everywhere positive, the latter of which implies that the orientations sum to $|W|$ rather than to $0$.
\end{remark}
\footnote{End of lecture \#12, Tuesday, 2 Apr}

\subsection{Consequences of the conjugacy theorem}
Let $K$ be a compact connected Lie group.  We saw above that for any maximal torus $T$, we have $K = \cup_{g \in K} g T g^{-1}$.  We now explore some consequences of this fact.

\begin{theorem}\label{thm:exp-onto}
  Every element of $K$ is contained in some maximal torus, the exponential map $\exp : \mathfrak{k} \rightarrow K$ is onto, and every $x \in K$ is contained in the connected component $Z_K(x)^0$ of its centralizer.
\end{theorem}
\begin{proof}
  If $T$ is a maximal torus, then its conjugates $g T g^{-1}$ are also maximal tori, and we've seen that every element of $K$ belongs to one of these.  Since for any torus $T$ the exponential map $\exp : \mathfrak{t} \rightarrow T$ is surjective, we deduce the corresponding result for $K$.  The final assertion is clear when $x$ is contained in the maximal torus $T$, because then $g \in T \leq Z_K(x)^0$, and follow in general from what we have just shown.
\end{proof}

It's worth illustrating the subtlety here with a ``near proof.''  Given an element $x \in K$, let $H := \overline{\langle x \rangle}$ denote the closure of the subgroup that it generates.  Then $H$ is closed in $K$, hence compact, and abelian.  If we knew that $H$ were connected, then $H$ would be a torus, and so we'd be done.  If we even knew merely that $x$ belonged to the connected component $H^0$ of $H$, then we could conclude similarly.  But in general $x \notin H^0$.  For instance, take $x = (-1,e^i) \in \U(1)^2$.  Then $H = \{\pm 1\} \times \U(1)$ and $H^0 = \{1\} \times \U(1)$, so $x \notin H^0$.  This illustrates some of the subtlety involved in Theorem \ref{thm:exp-onto}.


Anyway, continuing with the consequences:
\begin{theorem}\label{thm:conj-max-tori}
  Any two maximal tori $T, T'$ of $K$ are conjugate.
\end{theorem}
\begin{proof}
  Let $t' \in T'$ be a generator.  We've seen that $t' \in g T g^{-1}$ for some $g \in K$.  Since $g T g^{-1}$ is a closed subgroup and $t'$ is a generator, it follows that $T' \subseteq g T g^{-1}$.  Since $T$ is a maximal torus, so is $g T g^{-1}$, which forces the equality $T' = g T g^{-1}$.
\end{proof}

\begin{theorem}
  The center $Z(K)$ of $K$ is given by
  \begin{equation*}
    Z(K) = \cap _{\text{maximal tori } T} T.
  \end{equation*}
\end{theorem}
\begin{proof}
  \begin{itemize}
  \item ``$\supseteq$'': Let $z \in \cap T$ and $x \in K$.  By Theorem \ref{thm:exp-onto}, we may find a maximal torus $T$ with $x \in T$.  Then $z \in T$, so $x$ and $z$ commute.  Since $x$ was arbitrary, we conclude that $z \in Z(K)$.
  \item ``$\subseteq$'': Let $z \in Z(K)$, and let $T$ be a maximal torus.  Choose $g \in K$ so that $z \in g T g ^{-1}$.  Then $T \ni g^{-1} z g = z$, because $z$ is central.  Since $T$ was arbitrary, we conclude that $z \in \cap T$.
  \end{itemize}
\end{proof}

\begin{theorem}
  Let $T$ be a maximal torus, with Weyl group $W = N(T)/T$.  If $t, t' \in T$ are $K$-conjugate, then they have the same $W$-orbits.  Thus the map
  \begin{equation*}
    T/W \rightarrow \{\text{conjugacy classes in } K\}
  \end{equation*}
  is bijective.
\end{theorem}
\begin{proof}
  Let $H := Z_K(t')^0$ denote the connected component of the centralizer of $t' \in T$.  Choose $g \in K$ so that $t' = g t g^{-1}$; in paricular, $t' \in g T g^{-1}$.  Then the tori $g T g^{-1}$ and $T$ are both contained in $H$ and maximal in $K$, hence maximal in $H$, so by Theorem \ref{thm:conj-max-tori} applied to the compact connected Lie group $H$, we see that there is some $h \in H$ so that $T = h g T g^{-1} h^{-1}$, i.e., so that $h g \in N_K(T)$.  Let $w := h g T$ denote the class of $h g$ in the Weyl group $W$.  Then $w \cdot t = h g t g^{-1} h^{-1} = h t' h^{-1} = t'$, since $h$ centralizes $t'$.
\end{proof}

\begin{theorem}
  Let $S \subseteq K$ be any torus.  Then its centralizer is described by the formula
  \begin{equation*}
    Z_K(S) = \cup _{\text{maximal tori } T \supseteq S} T.
  \end{equation*}
\end{theorem}
\begin{proof}
  We start by noting that if $g \in Z_K(S)$, then $S \subseteq Z_K(g)^0$.  The remainder of the proof is left as an exercise in applying the ideas introduced above.
\end{proof}

\begin{corollary}
  For all tori $S$, the centralizer $Z_K(S)$ is connected.
\end{corollary}
This might sound at first like a very dry and boring result, but knowing that a subgroup is connected is quite powerful, because it reduces the determination of that subgroup to that of its Lie algebra.

\begin{corollary}
  Let $T$ be a maximal torus.  Then $Z_K(T) = T$.  Thus the Weyl group $W = N_K(T)/T = N_K(T)/ Z_K(T)$ acts faithfully on $T$, i.e., $W$ injects into $\Aut(T)$.
\end{corollary}

\begin{corollary}\label{cor:Z-k-x-conn}
  Let $x \in \mathfrak{k}$.  Then $Z_K(x) := \{g \in K : \Ad(g) x = x\}$ is connected.
\end{corollary}
\begin{proof}
  Set $S := \overline{\exp(\mathbb{R} x)}$.  Then $S$ is a torus, and $Z_K(x) = Z_K(S)$.
\end{proof}

\subsection{Basics on Weyl chambers}\label{sec:basics-weyl-chambers}
We continue to assume that $K$ is a compact connected Lie group.  We fix a maximal torus $T$.  We now define the ``Lie algebra variants'' of the sets of regular elements defined at the group level in \S\ref{sec:weyl-integr-form-general}.  To that end, let us denote by
\begin{equation*}
  D_0 : \mathfrak{g} \rightarrow \mathbb{R}
\end{equation*}
\begin{equation*}
  D_0(x) := \text{ coefficient of } \lambda^{\dim(T)} \text{ in } \det(\lambda - \ad(x))
\end{equation*}
the Lie algebra analogue of the map $D : G \rightarrow \mathbb{R}$ defined earlier.  We may compute $D_0$ on a toral element $x \in \mathfrak{t}$ as follows.  The root space decomposition \eqref{eq:root-space-decomp} for the action of $T$ on $\mathfrak{g}$ differentiates to a decomposition for the action of $\mathfrak{t}$ on $\mathfrak{g}$, namely if we decompose $x \in \mathfrak{g}$ as
\begin{equation}\label{eq:root-space-decomp-of-x-again}
  x = x_0 + \sum x_\alpha
  \in \mathfrak{t}_{\mathbb{C}} \oplus (\oplus_{\alpha \in \Phi}
  \mathfrak{g}^\alpha),
\end{equation}
then
\begin{equation}\label{eq:adjoint-actino-on-root-sapce-decmop}
  [z,x]
  = 0 + \sum \alpha(z) x_\alpha
  \text{ for all }
  z \in \mathfrak{t}.
\end{equation}
Thus $\det(\lambda - \ad(z)) = \lambda^{\dim(T)} \prod_{\alpha \in \Phi } (\lambda - \alpha(z))^{\dim(\mathfrak{g}^\alpha)}$.  Since the roots $\alpha$ come in pairs $\{\alpha, - \alpha \}$ with $\dim(\mathfrak{g}^\alpha) = \dim(\mathfrak{g}^{-\alpha})$, we have $\prod_{\alpha \in \Phi} (-1)^{\dim(\mathfrak{g}^\alpha)} = 1$, which gives the simple formula
\begin{equation*}
  D_0(z) = \prod_{\alpha \in \Phi} \alpha(z)^{\dim(\mathfrak{g}^\alpha)} \text{ for } z \in \mathfrak{t}.
\end{equation*}
We note again that we'll show in a bit that each root space is one-dimensional, so that the exponents $\dim(\mathfrak{g}^\alpha)$ may be omitted.

\begin{definition}
  We say that $x \in \mathfrak{k}$ is \emph{regular} if $D_0(x) \neq 0$.  We write $\mathfrak{k}^{\reg} \subseteq \mathfrak{k}$ for the subset of regular elements.  We more generally use a superscripted $\reg$, as in $\mathfrak{t}^{\reg}, \mathfrak{t}_{\mathbb{R}}^{\reg}$, etc., to denote the subset of regular elements.

  A \emph{Weyl chamber} $C$ is a connected component of $\mathfrak{t}_{\mathbb{R}}^{\reg}$.
\end{definition}

We note that $\mathfrak{k}^{\reg}$ is closed under $\Ad(K)$, since the characteristic polynomial is invariant by conjugation.  We leave it to the reader to check the instructive identity
\begin{equation*}
  \mathfrak{k}^{\reg} = \{ x \in \mathfrak{k} : \exp(i \eps x) \in K^{\reg} \text{ for all small } \eps > 0.  \}
\end{equation*}
Thus $\mathfrak{k}^{\reg}$ may be regarded as a sort of ``tangent cone'' at the identity to the open subset $K^{\reg}$ of $K$.  Note however that there are elements of $\mathfrak{k}^{\reg}$ whose exponentials do not lie in $K^{\reg}$; for example, $x = \diag(\pi i, - \pi i ) \in \su(2)$ is regular, but $\exp(x) = \diag(-1,-1) \in \SU(2)$ is not regular.

Let $C$ be a Weyl chamber.  Its image $\alpha(C)$ under any root $\alpha$ is then a connected subset of $\mathbb{R}_{ \neq 0} = \mathbb{R}_{>0} \cup \mathbb{R}_{<0}$, and is thus contaiend in either the positive or negative reals.  In other words, we may find signs $\eps_{\alpha} \in \{\pm 1\}$ ($\alpha \in \Phi$) so that $C \subseteq C_\eps := \{x \in \mathfrak{t}_{\mathbb{R}} : \eps_\alpha \alpha(x) > 0 \text{ for all } \alpha \in \Phi \}$.  On the other hand, it's clear from the definition that $C_{\eps}$ is a \emph{convex cone}, i.e., if $x_1,\dotsc,x_n \in C_{\eps}$ and $t_1,\dotsc,t_n > 0$, then $t_1 x_1 + \dotsb + t_n x_n$.  In particular, $C_{\eps}$ is a connected subset of $\mathfrak{t}_{\mathbb{R}}^{\reg}$.  Since $C$ is by definition a connected component of the latter set, we deduce that $C = C_{\eps}$; that is to say, every Weyl chamber is described by requiring that each root $\alpha$ have some prescribed sign $\eps_{\alpha}$ on all of $C$.  In particular, $C$ is a convex cone.

For instance, if $K = \U(n)$, then $\mathfrak{t}_{\mathbb{R}}$ identifies with the set $\mathbb{R}^n$ of $n$-tuples $x = (x_1,\dotsc,x_n)$ of real numbers, and the subset $\mathfrak{t}_{\mathbb{R}}^{\reg}$ with the subset consisting of those $x$ whose entries are distinct: $x_i \neq x_j$ for $i \neq j$.  Since the roots are of the form $\eps_i - \eps_j$ with $i \neq j$, any Weyl chamber $C$ is described by inequalities of the form $x_i > x_j$ or $x_i < x_j$ for all indices $i < j$.  There is thus a permutation $\sigma$ of $\{1..n\}$ so that $C = \{ x : x_{\sigma(1)} > \dotsb > x_{\sigma(n)} \}$.  In particular, the symmetric group $S(n)$ acts simply-transitively on the set of Weyl chambers.  This observation and the fact that $W \cong S(n)$ are no accident; we'll soon see that they are features of general compact connected Lie groups.

Let's briefly review the various actions at our disposal.  We have actions of $K$ on $K$ by conjugation, on $\mathfrak{k}, \mathfrak{g}$ by the adjoint representation $\Ad$, and on $\mathfrak{k}^*, \mathfrak{g}^*$ by the coadjoint representation $\Ad^*$; we also have the differentiated actions $\ad$ of $\mathfrak{k}$ on $\mathfrak{k}, \mathfrak{g}$ (or of $\mathfrak{g}$ on $\mathfrak{g}$) and $\ad^*$ of $\mathfrak{k}$ on $\mathfrak{k}^*, \mathfrak{g}^*$ (or of $\mathfrak{g}$ on $\mathfrak{g}^*$).  These are related as follows: if $g \in K$ and $x \in \mathfrak{g}$ and $\xi \in \mathfrak{g}^*$, then
\begin{equation*}
  \langle x, \xi \rangle = \langle g \cdot x, g \cdot \xi \rangle;
\end{equation*}
 here $\langle , \rangle$ denotes the canonical pairing $\mathfrak{g} \otimes \mathfrak{g}^* \rightarrow \mathbb{C}$ and we abbreviate $g \cdot x := \Ad(g) x$ and $g \cdot \xi := \Ad^*(g) \xi$.  In terms of the differentiated actions, this reads: for $z \in \mathfrak{g}$, we have
\begin{equation*}
\langle z \cdot x, \xi \rangle + \langle x, z \cdot \xi \rangle
  = 0,
\end{equation*}
 where $\cdot$ denotes the evident action, e.g., $z \cdot x = \ad(z) x = [z,x]$.

The Weyl group $W = N(T)/T$ acts on $T$ by conjugation, hence on $\mathfrak{t}, \mathfrak{t}_{\mathbb{R}}, \mathfrak{t}_{\mathbb{C}}$, etc., by the adjoint action inherited from $G$, and likewise on $\mathfrak{t}^*, \mathfrak{t}_{\mathbb{R}}^*$, etc., by the coadjoint action.  The action of $W$ on $T$ induces an action of $w \in W$ on the cocharacter group $X^\vee(T)$ and on the character group $X(T)$, denoted $\gamma \mapsto w(\gamma)$ and $\lambda \mapsto w(\lambda)$, characterized in the first case by requiring that
\begin{equation*}
z^{w (\gamma)} = w \cdot z^{\gamma}
\end{equation*}
(for $z \in \U(1)$ and $\gamma \in X^\vee(T)$) and in the second case by requiring that
\begin{equation*}
  (w \cdot t)^{w(\lambda)} = t^{\lambda}
\end{equation*}
(for $t \in T$ and $\lambda \in X(T)$).  The actions of $W$ on $X(T)$ and $X^\vee(T)$ and on $\mathfrak{t}_{\mathbb{R}}^*$ and $\mathfrak{t}_{\mathbb{R}}$ are compatible with the identifications $X(T) \cong \mathfrak{t}_{\mathbb{Z}}^* \subseteq \mathfrak{t}_{\mathbb{R}}^*$ and $X^\vee(T) \cong \mathfrak{t}_{\mathbb{Z}} \subseteq \mathfrak{t}_{\mathbb{R}}$, thus the actions of $W$ on $\mathfrak{t}_{\mathbb{R}}$ and on $\mathfrak{t}_{\mathbb{R}}^*$ stabilize the lattices $\mathfrak{t}_{\mathbb{Z}}$ and $\mathfrak{t}_{\mathbb{Z}}^*$.  Moreover:
\begin{lemma}\label{sec:weyl-group-acts-on-roots}
  \begin{enumerate}
[(i)]
  \item $W$ acts on the set $\Phi$ of roots, i.e., $w(\alpha) \in \Phi$ for all $\alpha \in \Phi$; more precisely,
    \begin{equation}\label{eq:w-permutes-root-spaces}
      w \cdot \mathfrak{g}^{\alpha}
      =
      \mathfrak{g}^{w(\alpha)}.
    \end{equation}
  \item $W$ acts on $\mathfrak{t}_{\mathbb{R}}^{\reg}$.
  \item $W$ acts on the set of Weyl chambers, i.e., if $C$ is a Weyl chamber, then so is $w(C)$ for each $w \in W$.
  \end{enumerate}
\end{lemma}
We might pause to convince ourselves that all of these properties are obvious in the familiar case of $\U(n)$.
\begin{proof}
  \begin{enumerate}
[(i)]
  \item We check \eqref{eq:w-permutes-root-spaces}.  It suffices to show that $w \cdot \mathfrak{g}^{\alpha}$.  (The same argument then gives $w^{-1} \cdot \mathfrak{g}^{w(\alpha)} \subseteq \mathfrak{g}^{\alpha}$, whence equality.)  Let $x \in \mathfrak{g}^\alpha$ Then for $t \in T$,
    \begin{equation*}
      t \cdot w \cdot x = \Ad(t) \Ad(w) x = \Ad(w) \Ad(w^{-1} \cdot t) x = (w^{-1} \cdot t)^{\alpha} \Ad(w) x = t^{w(\alpha)} \Ad(w) x.
    \end{equation*}
    Thus $\Ad(w) x \in \mathfrak{g}^{w(\alpha)}$, as required.
  \item Recall that $\mathfrak{t}_{\mathbb{R}}^{\reg}$ consists of $x \in \mathfrak{t}_{\mathbb{R}}$ for which $\langle x, \alpha \rangle = \alpha(x)$ is nonzero for all roots $\alpha$.  We have $\langle w \cdot x, \alpha \rangle = \langle x, w^{-1}(\alpha) \rangle$, and we've seen that $w^{-1}$ preserves the set of roots, so we conclude that $w$ stabilizes $\mathfrak{t}_{\mathbb{R}}^{\reg}$.
  \item We use the previous assertion and the continuity of the action of $W$.
  \end{enumerate}
\end{proof}


The following result illustrates well the power of knowing that tori have connected centralizers:
\begin{lemma}\label{lem:W-acts-freely-weyl-chambers}
  The action of $W$ on the set of Weyl chambers is free, i.e., if $C$ is a Weyl chamber and $w \in W$ satisfies $w(C) = C$, then $w = 1$.
\end{lemma}
\begin{proof}
  Let $W_C := \{w \in W : w(C) = C\}$.  We must show that $W_C = \{1\}$.  We've seen that $C$ is a convex cone, and also that $W$ and hence $W_C$ is finite.  Let $y_0 \in C$.  Then the element $y := \sum_{w \in W_C} w \cdot y_0$ is a positive linear combination of elements of $C$, hence belongs to $C$.  Let $w \in W_C$.  Then $w$ permutes the summands in the definition of $y$, so $w \cdot y = y$, i.e., $w T \subseteq Z_K(y)$.  By Corollary \ref{cor:Z-k-x-conn}, $Z_K(y)$ is connected.  By general Lie theory, the Lie algebra of $Z_K(y)$ is given by $Z_{\mathfrak{k}}(y) = \{x \in \mathfrak{k} : [y,x] = 0\}$.  Let $x \in Z_{\mathfrak{k}}(y)$.  Writing $x = x_0 + \sum x_\alpha$ as in \eqref{eq:root-space-decomp-of-x-again}, we have $0 = [y,x] = 0 + \sum \alpha(y) x_\alpha$.  Since $\alpha(y)$ is nonzero and the root space decomposition \eqref{eq:root-space-decomp} is a direct sum decomposition, it follows that $x_\alpha =0$ for all $\alpha$, whence that $x = x_0 \in \mathfrak{t}_{\mathbb{C}} \cap \mathfrak{k} = \mathfrak{t}$.  Therefore $Z_{\mathfrak{k}}(y) = \mathfrak{t}$.  By the noted connectivity, it follows that $Z_K(y) = T$.  Thus $w T \subseteq T$, i.e., $w = 1$ in $W$.  We conclude as required that $W_C = \{1\}$.
\end{proof}

We'll see later that the action of $W$ on the set of Weyl chambers is moreover \emph{transitive}; the above lemma will then tell us that it is simply-transitive, hence that the order $|W|$ of the Weyl group is equal to the number of Weyl chambers.  So far, we haven't given any way to \emph{produce} nontrivial elements of the Weyl group.  (For instance, why should $W$ be nontrivial when $K \neq T$?)  That's our next main objective.  The construction will involve the roots.  For each root $\alpha$ , we'll cook up a map $F_\alpha : \SU(2) \rightarrow K$.  We'll then use these maps to produce Weyl group elements, called \emph{root reflections}, which we'll eventually show generate the Weyl group.

\subsection{Notation and preliminaries concerning \texorpdfstring{$\SU(2)$}{SU(2)}}
Take $K = \SU(2)$, so that $\mathfrak{k} = \suLie(2)$, $G = \SL_2(\mathbb{C})$, $\mathfrak{g} = \slLie_2(\mathbb{C})$.  Let $T \leq K$ denote the standard maximal torus $\{\diag(t,t^{-1}) : t \in \U(1)\}$, so that $\mathfrak{t} \leq \mathfrak{k}$ and $\mathfrak{t}_{\mathbb{C}} \leq \mathfrak{g}$ are the diagonal subalgebras.  The map $\theta : \mathfrak{g} \rightarrow \mathfrak{g}$ given as usual by $\theta(x) = - x^* = - \overline{x}^t$ has the property that $\mathfrak{k}$ is the $\theta$-fixed subspace of $\mathfrak{g}$.  Define $X,Y,H \in \mathfrak{g}$ by
\begin{equation}\label{eq:defn-HXY}
  H := 
\begin{pmatrix}
    1 &  \\
    & -1
  \end{pmatrix}
, \quad X := 
\begin{pmatrix}
    0 & 1 \\
    0 & 0
  \end{pmatrix}
, \quad Y := 
\begin{pmatrix}
    0 & 0 \\
    1 & 0
  \end{pmatrix}
.
\end{equation}
Then
\begin{equation}\label{eq:relations-X-Y-H}
  [X,Y] = H,
  \quad
  [H, X] = 2 X,
  \quad
  [H, Y] = - 2 Y
\end{equation}
and
\begin{equation}\label{eq:relations-X-Y-H-theta}
  \theta(H) = - H,
  \quad
  \theta(X) = -Y,
  \quad
  \theta(Y) = -X.
\end{equation}
These properties give a ``presentation'' for $\suLie(2)$ in that for any pair $(\mathfrak{g} ', \theta')$ consisting of a three-dimensional Lie algebra $\mathfrak{g} '$ equipped with an antilinear involution $\theta'$ that admits a basis $H', X', Y'$ satisfying the analogue of \eqref{eq:relations-X-Y-H} and \eqref{eq:relations-X-Y-H-theta}, the map $\slLie_2(\mathbb{C}) \rightarrow \mathfrak{g} '$ given by $H \mapsto H', X \mapsto X', Y \mapsto Y'$ is an isomorphism of complex Lie algebras that intertwines $\theta$ with $\theta '$, hence induces an isomorphism of real Lie algebras $\suLie(2) \rightarrow \mathfrak{k} ' := \{x \in \mathfrak{g} ' : \theta '(x) = x\}$.

We have
\begin{equation*}
  \mathfrak{t}_{\mathbb{C}} = \mathbb{C} H, \quad \mathfrak{t}_{\mathbb{R}} = \mathbb{R} H, \quad \mathfrak{t}_{\mathbb{Z}} = \mathbb{Z} H.
\end{equation*}
(For the last of these, we note that every one-parameter subgroup $\U(1) \rightarrow T$ is given by $z \mapsto \diag(z^m, z^{-m})$ for some $m \in \mathbb{Z}$; we may rewrite this as $\exp(i \theta) \mapsto \exp(i m H)$, whence the claim.)  In view of \eqref{eq:relations-X-Y-H}, the root space decomposition of $\mathfrak{g}$ is given by
\begin{equation}\label{eq:}
  \mathfrak{g} =
  \underbrace{\mathbb{C} H}_{\mathfrak{t}_{\mathbb{C}}}
  \oplus
  \underbrace{\mathbb{C} X}_{\mathfrak{g}^{\alpha}}
  \oplus
  \underbrace{\mathbb{C} Y}_{\mathfrak{g}^{-\alpha}},
\end{equation}
where $\alpha \in \mathfrak{t} _{\mathbb{R} }^*$ is characterized by
\begin{equation}\label{eq:}
  \alpha(H) = 2.
\end{equation}
The set of roots is thus
\begin{equation}\label{eq:}
  \Phi = \{\alpha, - \alpha\}.
\end{equation}
We note that
\begin{equation}\label{eq:}
  \mathfrak{t}_{\mathbb{C}}^* = \mathbb{C} \alpha,
  \quad
  \mathfrak{t}_{\mathbb{R}}^* = \mathbb{R} \alpha,
  \quad
  \mathfrak{t}_{\mathbb{Z}}^*
  =  \tfrac{1}{2}\mathbb{Z} \alpha.
\end{equation}
(For the last of these, note that the characters of $T$ are given by $\diag(t,t^{-1}) \mapsto t^{l}$ for some $l \in \mathbb{Z}$, and that if we write $\diag(t,t^{-1}) = \exp(i \theta H)$, then $t^l = \exp(i \theta l) = \exp(i \theta l \alpha(H)/2) = \exp(i \theta H)^{l \alpha/2}$, whence the characters of $T$ are uniquely of the form $l \alpha/2$ with $l \in \mathbb{Z}$.)

Recall (\S\ref{sec:case-su2} and earlier) that any finite-dimensional representation $\pi : \SU(2) \rightarrow \GL(V)$ decomposes as a sum
\begin{equation*}
  V = \oplus_{k \in \mathbb{Z}} V^k
\end{equation*}
of weight spaces
\begin{equation*}
  V^k := \{v \in V : \diag(t,t^{-1}) v = t^k v \text{ for all } t \in \U(1) \}.
\end{equation*}
For example, if $(\pi,V) = (\Ad,\mathfrak{g})$, then $V^0 = \mathbb{C} H = \mathfrak{t}_{\mathbb{C}}, V^2 = \mathbb{C} X = \mathfrak{g}^{\alpha}, V^{-2} = \mathbb{C} Y = \mathfrak{g}^{-\alpha}$.  In terms of the differentiated representation $d \pi : \suLie(2) \rightarrow \End(V)$ and its complexification $d \pi : \slLie_2(\mathbb{C}) \rightarrow \End(V)$, we may write
\begin{equation*}
  V^k = \{v \in V : d \pi(H) v = k v \},
\end{equation*}
that is to say, the weight space decomposition of $(\pi,V)$ is simply its decomposition into eigenspaces for the operator $d \pi(H)$.  Note in particular that every eigenvalue of $d \pi(H)$ is an \emph{integer}.

For simplicity of notation we will often use abbreviations like $x v := d \pi(x) v$ for $x \in \mathfrak{g}$ and $v \in V$ when $\pi$ is clear from context.

In view of the general identity ``$\mathfrak{g}^\alpha \cdot V^{\lambda} \subseteq V^{\lambda+\alpha}$'' (Lemma \ref{lem:root-spaces-permute-weight-spaces}) and the fact that $\alpha(H) = 2$, we see that $X$ and $Y$ act on the $H$-eigenspaces by ``raising and lowering operators,'' i.e.,
\begin{equation}\label{eq:}
  d \pi(X) : V^k \rightarrow V^{k+2}, \quad d \pi(Y) : V^k
  \rightarrow V^{k-2}.
\end{equation}
For instance, this follows in the case $(\pi,V) = (\Ad,\mathfrak{g})$ from the identities \eqref{eq:relations-X-Y-H}.

We have also seen that the irreducible finite-dimensional representation of $\SU(2)$ are indexed by nonnegative integers $n$ and given explicitly by the $(n+1)$-dimensional space $V_n = \mathbb{C}[x,y]^{(n)}$ of homogeneous polynomials of degree $n$ in two variables, with $\SU(2)$ acting by right translation.  Moreover, any finite-dimensional representation $(\pi,V)$ of $\SU(2)$ decomposes uniquely as a finite direct sum $V = \oplus_{n \geq 0} V_n^{\oplus \mu(n)}$ for some multiplicities $\mu(n) \geq 0$,

The weight spaces of the irreducible representations $(\pi_n,V_n)$ are all one-dimensional, and the weight space decompositions read
\begin{align}\label{eq:}
  V_n
  &=
    V_n^{n} \oplus V_n^{n-2} \oplus V_n^{n-2} \oplus \dotsb
    \oplus V_{n}^{-n}
  \\
  &=
    \mathbb{C} x^n
    \oplus \mathbb{C} x^{n-1} y
    \oplus \mathbb{C} x^{n-2} y^2
    \oplus \dotsb \oplus
    \mathbb{C} y^n.
\end{align}
Using the definition \eqref{eq:defn-HXY}, we compute that the action of $\mathfrak{g}$ is given by
\begin{equation}\label{eq:}
  d \pi_n(H) = x \partial_x - y \partial _y,
  \quad d \pi_n(X)
  = x \partial_y,
  \quad
  d \pi_n(Y) = y \partial_x.
\end{equation}
It follows readily from this explicit description that the lowering maps
\begin{equation}\label{eq:}
  V_n^n \xrightarrow{Y} V_n^{n-2}
  \xrightarrow{Y} V_n^{n-2} \xrightarrow{Y} \dotsb
  \xrightarrow{Y} V_n^{-n}
\end{equation}
and the raising maps
\begin{equation}\label{eq:X-raising-isoms}
  V_n^{-n} \xrightarrow{X} V_n^{-n+2}
  \xrightarrow{X} V_n^{-n+4} \xrightarrow{X} \dotsb
  \xrightarrow{X} V_n^{n}
\end{equation}
are all isomorphisms of one-dimensional vector spaces.  Indeed, they send each monomial to an explicit nonzero multiple of another monomial.  Of course $d \pi(X) V_n^n$ and $d \pi(Y) V_n^{-n}$ are trivial, because the weight spaces $V_n^{n+2}$ and $V_n^{-n-2}$ are trivial.

Here's a typical application of the above explication of the representation theory of $\SU(2)$; this application will turn out to be the key for showing that the root spaces of a general compact Lie group are one-dimensional.
\begin{lemma}\label{lem:key-for-root-space-one-dimensional}
  Let $V$ be a finite-dimensional representation of $\SU(2)$ such that
  \begin{enumerate}
[(i)]
  \item all weights (i.e., eigenvalues of $H$) are even, and
  \item the weight space $V^0$ is one-dimensional.
  \end{enumerate}
  Then $V$ is irreducible, say $V \cong V_n$.  If moreover there exists a nonzero vector $v \in V^2$ such that $X v = 0$, then $n = 2$.
\end{lemma}
\begin{proof}
  Since the weights are even, when we decompose $V = \oplus V_n^{\oplus \mu(n)}$ as a sum of irreducibles $V_n$, only those with $n$ even occur with positive multiplicity $\mu(n)$.  For $n$ even, we have $\dim V_n^0 = 1$, thus $1 = \dim V^0 = \sum_n \mu(n)$.  Thus $\mu(n) = 1$ for some $n$ and vanishes for all other values, and so $V \cong V_n$.

  Suppose there exists $v$ with the stated property.  Since $v$ is nonzero and $X$ annihilates $v$, we see (from the assertion following \eqref{eq:X-raising-isoms}) that $n = 2$.
\end{proof}
\footnote{End of lecture \#13, Thursday, 4 Apr}

Similarly:
\begin{lemma}\label{lem:key-for-rational-multiples-of-roots}
  Let $V$ be a finite-dimensional representation of $\SU(2)$ that has some odd weight, i.e., the weight space $V^n$ is nonzero for some odd integer $n$.  Then the weight space $V^1$ is nonzero.
\end{lemma}
\begin{proof}
  We decompose $V = \oplus_{n \geq 0} V_n^{\oplus \mu(n)}$ into irreducibles.  Since the weights of $V_n$ all have the same parity as $n$, we see that $\mu(n)$ is positive for some odd $n$.  But for such $n$, the weights of $V_n$ are $n,n-2,n-4,\dotsc,3,1,-1,-3,\dotsc,-n$.  Thus $\dim V_n^1 > 0$ and so $\dim V^1 \geq \mu(n) \dim V_n^1 > 0$, as required.
\end{proof}

\subsection{From roots to distinguished \texorpdfstring{$\SU(2)$}{SU(2)}'s}\label{sec:from-roots-dist}
We're now ready to state and prove one of the most important theorems concerning the structure of compact Lie groups.

Recall the notation \eqref{eq:defn-HXY} for the standard basis elements $H,X,Y$ of $\slLie_2(\mathbb{C})$.
\begin{theorem}\label{thm:from-roots-dist-su2}
  Let $K$ be a compact connected Lie group, $T \leq K$ a maximal torus, $\Phi = \Phi(K:T)$ the associated set of roots, and $\mathfrak{g} = \mathfrak{k}_{\mathbb{C}}$ the complexified Lie algebra with the usual root space decomposition.  Let $\alpha \in \Phi \subseteq \mathfrak{t}_{\mathbb{Z}}^*$ be a root.
  \begin{enumerate}
  \item There is a morphism of Lie groups
    \begin{equation*}
      F_\alpha : \SU(2) \rightarrow K
    \end{equation*}
    whose differential $d F_\alpha : \su(2) \rightarrow \mathfrak{k}$ has complexification $d F_\alpha : \slLie_2(\mathbb{C}) \rightarrow \mathfrak{g}$ satisfying
    \begin{equation*}
      d F_\alpha (X) \in \mathfrak{g}^\alpha.
    \end{equation*}
  \item There is a cocharacter $\alpha^\vee \in X^\vee(T) = \Hom(\U(1),T)$ so that for each such $F_\alpha$, we have a commutative diagram
    \begin{equation}\label{eq:F-alpha-alpha-vee-commute}
      \begin{CD}         
        \U(1) @> \alpha^{\vee} >> T\\
        @V t \mapsto \diag(t, 1/t) VV  @VV V \\
        \SU(2) @>> F_{\alpha} > K.
      \end{CD}
    \end{equation}
    The cocharacter $\alpha^\vee$ is independent of the choice of $F_\alpha$.  More precisely, let
    \begin{equation*}
      H_\alpha \in \mathfrak{t}_{\mathbb{Z}} \cong X^\vee(T)
    \end{equation*}
    correspond to $\alpha^\vee$ in the sense of \S\ref{sec:notat-relat-torus}, thus
    \begin{equation*}
      \alpha^\vee(e^{i \theta}) = \exp(i \theta H_\alpha)
    \end{equation*}
    for all $\theta \in \mathbb{R}$.  Then $H_\alpha$ is the unique element of $[\mathfrak{g}^{\alpha}, \mathfrak{g}^{-\alpha}]$ such that $\alpha(H_\alpha) = 2$.
  \item $\dim(\mathfrak{g}^\alpha) = 1$.
  \item $\mathbb{C} \alpha \cap \Phi = \mathbb{Q} \alpha \cap \Phi = \{\alpha, - \alpha \}$.
  \item $F_\alpha$ is unique up to conjugation by the image of $\alpha^\vee$.
  \item \label{item:image-F-alpha} Let $T_\alpha \leq T$ denote the codimension one subtorus defined as the connected component of the kernel of $\alpha$, thus
    \begin{equation*}
      T_\alpha := \ker(\alpha)^0 = \{t \in T : t^\alpha = 1\}^0, \quad \mathfrak{t}_\alpha := \Lie(T_\alpha) = \{x \in \mathfrak{t} : \alpha(x) = 0\}.
    \end{equation*}
    Then the image of $F_\alpha$ is contained in the centralizer $Z_K(T_\alpha)$, and the composition
    \begin{equation}\label{eq:}
      \SU(2) \xrightarrow{F_\alpha } Z_K(T_\alpha)
      \rightarrow Z_K(T_\alpha)/T_\alpha
    \end{equation}
    is surjective, and induces an isomorphism $\su(2) \cong Z_{\mathfrak{k}}(\mathfrak{t}_\alpha) / \mathfrak{t}_\alpha$ of Lie algebras.
  \end{enumerate}
\end{theorem}

The point is that a root $\alpha$ on its own is fairly useless -- there's not much you can do -- but once you know that it comes from some copy of $\SU(2)$, everything becomes possible.  If we learn anything from this part of the course, it's that \emph{we should always think of a root of a compact connected Lie group as coming with an associated copy of $\SU(2)$} in the above sense.

Let's illustrate with the example of $K = \U(3)$, with $T = \U(1)^3$ the standard diagonal maximal torus and $\alpha = \eps_1 - \eps_2$, so that $\mathfrak{g}^{\alpha} = \mathbb{C} E_{12 }$.  We may then take
\begin{equation*}
  F_\alpha (
\begin{pmatrix}
    a & b \\
    c & d
  \end{pmatrix}
) = 
\begin{pmatrix}
    a & b & 0 \\
    c & d & 0 \\
    0 & 0 & 1
  \end{pmatrix}
,
\end{equation*}
so that $d F_\alpha(X) = E_{12} \in \mathfrak{g}^\alpha$.  The composition of the standard cocharacter $\U(1) \xrightarrow{t \mapsto \diag(t,1/t)} \SU(2)$ with $F_\alpha$ is the cocharacter
\begin{equation*}
  \alpha^\vee : \U(1) \rightarrow T
\end{equation*}
\begin{equation*}
  t \mapsto 
\begin{pmatrix}
    t &  &  \\
    & 1/t &  \\
    & & 1
  \end{pmatrix}
\end{equation*}
of $K$, and we have $\alpha^\vee(e^{i \theta}) = \exp(i \theta H_\alpha)$ with
\begin{equation*}
  H_\alpha = 
\begin{pmatrix}
    1 &  &  \\
    & -1 &  \\
    & & 0
  \end{pmatrix}
  = d F_\alpha(H).
\end{equation*}
We have $\Phi = \{\eps_i - \eps_j : i \neq j\}$, so it is clear that $\mathbb{Q} \alpha \cap \Phi = \{\alpha, - \alpha \}$.  We compute using the Lie algebra that
\begin{equation*}
  T_\alpha = \left\{ 
\begin{pmatrix}
      z &  &  \\
      & z &  \\
      & & t
    \end{pmatrix}
 : z, t \in \U(1) \right\},
\end{equation*}
\begin{equation*}
  Z_K(T_\alpha) = \left\{ 
\begin{pmatrix}
      a & b &  \\
      c & d &  \\
      & & t
    \end{pmatrix}
 : 
\begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
 \in \U(2), t \in \U(1) \right\}.
\end{equation*}
We may verify that $F_\alpha : \SU(2) \rightarrow Z_K(T_\alpha)/T_\alpha$ is surjective with kernel $\{\pm 1\}$, and in any event that $d F_\alpha$ defines an isomorphism of Lie algebras (which is simplest to check first for the complexified Lie algebras).

If we were instead given $\alpha = \eps_{1} - \eps_3$, then we'd take
\begin{equation*}
  F_\alpha (
\begin{pmatrix}
    a & b \\
    c & d
  \end{pmatrix}
) = 
\begin{pmatrix}
    a & 0 & b \\
    0 & 1 & 0 \\
    c & 0 & d
  \end{pmatrix}
.
\end{equation*}

\subsection{Classification of rank one groups}
Before proving Theorem \ref{thm:from-roots-dist-su2}, let's illustrate its power with a typical application.
\begin{definition}
  The \emph{rank} of a compact connected Lie group $K$ is defined to be the dimension of any maximal torus $T$ (which is well-defined by Theorem \ref{thm:conj-max-tori}, for instance):
  \begin{equation*}
    \rank(K) := \dim(T).
  \end{equation*}
\end{definition}
For instance, $\rank(\U(n)) = n$, while $\rank(\SU(n)) = n-1$.

It's clear from Lemma \ref{lem:maximal-tori-vs-maxl-ab-subalg} that every connected compact Lie group of rank $0$ is trivial.  The case of rank $1$ is more interesting:
\begin{corollary}\label{cor:classification-compact-lie-gps-rank-1}
  Every connected compact Lie groups $K$ of rank $1$ is isomorphic to exactly one of the following:
  \begin{itemize}
  \item $\U(1)$
  \item $\SU(2)$
  \item $\SU(2) / \{\pm 1\} \cong \SO(3)$
  \end{itemize}
\end{corollary}
\begin{proof}
  Let $T \leq K$ be a maximal torus and $\mathfrak{g} = \mathfrak{k}_{\mathbb{C}}$, with $\Phi$ the set of roots.  If $\Phi$ is empty, then the root space decomposition implies that $\mathfrak{g} = \mathfrak{t}_{\mathbb{C}}$, hence that $\mathfrak{k} = \mathfrak{t}$; since $K$ is connected, this forces $K = T \cong \U(1)$.  Suppose otherwise that there exists some $\alpha \in \Phi$.  Then the codimension $1$ subtorus $T_\alpha \leq T$ defined in Theorem \ref{thm:from-roots-dist-su2} has dimension $\dim(T_\alpha) = \dim(T) - 1 = 0$, and so $T_\alpha = \{1\}$.  Thus $Z_K(T_\alpha) = K$ and $Z_K(T_\alpha)/T_\alpha \cong K$.  Thus any map $F_\alpha : \SU(2) \rightarrow K$ as in Theorem \ref{thm:from-roots-dist-su2} defines a Lie algebra isomorphism $\su(2) \rightarrow K$.  Since $K$ is connected, $F_\alpha$ is surjective.  Since $d F_\alpha$ is injective, the kernel of $F_\alpha$ is a (discrete) subgroup of the center $\{\pm 1\}$ of $\SU(2)$ (see e.g. \S20 and especially Lemma 154 of my notes on Lie groups, on the course homepage; the general fact being used here is that if $f : G \rightarrow H$ is a morphism of Lie groups with $d f$ injective and $G$ connected, then $\ker(f)$ is a discrete subgroup of the center of $G$).  Thus either $K \cong \SU(2)$ or $K \cong \SU(2) / \{\pm 1\}$; the two cases are distinguished by (for instance) their centers.
\end{proof}

In fact, this application is representative of the power of Theorem \ref{thm:from-roots-dist-su2} in the sense that one could go backwards and derive much of Theorem \ref{cor:classification-compact-lie-gps-rank-1} assuming Corollary \ref{cor:classification-compact-lie-gps-rank-1}.  We will not do this here, but it's done that way in some references, e.g., the BTD course reference.

\subsection{Some invariant inner products}\label{sec:some-invariant-inner}
Before turning to the proof of Theorem \ref{thm:from-roots-dist-su2}, we pause to choose some inner products concerning the objects in its statement.  (The formulation of Theorem \ref{thm:from-roots-dist-su2} is independent of this choice, but the proof will not be.)  Choose an embedding $K \hookrightarrow \U(n)$ with respect to which $T$ is contained in the diagonal subgroup (see \S\ref{sec:notat-relat-torus} for details on this and what follows).  This choice defines an embedding $\mathfrak{g} \hookrightarrow \glLie_n(\mathbb{C})$ with respect to which $\mathfrak{t}_{\mathbb{R}}$ is contained in the subspace of diagonal matricse with real entries.  We have the usual involution $\theta : \glLie_n(\mathbb{C}) \rightarrow \glLie_n(\mathbb{C})$, preserving $\mathfrak{g}$, and given by $\theta(x) := - x^*$, where $x^* := \overline{x}^t$.  Then:
\begin{itemize}
\item The map
  \begin{equation*}
    \glLie_n(\mathbb{C}) \times \glLie_n(\mathbb{C}) \rightarrow \mathbb{R}
  \end{equation*}
  \begin{equation*}
    (x,y) \mapsto - \trace(x \theta(y))
  \end{equation*}
  defines an inner product.  Indeed,
  \begin{equation*}
    - \trace(x \theta(y)) = \trace(x y^*) = \sum_{i,j} x_{i j} \overline{y_{i j}}.
  \end{equation*}
  In particular, this map restricts to an inner product on $\mathfrak{g}$.
\item The map
  \begin{equation*}
    \mathfrak{t}_{\mathbb{R}} \times \mathfrak{t}_{\mathbb{R}} \rightarrow \mathbb{R}
  \end{equation*}
  \begin{equation*}
    (x,y) \mapsto \trace( x y)
  \end{equation*}
  is an inner product.  Indeed, for $y \in \mathfrak{t}_{\mathbb{R}}$, we have $y = - \theta(y)$, so this is just the restriction of the inner product defined above on $\glLie_n(\mathbb{C})$; explicitly, it is just the obvious inner product given in terms of the diagonal entries by the formula $\trace(x y) = \sum_{i} x_{i i} \overline{y_{i i}}$.
\item The inner product just defined on $\mathfrak{t}_{\mathbb{R}}$ defines an isomorphism
  \begin{equation*}
    \mathfrak{t}_{\mathbb{R}}^* \rightarrow \mathfrak{t}_{\mathbb{R}}
  \end{equation*}
  \begin{equation*}
    \lambda \mapsto u_\lambda
  \end{equation*}
  characterized by the identity
  \begin{equation}\label{eq:characterize-u-lambda}
    \lambda(z) = \trace(u_\lambda z)
  \end{equation}
  for all $z \in \mathfrak{t}_{\mathbb{R}}$.  The same identity extends $\mathbb{C}$-linearly to all $z \in \mathfrak{t}_{\mathbb{C}}$.
\item Using the above isomorphism, we may transport the given inner product on $\mathfrak{t}_{\mathbb{R}}$ to an inner product on $\mathfrak{t}_{\mathbb{R}}^*$ that we denote by $(,)$: for $\lambda_1, \lambda_2 \in \mathfrak{t}_{\mathbb{R}}^*$,
  \begin{equation}\label{eq:}
    (\lambda_1, \lambda_2) := \trace(u_{\lambda_1} u_{\lambda_2}).
  \end{equation}
  For example, if $K = \U(n)$, then under the identification $\mathfrak{t}_{\mathbb{R}}^* \cong \mathbb{R}^n$ as in \S\ref{sec:notat-relat-torus}, this inner product is the standard one.
\end{itemize}
We verify readily that the inner product that we defined on $\glLie_n(\mathbb{C})$ is $\Ad(\U(n))$-invariant, hence that on $\mathfrak{g}$ is $\Ad(K)$-invariant and that on $\mathfrak{t}_{\mathbb{R}}$ is $W$-invariant.

\subsection{Proofs concerning the passage from roots to \texorpdfstring{$\SU(2)$}{SU(2)}'s}\label{sec:proofs-conc-pass}
With these preliminaries out of the way, we're now prepared to give the:
\begin{proof}
[Proof of Theorem \ref{thm:from-roots-dist-su2}]
  \begin{enumerate}
[(i)]
  \item We'll construct $F_\alpha$ in terms of generators and relations (see \eqref{eq:relations-X-Y-H} and \eqref{eq:relations-X-Y-H-theta}).  We start by constructing elements $X_\alpha, Y_\alpha, H_\alpha \in \mathfrak{g}$ that we eventually intend to be the images of $X,Y,H \in \slLie_2(\mathbb{C})$ under $d F_\alpha$.

    First, let $c > 0$ denote a suitable constant to be specified later (in \eqref{eq:definition-of-c-equals-2-over-alpha-alpha}), and choose any $X_\alpha \in \mathfrak{g}^{\alpha}$ whose norm $\|X_\alpha \|^2 := - \trace(X_\alpha \theta(X_\alpha))$ is equal to $c$.  Set $Y_\alpha := - \theta(X_\alpha) \in \mathfrak{g}^{-\alpha}$ and $H_\alpha := [X_\alpha, Y_\alpha] \in \mathfrak{t}_{\mathbb{C}}$.  We get a linear map
    \begin{equation*}
      ``d F_\alpha'' : \slLie_2(\mathbb{C}) \rightarrow \mathfrak{g}
    \end{equation*}
    \begin{equation*}
      X \mapsto X_\alpha, Y \mapsto Y_\alpha, H \mapsto H_\alpha.
    \end{equation*}

    We want the above map to define a $\theta$-equivariant Lie algebra morphism.  For this to be the case, the non-obvious relations to be verified are that
    \begin{equation}\label{eq:HXalpha-2Xalph}
      [H_\alpha, X_\alpha] = 2 X_\alpha,
    \end{equation}
    \begin{equation}\label{eq:HYalpha-2Yalph}
      [H_\alpha, Y_\alpha] = - 2 Y_\alpha,
    \end{equation}
    \begin{equation}\label{eq:thetaHalphanegAlpha}
      \theta(H_\alpha) = - H_\alpha.
    \end{equation}
    Since $[H_\alpha,X_\alpha] = \alpha(H_\alpha) X_\alpha$ and $[H_\alpha,Y_\alpha] = - \alpha(H_\alpha) Y_\alpha$, the two relations \eqref{eq:HYalpha-2Yalph} are equivalent to the single relation
    \begin{equation}\label{eq:alpha-H-alpha-equals-2}
      \alpha(H_\alpha) = 2,
    \end{equation}
    while the third relation \eqref{eq:thetaHalphanegAlpha} is equivalent to requiring that
    \begin{equation}\label{eq:H-alpha-in-t-R}
      H_\alpha \in \mathfrak{t}_{\mathbb{R}}.
    \end{equation}

    Before checking the above relations, we note that for all $x \in \mathfrak{g}^{\alpha}, y \in \mathfrak{g}^{-\alpha}$ and $z \in \mathfrak{t}_{\mathbb{C}}$, we have
    \begin{equation}\label{eq:trace-x-y-z-alpha}
      \trace([x,y] z) = \alpha(z) \trace(x y).
    \end{equation}
    Indeed, since the trace of a product of two matrices doesn't change if we swap the two matrices, we have $\trace([x,y] z) = \trace(z[x,y]) = \trace(z x y - z y x) = \trace(z x y - x z y) = \trace([z,x] y)$, and since $[z,x] = \alpha(z)$ the required identity follows.  By the characterizing property \eqref{eq:characterize-u-lambda} of $u_\alpha$, we may rewrite \eqref{eq:trace-x-y-z-alpha} as the identity
    \begin{equation}\label{eq:x-comm-y-vs-trace-vs-u-alpha}
      [x,y] = \trace(x y) u_\alpha
    \end{equation}
    of elements of $\mathfrak{t}_{\mathbb{C}}$.

    Applying \eqref{eq:x-comm-y-vs-trace-vs-u-alpha} to $X_\alpha$ and $Y_\alpha$ gives
    \begin{equation}\label{eq:}
      H_\alpha = [X_\alpha, Y_\alpha ]
      = \trace(X_\alpha Y_\alpha) u_\alpha
      = \|X_\alpha \|^2 u_\alpha
      = c u_\alpha.
    \end{equation}
    using in the last two steps the definitions of $\|X_\alpha \|^2$ and of $Y_\alpha$.  On the other hand, the definition of $u_\alpha$ gives
    \begin{equation}\label{eq:}
      \alpha(u_\alpha) = \trace(u_\alpha u_\alpha)
      = (\alpha,\alpha).
    \end{equation}
    Combining the above two identites, we obtain $\alpha(H_\alpha) = c (\alpha,\alpha)$.  With the choice
    \begin{equation}\label{eq:definition-of-c-equals-2-over-alpha-alpha}
      c := \frac{2}{(\alpha,\alpha)},
    \end{equation}
    we obtain the required relation \eqref{eq:alpha-H-alpha-equals-2}.  Moreover, we have $H_\alpha = c u_\alpha \in \mathbb{R} u_\alpha \subseteq \mathfrak{t}_{\mathbb{R}}$, so \eqref{eq:H-alpha-in-t-R} holds.

    This completes the proof that the linear map ``$d F_\alpha$'' defines a $\theta$-equivariant $\mathbb{C}$-linear Lie algebra morphism $\slLie_2(\mathbb{C}) \rightarrow \mathfrak{g}$.  By passage to $\theta$-fixed subspaces, we obtain a Lie algebra morphism $\su(2) \rightarrow \mathfrak{k}$.  Since $\SU(2)$ is simply-connected, this lifts to a Lie group morphism $F_\alpha : K \rightarrow \SU(2)$, whose differential $d F_\alpha$ is the map ``$d F_\alpha$'' that we constructed.
  \item The composition
    \begin{equation}\label{eq:}
      \alpha^\vee : \U(1) \rightarrow \SU(2)
      \xrightarrow{F_\alpha } K
    \end{equation}
    is given by
    \begin{equation}\label{eq:}
      e^{i \theta}
      \mapsto
      \exp (i \theta d F_\alpha(H))
      =
      \exp(i \theta H_\alpha)
      \in T,
    \end{equation}
    so the diagram \eqref{eq:F-alpha-alpha-vee-commute} is defined and commutes.  Finally, we see from \eqref{eq:x-comm-y-vs-trace-vs-u-alpha} that
    \begin{equation}\label{eq:commutators-land-in-C-H-alpha}
      [\mathfrak{g}^\alpha, \mathfrak{g}^{-\alpha}]
      \subseteq \mathbb{C} u_\alpha
      = \mathbb{C} H_\alpha,
    \end{equation}
    so that $H_\alpha = [X_\alpha, Y_\alpha]$ admits the required characterization as the unique element of $[\mathfrak{g}^\alpha, \mathfrak{g}^{-\alpha}]$ for which $\alpha(H_\alpha) = 2$.
  \item Define
    \begin{equation}\label{eq:defn-V-relevant-for-dim-g-alpha-1}
      V := \mathbb{C} H_\alpha \oplus (\oplus_{0 \neq n \in
        \mathbb{Z} }
      \mathfrak{g}^{n \alpha}),
    \end{equation}
    and set $\slLie_2(\mathbb{C})_\alpha := d F_\alpha(\slLie_2(\mathbb{C}))$.  We may verify then $\slLie_2(\mathbb{C})_\alpha$ that acts $\mathbb{C}$-linearly on $V$ by the restriction of the adjoint representation $\ad$:
    \begin{itemize}
    \item $\ad_{H_\alpha}$ preserves each summand in \eqref{eq:defn-V-relevant-for-dim-g-alpha-1}, acting on $\mathbb{C} H_\alpha$ by the eigenvalue $0$ and on each $\mathfrak{g}^{n \alpha}$ by the eigenvalue $n \alpha(H_\alpha) = 2 n$.
    \item $\ad_{X_\alpha}(\mathfrak{g}^{n \alpha}) \subseteq \mathfrak{g}^{(n+1) \alpha}$ for all integers $n$.  Moreover, $\ad_{X_\alpha}(\mathfrak{g}^{-\alpha}) \subseteq \mathbb{C} H_\alpha$ thanks to \eqref{eq:commutators-land-in-C-H-alpha}.  Thus $\ad_{X_\alpha}$ maps $V$ to $V$.
    \item Similarly, $\ad_{Y_\alpha}(\mathfrak{g}^{n \alpha}) \subseteq \mathfrak{g}^{(n-1) \alpha}$ for all integers $n$, and $\ad_{Y_\alpha}(\mathfrak{g}^\alpha) \subseteq \mathbb{C} H_\alpha$.
    \end{itemize}
    Thus $V$ defines a finite-dimensional $\mathbb{C}$-linear representation of $\slLie_2(\mathbb{C})$, hence by Theorem \eqref{thm:unitary-trick} a finite-dimensional representation of $\SU(2)$.  This representation has the property that all weights (i.e., $H_\alpha$-eigenvalues) are even, the weight zero subspace $\mathbb{C} H_\alpha$ is one-dimensional, and the weight two subspace $\mathfrak{g}^{\alpha}$ contains the vector $X_\alpha$ for which $\ad_{X_\alpha} X_\alpha = 0$.  By Lemma \ref{lem:key-for-root-space-one-dimensional}, we deduce that $V$ is isomorphic to the irreducible three-dimensional representation $V_2$ of $\SU(2)$, whose weights are $\{0,2,-2\}$ each occurring with multiplicity one.  Thus
    \begin{equation}\label{eq:}
      \dim \mathfrak{g}^{\alpha}
      =
      \dim \mathfrak{g}^{-\alpha}
      = 1
    \end{equation}
    and
    \begin{equation}\label{eq:first-exclusion-dimensions-g-n-alpha}
      \dim \mathfrak{g}^{n \alpha} = 0 \text{ for } n \in \mathbb{Z} - \{0,1,-1\}.
    \end{equation}
  \item We now take instead
    \begin{equation}\label{eq:}
      V = \mathbb{C} H_\alpha
      \oplus (\oplus_{0 \neq n \in \mathbb{C} \alpha }
      \mathfrak{g}^{n \alpha}).
    \end{equation}
    (We might note in passing that, since the set $\Phi$ of roots is contained in the finite free $\mathbb{Z}$-module $\mathfrak{t}_{\mathbb{Z}}^*$, the sum over $n$ could be restricted from the start to $\mathbb{Q} \alpha$.)  We note as before that $\slLie_2(\mathbb{C})_\alpha$ acts on $V$ by the restriction of $\ad$.  Then $V$ defines a finite-dimensional representation of $\slLie_2(\mathbb{C})$, and so, as we have seen, all of the weights of $\ad_{H_\alpha}$ must be integers.  But the eigenvalue of $\ad_{H_\alpha}$ on $\mathfrak{g}^{n \alpha}$ is $n \alpha(H_\alpha) = 2 n$, which is an integer only if $n \in (1/2) \mathbb{Z}$.  We've seen already in \eqref{eq:first-exclusion-dimensions-g-n-alpha} that for $n \in \mathbb{Z}$, we have $n \alpha \notin \Phi$ unless $n = \pm 1$, so it remains only to consider the case that $n \alpha \in \Phi$ for some $n \in (1/2) \mathbb{Z} - \mathbb{Z}$.  But then $2 n$ is odd, so $\mathfrak{g}^{n \alpha}$ is a nonzero weight space for $V$ with odd weight, so by Lemma \ref{lem:key-for-rational-multiples-of-roots}, we see that the weight one subspace $V^1 = \mathfrak{g}^{\alpha/2}$ is nonzero, i.e., that $\alpha/2$ is a root.  But then $\alpha/2$ and $\alpha = 2 (\alpha/2)$ are both roots, which contradicts \eqref{eq:first-exclusion-dimensions-g-n-alpha} (applied to $\alpha/2$ in place of $\alpha$).  The proof that $\mathbb{C} \alpha \cap \Phi = \mathbb{Q} \alpha \cap \Phi = \{\alpha, -\alpha \}$ is now complete.\footnote{End of lecture \#14, Tuesday, 9 Apr}
    
  \item The uniqueness of $F_\alpha$ up to conjugation by $\alpha^\vee$ follows from the recently established one-dimensionality of $\mathfrak{g}^\alpha$ and the construction of $F_\alpha$.  Since we required $\|X_\alpha \|^2 = 2/(\alpha,\alpha)$ and the choices of $Y_\alpha$ and $H_\alpha$ were then forced by the requirement that $d F_\alpha$ be a $\theta$-equivariant Lie algebra morphism, any other map $F_\alpha '$ satisfying the same conditions as $F_\alpha$ is obtained by replacing $X_\alpha$ as in the construction of $F_\alpha$ with its multiple by some element $e^{i \theta} \in \U(1)$.  Since $[H_\alpha, X_\alpha] = 2 X_\alpha$, we have
    \begin{equation}\label{eq:}
      \Ad(\alpha^\vee(e^{i \theta/2}))
      X_\alpha 
      =
      \Ad(\exp(i \theta H_\alpha/2))
      X_\alpha
      =
      e^{i \theta} X_\alpha,
    \end{equation}
    which implies that $F_\alpha '$ is the conjugate of $F_\alpha$ by $\alpha^\vee(e^{i \theta/2})$, as required.
  \item We have $\mathfrak{t}_{\alpha} = \{x \in \mathfrak{t} : \alpha(x) =0\}$ and and $\Lie(Z_K(T_\alpha)) = Z_{\mathfrak{k}}(\mathfrak{t}_\alpha)$ and $(Z_{\mathfrak{k}}(\mathfrak{t}_\alpha))_{\mathbb{C}} = Z_{\mathfrak{g}}(\mathfrak{t}_\alpha) = \{x \in \mathfrak{g} : [x,z] = 0 \text{ for all } z \in \mathfrak{t} \text{ with } \alpha(z) = 0\}$.  Since $[z,X_\alpha] = \alpha(z) X_\alpha$ and $[z,Y_\alpha] = -\alpha(z) Y_\alpha$ for all $z \in \mathfrak{t}$, and since $\mathfrak{t}_{\mathbb{C}}$ is commutative, we see that $X_\alpha, Y_\alpha, H_\alpha$ all belong to $Z_{\mathfrak{g}}(\mathfrak{t}_\alpha)$.  Since $\SU(2)$ is connected, it follows likewise that the image of $F_\alpha$ is contained in $Z_K(T_\alpha)$.  It remains only to check that the map
    \begin{equation*}
      d F_\alpha : \su(2) \rightarrow Z_{\mathfrak{k}}(\mathfrak{t}_\alpha)/\mathfrak{t}_\alpha
    \end{equation*}
    is an isomorphism, or equivalently, that
    \begin{equation}\label{eq:required-lie-alg-isom-involving-F-alpha}
      d F_\alpha : \slLie_2(\mathbb{C}) \rightarrow
      Z_{\mathfrak{g}}(\mathfrak{t}_\alpha)
      / (\mathfrak{t}_{\alpha})_{\mathbb{C}}
    \end{equation}
    is an isomorphism.  We note that $Z_{\mathfrak{g}}(\mathfrak{t}_\alpha)$ is a Lie subalgebra of $\mathfrak{g}$ that contains $\mathfrak{t}_{\mathbb{C}}$, hence admits a root space decomposition
    \begin{equation}\label{eq:}
      Z_{\mathfrak{g}}(\mathfrak{t}_\alpha)
      = \mathfrak{t}_{\mathbb{C}}
      \oplus (\oplus_{\beta \in B}
      \mathfrak{g}^\beta),
    \end{equation}
    where $B$ denotes the set of all roots $\beta \in \Phi$ for which $\mathfrak{g}^\beta \subseteq Z_{\mathfrak{g}}(\mathfrak{t}_\alpha)$, i.e., for which $\beta(z) = 0$ for all $z \in \mathfrak{t}$ with $\alpha(z) = 0$.  Since $\alpha$ and $\beta$ are both nonzero functionals, it follows that $\beta \in B$ iff $\beta \in \mathbb{C} \alpha$, which we've seen is equivalent to asking that $\beta = \pm \alpha$.  Since $\alpha(H_\alpha) \neq 0$, we have a decomposition
    \begin{equation}\label{eq:decompose-t-C-via-H-alpha}
      \mathfrak{t}_{\mathbb{C}} =
      (\mathfrak{t}_{\alpha})_{\mathbb{C}}
      \oplus \mathbb{C} H_\alpha.
    \end{equation}
    Thus
    \begin{equation}\label{eq:}
      Z_{\mathfrak{g}}(t_\alpha)
      = (\mathfrak{t}_\alpha)_{\mathbb{C}}
      \oplus \mathbb{C} H_\alpha
      \oplus \mathfrak{g}^{\alpha}
      \oplus \mathfrak{g}^{-\alpha}
    \end{equation}
    and so
    \begin{equation}\label{eq:}
      Z_{\mathfrak{g}}(\mathfrak{t}_\alpha)
      / (\mathfrak{t}_{\alpha})_{\mathbb{C}}
      \cong
      \mathbb{C} H_\alpha
      \oplus \mathfrak{g}^{\alpha} \oplus \mathfrak{g}^{-\alpha}.
    \end{equation}
    The composition
    \begin{equation}\label{eq:}
      \slLie_2(\mathbb{C}) \xrightarrow{d F_\alpha }
      Z_{\mathfrak{g}}(t_\alpha)
      \twoheadrightarrow Z_{\mathfrak{g}}(t_\alpha)/
      (t_\alpha)_{\mathbb{C}}
      \cong
      \mathbb{C} H_\alpha
      \oplus \mathfrak{g}^{\alpha} \oplus \mathfrak{g}^{-\alpha}
      \cong \slLie_2(\mathbb{C})
    \end{equation}
    is the identity, so we deduce as required that \eqref{eq:required-lie-alg-isom-involving-F-alpha} is an isomorphism.
  \end{enumerate}
\end{proof}

\subsection{Construction of root reflections}
We finally develop a way to produce nontrivial elements of the Weyl group.  Start with $K = \SU(2)$, with $T \cong \U(1)$ the standard diagonal maximal torus.  It follows from our earlier analysis of $\U(n)$ that the Weyl group $W$ of $T$ is isomorphic to $\mathbb{Z}/2$.  One can see this by noting that $T$ consists of those elements of $K$ that are diagonalized by the decomposition $\mathbb{C}^2 = \mathbb{C} e_1 \oplus \mathbb{C} e_2$ of the standard representation into the lines spanned by its standard basis elements.  Hence $N(T)$ acts faithfully on such lines, with $T$ acting trivially.  Thus $W$ embeds as a subgroup of the permutation group of $\{\mathbb{C} e_1, \mathbb{C} e_2\}$.  The element $w := 
\begin{pmatrix}
  & 1 \\
  -1 &
\end{pmatrix}
 \in K$ swaps the two lines, so in fact $W = \{e T, w T\}$ is the full permutation group.  The same proof shows that the Weyl group of each of $\U(n), \SU(n), \PU(n)$ is the symmetric group $S(n)$.  Anyway, we compute that with $H = 
\begin{pmatrix}
  1 &  \\
  & -1
\end{pmatrix}
 \in \slLie_2(\mathbb{C})$,
\begin{equation}\label{eq:}
  w \cdot H = -H.
\end{equation}
More generally, $w$ acts on each of the spaces $\mathfrak{t}, \mathfrak{t}_{\mathbb{Z}}, \mathfrak{t}_{\mathbb{R}}, \mathfrak{t}_{\mathbb{C}}, \mathfrak{t}^*, \mathfrak{t}_{\mathbb{Z}}^*, \mathfrak{t}_{\mathbb{R}}^*, \mathfrak{t}_{\mathbb{C}}^*$ by negation.  Moreover, for any finite-dimensional representation $V$ of $\SU(2)$ with weight space decomposition $V = \oplus_{\lambda \in \mathbb{Z}} V^\lambda$ (here $V^\lambda = \{v \in V : H v = \lambda v \}$, as usual), the action of $w$ defines an isomorphism
\begin{equation}\label{eq:}
  w : V^{\lambda} \xrightarrow{\cong }V^{-\lambda},
\end{equation}
as follows from a routine computation as in the proof of Lemma \ref{sec:weyl-group-acts-on-roots}.

Returning now to the general setting:
\begin{theorem}
  Let $K$ be a compact connected Lie group, with maximal torus $T$.  Let $\Phi = \Phi(K:T)$ denote the set of roots, and $W = N(T)/T$ the Weyl group.
  
  Let $\alpha \in \Phi$.  Let $F_\alpha : \SU(2) \rightarrow K$, as in Theorem \ref{thm:from-roots-dist-su2}, be such that $F_\alpha (
\begin{pmatrix}
    & 1 \\
    &
  \end{pmatrix}
)$ is a nonzero element of $\mathfrak{g}^{\alpha}$.  Define
  \begin{equation}\label{eq:}
    w_\alpha := F_\alpha (
\begin{pmatrix}
      0 & 1 \\
      -1 & 0
    \end{pmatrix}
).
  \end{equation}
  Then:
  \begin{enumerate}
[(i)]
  \item $w_\alpha$ belongs to the normalizer $N(T)$ of $T$, hence defines a Weyl group element that we denote by
    \begin{equation}\label{eq:}
      s_\alpha := w_\alpha T \in W.
    \end{equation}
    The element $s_\alpha$ is independent of the choice of $F_\alpha$.
  \item $s_\alpha^2 = 1$.
  \item For $x \in \mathfrak{t}_{\mathbb{C}}$,
    \begin{equation}\label{eq:}
      s_\alpha \cdot x = x - \alpha(x) H_\alpha
    \end{equation}
    and for $t \in T$,
    \begin{equation}\label{eq:}
      s_\alpha \cdot t = t / \alpha^\vee(t^\alpha).
    \end{equation}
  \item For $\lambda \in \mathfrak{t}_{\mathbb{C}}^*$,
    \begin{equation}\label{eq:}
      s_\alpha \cdot \lambda  = \lambda  - \lambda(H_\alpha) \alpha.
    \end{equation}
  \end{enumerate}
\end{theorem}
\begin{proof}
  We check readily that assertions (iii) and (iv) are equivalent, that either implies (ii), and that the claimed uniqueness of $s_\alpha$ follows from the established uniqueness of $F_\alpha$.  On the other hand, each of the assertions (i) and (iii) will follow if we can show that
  \begin{equation}\label{eq:}
    \Ad(w_\alpha) x = x - \alpha(x) H_\alpha 
  \end{equation}
  for all $x \in \mathfrak{t}_{\mathbb{C}}$.  To that end, we apply the decomposition \eqref{eq:decompose-t-C-via-H-alpha} to write $x = y + z H_\alpha$, where $y \in \mathfrak{t}_{\mathbb{C}}$ with $\alpha(y) = 0$ and $z \in \mathbb{C}$, namely $z = (1/2) \alpha(x)$.  By part (vi) of Theorem \ref{thm:from-roots-dist-su2}, the image of $F_\alpha$ commutes with $y$, hence $\Ad(w_\alpha) y = y$.  Set $w := 
\begin{pmatrix}
    & 1 \\
    -1 &
  \end{pmatrix}
 \in \SU(2)$.  Then
  \begin{equation}\label{eq:}
    \Ad(w_\alpha) H_\alpha
    = \Ad(F_\alpha(w)) F_\alpha(H)
    = F_\alpha(\Ad(w) H)
    = F_\alpha(-H) = - H_\alpha,
  \end{equation}
  so
  \begin{equation}\label{eq:}
    \Ad(w_\alpha) x = y - z H_\alpha
    = x - 2 z H_\alpha
    = x - \alpha(x) H_\alpha,
  \end{equation}
  as required.
\end{proof}
\footnote{End of half-lecture \#15, Thursday, 11 Apr}

The theorem and its proof imply that the action of $s_\alpha$ on $\mathfrak{t}_{\mathbb{R}}$ may be characterized as the unique linear map that restricts to the identity on $\ker(\alpha)$ and sends $H_\alpha$ to $- H_\alpha$; similarly, the action on $\mathfrak{t}_{\mathbb{R}}^*$ is the unique linear map that is given by the identity on $\ker(H_\alpha)$ (here $H_\alpha \in \mathfrak{t}_{\mathbb{R}} = (\mathfrak{t}_{\mathbb{R}}^*)^*$) and sends $\alpha$ to $-\alpha$.  Suppose now that we choose an embedding $K \hookrightarrow \U(n)$.  Then, as discussed in \S\ref{sec:some-invariant-inner}, we get an $\Ad(K)$-invariant inner product on $\mathfrak{g}$ given by $(x,y) \mapsto - \trace(x \theta(y))$ and hence a $W$-invariant inner product $\langle , \rangle$ on $\mathfrak{t}_{\mathbb{R}}$ given by $\langle x, y \rangle := \trace(x y)$.  We may use this inner product to define a duality isomorphism $\mathfrak{t}_{\mathbb{R}}^* \cong \mathfrak{t}_{\mathbb{R}}$, $\lambda \mapsto u_\lambda$, where as before $\langle u_\lambda, v \rangle = \lambda(v)$ for $v \in \mathfrak{t}_{\mathbb{R}}$, and also an inner product $\langle , \rangle$ on $\mathfrak{t}_{\mathbb{R}}^*$ given by $\langle \lambda_1, \lambda_2 \rangle := \langle u_{\lambda_1}, u_{\lambda_{2}} \rangle$.  Using such inner products, we may interpret the $s_\alpha$ geometrically as follows.
\begin{definition}
  Let $(V, \langle , \rangle)$ be a finite-dimensional real inner product space, and let $0 \neq v \in V$.  The \emph{reflection in $v$} is the linear map $r_v : V \rightarrow V$ that restricts to the identity on the orthogonal complement $v^\perp$ of $v$ and that sends $v$ to $-v$, thus
  \begin{equation*}
    r_v(u) = u - 2 \frac{\langle u, v \rangle}{\langle v, v \rangle} v.
  \end{equation*}
\end{definition}
\begin{lemma}
  Define inner products on $\mathfrak{t}_{\mathbb{R}}$ and $\mathfrak{t}_{\mathbb{R}}^*$ as above.  Let $\alpha \in \Phi$.  The action of $s_\alpha$ on $\mathfrak{t}_{\mathbb{R}}$ is the reflection in $H_\alpha$, while that of $s_\alpha$ on $\mathfrak{t}_{\mathbb{R}}^*$ is the reflection in $\alpha$.
\end{lemma}
\begin{proof}
  We recall from \S\ref{sec:proofs-conc-pass} that $H_\alpha = \frac{2}{\langle \alpha, \alpha \rangle} u_\alpha$, so that $H_\alpha$ and $\alpha$ correspond to positive multiples of one another under the isomorphism $\mathfrak{t}_{\mathbb{R}} \cong \mathfrak{t}_{\mathbb{R}}^*$ defined above.  Thus $\ker(H_\alpha : \mathfrak{t}_{\mathbb{R}}^* \rightarrow \mathbb{R})$ is the orthogonal complement of $\alpha$, while $\ker(\alpha : \mathfrak{t}_{\mathbb{R}} \rightarrow \mathbb{R})$ is the orthogonal complement of $H_\alpha$.  The required conclusion then follows from the formulas for $s_\alpha$ noted previously.
\end{proof}

We've defined the regular subset $\mathfrak{t}_{\mathbb{R}}^{\reg}$ in $\mathfrak{t}_{\mathbb{R}}$ to be the complement of $\cup_{\alpha \in \Phi} \ker(\alpha)$, and we've defined a Weyl chamber $C$ in $\mathfrak{t}_{\mathbb{R}}$ to be a connected component of $\mathfrak{t}_{\mathbb{R}}^{\reg}$.  We may similarly define the regular subset $\mathfrak{t}_{\mathbb{R}}^{* \reg}$ in $\mathfrak{t}_{\mathbb{R}}^*$ to be the complement of $\cup_{\alpha \in \Phi} \ker(H_\alpha)$, where now $H_\alpha \in \mathfrak{t}_{\mathbb{R}}$ is viewed as an element of $(\mathfrak{t}_{\mathbb{R}}^*)^*$, and a Weyl chamber $C^\vee \subseteq \mathfrak{t}_{\mathbb{R}}^*$ to be a connected component of $\mathfrak{t}_{\mathbb{R}}^{*\reg}$.  Since the isometry $\mathfrak{t}_{\mathbb{R}} \cong \mathfrak{t}_{\mathbb{R}}^*$ defined above identifies $H_\alpha$ with a positive multiple of $\alpha$, we see that it identifies the kernel of $\alpha$ with the kernel of $H_\alpha$, and hence identifies each Weyl chamber $C$ of $\mathfrak{t}_{\mathbb{R}}$ with a corresponding Weyl chamber $C^\vee$ of $\mathfrak{t}_{\mathbb{R}}^*$.  This identification is independent of the embedding $K \hookrightarrow \U(n)$, since if $C$ is described as
\begin{equation}\label{eqn:}
  C = \{t  \in \mathfrak{t}_{\mathbb{R}} : \eps_\alpha \alpha(t) > 0 \text{ for all } \alpha \in
  \Phi  \}
\end{equation}
for some signs $\eps_\alpha \in \{\pm 1\}$ (see \S\ref{sec:basics-weyl-chambers}), then $C^\vee$ is described analogously as
\begin{equation}\label{eqn:}
  C^\vee
  = \{\lambda
  \in \mathfrak{t}_{\mathbb{R}}^*
  : \eps_\alpha
  \lambda(H_\alpha) > 0 \text{ for all } \alpha \in
  \Phi  \}.
\end{equation}
In this way all of the facts that we prove below involving the Weyl group action on $\mathfrak{t}_{\mathbb{R}}$ and Weyl chambers thereof have immediate counterparts for $\mathfrak{t}_{\mathbb{R}}^*$.

\begin{example}
  Take $K = \SU(3)$.  Then (with the usual notation) $\Phi = \{\eps_i - \eps_j : 1 \leq i < j \leq 3\}$ and $\mathfrak{t}_{\mathbb{R}}^* \cong \mathfrak{t}_{\mathbb{R}} \cong (\mathbb{R}^3)_0$, the space of triples $x = (x_1,x_2,x_3) \in \mathbb{R}^3$ with $\sum x_j = 0$.  Using the standard representation, the inner product on $\mathfrak{t}_{\mathbb{R}}^*$ is given by the restriction of the standard inner product on $\mathbb{R}^3$.  The elements
  \begin{equation*}
    \eps_1 - \eps_2 = (1,-1,0), \quad \eps_2 - \eps_3 = (0,1,-1), \quad \eps_3 - \eps_1 = (-1,0,1)
  \end{equation*}
  of $\mathfrak{t}_{\mathbb{R}}^*$ sum to zero, have the same lengths, and have the same inner products with one another (up to sign).  An isometric embedding of $\mathfrak{t}_{\mathbb{R}^*}$ into $\mathbb{R}^2$ is thus obtained by sending these elements to the vertices of an equilateral triangle with center the origin.  The following elements are then sent to the vertices of a regular hexagon with center the origin, traversed in the order they appear around the boundary:
  \begin{equation*}
    \eps_1 - \eps_2, \quad \eps_1 - \eps_3, \quad \eps_2 - \eps_3, \quad \eps_1 - \eps_2, \quad \eps_3 - \eps_1, \quad \eps_3 - \eps_2.
  \end{equation*}
  In lecture we drew a picture and described the root reflections, noting that they preserve the set of roots.  For each root $\alpha$, we drew the hyperplane perpendicular to $\alpha$, which we denoted by $H_{\alpha}$.  (The notation is not so terrible, because the hyperplane $H_\alpha \subseteq \mathfrak{t}_{\mathbb{R}}^*$ is also the kernel of the functional on $\mathfrak{t}_{\mathbb{R}}^*$ defined by the element $H_{\alpha} \in \mathfrak{t}_{\mathbb{R}}$.)  There are six such hyperplanes.  The connected components of their complements are the six Weyl chambers.  Each Weyl chamber is a $60$ degree open ``pizza slice'' in $\mathbb{R}^2$.  Each Weyl chamber contains exactly one root, which is positioned along the central ray of that Weyl chamber, $30$ degrees from each of its walls.  (TODO: replace this text with a picture.)
\end{example}


\subsection{Using root reflections to elucidate the Weyl group}
\begin{lemma}\label{lem:key-root-reflections}
  Let $C, C'$ be Weyl chambers in $\mathfrak{t}_{\mathbb{R}}$.  Then there exist Weyl chambers $C = C_0, C_1, C_2, \dotsc, C_{n-1}, C_n = C'$ and roots $ \alpha_1,\dotsc,\alpha_n \in \Phi$ with the following properties:
  \begin{enumerate}
[(i)]
  \item For $j=1..n$, we have $\alpha_j > 0$ on $C_0, \dotsc, C_{j-1}$ and $\alpha_j < 0$ on $C_j, \dotsc, C_{n}$.
  \item Each $\beta \in \Phi - \{\pm \alpha_1, \dotsc, \pm \alpha_n\}$ takes the same sign on $C_0,\dotsc,C_n$.
  \item For $j=1..n$, we have $s_{\alpha_j} C_{j-1} = C_j$.  In particular,
    \begin{equation}\label{eqn:}
      C'
      = s_{\alpha_n} \dotsb s_{\alpha_1} C.
    \end{equation}
  \end{enumerate}
  Analogous conclusions hold for any pair of Weyl chambers in $\mathfrak{t}_{\mathbb{R}}^*$.
\end{lemma}
The idea of the proof was illustrated in class on the picture of the Weyl chambers for $\SU(3)$.  Consider the straight line path from some $t \in C$ to some $t' \in C'$.  If $t, t'$ are chosen generically, then this path will hit each root hyperplane $\alpha^{\perp} := \{x \in \mathfrak{t}_{\mathbb{R}} : \alpha(x) = 0\}$ one at a time.  We take $\alpha_1,\dotsc,\alpha_n$ to correspond to the root hyperplanes encountered along the way from $t$ to $t'$ and $C_0,\dotsc,C_n$ the Weyl chambers, so that the path starts in the Weyl chamber $C_0$, then passes through the root hyperplane $\alpha_1^{\perp}$, then spends some time in the Weyl chamber $C_1$, then passes through the root hyperplane $\alpha_2^{\perp}$, and so on.  We'll give the actual proof next time.  For now, let's illustrate with some motivating applications.

\begin{corollary}
  The Weyl group $W = N(T)/T$ of a maximal torus $T$ in a compact connected Lie group $K$ is generated by the root reflections $s_\alpha$ ($\alpha \in \Phi = \Phi(K:T)$).
\end{corollary}
\begin{proof}
  Recall from Lemma \ref{lem:W-acts-freely-weyl-chambers} that $W$ acts freely on the set of Weyl chambers.  Let $C$ be any Weyl chamber, and let $w \in W$.  Then $C' := w(C)$ is a Weyl chamber.  By Lemma \ref{lem:key-root-reflections}, we may write $C' = s_{\alpha_n} \dotsb s_{\alpha_1} C$ for some $\alpha_1,\dotsc,\alpha_n \in \Phi$.  Since the action of $W$ on the set of Weyl chambers is free, we obtain $w = \alpha_n \dotsb \alpha_1$.
\end{proof}

\begin{corollary}
  With notation as assumptions as above, $W$ acts simply-transitively on the set of Weyl chambers.
\end{corollary}
\begin{proof}
  We've seen (Lemma \ref{lem:W-acts-freely-weyl-chambers}) that the action is defined and free; the final assertion in Lemma \ref{lem:key-root-reflections} implies that it is transitive.
\end{proof}
\footnote{End of half-lecture \#16, Tuesday, 16 Apr}


\begin{proof}
[Proof of Lemma \ref{lem:key-root-reflections}]
  We implement the proof sketch given above.  Fix $t' \in C'$.  Set $\Sigma := \{\alpha \in \Phi : \alpha(C) > 0, \alpha(C') < 0\}$.  Note that for each $t \in C$ and $\alpha \in \Phi$, we have $\alpha(t) > 0 > \alpha(t')$, so the line segment $\{(1 - \tau) t + \tau t ' : \tau \in [0,1]\}$ connecting $t$ to $t'$ intersects $\alpha^\perp$ in a unique point that we denote by $p_\alpha(t)$.  This defines a map
  \begin{equation*}
    p_\alpha : C \rightarrow \alpha^\perp.
  \end{equation*}
  This map is continuous and open.  Moreover, for each $\alpha \in \Phi$, the hyperplane $\alpha^\perp$ is distinct from $\beta^\perp$ for all $\beta \in \Phi - \{\pm \alpha \} = \Phi - \mathbb{C} \alpha$, so the set
  \begin{equation*}
    \alpha^\perp - \cup_{\beta \in \Phi - \{\pm \alpha \}} \alpha^\perp \cup \beta^{\perp}
  \end{equation*}
  is dense and open in $\alpha^\perp$.  By an exercise in topology, it follows that for each $\alpha \in \Sigma$, the set
  \begin{equation*}
    A_\alpha := \{t \in C : p_\alpha(t) \in \beta^\perp \text{ for all } \beta \in \Phi - \mathbb{C} \alpha \}
  \end{equation*}
  is dense and open in $C$.  Hence likewise $A := \cap_{\alpha \in \Sigma} A_\alpha$ is dense and open in $C$.  Choose any $t \in A$.  Let $\gamma : [0,1] \rightarrow \mathfrak{t}_{\mathbb{R}}$ denote the linear path connecting $t$ to $t'$, thus $\gamma(\tau) := (1 - \tau) t + \tau t'$.  For each $\alpha \in \Sigma$, abbreviate $p_\alpha := p_\alpha(t)$, and let $\tau_{\alpha} \in (0,1)$ be such that $\gamma(\tau_\alpha) = p_\alpha$.  By the construction of $t$, we have $\tau_\alpha \neq \tau_{\alpha '}$ whenever $\alpha \neq \alpha '$.  We may thus order $\Sigma = \{\alpha_1, \dotsc, \alpha_n\}$ in such a way that $\tau_{\alpha_1} < \dotsb < \tau_{\alpha_n}$.  Abbreviate $\tau_j := \tau_{\alpha_j}$ and set $\tau_0 := 0, \tau_{n+1} := 1$.  Note if $\beta \in \Phi - (\Sigma \cup (- \Sigma))$, then $\beta$ takes the same sign on $t$ and $t'$, hence also on the path $\gamma$ connecting them.  It follows that for $j=0..n$, the set $\{\gamma(\tau) : \tau_{j} < \tau < \tau_{j+1}\}$ belongs to some Weyl chamber $C_j$ of $\mathfrak{t}_{\mathbb{R}}$, with $C_0 = C$ and $C_n = C'$.  Moreover, for $j=1..n$, there is a neighborhood $U_j$ of $p_{\alpha_j}$ that doesn't intersect $\beta^\perp$ for any $\beta \in \Phi - \mathbb{C} \alpha_j$, thus $U_j = (U_j \cap C_{j-1}) \sqcup (U_j \cap \alpha_j^\perp) \sqcup (U_j \cap C_j)$.  The root reflection $s_{\alpha_j}$ satisfies $s_{\alpha_j}|_{\alpha_j^\perp} = \id$, thus $s_{\alpha_j} U_j \cap U_j \neq \emptyset$, thus $s_{\alpha_j} (U_j \cap C_{j-1}) \cap (U_j \cap C_{j-1}) \neq \emptyset$, hence $s_{\alpha_j} C_{j-1} = C_j$.  The remaining assertions follow from what we have already shown.
\end{proof}

\begin{corollary}
  Let $C$ be a Weyl chamber.  Any $x \in \mathfrak{t}_{\mathbb{R}}^{\reg}$ is $W$-conjugate to a unique element of $C$.  The $W$-stabilizer of $x$ is trivial.
\end{corollary}
\begin{proof}
  We use that $x$ belongs to some Weyl chamber and that $W$ acts transitively on the set of Weyl chambers.
\end{proof}

\begin{corollary}\label{cor:W-conj-to-unique-dominant-element}
  Let $C$ be a Weyl chamber.  Then any $x \in \mathfrak{t}_{\mathbb{R}}$ is $W$-conjugate to a unique element of the closure $\overline{C}$ of $C$.

  The $W$-stabilizer of $x$ is generated by $\{s_\alpha : \alpha(x) = 0\}$.
\end{corollary}
\begin{proof}
  We observe first that $\mathfrak{t}_{\mathbb{R}}^{\reg}$ is dense in $\mathfrak{t}_{\mathbb{R}}$.  Indeed, if $x \in \mathfrak{t}_{\mathbb{R}}$ and $y$ is any element of $\mathfrak{t}_{\mathbb{R}}^{\reg}$, then $x + \eps y \in \mathfrak{t}_{\mathbb{R}}^{\reg}$ for all small enough $\eps > 0$, and $x = \lim_{\eps \rightarrow 0} (x + \eps y)$.  Since $\mathfrak{t}_{\mathbb{R}}^{\reg}$ is the union of the $W$-conjugates of $C$, it follows that $\mathfrak{t}_{\mathbb{R}}^{\reg}$ is the union of the $W$-conjugates of $\overline{C}$.  This gives the existence of a $W$-conjugate of $x$ in $\overline{C}$.  For verifying the uniqueness, we may assume that $x$ belongs to $\overline{C}$, and must show that if $w \in W$ satisfies $w(x) \in \overline{C}$, then $w(x) = x$.  To see this, we apply Lemma \ref{lem:key-root-reflections} to $C$ and $C' := w^{-1}(C)$, giving us some chambers $C_0,\dotsc,C_n$ and roots $\alpha_1,\dotsc,\alpha_n$.  Our hypothesis implies that $x \in \overline{C} \cap \overline{C'}$.  For $j = 1..n$, we have $\alpha_j > 0$ on $C$ and $\alpha_j < 0$ on $C'$, thus $\alpha_j(x) = 0$, and so $s_{\alpha_j}(x) = x$.  On the other hand, $w^{-1} = s_{\alpha_n} \dotsb s_{\alpha_1}$, so $w = s_{\alpha_1} \dotsb s_{\alpha_n}$.  Thus $w(x) = x$.

  The final assertion follows from the argument just given; we note that it doesn't depend upon the choice of $C$.
\end{proof}

\subsection{Basics on dominance, positivity and simple roots}

\begin{definition}
  Choose a Weyl chamber $C \subseteq \mathfrak{t}_{\mathbb{R}}^{\reg}$, hence a corresponding dual Weyl chamber $C^\vee \subseteq \mathfrak{t}_{\mathbb{R}}^{\reg}$.  We say that $x \in \mathfrak{t}_{\mathbb{R}}$ (resp. $\lambda \in \mathfrak{t}_{\mathbb{R}}^*$) is \emph{dominant} if $x \in \overline{C}$ (resp. if $\lambda \in \overline{C^\vee}$), and \emph{strictly dominant} if $x \in {C}$ (resp. if $\lambda \in {C^\vee}$).
  
  We say that $\alpha \in \Phi$ is \emph{positive} if $\alpha(C) > 0$; we abbreviate this condition simply to $\alpha > 0$ when $C$ is clear by context.  We denote by $\Phi^+ := \Phi^+(C) \subseteq \Phi$ the subset of positive roots.  We say that a positive root $\alpha \in \Phi^+$ is \emph{simple} if it cannot be written in the form $\alpha_1 + \alpha_2$ for some $\alpha_1, \alpha_2 \in \Phi^+$.  We denote by $\Delta := \Delta(C) \subseteq \Phi^+$ the set of simple roots.

  For $x,y \in \mathfrak{t}_{\mathbb{R}}$ we say that $x$ is \emph{higher than} $y$, and write $x \geq y$, if $\lambda(x-y) \geq 0$ for all $\lambda \in C^\vee$.  Similarly, for $\lambda,\mu \mathfrak{t}_{\mathbb{R}}^*$, we say that $\lambda$ is \emph{higher than} $\mu$, and write $\lambda \geq \mu$, if $(\lambda - \mu)(x) \geq 0$ for all $x \in C$.  We use ``strictly higher than'' to mean ``higher than and not equal to.''
\end{definition}
We note that, for reasons explained earlier, all of these notions are compatible with the isometry $\mathfrak{t}_{\mathbb{R}} \cong \mathfrak{t}_{\mathbb{R}}^*$ defined by the inner product given by the trace pairing with respect to any embedding $K \hookrightarrow \U(n)$.


\begin{lemma}
  (Not proved in lecture, but useful facts to mention at this point.)
  \begin{enumerate}
[(i)]
  \item $\Phi = \Phi^+ \sqcup (-\Phi^+)$.
  \item $C = \{x \in \mathfrak{t}_{\mathbb{R}} : \alpha(x) > 0 \text{ for all } \alpha > 0\}$ and $C^\vee = \{\lambda \in \mathfrak{t}_{\mathbb{R}}^* : \lambda(H_\alpha) > 0 \text{ for all } \alpha > 0\}$.
  \item $\{ x \in \mathfrak{t}_{\mathbb{R}} : x \geq 0 \} = \sum_{\alpha > 0} \mathbb{R}_{\geq 0} H_{\alpha}$ and $\{ \lambda \in \mathfrak{t}_{\mathbb{R}}^* : \lambda \geq 0 \} = \sum_{\alpha > 0} \mathbb{R}_{\geq 0} \alpha$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  We've seen that we may write $C = \{x : \eps_\alpha \alpha(x) > 0 \text{ for all } \alpha \in \Phi \}$ for some signs $\eps_\alpha \in \{\pm 1\}$.  Then $C^\vee = \{\lambda : \eps_\alpha \lambda(H_\alpha) > 0 \text{ for all } \alpha \in \Phi \}$.  Since $\eps_{-\alpha} = - \eps_{\alpha}$ and $\Phi^+ = \{\alpha : \eps_\alpha = 1\}$, we deduce assertions (i) and (ii).  The final assertion (iii) boils down, as follows, to a general duality principle for convex cones.  Let $M_1$ and $M_2$ denote the LHS and RHS of the claimed identity.  Then $M_1$ and $M_2$ are closed convex cones containing the origin.  Clearly $M_2 \subseteq M_1$.  To prove the reverse containment $M_1 \subseteq M_2$, it suffices (by the separating hyperplane theorem, the finite-dimensional case of Hahn--Banach) to show that for each $x \in \mathfrak{t}_{\mathbb{R}}$ with $M_2(x) \geq 0$, we have $M_1(x) \geq 0$.  Indeed, if $M_2(x) \geq 0$, then $\alpha(x) \geq 0$ for all $\alpha > 0$; since $\lambda \geq 0$ for all $\lambda \in M_1$, the required implication follows.
\end{proof}


For instance, take $K = \U(n)$, with $T$ as usual and coordinates as before.  We might take
\begin{equation}
  C = \{x \in \mathfrak{t}_{\mathbb{R}} \cong \mathbb{R}^n : x_1
  > \dotsb > x_n\}.
\end{equation}
Then
\begin{equation}\label{eq:}
  C^\vee = \{\lambda \in \mathfrak{t}_{\mathbb{R}}^* \cong
  \mathbb{R}^n:
  \lambda_1 > \dotsb > \lambda_n\}
\end{equation}
and
\begin{equation}
  \Phi^+ = \{\eps_i - \eps_j : i < j\} = \{\eps_1 - \eps_2,
  \eps_1 - \eps_3, \dotsc\}
\end{equation}
and
\begin{equation}\label{eq:}
  \Delta = \{\eps_1 - \eps_2, \eps_2 - \eps_3, \dotsc, \eps_{n-1} - \eps_n\}.
\end{equation}



\begin{corollary}
  If $\lambda \in \mathfrak{t}_{\mathbb{R}}^*$ is dominant, then $\lambda \geq w(\lambda)$ for all $w \in W$.
\end{corollary}
\begin{proof}
  We apply Lemma \ref{lem:key-root-reflections} in its dual form to $C^\vee$ and $w C^\vee$.  This gives us a sequence of dual Weyl chambers $C_0^\vee = C^\vee, C_1^\vee, \dotsc, C_n^\vee = w C^\vee$ and roots $\alpha_1,\dotsc,\alpha_n$ satisfying (among other properties) $H_{\alpha_j} > 0$ on $C_0^{\vee}, \dotsc, C_{j-1}^{\vee}$ and $H_{\alpha_j} < 0$ on $C_j^{\vee}, \dotsc, C_{n}^{\vee}$.  Set $\lambda_j := s_{\alpha_j} \dotsb s_{\alpha_1} \lambda$.  Then $\lambda = \lambda_0$ and $w(\lambda) = \lambda_n$, so
  \begin{equation*}
    \lambda - w(\lambda) = (\lambda_0 - \lambda_1) + (\lambda_1 - \lambda_2) + \dotsb + (\lambda_{n-1} - \lambda_n).
  \end{equation*}
  For $j=1..n$, the formulas defining $\lambda_j$ and $s_{\alpha_j}$ give
  \begin{equation*}
    \lambda_{j-1} - \lambda_j = \lambda_{j-1} - s_{\alpha_j} \lambda_{j-1} = \lambda_{j-1}(H_{\alpha_j}) \alpha_j.
  \end{equation*}
  We have $\alpha_j > 0$ (because $H_{\alpha_j} > 0$ on $C^\vee$) and $\lambda_{j-1}(H_{\alpha_j}) \geq 0$ (because $H_{\alpha_j} \geq0$ on $C_{j-1}$), hence $\lambda_{j-1} - \lambda_j \geq 0$.  It follows that $\lambda - w(\lambda) \geq 0$.
\end{proof}

\begin{theorem}\label{thm:simple-system-is-linearly-indep}
  $\Delta$ is linearly independent over $\mathbb{R}$, and the $\mathbb{Z}$-module $\mathbb{Z} \Phi$ spanned by $\Phi$ admits the $\mathbb{Z}$-module basis $\Delta$.
\end{theorem}
We'll give the proof after a few lemmas.

\begin{lemma}\label{lem:pos-roots-via-simple-roots}
  Every $\beta \in \Phi^+$ may be written in the form $\beta = \sum_{\alpha \in \Delta} n_\alpha \alpha$ with $n_\alpha \in \mathbb{Z}_{\geq 0}$.
\end{lemma}
\begin{proof}
  Fix $t \in C$.  As $\beta$ varies over $\Phi^+$, the quantities $\beta(t)$ vary over a finite subset of $\mathbb{R}_{>0}$.  We argue by induction on $\beta(t)$.  If $\beta \in \Delta$, then we are done.  Otherwise $\beta = \beta_1 + \beta_2$ with $\beta_1, \beta_2 \in \Delta$.  Then $\beta_1(t), \beta_2(t) < \beta(t)$, so we may conclude by our inductive hypothesis.
\end{proof}

Let $\alpha,\beta \in \Phi$ be roots.  Recall that, if we choose an embedding $K \hookrightarrow \U(n)$ and hence an inner product $\langle x,y \rangle = \trace( x y)$ on $\mathfrak{t}_{\mathbb{R}}$ and hence an isometry $\mathfrak{t}_{\mathbb{R}}^* \ni \lambda \mapsto u_\lambda \in \mathfrak{t}_{\mathbb{R}}$, then $H_\alpha = \frac{2}{( \alpha, \alpha )} u_\alpha$.  Thus
\begin{equation}\label{eq:identity-alpha-beta-inner-prod-vs-H-stuff}
  \langle \alpha, \beta  \rangle
  =
  \beta(u_\alpha)
  =
  \alpha(u_\beta)
  =
  \frac{\langle\alpha,\alpha\rangle}{2}
  \beta(H_\alpha)
  = 
  \frac{\langle\beta,\beta\rangle}{2}
  \alpha(H_\beta).
\end{equation}
In particular, the quantities
\begin{equation}\label{eq:}
  (\alpha,\beta),
  \quad
  \alpha(H_\beta),
  \quad 
  \beta(H_\alpha)
\end{equation}
all have the same sign.  We say that the roots $\alpha,\beta$
\begin{itemize}
\item are \emph{orthogonal} if these quantities are zero,
\item form an \emph{acute} angle if these quantities are positive, and
\item form an \emph{obtuse} angle if they are negative.
\end{itemize}
These notions are evidently independent of the choice of embedding $K \hookrightarrow \U(n)$.


\begin{lemma}\label{lem:non-proportional-roots-inner-products-etc}
  Let $\alpha,\beta \in \Phi$ be roots that are non-proportional (thus $\alpha \neq \pm \beta$).
  \begin{itemize}
  \item If $\alpha,\beta$ are orthogonal, then $\alpha(H_\beta) = \beta(H_\alpha) = 0$.
  \item Suppose $\alpha,\beta$ form an acute angle.  Then the pair $(\alpha(H_\beta), \beta(H_\alpha))$ is one of the following:
    \begin{equation*}
      (1,1), (1,2), (2, 1), (1, 3), (3, 1).
    \end{equation*}
    Moreover, $\alpha - \beta$ is a root.
  \item Suppose $\alpha,\beta$ form an obtuse angle.  Then the pair $(\alpha(H_\beta), \beta(H_\alpha))$ is one of the following:
    \begin{equation*}
      (-1,-1), (-1,-2), (-2, -1), (-1, -3), (-3, -1).
    \end{equation*}
    Moreover, $\alpha + \beta$ is a root.
  \end{itemize}
\end{lemma}
\begin{proof}
  Suppose for instance that $\alpha,\beta$ form an acute angle.  Since $\alpha,\beta \in \mathfrak{t}_{\mathbb{Z}}^*$ and $H_\alpha, H_\beta \in \mathfrak{t}_{\mathbb{Z}}$, we then have $\alpha(H_\beta), \beta(H_\alpha) \in \mathbb{Z}_{\geq 1}$.  Since $\alpha$,$\beta$ are non-proportional, we may apply the Cauchy--Schwartz inequality in its strict form and the identity \eqref{eq:identity-alpha-beta-inner-prod-vs-H-stuff} to see that
  \begin{equation*}
    \alpha(H_\beta) \beta(H_\alpha) = 4 \frac { \left\lvert \langle \alpha, \beta \rangle \right\rvert^2 } { \langle \alpha, \alpha \rangle \langle \beta, \beta \rangle } < 4.
  \end{equation*}
  Thus $\alpha(H_\beta) \beta(H_\alpha) \in \{1,2,3\}$, which leads to the possibilities indicated.  In particular, either $\alpha(H_\beta)$ or $\beta(H_\alpha)$ is equal to $1$.  Suppose for instance that $\beta(H_\alpha) = 1$.  Then
  \begin{equation*}
    s_{\alpha}(\beta) = \beta - \beta(H_\alpha) \alpha = \beta - \alpha,
  \end{equation*}
  so $\beta - \alpha$ is a root, thus $\alpha - \beta = - (\beta - \alpha)$ is a root, as required.  The case that $\alpha(H_\beta) = 1$ is treated similarly.  The obtuse case is treated similarly.
\end{proof}

\begin{corollary}\label{cor:simple-roots-obtuse-angles}
  If $\alpha, \beta$ are distinct elements of $\Delta$, then $\langle \alpha, \beta \rangle \leq 0$.
\end{corollary}
\begin{proof}
  We've seen that otherwise $\gamma := \alpha - \beta$ is a root.  If $\gamma \in \Phi^+$, then the identity $\alpha = \beta + \gamma$ implies that $\alpha \notin \Delta$, while if $-\gamma \in \Phi^+$, then the identity $\beta = \alpha + (-\gamma)$ implies that $\beta \notin \Delta$.  In either case we obtain the required contradiction.
\end{proof}

\begin{proof}
[Proof of Theorem \ref{thm:simple-system-is-linearly-indep}]
  Suppose there are distinct elements $\alpha_1,\dotsc,\alpha_m, \beta_1, \dotsc, \beta_n \in \Delta$ and positive reals $a_1,\dotsc,a_m,b_1,\dotsc,b_n$ so that
  \begin{equation*}
    x := \sum a_i \alpha_i = \sum b_j \beta_j =: y.
  \end{equation*}
  Corollary \ref{cor:simple-roots-obtuse-angles} then implies that $\langle \alpha_i, \beta_j \rangle \leq 0$, so
  \begin{equation*}
    0 \leq \langle x, x \rangle = \langle x, y \rangle = \sum_{i,j} \underbrace{a_i b_j}_{>0} \underbrace{\langle \alpha_i, \beta_j \rangle}_{\leq 0} \leq 0,
  \end{equation*}
  so equality holds, and thus $x = y = 0$.  Choose any $t \in C$.  Then $a_i(t) > 0$, so $0 = x(t) = \sum_{i} a_i \alpha_i(t)$ and $0 = y(t) = \sum_{j} b_j \beta_j(t)$ with each summand positive.  Thus $m = n = 0$.
\end{proof}

\begin{theorem}
  The Weyl group is generated by the simple reflections, i.e., $W = \langle s_\alpha : \alpha \in \Delta \rangle$.
\end{theorem}
\begin{proof}
  We'll leave this to the homework.  It can be proved using Lemma \ref{lem:key-root-reflections}; the main point is that $\alpha_1 \in \Delta(C)$ for any $\alpha_1$ in the conclusion of that lemma.
\end{proof}


\subsection{Cartan matrices}
Let $K$ be a compact connected Lie group, with maximal torus $T$, Weyl chamber $C$ and accompanying notation as above.

\begin{definition}
  The Cartan matrix is $(N_{\alpha \beta})_{\alpha, \beta \in \Delta}$ where $N_{\alpha \beta} := \alpha(H_\beta)$.
\end{definition}

We'll note that the Cartan matrix, as we've defined it here, depends only upon the isomorphism class of the compact Lie group $K$.  (Use that any two maximal tori are $G$-conjugate and that any two Weyl chambers are $W$-conjugate.)

\begin{lemma}
  We have $N_{\alpha \alpha} = 2$.  For $\alpha \neq \beta$, the pair $(N_{\alpha \beta},N_{\beta \alpha})$ is one of
  \begin{equation*}
    (0,0), \quad (-1,-1), \quad (-1,-2), \quad (-2,-1), \quad (-1,-3), \quad (-3,-1).
  \end{equation*}
\end{lemma}
\begin{proof}
  By Lemma \ref{lem:non-proportional-roots-inner-products-etc} and Corollary \ref{cor:simple-roots-obtuse-angles}.
\end{proof}


We'll prove the following (and some variants) after the break:

\begin{theorem}\label{thm:cartan-matrix-determines-K}
  The tuple $(T, \Delta, \Delta^\vee)$ determines $K$ up to isomorphism, in the following sense:

  Suppose given two compact connected Lie groups $K', K''$ equipped with maximal tori $T', T''$, Weyl chambers $C', C''$.  Let $\Delta ' = \{\gamma_1 ', \dotsc, \gamma_n'\}$ and $\Delta '' = \{\gamma_1 '', \dotsc, \gamma_n''\}$ denote the associated sets of simple roots, and $H_i' := H_{\gamma_i'}$, $H_i'' := H_{\gamma_i''}$ the simple coroots.  Suppose given an isomorphism $\kappa : T' \rightarrow T''$ such that $\kappa^* \gamma_i'' = \gamma_i'$ and $\kappa_* H_i' = H_i''$.  Then $\kappa$ extends to an isomorphism $K' \rightarrow K''$.

  More precisely, suppose given maps $F_i' : \SU(2) \rightarrow K'$ and $F_i'' : \SU(2) \rightarrow K''$ attached to the roots $\gamma_i'$ and $\gamma_i''$.  Let $X_i', Y_i', H_i'$ and $X_i'', Y_i'', H_i''$ be defined accordingly.  Then there exists a unique extension of $\kappa$ whose differential maps $X_i'$ to $X_i''$ for each $i$.
\end{theorem}
This has the following consequence:
\begin{corollary}
  Suppose that $K$ has finite center, or equivalently, that $\mathfrak{k}$ has trivial center.  Then the pair $(T, (N_{\alpha \beta})_{\alpha,\beta \in \Delta})$ determines $K$ up to isomorphism in the following sense:

  Let $K', K''$ be two compact connected Lie groups having finite center, with maximal tori $T', T''$ and Weyl chambers $C', C''$ and sets of simple roots $\Delta ', \Delta ''$ so that there is an isomorphism $T' \xrightarrow{\simeq } T''$ for which the induced isomorphism $X(T'') \xrightarrow{\simeq } X(T')$ defines a bijection $\Delta '' \xrightarrow{\simeq } \Delta '$ with respect to which the Cartan matrices coincide, i.e., $\alpha ''(H_{\beta ''}) = \alpha '(H_{\beta '})$ whenever $\alpha ', \beta ' \in \Delta '$ correspond to $\alpha '', \beta '' \in \Delta ''$.  Then the isomorphism $T' \xrightarrow{\simeq } T''$ extends to an isomorphism $K' \xrightarrow{\simeq } K''$.  The set of possible isomorphisms may be described as in theorem \ref{thm:cartan-matrix-determines-K}.
\end{corollary}
% TODO: maybe some explanation about splitting a general $K$ as an almost direct product of a central torus and some simple Lie groups?


\footnote{End of lecture \#17, Thursday, 18 Apr} The proof will be given in \S??? after some preliminaries.

\subsection{Strings of roots}
Let $K,T,\Phi$ be as usual: a compact connected Lie group, a maximal torus, and the set of roots.  Let $\alpha, \beta \in \Phi$.
\begin{lemma}\label{lem:strings-roots}
  \begin{enumerate}
[(i)]
  \item If $\alpha+\beta \in \Phi$, then $[\mathfrak{g}^\alpha, \mathfrak{g}^{\beta}] = \mathfrak{g}^{\alpha+\beta}$.
  \item Suppose that $\alpha,\beta$ are non-proportional, i.e., $\mathbb{Q} \alpha \neq \mathbb{Q} \beta$.  The set
    \begin{equation*}
      E := \{k \in \mathbb{Z} : \beta + k \alpha \in \Phi \}
    \end{equation*}
    contains $0$ and is finite, hence has a minimal element of the form $-p\in \mathbb{Z}_{ \leq 0}$ and a maximal element of the form $q \in \mathbb{Z}_{\geq 0}$.  We have
    \begin{equation}\label{eq:E-equals-Z-cap-interval}
      E = \mathbb{Z} \cap [-p,q].
    \end{equation}
    Moreover, for any nonzero elements $X_\alpha \in \mathfrak{g}^{\alpha}, Y_\alpha \in \mathfrak{g}^{-\alpha}$, each of the maps
    \begin{equation*}
      \mathfrak{g}^{\beta - p \alpha} \xrightarrow{\ad_{X_\alpha}} \mathfrak{g}^{\beta - (p-1) \alpha} \xrightarrow{\ad_{X_\alpha}} \dotsb \xrightarrow{\ad_{X_\alpha}} \mathfrak{g}^{\beta + (q-1) \alpha} \xrightarrow{\ad_{X_\alpha}} \mathfrak{g}^{\beta + q \alpha},
    \end{equation*}
    \begin{equation*}
      \mathfrak{g}^{\beta - p \alpha} \xleftarrow{\ad_{Y_\alpha}} \mathfrak{g}^{\beta - (p-1) \alpha} \xleftarrow{\ad_{Y_\alpha}} \dotsb \xleftarrow{\ad_{Y_\alpha}} \mathfrak{g}^{\beta + (q-1) \alpha} \xleftarrow{\ad_{Y_\alpha}} \mathfrak{g}^{\beta + q \alpha},
    \end{equation*}
    is an isomorphism.  Moreover,
    \begin{equation}\label{eq:beta-H-alpha-diff-p-q}
      \beta(H_\alpha) = p - q.
    \end{equation}
  \end{enumerate}
\end{lemma}
\begin{proof}
  Note first that (ii) implies (i): if $\alpha+\beta \in \Phi$, then (since $\mathbb{Q} \alpha \cap \Phi = \{\pm \alpha \}$) $\alpha,\beta$ are non-proportional, the set $E$ as defined in (ii) contains the elements $0,1$, and $\ad(X_\alpha) : \mathfrak{g}^{\beta} \rightarrow \mathfrak{g}^{\beta+\alpha}$ is an isomorphism of one-dimensional vector spaces, which implies that $[\mathfrak{g}^\alpha,\mathfrak{g}^{\beta}] = \mathfrak{g}^{\alpha+\beta}$.

  We turn to the proof of (ii).  Since $\beta, \alpha$ are non-proportional, we have $\beta + k \alpha \neq 0$ for all $k \in \mathbb{Z}$.  Thus
  \begin{equation}
    V := \oplus_{k \in \mathbb{Z}} \mathfrak{g}^{\beta + k
      \alpha}
    =
    \oplus _{k \in E}
    \mathfrak{g}^{\beta + k \alpha}.
  \end{equation}
  We check easily that $V$ is an $\slLie_2(\mathbb{C})_\alpha$-module whose weight spaces, i.e., $\ad(H_\alpha)$-eigenspaces, are the summands $\mathfrak{g}^{\beta+k \alpha}$, with correspond eigenvalues $(\beta+k \alpha)(H_\alpha) = \beta(H_\alpha) + 2 k$.  The weight spaces are thus one-dimensional and the weights all have the same parity, so by the structure theory for $\slLie_2(\mathbb{C})$-modules, we deduce that $V$ is irreducible, that the weights of $V$ are of the form $\{-n, -n+2, \dotsc, n-2, n\}$ for some nonnegative integer $n$, that \eqref{eq:E-equals-Z-cap-interval} holds, and that the connecting maps between adjacent weight spaces defined by $\ad(X_\alpha)$ and $\ad(Y_\alpha)$ are isomorphisms.  Moreover, $n = \beta(H_\alpha) + 2 q$ and $-n = \beta(H_\alpha) - 2 p$, which gives \eqref{eq:beta-H-alpha-diff-p-q}.
\end{proof}


\subsection{Generating the Lie algebra using simple root
  vectors}\label{sec:gener-lie-algebra}
For the next few results, let $K,T,C,\Phi,\Phi^+,\Delta$ be as usual.  Write
\begin{equation*}
  \Delta = \{\gamma_1,\dotsc,\gamma_n\}.
\end{equation*}
Fix $F_{\gamma_j} : \SU(2) \rightarrow K$ as in \S\ref{sec:from-roots-dist} and associated elements $X_j := X_{\gamma_j}$, $Y_j := Y_{\gamma_j}$, $H_j := H_{\gamma_j}$.


\begin{lemma}\label{lem:roots-partial-sums}
  Let $\beta \in \Phi^+$.  Then there exist $\alpha_1,\dotsc,\alpha_r \in \Delta$ so that
  \begin{itemize}
  \item $\beta = \alpha_1 + \dotsb + \alpha_r$, and
  \item $\alpha_1 + \dotsb + \alpha_s \in \Phi$ for $s=1..r$.
  \end{itemize}
\end{lemma}
\begin{proof}
  The proof is an adaptation of that of Lemma \ref{lem:pos-roots-via-simple-roots}.  Fix $t \in C$.  Induct on $\beta(t) \in \mathbb{R}_{>0}$.  If $\beta \in \Delta$, then the required conclusion holds with $r = 1$ and $\alpha_1 = \beta$, so suppose $\beta \notin \Delta$.

  We claim that there exists $j \in \{1..n\}$ so that $\beta$ and $\gamma_j$ form an acute angle.  Otherwise, arguing as in the proof of Theorem \ref{thm:simple-system-is-linearly-indep}, we see that the set $\{\beta, \gamma_1, \dotsc, \gamma_n\}$ is linearly independent over $\mathbb{R}$.  Since $\beta \in \oplus_{j=1..n} \mathbb{Z}_{\geq 0} \gamma_j$, we obtain the required contradiction.

  Fix one such $j$.  Since $\beta \in \Phi^+ - \Delta$, the roots $\beta$ and $\gamma_j$ are non-proportional, so by Lemma \ref{lem:non-proportional-roots-inner-products-etc}, the difference $\beta - \gamma_j$ is a root.  We claim that $\beta - \gamma_j$ is positive.  If not, then $\gamma_j - \beta$ is positive.  Writing $\beta = \sum_i c_i \gamma_i$ with $c_i \in \mathbb{Z}_{\geq 0}$, the decomposition $\gamma _j - \beta = \sum_i (\delta_{i j} - c_i) \gamma_i$ then has nonnegative coefficients, so $c_j \leq 1$ and $c_i \leq 0$ for $i \neq j$.  These conditions force either $\beta = 0$ (which is impossible because $\beta$ is a root) or $\beta = \gamma_j$ (which is impossible because $\beta \notin \Delta$).

  We now apply our inductive hypothesis to $\beta - \gamma_j$, writing it as $\alpha_1 + \dotsb + \alpha_r$ for some simple roots satisfying the required conclusion of the lemma, and take $\alpha_{r+1} := \gamma_j$.  Then $\beta = \alpha_1 + \dotsb + \alpha_{r+1}$ gives the required decomposition.
\end{proof}


\begin{corollary}\label{cor:g-gen-via-simple-stuff}
  $\mathfrak{g}$ is generated (as a complex Lie algebra) by $\mathfrak{t}_{\mathbb{C}} \cup \{X_i\} \cup \{Y_j\}$.  More precisely,
  \begin{equation}\label{eqn:decomposition-frak-g-via-simple-root-vecs}
    \mathfrak{g} =
    \mathfrak{t}_{\mathbb{C}}
    \oplus
    (
    \sum _{ \substack{
        r \geq 1 \\
        i = (i_1,\dotsc,i_r) \in \mathbb{Z}_{\geq 1}^r
      }
    }
    \mathbb{C} e_i
    )
    \oplus
    (
    \sum _{ \substack{
        r \geq 1 \\
        i = (i_1,\dotsc,i_r) \in \mathbb{Z}_{\geq 1}^r
      }
    }
    \mathbb{C} f_i
    ),
  \end{equation}
  where
  \begin{equation}\label{eqn:defn-e-i}
    e_i := [X_{i_1},\dotsc,[X_{i_{r-1}}, X_{i_r}]\dotsb]
    \in \mathfrak{g}^{\gamma_{i_1} + \dotsb + \gamma_{i_r}},
  \end{equation}
  \begin{equation}\label{eqn:defn-f-i}
    f_i := [Y_{i_1},\dotsc,[Y_{i_{r-1}}, Y_{i_r}]\dotsb]
    \in \mathfrak{g}^{-\gamma_{i_1} - \dotsb - \gamma_{i_r}}.
  \end{equation}
  The action of the indicated generators for $\mathfrak{g}$ on the above decomposition is described as follows: For $H \in \mathfrak{t}_{\mathbb{C}}$, we have
  \begin{equation}\label{eqn:}
    \ad_{X_j}
    H = -\gamma_j(H) X_j,
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{Y_j}
    H = -\gamma_j(H) X_j,
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{X_j}
    e_i
    =
    [X_j, [X_{i_1},\dotsc,[X_{i_{r-1}}, X_{i_r}]\dotsb]]
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{H}
    e_i
    =
    \langle H, \gamma_{i_1} + \dotsb + \gamma_{i_r} \rangle
    e_i
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{Y_j}
    f_i
    =
    [Y_j, [Y_{i_1},\dotsc,[Y_{i_{r-1}}, Y_{i_r}]\dotsb]]
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{H}
    f_i
    =
    -\langle H, \gamma_{i_1} + \dotsb + \gamma_{i_r} \rangle
    f_i
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{Y_j} X_i
    =
    \begin{cases}
      - H_i & \text{ if }i = j \\
      0 & \text{ if } i \neq j,
    \end{cases}
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{Y_j} e_i
    =
    [\ad_{Y_j} X_{i_1},\dotsc,[X_{i_{r-1}}, X_{i_r}]\dotsb]
    + \dotsb
    +
    [ X_{i_1},\dotsc,[X_{i_{r-1}}, \ad_{Y_j} X_{i_r}]\dotsb],
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{X_j} Y_i
    =
    \begin{cases}
      H_i & \text{ if }i = j \\
      0 & \text{ if } i \neq j,
    \end{cases}
  \end{equation}
  \begin{equation}\label{eqn:}
    \ad_{X_j} f_i
    =
    [\ad_{X_j} Y_{i_1},\dotsc,[Y_{i_{r-1}}, Y_{i_r}]\dotsb]
    + \dotsb
    +
    [ Y_{i_1},\dotsc,[Y_{i_{r-1}}, \ad_{X_j} Y_{i_r}]\dotsb].
  \end{equation}
\end{corollary}
\begin{proof}
  Let $\mathfrak{h}$ denote the subalgebra of $\mathfrak{g}$ generated by the indicated set.  In particular, $\mathfrak{h}$ is a $\mathfrak{t}_{\mathbb{C}}$-module containing $\mathfrak{g}^{\pm \gamma_j}$ for $j=1..n$, so we may write
  \begin{equation*}
    \mathfrak{h} = \mathfrak{t}_{\mathbb{C}} \oplus (\oplus_{\alpha \in E} \mathfrak{g}^{\alpha})
  \end{equation*}
  for some subset $E \subseteq \Phi$ containing $\{\pm \gamma_1, \dotsc, \pm \gamma_n\}$.  We must show that in fact $E = \Phi$.  So let $\beta \in \Phi$; we must show that $\beta \in E$.  We assume that $\beta \in \Phi^+$; a similar proof applies if $- \beta \in \Phi^+$.  By Lemma \ref{lem:roots-partial-sums}, we may write $\beta = \alpha_1 + \dotsb + \alpha_r$ with each partial sum $\alpha_1 + \dotsb + \alpha_s$ also a root.  We show by induction on $r$ that $\beta \in E$.  If $r = 1$, then $\beta \in \Delta \subseteq E$, so suppose $r \geq 2$.  By our inductive hypothesis, $\delta := \alpha_1 + \dotsb + \alpha_{r-1} \in E$.  We have $\beta = \delta + \alpha_r$, with each term a root, so by Lemma \ref{lem:strings-roots}, $\mathfrak{g}^{\beta} = [\mathfrak{g}^\delta, \mathfrak{g}^{\alpha_r}]$.  Our inductive hypothesis gives $\mathfrak{g}^\delta, \mathfrak{g}^{\alpha_r} \subseteq \mathfrak{h}$.  Since $\mathfrak{h}$ is a subalgebra, it follows that $\mathfrak{g}^{\beta} \subseteq \mathfrak{h}$, whence as required that $\beta \in E$.

  This establishes that the indicated set is indeed a generating set for $\mathfrak{g}$.  The remaining assertions follow from the argument just given together and some routine calculation.
\end{proof}

\begin{corollary}\label{cor:nonzero-ideals-detected-via-t}
  Any nonzero ideal $\mathfrak{a}$ of $\mathfrak{g}$ contains a nonzero element of $\mathfrak{t}_{\mathbb{C}}$.
\end{corollary}
\begin{proof}
  The proof is similar to that of the previous corollary (and could even be deduced from its conclusion by considering orthogonal complements).  Since $\mathfrak{a}$ is in particular a $\mathfrak{t}_{\mathbb{C}}$-module, we may write $\mathfrak{a} = \mathfrak{b} \oplus (\oplus_{\alpha \in E} \mathfrak{g}^{\alpha})$ for some subspace $\mathfrak{b} \subseteq \mathfrak{t}_{\mathbb{C}}$ and some subset $E \subseteq \Phi$.  We want to show that $\mathfrak{b}$ is nonzero.  Suppose otherwise that $\mathfrak{b}$ is zero.  Then $E$ is nonempty.  Let $\beta \in E$.  Write $\beta = \alpha_1 + \dotsb + \alpha_r$ as in Lemma \ref{lem:roots-partial-sums}.  Set $Z := [Y_{\alpha_1}, \dotsb, [Y_{\alpha_{r-1}}, [Y_{\alpha_r}, X_{\beta}]] \dotsb ]$.  Then $Z \in \mathfrak{t}_{\mathbb{C}} \cap \mathfrak{b} = \mathfrak{b} = \{0\}$, but several applications of Lemma \ref{lem:strings-roots} (and \eqref{eq:commutators-land-in-C-H-alpha}) implies that $Z \neq 0$, giving the required contradiction.
\end{proof}

\subsection{Proof of Theorem
  \ref{thm:cartan-matrix-determines-K}}
We need one final miscellaneous lemma:
\begin{lemma}\label{lem:pi-1-T-onto-pi-1-K}
  Let $K$ be a compact connected Lie group with maximal torus $T$.  Then the map $\pi_1(T) \rightarrow \pi_1(K)$ induced by the inclusion $T \rightarrow K$ is surjective.
\end{lemma}
\begin{proof}
  (Omitted in lecture; see for instance Section 36.4 of my Fall 2016 notes on Lie groups, linked on the course homepage.)
\end{proof}

We now give the proof of Theorem \ref{thm:cartan-matrix-determines-K}.  Let notation be as in its statement.  We may write $\Delta ' = \{\gamma_1 ', \dotsc, \gamma_n' \}$ and $\Delta '' = \{\gamma_1'', \dotsc, \gamma_n''\}$ so that $\gamma_i'$ corresponds to $\gamma_i''$.  We define $\mathfrak{g} ', \mathfrak{g} ''$ in the obvious way and define $X_i', Y_j' \in \mathfrak{g} '$ and $X_i'', Y_j'' \in \mathfrak{g} ''$ as in \S\ref{sec:gener-lie-algebra}.  We will show more precisely that there exists a unique isomorphism $K' \xrightarrow{\cong } K''$ extending the given isomorphism $\kappa : T' \xrightarrow{\cong } T''$ with the additional property that the induced isomorphism $\mathfrak{g} ' \xrightarrow{\cong } \mathfrak{g} ''$ maps $X_i'$ to $X_i''$ for $i=1..n$.

To that end, we fix linear embeddings of $K'$ and $K''$, hence involutions $\theta$ of $\mathfrak{g} '$ and $\mathfrak{g} ''$ with fixed subspaces $\mathfrak{k} '$ and $\mathfrak{k} ''$.  It suffices to show that there exists a unique $\theta$-equivariant isomorphism $\mathfrak{g} ' \xrightarrow{\cong } \mathfrak{g} ''$ given on $\mathfrak{t}_{\mathbb{C}} '$ by $d \kappa$ and which maps $X_i'$ to $X_i''$.  Indeed, such an isomorphism of complexified Lie algebras induces an isomorphism $\tau : \mathfrak{k} ' \xrightarrow{\cong } \mathfrak{k} ''$ of their $\theta$-fixed subspaces.  What remains to be checked is just that any such isomorphism arises from a Lie group morphism $K' \rightarrow K''$, and similarly for the inverse isomorphism.  From basic Lie theory (see e.g. Theorem 155 of the Fall 2016 notes linked on the course webpage), the Lie group $K'$ admits a universal connected covering group $\widetilde{K'}$, which fits into a short exact sequence $1 \rightarrow \pi_1(K) \rightarrow \widetilde{K'} \rightarrow K' \rightarrow 1$, where $\pi_1(K)$ identifies with a discrete central subgroup of $\widetilde{K'}$.  The given Lie algebra map $\mathfrak{k} ' \rightarrow \mathfrak{k} ''$ extends to a Lie group map $\widetilde{K'} \rightarrow K''$ (see e.g. Theorem 146 of the aforementioned notes); we just need to check that $\pi_1(K')$ lies in the kernel of the latter.  But by Lemma \ref{lem:pi-1-T-onto-pi-1-K}, it suffices to show that $\pi_1(T')$ lies in the indicated kernel, which follows from the fact that the given map $T' \rightarrow T''$ is a well-defined Lie group map.  The completes the proof of the required reduction.

It remains to produce the required $\theta$-equivariant map $\mathfrak{g} ' \rightarrow \mathfrak{g} ''$.  We will do so by constructing its graph.  Let $\mathfrak{g}$ denote the subalgebra of $\mathfrak{g} ' \oplus \mathfrak{g} ''$ generated by $\mathfrak{t}_{\mathbb{C}} \cup \{X_i\} \cup \{Y_i\}$, where
\begin{equation*}
  \mathfrak{t}_{\mathbb{C}} := \{(x', x'' ) \in \mathfrak{t}_{\mathbb{C}} ' \oplus \mathfrak{t}_{\mathbb{C}} '' : d \kappa(x') = x'' \}
\end{equation*}
and $X_i := (X_i', X_i'')$ and $Y_i := (Y_i', Y_i'')$ with $Y_i' := -\theta(X_i'), Y_i'' := -\theta(X_i'')$.  \footnote{End of lecture \#18, Tuesday, 30 Apr 2019} It is enough to check that $\mathfrak{g}$ is the graph of a $\theta$-equivariant isomorphism of Lie algebras; note that if such a morphism exists, then it is uniquely defined and restricts on $\mathfrak{t}_{\mathbb{C} }'$ to $d \kappa$.  The $\theta$-equivariance will follow from the construction of $\mathfrak{g}$.  By applying the same argument with the roles of $\mathfrak{g} '$ and $\mathfrak{g} ''$ reversed, we reduce to showing that $\mathfrak{g}$ is the graph of a morphism of Lie algebras.  Let $\pi ' : \mathfrak{g} \rightarrow \mathfrak{g} '$ and $\pi '' : \mathfrak{g} \rightarrow \mathfrak{g} ''$ denote the projections.  By Corollary \ref{cor:g-gen-via-simple-stuff}, we see that $\pi '$ and $\pi ''$ are surjective.  We might thus hope to define a morphism $f : \mathfrak{g} ' \rightarrow \mathfrak{g} ''$ by requiring that $f(x) := y$ whenever $(x,y) \in \mathfrak{g}$.  Since $\mathfrak{g}$ is a subalgebra, we know that $f$ is a morphism provided that it is well-defined as a set-theoretic map.  We thus need only check that if $(x,y_1)$ and $(x,y_2)$ both belong to $\mathfrak{g}$, then $y_1 = y_2$.  Equivalently, we need to check that if $(0,y)$ belongs to $\mathfrak{g}$, then $y = 0$; in other words, we must show that $\ker(\pi '')$ is trivial.

We claim that
\begin{equation}\label{eq:key-claim-for-proof-of-that-Cartan-determines-K}
  \mathfrak{g} \cap (\mathfrak{t}_{\mathbb{C}}' \oplus
  \mathfrak{t}_{\mathbb{C} }'')
  =
  \mathfrak{t}_{\mathbb{C}}.
\end{equation}
Before proving the claim, we explain why it suffices.  Suppose for the sake of contradiction that $\ker(\pi '')$ is nonzero.  Then $\mathfrak{a} ' := \pi '(\ker(\pi ''))$ is a nonzero ideal of $\mathfrak{g}'$.  By Corollary \ref{cor:nonzero-ideals-detected-via-t}, we have $\mathfrak{a} ' \cap \mathfrak{t}_{\mathbb{C}}' \neq \{0\}$.  We may thus find $0 \neq x' \in \mathfrak{t}_{\mathbb{C} }'$ so that $(x', 0) \in \ker(\pi '')$.  Clearly
\begin{equation}\label{eq:}
  (x',0) \in \mathfrak{g} \cap (\mathfrak{t}_{\mathbb{C}}' \oplus
  \mathfrak{t}_{\mathbb{C} }'').
\end{equation}
From the claim \eqref{eq:key-claim-for-proof-of-that-Cartan-determines-K}, it follows that $(x', 0) \in \mathfrak{t}_{\mathbb{C}}$, whence that $0 = d \kappa(x')$.  Since $d \kappa$ is an isomorphism, this forces $x' = 0$, giving the required contradiction.

We turn finally to the proof of the claim \eqref{eq:key-claim-for-proof-of-that-Cartan-determines-K}.  Let $\mathfrak{h} \subseteq \mathfrak{g} ' \oplus \mathfrak{g} ''$ denote the set defined as on the RHS of \eqref{eqn:decomposition-frak-g-via-simple-root-vecs}, with $e_i, f_i$ defined in terms of $X_i, Y_i \in \mathfrak{g}$ as in \eqref{eqn:defn-e-i}, \eqref{eqn:defn-f-i}.  Then $\mathfrak{t}_{\mathbb{C}} \subseteq \mathfrak{h}$; moreover, $\mathfrak{t}_{\mathbb{C}}$ is self-centralizing in $\mathfrak{h}$, since the $\mathfrak{t}_{\mathbb{C}}$-weights of $\oplus \mathbb{C} e_i$ (resp. of $\oplus \mathbb{C} f_i$) are positive (resp. negative).  In particular, $(\mathfrak{t}_{\mathbb{C}}' \oplus \mathfrak{t}_{\mathbb{C}} '') \cap \mathfrak{h} = \mathfrak{t}_{\mathbb{C}}$, so to establish \eqref{eq:key-claim-for-proof-of-that-Cartan-determines-K}, we reduce to showing that $\mathfrak{g} = \mathfrak{h}$.  To that end, observe first that $\mathfrak{h}$ is contained in $\mathfrak{g}$ and contains the generating set $S := \mathfrak{t}_{\mathbb{C}} \cup \{X_i\} \cup \{Y_i\}$ for $\mathfrak{g}$.  We thereby reduce to verifying that $\mathfrak{h}$ is a subalgebra.  Since $\mathfrak{h} \subseteq \mathfrak{g}$, we may reduce further to showing that $\mathfrak{h}$ is closed under $\ad_{Z}$ for each $Z \in S$.  For this we appeal to the relations noted in Corollary \ref{cor:g-gen-via-simple-stuff}, applied to each component $\mathfrak{g} ', \mathfrak{g} ''$ of $\mathfrak{g} ' \oplus \mathfrak{g} ''$, together with the assumed compatibility between $\gamma_i'$ and $\gamma_i''$.

This completes the proof.

\subsection{Dynkin diagrams}
They provide a convenient pictorial representation of the Cartan matrix.  Here we just summarize what was discussed in class; look at Wikipedia or any of the course references for a detailed discussion.

The vertices of the Dynkin diagram correspond to the simple roots.  For any pair $(\alpha,\beta)$ of \emph{distinct} simple roots, we have seen that there are at most seven possibilities for the pair $(N_{\alpha \beta}, N_{\beta \alpha})$ of integers appearing in the Cartan matrix:
\begin{itemize}
\item $(0,0)$: we draw no edge between $\alpha$ and $\beta$
\item $(-1,-1)$: we draw a simple undirected edge between $\alpha$ and $\beta$
\item $(-1,-2)$: we draw a double oriented edge from $\alpha$ to $\beta$
\item $(-2,-1)$: we draw a double oriented edge from $\beta$ to $\alpha$
\item $(-1,-3)$: we draw a triple oriented edge from $\alpha$ to $\beta$
\item $(-3,-1)$: we draw a triple oriented edge from $\beta$ to $\alpha$
\end{itemize}
We illustrated with $K = \U(n)$ or $\SU(n)$ or $\PU(n)$, where the Cartan matrix looks like (for $n=5$)
\begin{equation*}
  \begin{pmatrix}
    2 & -1 & 0 & 0 & 0  \\
    -1 & 2 & -1 & 0 & 0 \\
    0 & -1  & 2 & -1 & 0 \\
    0 & 0 & -1  & 2 & -1 \\
    0 & 0 & 0 & -1 & 2
  \end{pmatrix}
,
\end{equation*}
so the Dynkin diagram looks like a series of vertices connected by simple edges.

We've focused in this class on Lie groups rather than Lie algebras.  A variant of Theorem \ref{thm:cartan-matrix-determines-K} (with similar proof) is that if $K', K''$ are connected compact Lie groups with maximal tori $T', T''$ of the same dimension, then $\mathfrak{k} ' \cong \mathfrak{k} ''$ iff the Cartan matrices are the same (up to relabeling indices) iff the Dynkin diagrams are the same.

\subsection{Root data}\label{sec:root-data}

\begin{definition}
  A \emph{root datum} is a quadruple $\Psi = (X, \Phi, X^\vee, \Phi^\vee)$ consisting of
  \begin{enumerate}
[(i)]
  \item finitely-generated free abelian groups $X, X^\vee$ of the same rank $n$, thus $X \cong \mathbb{Z}^n, X^\vee \cong \mathbb{Z}^n$,
  \item a perfect $\mathbb{Z}$-linear pairing $X \otimes_{\mathbb{Z}} X^\vee \xrightarrow{(,)} \mathbb{Z}$,
  \item finite subsets $\Phi \subseteq X, \Phi^\vee \subseteq X$, and
  \item a bijection $\Phi \leftrightarrow \Phi^\vee$ denoted $\alpha \leftrightarrow \alpha^\vee$ such that
  \item $(\alpha,\alpha^\vee) = 2$ for all $\alpha \in \Phi$,
  \item $s_\alpha(\lambda) := \lambda - (\lambda,\alpha^\vee) \alpha$ defines a linear automorphism $s_\alpha : X \rightarrow X$ such that $s_\alpha(\Phi) = \Phi$, and
  \item $s_{\alpha^\vee}(\lambda^\vee) := \lambda^\vee - (\alpha,\lambda^\vee) \alpha^\vee$ defines a linear automorphism $s_{\alpha^\vee} : X^\vee \rightarrow X^\vee$ such that $s_{\alpha^\vee}(\Phi^\vee) = \Phi^\vee$.
  \end{enumerate}
  We say that $\Psi$ is \emph{reduced} if $\mathbb{Q} \alpha \cap \Phi = \{\alpha, - \alpha \}$ for all $\alpha \in \Phi$.

  There is an obvious notion of an isomorphism of root data.
\end{definition}

For example, if $K$ is a compact connected Lie group with maximal torus $T$, then
\begin{equation*}
  \Psi(K:T) :=(X(T), \Phi(K:T),X^\vee(T), \Phi^\vee(K:T))
\end{equation*}
is a root datum; here $\Phi^\vee(K:T) := \{\alpha^\vee : \alpha \in \Phi(K:T)\}$.  Using the conjugacy of maximal tori, we see that the isomorphism class of $\Psi(K:T)$ is independent of $T$.
\begin{theorem}
  The map
  \begin{equation*}
    \{\text{compact connected Lie groups } K\}/\sim \, \longrightarrow \, \{\text{reduced root data } \Psi \} / \sim,
  \end{equation*}
  given by sending $K$ to $\Psi(K:T)$ for some maximal torus $T$, is a bijection.
\end{theorem}
\begin{proof}
[Sketch of proof]
  Injectivity follows readily from Theorem \ref{thm:cartan-matrix-determines-K} (exercise).  We omit the proof of surjectivity; one way to proceed would be to argue as in the proof of Theorem \ref{thm:cartan-matrix-determines-K}, but incorporating the Serre relations $\ad_{X_i}^{-N_{i j}+1} X_j = \ad_{Y_i}^{-N_{ij}+1} Y_j = 0$.
\end{proof}
\footnote{End of lecture \#???}



\section{Representations of compact Lie groups}\label{sec:repr-comp-lie}
Let $K$ be a compact connected Lie group.  Our aim in this section is to generalize the results established in \S\ref{sec:char-theory-comp} when $K = \U(n)$.

\subsection{Setup and preliminaries}
Choose a maximal torus $T$ and then a Weyl chamber $C \subseteq \mathfrak{t}_{\mathbb{R}}^{\reg}$.  These choices define a Weyl group $W$ and sets $\Phi \supseteq \Phi^+ \supseteq \Delta$ of roots, positive roots and simple roots, respectively.  Recall that $C$ defines a notion of $\lambda \in \mathfrak{t}_{\mathbb{R}}^*$ being \emph{dominant}, which means that it belongs to the closure $\overline{C^\vee } = \{\lambda : \lambda(H_\alpha) \geq 0 \text{ for all } \alpha \in \Phi^+\}$ of the Weyl chamber $C^\vee = \{\lambda : \lambda(H_\alpha) > 0 \text{ for all } \alpha \in \Phi^+\}$, and \emph{strictly dominant} if in fact $\lambda \in C^\vee$.  Recall that $C$ defines a partial on $\mathfrak{t}_{\mathbb{R}^*}$, where we say that $\lambda_1 \geq \lambda_2$ if $(\lambda_1 - \lambda_2)(C) \geq 0$, or equivalently, if $\lambda_1 - \lambda_2 \in \sum_{\alpha \in \Phi^+} \mathbb{R}_{\geq 0} \alpha$; we say also that $\lambda$ is \emph{positive} if $\lambda \geq 0$.

We remark that one can characterize the subsets $\Phi^+ \subseteq \Phi$ arising in the above way as those for which
\begin{itemize}
\item $\alpha, \beta \in \Phi^+, \alpha + \beta \in \Phi \implies \alpha+\beta \in \Phi^+$ and
\item $\Phi$ is the disjoint union of $\Phi^+$ and $- \Phi^+$.
\end{itemize}
(See any of the main course references.)  The choice of $\Phi^+$ determines $C = \{x : \alpha(x) > 0 \text{ for all } \alpha \in \Phi^+ \}$ and $C^\vee$.  A convenient way to choose $\Phi^+$ is via a \emph{lexicographical ordering}: choose a faithful embedding $K \hookrightarrow \U(n)$, hence an embedding $\mathfrak{t}_{\mathbb{R}} \hookrightarrow \mathbb{R}^n$, identify $\mathfrak{t}_{\mathbb{R}}^*$ with $\mathfrak{t}_{\mathbb{R}}$ in the usual way via the pairing $(x,y) \mapsto \trace(x y)$ (the restriction of the standard Euclidean norm on $\mathbb{R}^n$), and equip $\mathfrak{t}_{\mathbb{R}}^*$ with the lexicographical order coming from $\mathbb{R}^n$.  This is what we did earlier (implicitly) in the case $K = \U(n)$.

We recall some additional facts established in the preceeding section:
\begin{enumerate}
[(i)]
\item $W$ acts simply-transitively on the set of Weyl chambers.
\item (dual form of Corollary \ref{cor:W-conj-to-unique-dominant-element}) Any $\lambda \in \mathfrak{t}_{\mathbb{R}}^*$ is $W$-conjugate to a unique dominant element.
\item Any dominant $\lambda \in \mathfrak{t}_{\mathbb{R}}^*$ satisfies $\lambda \geq w(\lambda)$ for all $w \in W$.
\end{enumerate}

Recall finally the Weyl integral formula (\S\ref{sec:weyl-integr-form-general})
\begin{equation}\label{eqn:WIF-recall-for-reps}
  \int_K f
  = \frac{1}{|W|}
  \int_{g \in K/T}
  \int_{t \in T}
  D(t) f (g t g^{-1}),
\end{equation}
where we integrate with respect to probability Haar measures and where the Jacobian factor $D$ is given explicitly by
\begin{equation}\label{eqn:}
  D(t) = |\det( t - 1  |
  \mathfrak{g}/\mathfrak{t}_{\mathbb{C}})|
  =
  \prod_{\alpha \in   \Phi}
  |t^{\alpha}-1|.
\end{equation}

Our first aim is to factor $D= \Delta \overline{\Delta } = |\Delta|^2$ as in the case $K = \U(n)$.  Such a factorization holds formally with
\begin{equation}\label{eqn:}
  \Delta(t) :=
  \prod_{\alpha > 0}
  (t^{\alpha/2} - t^{-\alpha/2}).
\end{equation}
Unfortunately, $\Delta$ is not in general well-defined as a function on $T$; the source of this issue is that the expressions $t^{\alpha/2}$ themselves are not well-defined, since it may happen that $\alpha \in \mathfrak{t}_{\mathbb{Z}}^*$ but $(1/2) \alpha \notin \mathfrak{t}_{\mathbb{Z}}^*$.  To circumvent this issue, we pass to a covering torus $\tilde{T}$ of $T$.  Recall that the normalized exponential map $e(x) := \exp(2 \pi i x)$ induces an isomorphism $e : \mathfrak{t}_{\mathbb{R}} / \mathfrak{t}_{\mathbb{Z}} \xrightarrow{\sim} T$.  We denote by $\tilde{T}$ the covering torus of $T$ corresponding to the sublattice $2 \mathfrak{t}_{\mathbb{Z}}$ of $\mathfrak{t}_{\mathbb{Z}}$; it comes with an isomorphism $e : \mathfrak{t}_{\mathbb{R}} / 2 \mathfrak{t}_{\mathbb{Z}} \xrightarrow{\sim} \tilde{T}$ and a canonical surjection $\tilde{T} \rightarrow T$.  In other words, choosing a basis for $\mathfrak{t}_{\mathbb{Z}}$, we may identify $T$ with $\mathbb{R}^n/\mathbb{Z}^n$ and $\tilde{T}$ with $\mathbb{R}^n/2 \mathbb{Z}^n$.  In any event, for $\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^*$, we have $\lambda(2 \mathfrak{t}_{\mathbb{Z}}) \subseteq \mathbb{Z}$, and so the character $e^{\lambda} : \tilde{T} \rightarrow \U(1)$ given by $\exp(x) \mapsto \exp(\lambda(x))$ is well-defined.  In particular, the above formula defines $\Delta : \tilde{T} \rightarrow \mathbb{C}$.  We identify $D : T \rightarrow \mathbb{C}$ with a function $D : \tilde{T} \rightarrow \mathbb{C}$ via pullback.  The identity $|\Delta|^2 = D$ then holds.  For a class function $f$ on $K$, the Weyl integral formula \eqref{eqn:WIF-recall-for-reps} implies that
\begin{equation}\label{eqn:}
  \langle f, f \rangle_K
  =
  \frac{1}{W}
  \langle \Delta f, \Delta f \rangle_{\tilde{T}},
\end{equation}
where we identify $f$ with a function on $\tilde{T}$ by restricting to $T$ and then pulling back to $\tilde{T}$, and where we integrate with respect to the probability Haar on $\tilde{T}$.

Set
\begin{equation}\label{eqn:}
  \rho := \frac{1}{2} \sum_{\alpha \in \Phi^+} \alpha.
\end{equation}
It defines an element of $(1/2) \mathfrak{t}_{\mathbb{Z}}^*$, hence a character of $\tilde{T}$.  We may rewrite
\begin{equation}\label{eqn:}
  \Delta(t) =
  t^{\rho}
  \prod_{\alpha > 0}
  (1 - t^{-\alpha})
\end{equation}
We pause to record some basic properties of $\rho$.
\begin{lemma}\label{lem:rho-basic-props}
  \begin{enumerate}
[(i)]
  \item $\rho$ is strictly dominant.  Its $W$-stabilizer is trivial.
  \item For all $w \in W$, we have
    \begin{equation}\label{eqn:formula-for-rho-minus-w-rho}
      \rho - w(\rho)
      = \sum_{
        \substack{
          \alpha \in \Phi : \\
          \alpha > 0, \\
          w^{-1}(\alpha) < 0
        }
      }
      \alpha.        
    \end{equation}
    In particular, $\rho - w(\rho)$ defines a positive element of $\mathfrak{t}_{\mathbb{Z}}^*$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  We first establish the formula \eqref{eqn:formula-for-rho-minus-w-rho}.  We have
  \begin{equation*}
    w(\rho) = \frac{1}{2} \sum_{\alpha > 0} w(\alpha) = \sum_{\alpha: w^{-1}(\alpha) > 0} \alpha = \sum_{\alpha > 0} \eps_\alpha \alpha,
  \end{equation*}
  where
  \begin{equation*}
    \eps_\alpha =
    \begin{cases}
      1 & \text{ if }
      w^{-1}(\alpha) > 0, \\
      -1 & \text{ if } w^{-1}(\alpha) < 0.
    \end{cases}
  \end{equation*}
  Thus
  \begin{equation*}
    \rho - w(\rho) = \sum _{\alpha > 0} \frac{1 - \eps_\alpha }{2} \alpha.
  \end{equation*}
  We have
  \begin{equation*}
    \frac{1 - \eps_\alpha }{2} =
    \begin{cases}
      1 & \text{ if } w^{-1}(\alpha) < 0, \\
      0 & \text{ otherwise},
    \end{cases}
  \end{equation*}
  which leads to the required formula.  It follows immediately that $\rho - w(\rho)$ defines a positive element of $\mathfrak{t}_{\mathbb{Z}}^*$.
  
  Suppose now that $w \in W$ fixes $\rho$.  Then $w$ stabilizes $\Phi^+$, hence $w$ stabilizes $C$.  Since we have seen (in Lemma \ref{lem:W-acts-freely-weyl-chambers}) that $W$ acts freely on the set of Weyl chambers, it follows that $w = 1$.  Thus $\rho$ has trivial $W$-stabilizer.

  It remains only to verify that $\rho$ is strictly dominant, i.e., that for each $\alpha \in \Phi^+$, we have $\rho(H_\alpha) > 0$ .  To that end, note first that since $\rho$ has trivial $W$-stabilizer, it is not fixed by the root reflection $s_\alpha$, and so $\rho(H_\alpha) \neq 0$.  Suppose for the sake of contradiction that $\rho(H_\alpha) < 0$.  Then $s_\alpha(\rho) - \rho = - \rho(H_\alpha) \alpha > 0$.  But we have seen already using the formula \eqref{eqn:formula-for-rho-minus-w-rho} that $s_\alpha(\rho) - \rho < 0$, giving the required contradiction.
\end{proof}



Let $L$ denote the ring of Laurent polynomials on $\tilde{T}$, thus
\begin{equation*}
  L = \oplus_{\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^*} \mathbb{C} e^{\lambda},
\end{equation*}
where as usual $e^\lambda = [t \mapsto t^{\lambda}]$.  The Weyl group $W$ acts on $L$.  We set
\begin{equation*}
  L^{\sym} := \{f \in L : w(f) = f \text{ for all } w \in W\}.
\end{equation*}
For $w \in W$, we denote by $(-1)^w$ the determinant of the action of $w$ on $\mathfrak{t}_{\mathbb{R}}$.  For instance, for the root reflections $s_\alpha$, we have $(-1)^{s_\alpha} = -1$, because reflections have determinant $-1$.  More generally, if $w \in W$ may be written as a product of exactly $k \geq 0$ root reflections, then $(-1)^w = (-1)^k$.  This observation justifies the notation.  We set
\begin{equation*}
  L^{\alt} := \{f \in L : w(f) = (-1)^w f \text{ for all } w \in W\}.
\end{equation*}
Since the (simple) root reflections generate $W$, we may also write
\begin{equation*}
  L^{\alt} = \{f \in L : s_\alpha(f) = - f \text{ for all } \alpha \in \Delta\}.
\end{equation*}
For $\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^*$, define
\begin{equation}\label{eq:}
  A(\lambda)
  :=
  \sum_{w \in W}
  (-1)^w
  e^{w(\lambda)}.
\end{equation}
\begin{lemma}
  \begin{enumerate}
[(i)]
  \item $A(\lambda) \in L^{\alt}$ for all $\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^*$.
  \item $L^{\alt} = \oplus_{\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^* \cap C^\vee} \mathbb{C} A(\lambda)$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item Direct calculation.
  \item Let $f \in L^{\alt}$.  Then $f = \sum_{\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^*} c(\lambda) e^{\lambda}$, where $c(w(\lambda)) = (-1)^w c(\lambda)$ for all $w \in W$.  If $\lambda$ is irregular, i.e., if $\lambda(H_\alpha) = 0$ for some root $\alpha$, then $s_\alpha(\lambda) = \lambda$, while $(-1)^{s_\alpha} = -1$, and so $c(\lambda) = 0$.  If $\lambda$ is regular, then its unique dominant $W$-conjugate $w(\lambda)$ belongs to $C^\vee$, and we have $c(\lambda) = (-1)^w c(w(\lambda))$.  It follows that $f = \sum_{\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^* \cap C^\vee} c(\lambda) A(\lambda)$.  The uniqueness of this decomposition follows from the fact that if $\lambda \in (1/2) \mathfrak{t}_{\mathbb{Z}}^* \cap C^\vee$, then $w(\lambda) \notin C^\vee$ for all nontrivial $w \in W$.
  \end{enumerate}
\end{proof}



Let $\pi$ be a finite-dimensional representation of $G$.  Recall that the \emph{weights} of $\pi$ are those $\mu \in X(T)$ for which the weight space $\pi[\mu] := \{v \in \pi : t v = t^{\mu} v \text{ for all } t \in T\}$ is nonzero.  The character $\chi_\pi$ of $\pi$ is a class function on $G$, which we may identify with a $W$-invariant function on $T$, and in fact an element $\chi_\pi \in L^{\sym}$, given by $\chi_\pi = \sum_{\mu \in X(T)} m_\pi(\mu) e^{\mu}$.  For any element $f \in L$, we may speak more generally of the \emph{weights} of $f$ (i.e., those $\mu \in X(\tilde{T})$ for which the coefficient of $e^{\mu}$ in $f$ is nonzero), and we say that $\lambda$ is
\begin{itemize}
\item a \emph{maximal weight} of $f$ if there does not exist a weight $\mu$ of $f$ with $\mu > \lambda$, and
\item the \emph{highest weight} of $f$ if every weight $\mu$ of $f$ satisfies $\mu \leq \lambda$.
\end{itemize}
Note that $f$ has (in general, many) maximal weights, and that $f$ has a highest weight iff it has a unique maximal weight.  This terminology applies also to $\pi$ in place of $f$ by considering the character $\chi_\pi$.  For instance, $\Delta = \prod_{\alpha > 0} (e^{\alpha/2} - e^{-\alpha/2})\in L^{\alt}$ and $A(\rho) = \sum_{w \in W} (-1)^w e^{w(\rho)} = A(\rho)$ are each of the form $e^{\rho} + \dotsb$ where $\dotsb$ denotes the contributions of weights $w(\rho)$ strictly less than $\rho$, and so $\Delta$ and $A(\rho)$ both have highest weight $\rho$.

\begin{lemma}\label{lem:props-of-delta-vs-L}
  \begin{enumerate}
[(i)]
  \item $\Delta \in L^{\alt}$.
  \item For all $f \in L^{\alt}$, we have $f/\Delta \in L^{\sym}$.
  \item $\Delta = A(\rho)$.
  \item Let $\lambda$ be a dominant element of $(1/2) \mathfrak{t}_{\mathbb{Z}}^*$.  Then $\lambda + \rho$ is a strictly dominant element $(1/2) \mathfrak{t}_{\mathbb{Z}}^*$, and
    \begin{equation}\label{eq:evaluate-schur-poly-at-1-general-group}
      \frac{A(\lambda+\rho)}{A(\rho)}(1)
      = \prod _{\alpha > 0}
      \frac{\langle \lambda + \rho, H_\alpha  \rangle
      }{
        \langle \rho, H_\alpha  \rangle}.
    \end{equation}
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
[(i)]
  \item It is enough to check for each simple root $\beta$ that
    \begin{equation}\label{eq:}
      \Delta(s_\beta(t)) = - \Delta(t).
    \end{equation}
    Using that $s_\beta^2 =1$, we compute that
    \begin{equation}\label{eq:}
      \Delta(s_\beta(t))
      =
      \prod_{\alpha>0}
      (t^{s_\beta(\alpha)/2}
      - t^{-s_\beta(\alpha)/2}).
    \end{equation}
    Since $s_\beta(\beta) = -\beta$, we have
    \begin{equation}\label{eq:}
      (t^{s_\beta(\beta)/2}
      - t^{-s_\beta(\beta)/2})
      =
      -(t^{\beta/2} - t^{-\beta/2}).
    \end{equation}
    Let $\alpha \in \Phi^+ - \{\beta\}$.  Then (see the homework for details concerning the following terminology and facts) the Weyl chambers $C$ and $s_\beta(C)$ are adjacent, separated by the wall $\beta^\perp$ and no other wall.  In particular, $\alpha$ takes the same sign on $C$ and $s_\beta(C)$, hence $\alpha$ is positive on $s_\beta(C)$; equivalently, $s_\beta(\alpha)$ is positive on $C$, i.e., $s_\beta(\alpha) > 0$.  Thus $s_\beta$ acts on $\Phi^+ - \{\beta\}$, and so
    \begin{equation}\label{eq:}
      \prod_{\alpha>0, \alpha \neq \beta }
      (t^{s_\beta(\alpha)/2}
      - t^{-s_\beta(\alpha)/2})
      =
      \prod_{\alpha>0, \alpha \neq \beta }
      (t^{\alpha/2}
      - t^{-\alpha/2}).
    \end{equation}
    The required identity follows.
  \item It is enough to show that $f$ is divisible by $\Delta$ in $L$.  We record two proofs (the second of which was given in lecture, the first of which I find a bit more natural).  Both proofs use the fact that for distinct positive roots $\alpha$ and $\beta$,
    \begin{equation}\label{eq:C-alpha-not-C-beta}
      \mathbb{C} \alpha \cap \mathbb{C} \beta = \{0\}.
    \end{equation}
    (To see this, it suffices to check that $\beta \notin \mathbb{C} \alpha$, which follows from the identity $\mathbb{C} \alpha \cap \Phi = \{\alpha, - \alpha \}$ and our assumptions.)
    \begin{enumerate}
    \item (First proof) Set $Z := \{z \in (\mathbb{C}^\times)^n : \Delta(z) = 0 \}$.  Then $z \in Z$ iff $z^\alpha = 1$ for some positive root $\alpha$.  The identity $z^\alpha = 1$ is equivalent to $s_\alpha(z) = z$.  Let $f \in L^{\alt}$ and $z \in Z$.  Then, choosing $\alpha$ so that $s_\alpha(z) = z$, we have $f(z) = f(s_\alpha(z)) = - f(z)$, hence $f(z) = 0$.  Thus $f$ vanishes on $Z$.  By the Nullstellensatz, it follows that $f$ belongs to the radical of the ideal in $\Delta$ generated by $\Delta$.  Note that $L$ is a UFD (being a localization of the UFD $\mathbb{C}[t_1,\dotsc,t_n]$).  To conclude, it suffices to show that $\Delta$ defines a squarefree element of $L$, i.e., that no prime factor of $\Delta$ occurs with multiplicity $> 1$.

      We first compute the prime factorization of $t^\alpha - 1$ for each nontrivial character $\alpha$ of $\tilde{T}$.  The character group $X(\tilde{T})$ is isomorphic to $\mathbb{Z}^n$, where $n = \dim(\tilde{T})$.  Suppose that $\alpha$ is of the form $d \beta$ for some natural number $d$ and some character $\beta$ of $\tilde{T}$, with $d$ chosen maximally.  Then
      \begin{equation}\label{eq:prime-factorization-t-alpha-minus-one}
        t^\alpha - 1 = \prod (t^\beta - \zeta),
      \end{equation}
      with the product taken over all $d$th roots of unity $\zeta$.  We claim that each factor $t^\beta - \zeta$ is irreducible.  For this it suffices to show more generally that if $\alpha$ is a \emph{primitive} nontrivial character of $\tilde{T}$ (thus $\alpha$ corresponds to a nonzero element of $\mathbb{Z}^n$ whose coordinates are relatively prime) and $\zeta \in \mathbb{C}^\times$, then $t^\alpha - \zeta$ is irreducible.  Indeed, since $\alpha$ is primitive, we may apply a linear change of variables on $\mathbb{Z}^n$ to reduce to the case that $\alpha = (1,0,\dotsc,0)$, so that $t^\alpha - \zeta = t_1 - \zeta$, which is clearly irreducible.  (The existence of such a change of variables is a simple exercise in linear algebra, closely related to the fact that finitely-generated torsion-free modules over $\mathbb{Z}$ are free; indeed, the latter fact implies that the quotient $X(\tilde{T})/\mathbb{Z} \alpha$ is free, hence that the surjection to that quotient from $X(\tilde{T})$ splits, leading to a direct sum decomposition $X(\tilde{T}) = \mathbb{Z} \alpha \oplus (\dotsb)$ and hence the required change of variables.)
      

      Since no prime factor in \eqref{eq:prime-factorization-t-alpha-minus-one} occurs twice, we see in particular that $t^\alpha - 1$ is squarefree.  To establish the same for $\Delta$, we need to check that no prime factor of $t^\alpha - 1$ divides $t^\beta - 1$ whenever $\alpha$ and $\beta$ are distinct positive roots.  This follows from \eqref{eq:prime-factorization-t-alpha-minus-one} and \eqref{eq:C-alpha-not-C-beta}.
    \item (Second proof) We show first that if $f \in L$ vanishes on $\{z : z^\alpha = 1\}$, then $f / (t^\alpha - 1) \in L$.  Choosing coordinates $t_1,\dotsc,t_n$ on $\tilde{T}$, we may represent $\alpha$ as $(\alpha_1,\dotsc,\alpha_n)$ for some integers $\alpha_j$.  We may assume for convenience, by inverting the coordinate $t_n$ if necessary, that $\alpha_n \geq 0$.  Setting $R := \mathbb{Z}[t_1^{\pm 1}, \dotsc, t_{n-1}^{\pm 1}]$, we then have $t^{\alpha} - 1 \in R[t_n]$.  We may assume, after multiplying $f$ by a sufficiently large power of $t_n$, then $f \in R[t_n]$.  By division with remainder in the polynomial $t_n$, we then have $f = (t_n^{\alpha_n} - \prod_{j < n} t_j^{-\alpha_j}) q + r$ for some $q, r \in R[t_n]$ for which the degree in $t_n$ of $r$ is strictly less than $\alpha_n$.  This identity implies that $r$ vanishes on the solution set to the equation $t^\alpha = 1$.  For any given values $t_1,\dotsc,t_{n-1} \in \mathbb{C}^\times$ of the first $n-1$ coordinates, that equation has exactly $\alpha_n$ solutions in the variable $t_n$.  The noted degree bound on $r$ thus implies that $r=0$.  It follows as required that $f$ is divisible by $t^\alpha - 1$.

      From \eqref{eq:C-alpha-not-C-beta} we see that for distinct positive roots $\alpha$ and $\beta$, the tangent planes to the hypersurfaces $\{t : t^\alpha = 1\}$ and $\{t : t^\beta = 1\}$ are traverse at any point of the intersection $\{t : t^\alpha = t^\beta = 1\}$.  Thus if $f$ vanishes on $\{z : z^\beta = 1\}$ and is divisible by $t^\alpha -1$, then $f / (t^\alpha - 1)$ likewise vanishes on $\{z : z^\beta = 1\}$.  We may thus iteratively apply the argument of the preceeding paragraph to obtain the required conclusion.
    \end{enumerate}
    
  \item By the previous item, we have $A(\rho)/\Delta \in L^{\sym}$.  We've noted already that $\Delta$ and $A(\rho)$ each have $\rho$ as highest weight.  If $\lambda$ is any maximal weight of $A(\rho)/\Delta$, then $\lambda + \rho = \rho$, whence $\lambda = 0$.  Thus $A(\rho)/\Delta$ is a complex scalar; since $A(\rho)$ and $\Delta$ are each of the form $e^{\rho} + (\dotsb)$, we conclude that $A(\rho)/\Delta = 1$.
  \item We have seen that $\rho$ is strictly dominant.  Since $\lambda$ is dominant, it follows that $\lambda + \rho$ is strictly dominant.  It remains to check the formula \eqref{eq:evaluate-schur-poly-at-1-general-group}.  This requires taking a limit as in the proof of the analogous result for $\U(n)$ (Lemma \ref{lem:schur-evalu-at-1-U-n}).  Let us first rewrite the identity $\Delta = A(\rho)$ established above in the form: for $x \in \mathfrak{t}_{\mathbb{C}}$,
    \begin{equation}
      \sum_{W}
      (-1)^w
      e^{\langle w(x), \rho \rangle}
      =
      \prod_{\alpha > 0}
      (
      e^{\alpha(x)/2} - e^{-\alpha(x)/2}
      ).
    \end{equation}
    A similar argument (applied with the roles of $\mathfrak{t}_{\mathbb{C}}$ and $\mathfrak{t}_{\mathbb{C}}^*$ reversed) gives for $\lambda \in \mathfrak{t}_{\mathbb{C}}^*$ that
    \begin{equation}
      \sum_{W}
      (-1)^w
      e^{\langle w(\lambda),  H_\rho \rangle}
      =
      \prod_{\alpha > 0}
      (
      e^{\lambda(H_\alpha)/2} - e^{-\lambda(H_\alpha)/2}
      ).
    \end{equation}
    It follows that for $t = \exp(\eps H_\rho)$ with $\eps > 0$ small, we have
    \begin{equation*}
      \sum_{W} (-1)^w t^{w(\lambda)} = \prod_{\alpha > 0} ( e^{\eps \lambda(H_\alpha)/2} - e^{-\eps \lambda(H_\alpha)/2} ) \sim \eps^{|\Phi_+|} \prod_{\alpha > 0} \langle \lambda, H_\alpha \rangle.
    \end{equation*}
    Thus
    \begin{equation*}
      \frac{ \sum_{W} (-1)^w t^{w(\lambda + \rho )} } { \sum_{W} (-1)^w t^{w(\rho )} } |_{t=1} = \prod_{\alpha > 0} \frac{ \langle \lambda + \rho , H_\alpha \rangle }{ \langle \rho, H_\alpha \rangle },
    \end{equation*}
    as required.
  \end{enumerate}
\end{proof}

\subsection{Classification of irreducibles}
\begin{theorem}\label{thm:highest-weight-general-gp}
  Let $K$ be a compact connected Lie group with maximal torus $T$, hence root set $\Phi$ and Weyl group $W$.  Choose a Weyl chamber $C$, hence sets $\Phi^+$ and $\Delta$ of positive and simple roots, respectively, as well as a partial order $\geq$ on $\mathfrak{t}_{\mathbb{R}}^*$.

  Each $\pi \in \Irr(K)$ has a highest weight $\lambda \in X(T) = \mathfrak{t}_{\mathbb{Z}}^*$; this is a dominant element for which the Weyl character formula
  \begin{equation*}
    \chi_{\pi} = \frac{A_{\lambda+\rho}}{A_\rho }
  \end{equation*}
  holds, with
  \begin{equation*}
    A_{\mu} := \sum_{w \in W} (-1)^w e^{w(\mu)} \in X(\tilde{T}) = \frac{1}{2} \mathfrak{t}_{\mathbb{Z}}^*, \quad \rho := \frac{1}{2} \sum_{\alpha > 0} \alpha.
  \end{equation*}
  The map $\pi \mapsto \lambda$ defines a bijection
  \begin{equation}\label{eq:}
    \Irr(K)
    \leftrightarrow \{\text{dominant }
    \lambda \in X(T)\}
    \leftrightarrow
    X(T)/W.
  \end{equation}
\end{theorem}
\begin{proof}
  Let $\pi \in \Irr(K)$.  Let $\lambda \in X(T)$ be any maximal weight of $\pi$.  Since $\chi_\pi$ is $W$-invariant, and since every element $\mu$ of $X(T)$ is $W$-conjugate to a unique dominant element which is then $\geq \mu$, we know that $\lambda$ is dominant.  Since $\Delta$ has highest weight $\rho$ with $\Delta = e^\rho + \dotsb$, we know that $\Delta \chi_\pi \in L^{\alt}$ has $\lambda + \rho$ as a maximal weight, with $\Delta \chi_\pi = m_\pi(\lambda) e^{\lambda+\rho} + \dotsb$.  Regrouping terms into monomial alternating functions gives
  \begin{equation}\label{eq:WCF-Delta-chi-pi}
    \Delta \chi_\pi
    = m_\pi(\lambda) A_{\lambda+\rho} + \dotsb,
  \end{equation}
  where $\dotsb$ denotes a linear combination of $A_{\mu+\rho}$ taken over $\mu$ with $\mu+\rho$ strictly dominant and $\mu$ not greater than $\lambda$.  By the Weyl integral formula and Schur orthogonality, we have (as in the case of $\U(n)$)
  \begin{equation}\label{eqn:WCF-general-long}
    1 = \langle \chi_\pi, \chi_\pi  \rangle_G
    =
    \frac{\langle D \chi_\pi, \chi_\pi  \rangle_T}{|W|}
    =
    \frac{\langle \Delta \chi_\pi, \Delta \chi_\pi
      \rangle_{\tilde{T}}}  {|W|}
    =
    \frac{\|m_\pi(\lambda) A_{\lambda+\rho} + \dotsb \|^2}{|W|}
    \geq |m_\pi(\lambda)|^2.
  \end{equation}
  Here we used that $\lambda+\rho$ is strictly dominant and that for strictly dominant $\lambda_1, \lambda_2$,
  \begin{equation*}
    \langle A_{\lambda_1}, A_{\lambda_2} \rangle = 
\begin{cases}
      |W| & \text{ if } \lambda_1 = \lambda_2, \\
      0 & \text{ otherwise}
    \end{cases}
.
  \end{equation*}
  Since $m_\pi(\lambda) \in \mathbb{Z}_{\geq 1}$, we deduce from \eqref{eqn:WCF-general-long} that $m_\pi(\lambda) = 1$ and hence that equality holds in each step, i.e., that the $\dotsb$ in \eqref{eq:WCF-Delta-chi-pi} vanishes.  This gives the required formula for $\chi_\pi$ in terms of $\lambda$.  The proof shows also that $\lambda$ is uniquely determined, hence is the unique maximal weight, i.e., the highest weight of $\pi$.  We thus obtain an injective map $\Irr(K) \rightarrow \{\text{dominant } \lambda \in X(T)\}$ as in the statement of the theorem; to complete the proof, it remains only to show that this map is surjective.  We imitate the first proof given earlier for $\U(n)$.  Suppose $\lambda$ is a dominant element of $X(T)$ not arising as the highest weight of any element of $\Irr(K)$.  By Lemma \ref{lem:props-of-delta-vs-L}, the ratio $A_{\lambda+\rho}/A_{\rho}$ defines an element of $L^{\sym}$, hence in particular a function $\tilde{T} \rightarrow \mathbb{C}$.  We claim that this function factors through $T$.  To see this, we write
  \begin{equation*}
    \frac{A_{\lambda+\rho}}{A_{\rho}} = \frac { \sum_{w \in W} (-1)^w e^{w(\lambda+\rho) - \rho } } { e^{\rho} \prod_{\alpha > 0} (1 - e^{-\alpha}) },
  \end{equation*}
  and observe that having rewritten the ratio in this way, both the numerator and denominator factor through $T$: for the denominator, this is clear, while for the numerator, we use that $w(\lambda+\rho) - \rho = w(\lambda) - (\rho - w(\rho))$, which belongs to $\mathfrak{t}_{\mathbb{Z}}^*$ thanks to Lemma \ref{lem:rho-basic-props}.  This completes the verification of the claim.  Thus $A_{\lambda+\rho}/A_\rho$ defines a $W$-invariant function $T \rightarrow \mathbb{C}$, hence a class function $K \rightarrow \mathbb{C}$.  For each $\pi \in \Irr(K)$, say with highest weight $\mu$, then the Weyl integral formula gives
  \begin{equation*}
    \langle A_{\lambda+\rho}/A_\rho, \chi_\pi \rangle_K = \langle A_{\lambda+\rho}, \Delta \chi_\pi \rangle_{\tilde{T}} = \langle A_{\lambda+\rho}, A_{\mu+\rho} \rangle_{\tilde{T}}= 0,
  \end{equation*}
  using in the last step that $\lambda, \mu$ are dominant and distinct.  The Peter--Weyl theorem then implies that $A_{\lambda+\rho}/A_\rho = 0$, which is absurd.  This completes the proof of the required contradiction.
\end{proof}

\begin{remark}
  We indicate a way to formulate the conclusion of Theorem \ref{thm:highest-weight-general-gp} without explicit reference to a maximal torus.  Recall from \S\ref{sec:root-data} that there is a bijection
  \begin{equation*}
    \{\text{compact connected } K\}/\sim \quad \leftrightarrow \quad \{\text{reduced root data } \Psi = (X, \Phi, X^\vee, \Phi^\vee)\}
  \end{equation*}
  \begin{equation*}
    K \mapsto \Psi(K) := (X(T), \Phi(K:T), X^\vee(T), \Phi^\vee).
  \end{equation*}
  There is a ``duality'' $\Psi \mapsto \Psi^\vee$ on the set of (reduced) root data given by
  \begin{equation*}
    \Psi^\vee := (X^\vee, \Phi^\vee, X, \Phi).
  \end{equation*}
  We may define the dual group $K^\vee$ of $K$ to be the connected compact Lie group (up to isomorphism) having the dual root datum, i.e., $\Psi(K^\vee) = \Psi(K)^\vee$; it comes with a maximal torus $T^\vee \subseteq K^\vee$ having natural identifications $X(T^\vee) = X^\vee(T)$ and $X^\vee(T^\vee) = X(T)$, and we may furthermore identify the Weyl group $W$ of $T$ with the Weyl group of $T^\vee$.  We then have natural bijections
  \begin{align*}
    \Irr(K)
    &\cong
      \{\text{dominant } \lambda \in X^\vee(T)\}
    \\
    &\cong
      X(T) / W
    \\
    &\cong
      X^\vee(T^\vee) / W
    \\
    &\cong
      \Hom(\U(1), K^\vee)/\sim,
  \end{align*}
  where the equivalence relation $\sim$ in the last expression is given by conjugation.
\end{remark}

\subsection{Borel--Weil}
\label{sec:borel-weil}
Recall the usual notation: $K$ is a compact connected Lie group with Lie algebra $\mathfrak{k}$, complexification $G$, and complexified Lie algebra $\mathfrak{g}$.  Recall that $\Irr(K)$ is in natural bijection with the set of isomorphism classes of holomorphic (equivalently, algebraic) finite-dimensional irreducible representation of $G$.  We fix a maximal torus $T \leq K$, with Lie algebra denoted $\mathfrak{t}$, and set $\mathfrak{t}_{\mathbb{R}} := i \mathfrak{t} \subseteq \mathfrak{t} _{\mathbb{C}}$, as usual.  We denote by $A \leq G$ the connected (complex) Lie subgroup with Lie algebra $\Lie(A) = \mathfrak{t}_{\mathbb{C}}$.  For example, if $K = \U(n)$, so that $G = \GL_n(\mathbb{C})$, then with the choice
\begin{equation*}
  T = 
\begin{pmatrix}
    \U(1) &  &  \\
    & \ddots &  \\
    & & \U(1)
  \end{pmatrix}
\end{equation*}
we have
\begin{equation*}
  A = 
\begin{pmatrix}
    \mathbb{C}^\times &  &  \\
    & \ddots &  \\
    & & \mathbb{C}^\times
  \end{pmatrix}
.
\end{equation*}
Fix a Weyl chamber $C \subseteq \mathfrak{t}_{\mathbb{R}}$ and hence choices of positive roots $\Phi^+$ and simple roots $\Delta$, as usual.  Set
\begin{equation*}
  \mathfrak{n} := \oplus_{\alpha \in \Phi^+}
\end{equation*}
and
\begin{equation*}
  \mathfrak{b} := \mathfrak{n} \oplus \mathfrak{t}_{\mathbb{C}} \leq \mathfrak{g}.
\end{equation*}
(The directness of the sum follows immediately from the root space decomposition of $\mathfrak{g}$.)  For example, if we take $K = \U(n)$, $T$ as above, and for $C$ the ``standard'' Weyl chamber $\{(x_1,\dotsc,x_n) : x_i > x_j \text{ for } i < j \}$, then $\Phi^+ = \{\eps_i - \eps_j : i < j\}$, and (for $n=3$, say)
\begin{equation*}
  \mathfrak{n} = \oplus_{i < j} \mathbb{C} E_{i j } = 
\begin{pmatrix}
    0 & \ast & \ast \\
    & 0 & \ast \\
    & & 0
  \end{pmatrix}
,
\end{equation*}
\begin{equation*}
  \mathfrak{b} = 
\begin{pmatrix}
    \ast & \ast & \ast \\
    & \ast & \ast \\
    & & \ast
  \end{pmatrix}
,
\end{equation*}
\begin{equation*}
  N = 
\begin{pmatrix}
    1 & \ast & \ast \\
    & 1 & \ast \\
    & & 1
  \end{pmatrix}
,
\end{equation*}
\begin{equation*}
  B = 
\begin{pmatrix}
    \ast & \ast & \ast \\
    & \ast & \ast \\
    & & \ast
  \end{pmatrix}
.
\end{equation*}
Using that the sum of two positive roots, if it is a root, is positive, we see that $\mathfrak{n}$ and $\mathfrak{b}$ are Lie subalgebras of $\mathfrak{g}$, with $\mathfrak{n}$ an ideal of $\mathfrak{b}$.  Hence they correspond to some connected (complex) Lie subgroups $N$ and $B$ of $G$, with $N$ a normal Lie subgroup of $B$.
\begin{lemma}
  \begin{enumerate}
[(i)]
  \item The Lie subgroups $N, A$ and $B$ of $G$ are closed.
  \item $N \cap A = \{1\}$.
  \item $B = N A$; more precisely, $B$ is the semidirect product of $N$ and $A$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  In the special case $K = \U(n)$, these conclusions are clear by inspection of the explicit descriptions given above (which depended upon the choice of $C$, but the choice doesn't matter, because any two choices are $W$-conjugate).  In general, choose a faithful representation $(\pi,V)$ of $K$ and a basis $e_1,\dotsc,e_n$ of $V$ consisting of weight vectors for $T$, thus $t e_j = t^{\lambda_j} e_j$ for some $\lambda_j \in X(T)$.  We may assume that this basis has been ordered in such a way that if $\lambda_i < \lambda_j$ (with respect to the partial order defined by $C$), then $i < j$.  Since for $x \in \mathfrak{g}^\alpha$ with $\alpha > 0$ we have $d \pi(x) : V^{\lambda} \rightarrow V^{\lambda+\alpha}$ and $\lambda + \alpha > \lambda$, we see that $d \pi(x) \in 
\begin{pmatrix}
    0 & \ast & \ast \\
    & 0 & \ast \\
    & & 0
  \end{pmatrix}
$.  On the other hand, $d \pi(\mathfrak{t}_{\mathbb{C}}) \subseteq 
\begin{pmatrix}
    \ast & \ast & \ast \\
    & \ast & \ast \\
    & & \ast
  \end{pmatrix}
$.  Exponentiating, it follows that $\pi(N) \subseteq 
\begin{pmatrix}
    1 & \ast & \ast \\
    & 1 & \ast \\
    & & 1
  \end{pmatrix}
$ and $\pi(A) \subseteq 
\begin{pmatrix}
    \ast &  &  \\
    & \ast &  \\
    & & \ast
  \end{pmatrix}
$.  In particular, $A \cap N = \{1\}$.

  We claim now that $A$ and $N$ are closed.  (The remaining assertions then follow readily.)  We identify $G$ with its image under $\pi$.  (Apologies in advance if these proofs are sloppy.  The customary way to develop this material is in the language of algebraic groups.  I don't know offhand of a reference for the approach taken here.)

  Start with $N$.  Recall that, by definition, $N$ is generated by the image under $\exp$ of small elements of $\mathfrak{n}$.  Let $\mathfrak{n}_1 := 
\begin{pmatrix}
    0 & \ast & \ast \\
    & 0 & \ast \\
    & & 0
  \end{pmatrix}
 \geq \mathfrak{n}$ denote the Lie algebra consisting of strictly upper-triangular matrices, and $N_1 := 
\begin{pmatrix}
    1 & \ast & \ast \\
    & 1 & \ast \\
    & & 1
  \end{pmatrix}
  \geq N$ the corresponding Lie gruop.
  Then we have mutually inverse diffeomorphisms $\exp : \mathfrak{n}_1 \rightarrow N_1$ and $\log : N_1 \rightarrow \mathfrak{n}_1$ given by finite Taylor series.  In particular, the exponential map is defined on all of $\mathfrak{n}$.  Using the BCH formula, we see that in fact $N = \exp(\mathfrak{n})$.  Since $\mathfrak{n}$ is closed in $\mathfrak{n}_1$, it follows that $N$ is closed in $N_1$.  Since $N_1 \cap G$ is closed in $G$, we conclude that $N$ is closed in $G$.

  We turn to $A$.  Let $A_1 := 
  \begin{pmatrix}
    \mathbb{C}^\times  &  &  \\
    & \mathbb{C}^\times  &  \\
    & & \mathbb{C}^\times
  \end{pmatrix}
  \geq A$ denote the diagonal subgroup of $\GL_n(\mathbb{C})$, and $T_1 := 
\begin{pmatrix}
    \U(1) &  &  \\
    & \U(1) &  \\
    & & \U(1)
  \end{pmatrix}
  $ its standard maximal torus, so that $A_1 \geq T_1 \geq T$.  We get $\mathfrak{t}_{1,\mathbb{R}} \geq \mathfrak{t}_{\mathbb{R}}$ and $\mathfrak{t}_{1,\mathbb{Z}} \geq \mathfrak{t}_{\mathbb{Z}}$.  Since $T_1 \hookrightarrow T$, we have $\mathfrak{t}_{\mathbb{Z}} = \mathfrak{t}_{1,\mathbb{Z}} \cap \mathfrak{t}_{\mathbb{R}}$.  Thus we can find a lattice $L \subseteq \mathfrak{t}_{1,\mathbb{Z}}$ complementary to $\mathfrak{t}_{\mathbb{Z}}$, i.e., so that $\mathfrak{t}_{1,\mathbb{Z}} = \mathfrak{t}_{\mathbb{Z}} \oplus L$.  Choosing bases for these lattices, we can identify $\mathfrak{t}_{1,\mathbb{Z}}$ with $\mathbb{Z}^n$ and $\mathfrak{t}_{\mathbb{Z}}$ with $\mathbb{Z}^m$ for some $0 \leq m \leq n$ in such a way that the inclusion $\mathfrak{t}_{\mathbb{Z}} \hookrightarrow \mathfrak{t}_{1,\mathbb{Z}}$ is the standard one.  In the coordinates defined by these bases, the inclusions $T = \U(1)^m \hookrightarrow T_1 = \U(1)^n$ and $A = (\mathbb{C}^\times)^m \hookrightarrow (\mathbb{C}^\times)^n = A_1$ are then likewise the standard ones; in particular, the latter has closed image.
\end{proof}

In particular, $B/N \cong A$.  We may and shall thus extend each homomorphism $\chi : A \rightarrow \mathbb{C}^\times$ to a homomorphism $\chi : B \rightarrow \mathbb{C}^\times$ that is trivial on $N$, thus $\chi(n a) = \chi(a)$.  We'll often write $b^{\chi} := \chi(b)$.

\begin{theorem}\label{thm:highest-weight-borel-weil}
  Let $\pi$ be an irreducible holomorphic finite-dimensional representation of $G$.  There is a unique holomorphic character $\lambda : A \rightarrow \mathbb{C}^\times$ and a unique-up-to-scalars nonzero vector $v \in \pi$ so that
  \begin{equation}\label{eqn:v-is-a-B-eigenvector}
    \pi(b) v = b^{\lambda} v
    \text{ for all }
    b \in B.
  \end{equation}
  Moreover, $\lambda$ is the highest weight of $\pi$ and $v$ is a highest weight vector.
\end{theorem}
\begin{proof}
  Existence: Let $\lambda \in X(T)$ be the highest weight of $\pi$ (via Theorem \ref{thm:highest-weight-general-gp}, and using Theorem \ref{thm:alg-rep} to regard $\pi$ as an irreducible representation of $K$), and $v \in \pi[\lambda]$ a highest weight vector.  We extend $\lambda$ to a holomorphic character $A \rightarrow \mathbb{C}^\times$.  The vector $v$ is an eigenvector of $T$ with eigenvalue $\lambda$, hence also of $A$ with eigenvalue $\lambda$.  A short calculation as in the proof of Lemma \ref{lem:root-spaces-permute-weight-spaces} shows that $d \pi(X_\alpha) : \pi[\lambda] \rightarrow \pi[\lambda+\alpha]$.  Since $\lambda$ is the highest weight of $\pi$, we have $\pi[\lambda+\alpha] = \{0\}$.  Thus $X_\alpha v = 0$.  Since $\alpha$ was arbitrary, $v$ is annihilated by $\mathfrak{n}$.  Since $N$ is connected (by construction), it follows that $v$ is fixed by $N$.  Thus $v$ is a $B$-eigenvector, as required.

  Uniqueness: Suppose given a pair $(v,\lambda)$ as in the conclusion of the theorem.  Our task is to show that $\lambda$ is in fact the highest weight $\pi$.  For each finite ordered tuple $(\alpha_1,\dotsc,\alpha_n)$ of positive roots, the vector
  \begin{equation}\label{eq:}
    X_{-\alpha_1} \dotsb X_{-\alpha_n} v
    :=
    d \pi(X_{-\alpha_1}) \dotsb d \pi(X_{-\alpha_n}) v
  \end{equation}
  is a $T$-eigenvector of weight $\lambda - \alpha_1 - \dotsb - \alpha_n$, which is strictly lower than $\lambda$ when $n \geq 1$.  Call the span of the indicated vectors $V$.  It suffices to show that $V = \pi$.  Since $v \in V$ and $\pi$ is irreducible, it suffices to show that $V$ is $K$-invariant.  Since $K$ is connected, it is enough to show that $V$ is $\mathfrak{g}$-invariant.  To that end, recall that $\mathfrak{g} = \mathfrak{t}_{\mathbb{C}} \oplus (\oplus_{\alpha >0} \mathbb{C} X_\alpha) \oplus (\oplus_{\alpha > 0} \mathbb{C} X_{-\alpha})$.  It's clear that $V$ is invariant by $\mathfrak{t}_{\mathbb{C}}$ and by $X_{-\alpha}$ for $\alpha > 0$, so it remains only to show for $\alpha > 0$ that $V$ is $X_{\alpha}$-invariant.  Differentiating the condition \eqref{eqn:v-is-a-B-eigenvector}, we see that $v$ is annihilated by $X_{\alpha}$.  It remains to check for $n \geq 1$ that $X_{\alpha} X_{-\alpha_1} \dotsb X_{-\alpha_n} v$ belongs to $V$.  Set $u := X_{-\alpha_2} \dotsb X_{-\alpha_n} v$.  Then $X_{\alpha} X_{-\alpha_1} u = X_{-\alpha_1} X_\alpha u + [X_{\alpha}, X_{-\alpha_1}] u$.  By induction on $n$, both $X_{-\alpha} u$ and $[X_{\alpha}, X_{-\alpha_1}] u$ belong to $V$.  Since $X_{-\alpha_1}$ stabilizes $V$, the required conclusion follows.
  
  % Let $X \in \mathfrak{t}_{\mathbb{C}} \cup \{X_\alpha : \alpha > 0\}$ and $\beta > 0$.  Then
  % \begin{equation*}
  %   X \cdot X_{-\beta} \cdot v = X_{-\beta} \cdot X \cdot v + [X, X_{-\beta}] \cdot v.
  % \end{equation*}
  % We observe next that
  % \begin{equation*}
  %   X \cdot v { = \begin{cases}
    %     0 & \text{ if } X = X_\alpha, \\
  %     \lambda(X) v & \text{ if } X \in \mathfrak{t}_{\mathbb{C}}
  %   \end{cases}
  % }.
  % \end{equation*}
  % !!! TODO
\end{proof}

Now let $\lambda \in X(T)$ be a dominant weight, and consider the vector space
\begin{equation}\label{eq:}
  F_\lambda := \{f : G \rightarrow \mathbb{C} :
  \text{holomorphic},
  f(b g) = b^{-\lambda} f(g) \text{ for } b \in B, g \in G
  \}.
\end{equation}
The group $G$ (and hence also its subgroup $K$) acts on $F_\lambda$ by right translation.  We will show the following:
\begin{theorem}\label{thm:borel-weil-construction}
  Let $\pi_\lambda$ denote the irreducible representation of $K$ of highest weight $\lambda$.  Then $\pi_\lambda \cong F_\lambda^*$.  Here $F_\lambda^*$ denotes the dual representation of $F_\lambda$, regarded as a representation of $K$; alternatively, we may use Theorem \ref{thm:alg-rep} to extend $\pi_\lambda$ to an algebraic representation of $G$, and the isomorphism is then of algebraic representations of $G$.
\end{theorem}

We compute in passing the dual of $\pi_\lambda$.  Note that if $C$ is a Weyl chamber, then so is $-C$; since the Weyl group acts simply-transitively on the chambers, there is thus a unique $w_0 \in W$ for which $w_0(C) = - C$.  (It is often called the \emph{longest Weyl element}, with the notion of ``longest'' corresponding to $C$).  For instance, if $K = \U(n)$ and $C$ is chosen in the usual way, then $w_0 = 
\begin{pmatrix}
  &  & 1 \\
  & \dotsb &  \\
  1 & &
\end{pmatrix}
$.
\begin{lemma}
  $\pi_\lambda^* \cong \pi_{-w_0(\lambda)}$.
\end{lemma}
\begin{proof}
  It's clear that $\pi_\lambda^*$ is an irreducible, hence is isomorphic to $\pi_{\mu}$ for some dominant weight $\mu$.  We must verify that $\mu = - w_0 (\lambda)$, or equivalently, that $\lambda = - w_0(\mu)$.  We check first that $-\mu$ is the lowest weight of $\pi_\lambda$.  From this it follows that $- w_0 (\mu)$ is the highest weight of $\pi_\lambda$, and so $\lambda = - w_0(\mu)$, as required.
\end{proof}
Thus Theorem \ref{thm:borel-weil-construction} tells us that
\begin{equation}\label{eq:}
  \pi_{\lambda}
  \cong F_{-w_0(\lambda)}.
\end{equation}

\begin{proof}
[Proof of Theorem \ref{thm:borel-weil-construction}]
  Let $F$ denote the space of all holomorphic functions $f : G \rightarrow \mathbb{C}$, and let $f \in F$.  Using the identity principle for holomorphic functions on $G \cong K \times \mathfrak{p}$, we may identify $f$ with its restriction to $K$, and then further with an element of the space $L^2(K)$, on which the group $K \times K$ acts by the regular representation $(k_1,k_2) \cdot f(x) = f(k_1^{-1} x k)$.  We accordingly have an $L^2(K)$-decomposition $f = \sum_{\mu} f_\mu$, where $\mu \in X(T)$ runs over the dominant weights and $f_\mu$ belongs to the space $\mathcal{A}(\pi_\mu)$ of matrix coefficients of the representation $\pi_\mu$.  Let $L$ and $R$ denote respectively the left and right regular representations of $K$ on $L^2(K)$.  Since $\mathcal{A}(\pi_\mu) \cong \pi_\mu^* \otimes \pi_\mu$ is the $\pi_\mu$-isotypic component (see \S\ref{sec:isotyp-decomp}) of $L^2(K)$ under $R$, we have $f_\mu = \dim(\pi_\mu) R(\overline{\chi_{\mu}}) f$.  Suppose now that $f \in F_\lambda$, i.e., that $L(b) f = b^{\lambda} f$ for all $b \in B$.  Since $R$ and $L$ commute, it follows that $L(b) f_\mu = b^{\lambda} f_\mu$, i.e., that $f_\mu$ belongs to $F_\lambda$.  If $f_\mu$ is nonzero, then we may find a nonzero vector $v \in \pi_\mu^*$ so that $b v = b^{\lambda} v$ for all $b \in B$.  Then Theorem \ref{thm:highest-weight-borel-weil} implies that $\pi_\mu^* \cong \pi_\lambda$ and that $v \in \pi_\lambda$ is a highest weight vector.  Thus
  \begin{equation}\label{eq:}
    f \in (\pi_\lambda \otimes \pi_\lambda^*) \cap F_\lambda
    = \mathbb{C} v \otimes \pi_\lambda^*.
  \end{equation}
  Thus $F_\lambda = \mathbb{C} v \otimes \pi_\lambda^*$, hence $F_\lambda \cong \pi_\lambda^*$.
\end{proof}

\section{Plancherel formula for complex reductive groups}
With the remaining fewl ectures in the course, we aim to treat some of the simplest aspects of the representation theory of non-compact groups.  We focus on the case of complex reductive groups, which are simpler than real reductive groups for reasons to be explained later.

Let $G$ be a unimodular group equipped with a Haar measure $d g$, and let $f : G \rightarrow \mathbb{C}$ be nice enough (e.g., smooth and compactly-supported in the case of a Lie group).  We've seen in \S\ref{sec:four-analys-comp} that for $G$ compact, one has the Fourier inversion formula
\begin{equation}\label{eqn:}
  f(1)
  =
  \sum_{\pi \in \Irr(G)}
  \frac{\dim(\pi)}{\vol(G)}
  \trace(\pi(f)).
\end{equation}
(We showed this when $\vol(G) = 1$, but the general case follows because $\pi(f)$ and $\vol(G)$ scale in the same way when one scales $d g$.)  In the non-compact case, one seeks a formula of the shape
\begin{equation}\label{eqn:general-plancherel}
  f(1)
  = \int_{\pi \in \hat{G}}
  \trace(\pi(f))
  \, d \mu_P(\pi),
\end{equation}
where now $\hat{G}$ denotes the set of isomorphism classes of irreducible unitary representations of $G$ and $d \mu_P$ denotes a measure on $\hat{G}$, called \emph{Plancherel measure}, which scales inversely to $d g$.  For instance, the Fourier inversion formulas on the circle
\begin{equation}\label{eqn:}
  f(0)
  = \sum_{n \in \mathbb{Z}}
  \hat{f}(n),
  \quad
  \hat{f}(n) := \int_{x \in \mathbb{R}/\mathbb{Z}}
  f(x) e^{2 \pi i n x} \, d x,
  \quad
  f \in C^\infty(\mathbb{R}/\mathbb{Z})
\end{equation}
and on the real line
\begin{equation}\label{eqn:}
  f(0)
  = \sum_{\xi \in \mathbb{R}}
  \hat{f}(\xi)
  \, d \xi,
  \quad
  \hat{f}(\xi) := \int_{x \in \mathbb{R}/\mathbb{Z}}
  f(x) e^{2 \pi i n x} \, d x,
  \quad
  f \in C_c^\infty(\mathbb{R})
\end{equation}
may be interpreted in this way; for instance, in the second example, $\hat{f}(\xi)$ is the trace of $\pi_\xi(f)$, where $\pi_\xi$ denotes the one-dimensional representation of $\mathbb{R}$, spanned by the function $\mathbb{R} \ni x \mapsto e^{2 \pi i \xi x}$, and with $\mathbb{R}$ acting by right translation.

In a course emphasizing the functional analytic aspects of representation theory one might study general conditions under which such formulas \eqref{eqn:general-plancherel} exist (e.g., look up the definition of a ``Type I'' group).  Here we will focus instead on the problem of determining $\mu_P$ explicitly for a more restricted class of groups, namely the connected complex reductive groups.

We will use much of the same notation as above.  We start with a compact connected Lie group $K$.  We denote by $G$ its complexification.  We fix a maximal torus $T$ and Weyl chamber $C$.  We use these to define subgroups $N$, $A$ and $B$ of $G$, as in \S\ref{sec:borel-weil}.  The example $K = \U(n)$ depicted earlier is worth keeping in mind.

We note that $G$ is unimodular.  To see this, we must show that $|\det(\Ad(g))| = 1$ for all $g \in G$.  It suffices to verify the stronger identity
\begin{equation}\label{eqn:det-circ-Ad-equals-1}
  \det(\Ad(g)) = 1
\end{equation}
By the identity principle for holomorphic functions applied to $\det \circ \Ad : G \rightarrow \mathbb{C}^\times$, we need only verify \eqref{eqn:det-circ-Ad-equals-1} holds for $g \in K$.  Since $\det \circ \Ad$ is a class function, we reduce further to verifying \eqref{eqn:det-circ-Ad-equals-1} when $g = t \in T$, and indeed, $\det(\Ad(t)) = \prod_{\alpha \in \Phi} t^\alpha = \prod_{\alpha > 0} t^{\alpha} t^{-\alpha} = 1$.

\subsection{Iwasawa decomposition}
\label{sec:iwas-decomp}
In its most basic form it asserts that
\begin{equation}\label{eqn:}
  G = B  K.
\end{equation}
Moreover,
\begin{equation}\label{eqn:}
  B \cap K = T.
\end{equation}
In the case $G = \GL_n(\mathbb{C})$, this amounts to the Gram--Schmidt procedure (explanation given in class).  We record the proof for general $G$.  We consider first the analogous Lie algebra question.  To start, we might write any element of $\mathfrak{g}$ as $x + y + z$ using the decomposition
\begin{equation*}
  \mathfrak{g} = \mathfrak{t}_{\mathbb{C}} \oplus \mathfrak{n} \oplus \overline{\mathfrak{n}},
\end{equation*}
where $\mathfrak{n} = \oplus_{\alpha > 0} \mathfrak{g}^\alpha$ and $\overline{ \mathfrak{n} } = \oplus_{\alpha < 0} \mathfrak{g}^\alpha$.  We may write $x = x_0 + x_1$ with $x_0 \in \mathfrak{t}_{\mathbb{R}}, x_1 \in i \mathfrak{t}_{\mathbb{R}}$, say.  Then the element $w := x_1 + z + \theta (z)$ is $\theta$-invariant, hence belongs to $\mathfrak{k}$.  On the other hand, $y - \theta(z)$ belongs to $\mathfrak{n}$.  Thus
\begin{equation*}
  x + y + z = w + x_0 + (y - \theta(z)) \in \mathfrak{k} + \mathfrak{t}_{\mathbb{R}} + \mathfrak{n} .
\end{equation*}
In particular,
\begin{equation}\label{eqn:Lie-alg-iwasawa}
  \mathfrak{g} =
  \mathfrak{b} + \mathfrak{k}.
\end{equation}
Using that $\mathfrak{k} = \{x \in \mathfrak{g} : \theta(x) = x\}$ and that $\theta$ swaps $\mathfrak{n}$ and $\overline{\mathfrak{n}}$ and acts on $\mathfrak{t}_{\mathbb{C}}$ with fixed subspace $I \mathfrak{t}_{\mathbb{R}}$, we see that
\begin{equation}\label{eqn:}
  \mathfrak{b} \cap \mathfrak{k} = i \mathfrak{t}_{\mathbb{R}}.
\end{equation}

We now turn to the groups.  We've seen that $B$ is a closed subgroup of $G$.  Since $K$ is compact, it follows that $B K$ is closed.  Using \eqref{eqn:Lie-alg-iwasawa} and computing the derivative of the map $B \times K \xrightarrow{(b,g) \mapsto b g } G$ as in the proof of the Weyl integral formula, we see moreover that $B K$ is open (see for instance Knapp, Lemma 5.11).  Since $G$ is connected, we conclude that $G = B K$.

% We claim moreover that

% Let $A_0 \leq A$ denote the closed subgroup with Lie algebra $\mathfrak{t}_{\mathbb{R}}$, given in the basic $\GL_3(\mathbb{C})$ example by $A_0 = \begin{pmatrix}
%   \mathbb{R}^\times_+ &  &  \\
%   & \mathbb{R}^\times_+ &  \\
%   & & \mathbb{R}^\times_+
% \end{pmatrix}
% \leq A = \begin{pmatrix}
%   \mathbb{C}^\times  &  &  \\
%   & \mathbb{C}^\times  &  \\
%   & & \mathbb{C}^\times
% \end{pmatrix}$.




% say with $n = 3$.  We may take
% \begin{equation*}
%   A = \begin{pmatrix}
%     \ast &  &  \\
%     & \ast &  \\
%     & & \ast
%   \end{pmatrix}, \quad B = \begin{pmatrix}
%     \ast & \ast & \ast \\
%     & \ast & \ast \\
%     & & \ast
%   \end{pmatrix}.
% \end{equation*}
% We may write $g \in G$ as
% \begin{equation*}
%   g = \begin{pmatrix}
%     a_{1  1} & a_{1 2} & a_{1 3} \\
%     a_{2 1} &  a _{2 2} & a _{2 3} \\
%     a _{3 1} & a _{3 2} & a _{3 3}
%   \end{pmatrix}
%   =:
%   \begin{pmatrix}
%     v_1  \\
%     v_2  \\
%     v_3
%   \end{pmatrix},
% \end{equation*}
% say.  By Gram--Schmidt,
% \end{example}<++>


\subsection{Integration on \texorpdfstring{$B \backslash G$}{B \ G}}\label{sec:integr-b-backsl}
We fix a Haar measure $d g$ on $G$ and a left Haar measure $d b$ on $B$.  Using these choices we will define a notion of integration over $B \backslash G$.

Let $\mathcal{F}$ denote the space of measurable functions $f : G \rightarrow \mathbb{C}$ such that
\begin{equation}\label{eqn:}
  f(b g) = \delta(b) f(g)
\end{equation}
for all $b \in B, g \in G$ and $\int_K |f| < \infty$, the latter integral taken with respect to any Haar measure on $K$.

\begin{lemma}
  Each $f \in \mathcal{F}$ is of the form $f(g) = \int_{B} \tilde{f}(b g) \, d b$ for some $\tilde{f} : G \rightarrow \mathbb{C}$.  The map $\mathcal{F} \ni f \mapsto \int_G \tilde{f}(g) \, d g$ is a well-defined linear functional on $\mathcal{F}$ that is invariant under right translation by $G$.  We denote this linear functional by $f \mapsto \int_{B \backslash G} f$.  We have $\int_{B \backslash G} f = c \int_K f$ for some constant $c > 0$, depending only upon the various choices of Haar measure.
\end{lemma}
\begin{proof}
  Left to the reader; this or some variant was treated in ``Lie groups I''.
\end{proof}

\subsection{Principal series representations}
We denote by $A^\wedge$ the group of unitary characters of $A$, thus $A^\wedge$ consists of continuous homomorphisms $\chi : A \rightarrow \U(1)$.  We similarly define the group $\mathfrak{a}^\wedge$ of unitary characters of $A$.  Using the surjective quotient map $\mathfrak{a} \rightarrow A$ given by exponentiation, we may identify $A^\wedge$ with a subgroup of $\mathfrak{a}^\wedge$.  On the other hand, by differentiating at the origin, we may identify $\mathfrak{a}^\wedge$ with the $\mathbb{R}$-vector space consisting of linear functionals $\mathfrak{a} \rightarrow i\mathbb{R}$.  Concretely, suppose $A$ is the group of diagonal matrices in $\GL_n(\mathbb{C})$.  Write $a \in A$ in coordinates as $a = (a_1,\dotsc,a_n)$ with $a_j \in \mathbb{C}^\times$.  Then every $\chi \in A^\wedge$ is of the form $\chi(a) = \prod_j a_j^{s_j} (a_j/|a_j|)^{k_j}$ for some $s_j \in i \mathbb{R}$ and $k_j \in \mathbb{Z}$, and this defines an isomorphism $A^\wedge \cong (i \mathbb{R})^n \times \mathbb{Z}^n$.  Similarly, we may identify $\mathfrak{a}$ with the group $\mathbb{C}^n$ of diagonal matrices $x = (x_1,\dotsc,x_n)$ in $M_n(\mathbb{C})$ and $\mathfrak{a}^\wedge$ with the group $(i \mathbb{R})^n \times \mathbb{R}^n$.

To each $\lambda \in \Hom(A,\mathbb{C}^\times)$ we attach a ``principal series representation'' $\pi_\lambda$, consisting of measurable functions $v : G \rightarrow \mathbb{C}$ satisfying
\begin{equation}\label{eqn:}
  v(b x)
  =
  \delta^{1/2}(b) b^{\lambda} v(x)
\end{equation}
for all $b \in B, x \in G$ and
\begin{equation}\label{eqn:}
  \int_{K} |v|^2 < \infty.
\end{equation}
Thanks to the Iwasawa decomposition, we may regard $\pi_{\lambda}$ as a closed subspace of the Hilbert space $L^2(K)$.  The group $G$ acts on $\pi_\lambda$ by right translation.  We note that if $f_1 \in \pi_{\lambda}$ and $f_2 \in \pi_{-\lambda}$, then $f_1 f_2$ belongs to the space $\mathcal{F}$ considered in \S\ref{sec:integr-b-backsl}, and so we obtain a $G$-invariant pairing
\begin{equation*}
  \pi_{\lambda} \otimes \pi_{-\lambda} \rightarrow \mathbb{C}
\end{equation*}
\begin{equation*}
  f_1 \otimes f_2 \mapsto \int_{B \backslash G} f_1 f _2.
\end{equation*}
Also, $\overline{\pi_{\lambda}} = \pi_{- \overline{\lambda }}$.  Thus if $\lambda$ belongs to $A^\wedge = \Hom(A,\U(1))$, so that $\lambda = - \overline{\lambda }$, then integration over $B \backslash G$ defines a $G$-invariant inner product on $\pi_{\lambda}$, hence $\pi_{\lambda}$ is a unitary representation of $G$.

\subsection{Characters of principal series representations}
Recall that we equip $G$ with a Haar measure $d g$ and $B$ with a left Haar measure $d b$; this normalizes an integral over $B \backslash G$ in the sense described above.  To each $f \in C_c(G)$ we attach an integral operator $\pi(f) \in \End(\pi)$, as in \S\ref{sec:integral-operators}, by the formula $\pi(f) := \int_{g \in G} f(g) \pi(g) \, d g$.
\begin{theorem}
  For each $f \in C_c^\infty(G)$, the operator $\pi_\lambda(f)$ is trace class.
  % with trace norm bounded continuously in $f$.
  We have
  \begin{equation}\label{eq:preliminary-formula-character-ps}
    \trace(\pi_\lambda(f))
    = \int_{g \in B \backslash G}
    \int_{b \in B}
    f(g^{-1} b g) \delta^{1/2}(b) b^{\lambda}.
  \end{equation}
\end{theorem}
% The continuity just says that the trace norm in question may be bounded by an expression of the form $C_U \sum_{|\alpha| \leq N_U} \|\partial^\alpha f\|_{\infty}$, where $U$ is a compact set containing the support of $f$, the scalars $C_U$ and $N_U$ depend only upon $U$, and the sum is over multi-indices $\alpha \in \mathbb{Z}_{\geq 0}^{\dim(G)}$, with $\partial^\alpha$ the corresponding differential operators defined in suitable local coordinates.

% The theorem allows us to define The theorem multiples of $d g$ by a compactly-supported continuous function By a \emph{generalized function} on $G$ we will mean simply an element of the dual space of the space of compactly-supported measures on $G$.

% Let $M$ be a smooth manifold.  Recall that a \emph{distribution} on $M$ is an element of the continuous dual of the space $C_c^\infty(M)$ of test functions.  By a \emph{test measure} on $M$ we will mean a compactly-supported measure on $M$ given in local coordinates by a smooth multiple of Lebesgue measure.  For instance, the test measures on $G$ are precisely those of the form $f(g) \, d g$ for some $f \in C_c^\infty(G)$.  By a \emph{generalized function} on $M$ we will mean an element of the continuous dual of the space of test measures.  For example, any locally integrable measurable function $M \rightarrow \mathbb{C}$ defines a generalized function.  The

\begin{proof}
  Let $v \in \pi_\lambda$.  By definition,
  \begin{equation*}
\pi(f) v(x)
    = \int _{g \in G} f(g) v(x g),
  \end{equation*}
  where here and henceforth we the omit Haar measures for notational simplicity.  We apply the substitutinon $g \mapsto x^{-1} g$ to rewrite the above as
  \begin{equation*}
    \int_{g \in G} f(x^{-1} g) v(g).
  \end{equation*}
  We next use the defining property of $\int_{B \backslash G}$ to rewrite the above as
  \begin{equation*}
    \int _{y \in B \backslash G} \int _{b \in B} f (x ^{-1} b y) v (b y).
  \end{equation*}
  Since $v(b y) = \delta^{1/2}(b) b^{\lambda} v(y)$, it follows that
  \begin{equation}\label{eq:pi-of-f-via-k}
    \pi(f) v(x)
    = \int _{y \in B \backslash G}
    v(y) k(x,y)
  \end{equation}
  with
  \begin{equation*}
    k(x,y) := \int _{b \in B} \delta^{1/2}(b) b^{\lambda} f(x^{-1} b y).
  \end{equation*}
  Since $B$ is closed and $f \in C_c^\infty(G)$, we have $k \in C^\infty(G \times G)$.  It follows readily from the definition that
  \begin{equation}\label{eq:functional-eqn-for-k-kernel-ps}
    k(b_1 x, b_2 y) = \delta^{1/2} (b_1) b_1^{\lambda}
    \delta^{1/2} (b_2) b_2^{-\lambda} k(x,y)
    \text{ for } b_1, b_2 \in B.
  \end{equation}
  Note that the diagonal kernel integral
  \begin{equation}\label{eq:diagonal-kernel-integral}
    \int_{x \in B \backslash G} k(x,x)
  \end{equation}
  is the RHS of \eqref{eq:preliminary-formula-character-ps}.

  The remainder of the argument is a bit of standard functional analysis.  It suffice to show more generally that if $T$ is any operator on $\pi$ defined as in \eqref{eq:pi-of-f-via-k} by a smooth kernel $k \in C^\infty(G \times G)$ satisfying \eqref{eq:functional-eqn-for-k-kernel-ps}, then $T$ defines a trace class operator with trace given by \eqref{eq:diagonal-kernel-integral}.  This can be proved in the same way as the following statement, which may have come up in a functional analysis course:
  \begin{center}
    Let $(M,\mu)$ be a compact manifold equipped with a smooth finite measure, and let $k \in C^\infty(M \times M)$.  Then the operator $T$ on $L^2(M,\mu)$ defined by $T v(x) := \int v(y) k(x,y) \, d \mu(y)$ is trace class, with $\trace(T) = \int k(x,x) \, d \mu(x)$.
  \end{center}
  One can prove this by reducing first to the case that $M$ is a compact torus and then appealing to Fourier series.
  % The set $\supp(k)$ is left invariant under $B \times B$, and so may be identified with a subset of $B \backslash G \times B \backslash G$.  Since $B \backslash G$ is compact and the assignment $k \mapsto T$ is linear, we may assume by applying a suitable partition of unity that $\supp(k)$ is sufficiently small.  There are two cases:
  % \begin{enumerate}
  % \item $\supp(k) \subseteq U \times U$ for some small open $U \subseteq B \backslash G$.  As in the proof that quotients of Lie groups are manifolds, we may choose a fundamental domain $\tilde{U} \subseteq G$ for $U$: an open subset of a submanifold of $G$ such that the natural map $\tilde{U} \rightarrow U$ is a diffeomorphism.

  %   Gahh.  Okay, you really want to trivialize the representation.  $L^2(\tilde{U})$ should embed in $\pi_\lambda$.  Given $v_0$ on $\tilde{U}$, try to define $v$ on $G$ by $v(b u) := \delta^{1/2}(b) b^{\lambda} v_0(u)$.  Then $\|v\|^2 = \|v_0\|^2_{L^2(\tilde{U},\mu)}$ for some smooth measure $\mu$ on $\tilde{U}$.  Now what?  Now we want to estimate stuff like $\langle T v_i, v_j \rangle$ for a suitable ONB.  You were hoping that you could reduce to the case that the smooth measure in question is just given by Lebesgue measure?
  % \end{enumerate}
\end{proof}

\subsection{Some basics concerning conjugacy classes in \texorpdfstring{$G$}{G}}
Every such class intersects $B$, that is to say, $G = \cup_{x \in B \backslash G} x^{-1} B x$.  For instance, if $G = \GL_n(\mathbb{C})$, then this follows from the Jordan normal form.  To sketch a proof in general, it suffices to show for each $g \in G$ that the map
\begin{equation*}
  g : B \backslash G \rightarrow B \backslash G
\end{equation*}
\begin{equation*}
B x \mapsto B x g
\end{equation*}
has a fixed point (because if $B x = B x g$, then $g \in x^{-1} B x$).  One way to see this is to use that $B \backslash G \cong T \backslash K$ (a consequence of the Iwasawa decomposition) and then to show as in one of the standard proofs of the conjugacy of maximal tori (not the one presented in this course) that any map $T \backslash K \rightarrow T \backslash K$ homotopic to the identity has a fixed point.  One can also argue algebraically using the Borel fixed point theorem.


\subsection{A change of variables}
The groups $N$ and $A$ are unimodular (indeed, their adjoint representations are unimodular, hence have trivial determinant).  We may thus equip them with Haar measures, compatibly with the chosen left Haar measure on $B$ in the sense that
\begin{equation}\label{eqn:}
  \int_B h
  = \int_{n \in N, a \in A} h(a n)
  = \int_{n \in N, a \in A}
  h(n a) \delta^{-1}(a).
\end{equation}
For instance, if $G = \SL_2(\mathbb{C})$, then a left Haar on $B$ is given by
\begin{equation}\label{eqn:}
  \int_B h
  = \int_{x \in \mathbb{C}, y \in \mathbb{C}^\times}
  h (
  \begin{pmatrix}
    1 & x \\
    & 1
  \end{pmatrix}
  \begin{pmatrix}
    y &  \\
    & y^{-1}
  \end{pmatrix}
  )
  \, \frac{d x \, d ^\times y}{|y|_{\mathbb{C}}^2},
\end{equation}
where $d x$ and $d^\times y$ denote Haar measures on $\mathbb{C}$ and $\mathbb{C}^\times$ respectively and where
\begin{equation}\label{eqn:}
  |y|_{\mathbb{C}} := |y|^2
\end{equation}
denotes the complex modulus, given by the square of the usual complex absolute value, so that $\int_{x \in \mathbb{C}} h(x) \, d x = |y|_{\mathbb{C}} \int_{x \in \mathbb{C}} h(y x) \, d x$ for any $h \in C_c(\mathbb{C})$, and so that $d^\times y = d y / |y|_{\mathbb{C}}$ defines a Haar measure on $\mathbb{C}^\times$ for each Haar measure $d y$ on $\mathbb{C}$.

We want to rewrite this integral formula for $B$ so that it interacts better with conjugacy classes.  To start, note that
\begin{equation}\label{eqn:}
  \begin{pmatrix}
    1 & x \\
    & 1
  \end{pmatrix}
  \begin{pmatrix}
    y &  \\
    & y^{-1}
  \end{pmatrix}
  = 
\begin{pmatrix}
    y & x y^{-1} \\
    & y^{-1}
  \end{pmatrix}
  .
\end{equation}
Now observe that ``generically'' (i.e., for $y \neq 1$), the above matrix has distinct eigenvalues, hence is conjugate to the diagonal matrix with the same entries.  More explicitly, we have
\begin{equation}\label{eqn:}
  \begin{pmatrix}
    1 & z \\
    & 1
  \end{pmatrix}
  \begin{pmatrix}
    y &  \\
    & y^{-1}
  \end{pmatrix}
  \begin{pmatrix}
    1 & z \\
    & 1
  \end{pmatrix}
  ^{-1} =
  \begin{pmatrix}
    y & (y^{-1} - y ) z \\
    & y^{-1}
  \end{pmatrix}
  ,
\end{equation}
which equals $
\begin{pmatrix}
  y & x y^{-1} \\
  & y^{-1}
\end{pmatrix}
$ when $z = x y^{-1} / (y^{-1} - y)$.  Thus
\begin{equation}\label{eqn:sl2-change-of-var}
  \int_{x \in \mathbb{C}}
  h (
\begin{pmatrix}
    1 & x \\
    & 1
  \end{pmatrix} 
\begin{pmatrix}
    y &  \\
    & y ^{-1}
  \end{pmatrix}
  ) = \frac {|y^{-1} - y|_{\mathbb{C}}}{|y^{-1}|_{\mathbb{C}}} \int_{z \in \mathbb{C}} h (
\begin{pmatrix}
    1 & z \\
    & 1
  \end{pmatrix} 
\begin{pmatrix}
    y &  \\
    & y^{-1}
  \end{pmatrix}
  \begin{pmatrix}
    1 & z \\
    & 1
  \end{pmatrix}
  ^{-1} )
\end{equation}
and so
\begin{equation}\label{eqn:}
  \int_B h
  =
  \frac {|y^{-1} - y|_{\mathbb{C}}}{|y|_{\mathbb{C}}}
  \int_{x \in \mathbb{C}}
  h (
\begin{pmatrix}
    1 & x \\
    & 1
  \end{pmatrix} 
\begin{pmatrix}
    y &  \\
    & y^{-1}
  \end{pmatrix}
  \begin{pmatrix}
    1 & x \\
    & 1
  \end{pmatrix}
  ^{-1} ) \, d x \, d^\times y.
\end{equation}
Here it is understood that we integrate away from the measure zero set of $y$ for which $y^2 = 1$.

A similar argument applied ``one root at a time'' gives the following more general identity:
\begin{lemma}\label{lem:change-variables-N-A}
  With Haar measures on $B,N,A$ compatibly normalized as above, we have for $h \in C_c(B)$
  \begin{equation}\label{eqn:}
    \int_B
    h
    =
    \int_{a \in A^{\reg}}
    \frac{\prod _{\alpha > 0}  | a^{\alpha/2} -  a^{-\alpha/2}
      |_{\mathbb{C}}}
    {
      \delta^{1/2}(a)
    }
    \int_{n \in N}
    h (n a n^{-1}),
  \end{equation}
  with the integral taken over the full measure subset
  \begin{equation}\label{eqn:}
    A^{\reg}  := \{a \in A : a^\alpha \neq 1 \text{ for all }
    \alpha \in \Phi\}.
  \end{equation}
\end{lemma}
\begin{proof}
  Compare with Knapp, Lemma 10.16.  The key point is to verify the following general form of \eqref{eqn:sl2-change-of-var}, valid for any $a \in A^{\reg}$:
  \begin{equation}\label{eqn:general-change-of-var}
    \int_{n \in N}
    h (n a)
    =
    \frac {
      \prod_{\alpha > 0}
      |a^{\alpha/2} - a^{-\alpha/2}|_{\mathbb{C}}
    }
    {
      \delta^{-1/2}(a)
    }
    \int_{n \in N}
    h (n a n^{-1}).
  \end{equation}  
\end{proof}
As a consequence, we can rewrite the formula given above for $\chi_\lambda$, as follows.  We first define $F : A^{\reg} \rightarrow \mathbb{C}$ by the formula
\begin{equation}\label{eqn:formula-for-F-on-regular-a}
  F(a) :=
  (\prod_{\alpha > 0}
  |a^{\alpha/2} - a^{-\alpha/2}|_{\mathbb{C}})
  \int_{g \in A \backslash G}
  f(g^{-1} a g).
\end{equation}
By working backwards through the above calculations, we see that $F$ extends to an element of $C_c^\infty(A)$; indeed, by \eqref{eqn:general-change-of-var}, we have for $a \in A^{\reg}$
\begin{align*}
  F(a)
  &=
    (\prod_{\alpha > 0}
    |a^{\alpha/2} - a^{-\alpha/2}|_{\mathbb{C}})
    \int_{g \in B \backslash G}
    \int_{n \in N}
    f(g^{-1} n a n^{-1} g)
  \\
  &=
    \delta^{-1/2}(a)
    \int_{g \in B \backslash G}
    \int_{n \in N}
    f(g^{-1} n a g)
  \\
  &=
    \delta^{-1/2}(a)
    \int_{k \in K}
    \int_{n \in N}
    f(k^{-1} n a g)
\end{align*}
for a suitable Haar measure on $K$, but this last expression defines for $a \in A$ an element of $C_c^\infty(A)$, using here that $K$ is compact.  We define the Fourier transform $F^\wedge : A^\wedge \rightarrow \mathbb{C}$ by
\begin{equation*}
  F^\wedge(\lambda) := \int_{a \in A} a^{\lambda} F(a).
\end{equation*}
We then obtain
\begin{equation}\label{eqn:}
  \chi_\lambda(f) = F^\wedge(\lambda).
\end{equation}
We equip $A^\wedge$ with the Haar measure dual to that on $A$, so that the Fourier inversion formula $F(1) = \int_{\lambda \in A^\wedge} F^\wedge(\lambda)$ holds.

\subsection{Passage to the Lie algebra}
Suppose now that $f \in C_c^\infty(G)$ as above is supported in a ``small conjugation-invariant neighborhood'' $U$ of the identity element $1$ of the Lie group $G$.  For example, in the case $G = \GL_n(\mathbb{C})$, we might define $U$ to be the set of all $g \in G$ each of whose eigenvalues $c$ satisfies $|c - 1 | < \eps$ for some small $\eps > 0$; alternatively, we might ask that the subleading coefficients of the characteristic polynomial of $g$ be bounded by $\eps$.  We may then find a small $\Ad(G)$-invariant neighborhood $Y$ of the origin $0$ of the Lie algebra $\mathfrak{g}$ such that $\exp : Y \rightarrow U$ is a diffeomorphism onto its image.  We may define an element $f_0 \in C_c^\infty(Y)$ by requiring that $f_0(x) = f(\exp(x))$.  We define $F_0 \in C_c^\infty(\mathfrak{a})$ to vanish off elements $x \in \mathfrak{a} \cap Y$, for which we set $F_0(x) := F(\exp(x))$, with $F$ attached to $f$ as above.  We equip $\mathfrak{a}$ with the Haar measure matching up near the origin under the exponential map with the Haar measure on $A$ near the identity.  For $\lambda \in \mathfrak{a}^\wedge$ (hence, in particular, for $\lambda \in A^\wedge)$, we may define the Fourier transform $F_0^\wedge(\lambda)$; we have $F_0^\wedge(\lambda) = F^\wedge(\lambda)$ when $\lambda \in A^\wedge$.  (Recall from before that $\mathfrak{a}^\wedge \cong (\mathbb{R} \times \mathbb{R})^n$ while $A^\wedge \cong (\mathbb{R} \times \mathbb{Z})^n$.)  We equip $\mathfrak{a}^\wedge$ with the dual Haar measure, so that the Fourier inversion formula $F_0(0) = \int_{\lambda \in \mathfrak{a}^\wedge} F_0^\wedge(\lambda)$ holds.

Recall that we've fixed a Haar measure $d g$ on $G$.  We equip $\mathfrak{g}$ with the Haar measure $d x$ so that for small $x \in \mathfrak{g}$, writing $g = \exp(x)$, we have $d g = j(x) \, d x$, with $j(0) = 1$.  Using Lemma \ref{lem:deriv-of-expo}, one may check then that the inverse square root of $j$ admits the following formula for small enough regular elements $x \in \mathfrak{a}$:
\begin{equation}\label{eqn:}
  j^{-1/2}(x)
  = \prod_{\alpha > 0}
  \left\lvert
    \frac
    {
      e^{\alpha(a)/2}
      -
      e^{-\alpha(a)/2}
    }
    {
      \alpha(x)
    }
  \right\rvert.
\end{equation}

\subsection{Main result}
We define a polynomial function $P : \mathfrak{a}^\wedge \rightarrow \mathbb{R}$ by the formula $P(\lambda) := \prod_{\alpha > 0} | \lambda(H_\alpha)|_{\mathbb{C}}$.  We aim to show the following:
\begin{theorem}
  For $f \in C_c^\infty(G)$, and with measures as normalized above,
  \begin{equation}\label{eqn:}
    f(1)
    = \int_{\lambda \in A^\wedge}
    \frac{P(\lambda)}{|W|}
    \chi_\lambda(f).
  \end{equation}
\end{theorem}
This is a theorem of Gelfand--Naimark.  We'll loosely follow Harish-Chandra's proof technique, as exposed in the textbook of Varadarajan.  It is a fact that we will not have time to prove that the $\pi_\lambda$ are \emph{irreducible} unitary representations of $G$, so this theorem gives the desired Plancherel formula.

Let's suppose first that $f$ is supported in a small conjugation-invariant neighborhood of $1$, in the sense described above, so that we may define $F, f_0, F_0$.  The formula \eqref{eqn:formula-for-F-on-regular-a} implies that for small regular $x \in \mathfrak{a}$, we have
\begin{equation}\label{eqn:}
  F_0(x)
  =
  (
  \prod _{\alpha > 0 }
  e ^{\alpha(x)/2}
  - e^{-\alpha(x)/2}
  )
  \int_{g \in G/B}
  \int_{n \in N}
  f_0(\Ad(g n) x).
\end{equation}
The exponential map restricts to a polynomial diffeomorphism $\exp : \mathfrak{n} \rightarrow N$ with polynomial inverse $\log : N \rightarrow \mathfrak{n}$.  We may normalize the Haar measure on $\mathfrak{n}$ to be compatible with $\exp$.  With the abbreivation $f_0^g(y) := f_0(\Ad(g) y)$, we then obtain
\begin{equation}\label{eqn:}
  F_0(x)
  = 
  (
  \prod _{\alpha > 0 }
  e ^{\alpha(x)/2}
  - e^{-\alpha(x)/2}
  )
  \int_{g \in G/B}
  \int_{y \in \mathfrak{n}}
  f_0^g(\Ad(e^y) x).
\end{equation}
By the Lie algebra analogue of Lemma \ref{lem:change-variables-N-A}, we have
\begin{equation}\label{eqn:lie-algebra-change-of-var}
  \int_{y \in \mathfrak{n}}
  f_0^g(\Ad(e^y) x)
  =
  (
  \prod_{\alpha>0} |\alpha(x)|_{\mathbb{C}}^{-1}
  )
  \int_{y \in \mathfrak{n}}
  f_0^g(y + x).
\end{equation}
Thus
\begin{equation}\label{eqn:}
  F_0(x)
  = j^{-1/2}(x)
  \int_{g \in G/B, y \in \mathfrak{n}}
  f_0^g(x+y).
\end{equation}
The function $j^{-1/2}$ extends to an $\Ad(G)$-invariant function on $\mathfrak{g}$.  Setting $\phi := j^{-1/2} f_0$, we may rewrite the above formula as
\begin{equation}\label{eqn:}
  F_0(x)
  = \int_{g \in G/B, y \in \mathfrak{n}}
  \phi^g(x+y).
\end{equation}
Recall the decomposition $\mathfrak{g} = \mathfrak{n} \oplus \mathfrak{a} \oplus \overline{\mathfrak{n} }$.  Fix $K \hookrightarrow \U(n)$ and hence $G \hookrightarrow \GL_n(\mathbb{C})$ as usual.  We then get a perfect $\Ad(G)$-equivariant pairing $\mathfrak{g} \otimes \mathfrak{g} \rightarrow \mathbb{C}$ given by $(x,y) \mapsto \trace(x y)$.  Using this we may identify $\mathfrak{g}^\wedge$ with $\mathfrak{g}$.  Under this identification, $\mathfrak{n}$ and $\overline{\mathfrak{n}}$ are dual to one another, while $\mathfrak{a}$ is dual to itself.  Integrating the above formula for $F_0$ over regular $x \in \mathfrak{a}$ thus gives for all $\lambda \in \mathfrak{a}^\wedge$ that
\begin{equation}\label{eqn:}
  F_0^\wedge(\lambda)
  = \int_{g \in G/B}
  \int_{\eta \in \mathfrak{b}^\perp \subseteq
    \mathfrak{g}^\wedge}
  \hat{\phi}^g(\lambda+\eta).
\end{equation}
Here $\hat{\phi}$ denotes the Fourier transform and $\mathfrak{b}^\perp$ the subspace of $\mathfrak{g}^\wedge$ that annihilates $\mathfrak{b}$; by the above discussion, it identifies with $\mathfrak{n}$.

We say that $\lambda \in \mathfrak{a}^\wedge$ is \emph{regular} if $\lambda(H_\alpha) \neq 0$ for all roots $\alpha$.  By the analogue of \eqref{eqn:lie-algebra-change-of-var} for $\mathfrak{g}^\wedge$, we see that for regular $\lambda \in \mathfrak{a}^\wedge$,
\begin{equation}\label{eqn:}
  F_0^\wedge(\lambda)
  = (\prod_{\alpha > 0}
  |\lambda(H_\alpha)|_{\mathbb{C}})
  \int_{g \in G/A}
  \hat{\phi}(\Ad(g) \lambda).
\end{equation}
Let's denote the latter expression by $\int_{\mathcal{O}_\lambda} \hat{\phi}$; it's a normalized integral over the $G$-orbit $\mathcal{O}_\lambda$ of $\lambda$.  Then we have the following analogue of the Weyl integral formula for $h \in C_c(\mathfrak{g}^\wedge)$:
\begin{equation}\label{eqn:}
  \int_{\mathfrak{g}^\wedge}
  h
  = \frac{1}{|W|}
  \int_{\lambda \in \mathfrak{a}^\wedge,\text{regular}}
  P(\lambda)
  \int_{\mathcal{O}_\lambda} h.
\end{equation}
(The key point here is that the union of the orbits $\mathcal{O}_\lambda$ is open and dense in $G$; we may thus reduce to the usual Jacobian calculation.)

We're now ready to prove the theorem.  We denote by $\partial_P$ the translation-invariant differential operator on $C_c^\infty(A)$ or on $C_c^\infty(\mathfrak{a})$ with symbol $P$, so that $(\partial_P h)^\wedge = P \cdot \hat{h}$.  Let $f \in C_c^\infty(G)$.  Define $F$ as above.  Then
\begin{equation}\label{eqn:}
  \int_{\lambda \in \mathfrak{a}^\wedge}
  \frac{P(\lambda)}{|W|}
  \underbrace{\chi_\lambda(f)}_{=F^\wedge(\lambda)}
  =
  \frac{(\partial_P F)(1)}{|W|}.
\end{equation}
We want to show that this last expression equals $f(1)$.  By smoothly and $\Ad(G)$-invariantly truncating, we may assume that $f$ is supported in a small conjugation-invariant neighborhood of the identity, so that we may define $f_0,F_0,\phi$ as above.  Then, combining everything we've shown thus far, we conclude that
\begin{align}\label{eqn:}
  \frac{(\partial_P F)(1)}{|W|}
  &=
    \frac{\partial_P F_0(0)}{|W|}
  \\
  &=
    \int_{\lambda \in \mathfrak{a}^\wedge}
    \frac{P(\lambda)}{|W|}
    \int_{\mathcal{O}_\lambda}
    \hat{\phi}
  \\
  &= \int_{\mathfrak{g}^\wedge}
    \hat{\phi}
  \\
  &=
    \phi(0)
  \\
  &= f_0(0)
  \\
  &= f(1),
\end{align}
as required.





% Recall that for regular $a \in A$, we have !!!

% Let $M$ be a manifold.  By a \emph{generalized function} on $M$ we will mean an element of the linear dual of the space of measures $\mu$ on $M$ that are compactly-supported and given in local coordinates by smooth multiples of Lebesgue measure.  For instance, any locally integrable measurable function on $M$ defines a generalized function.  On $G$, each measure $\mu$ as above is of the form $f \, d g$ for some unique $f \in C_c^\infty(G)$.  We may thus define the \emph{character} $\chi_\lambda$ of $\pi_\lambda$ to be the generalized function $\chi_\lambda : f \, d g \mapsto \trace(\pi(f))$ furnished by the theorem.  It makes sense to ask whether a generalized function is represented by an ``actual'' function.  We will see that this is so for the restriction of $\chi_\lambda$ to the dense open set of regular elements in $G$.

% TODO: continue.  Things shown last time:
% \begin{itemize}
% \item
% \end{itemize}

% Things left to do:
% \begin{itemize}
% \item Verify that the character looks like a function
% \end{itemize}

\bibliography{refs}{} \bibliographystyle{plain}
\end{document}
